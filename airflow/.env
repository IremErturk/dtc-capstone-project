# Custom
COMPOSE_PROJECT_NAME=airflow
GOOGLE_APPLICATION_CREDENTIALS=/.google/credentials/terraform-admin.json
AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT=google-cloud-platform://?extra__google_cloud_platform__key_path=/.google/credentials/terraform-admin.json

AIRFLOW_UID= 50000

# Google Cloud Resource Parameters
GCP_PROJECT_ID=dtc-capstone-344019
GCP_GCS_BUCKET=dtc_capstone_344019_data-lake
BIGQUERY_DATASET=dtc_capstone_344019_all_data

# Postgres
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=airflow

# Airflow
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC=10

AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
AIRFLOW_CONN_METADATA_DB=postgres+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/airflow
AIRFLOW_VAR__METADATA_DB_SCHEMA=airflow

_AIRFLOW_WWW_USER_CREATE=True
_AIRFLOW_WWW_USER_USERNAME=${_AIRFLOW_WWW_USER_USERNAME:airflow}
_AIRFLOW_WWW_USER_PASSWORD=${_AIRFLOW_WWW_USER_PASSWORD:airflow}

AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
AIRFLOW__CORE__LOAD_EXAMPLES=False

_PIP_ADDITIONAL_REQUIREMENTS=${_PIP_ADDITIONAL_REQUIREMENTS:- pyarrow==5.0.0 apache-airflow==2.2.3 apache-airflow-providers-google==6.2.0 pyspark==3.2.0 }
