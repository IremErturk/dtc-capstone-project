{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35d028e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "import re\n",
    "import os\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa7bfaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77830202",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/Cellar/apache-spark/3.2.1/libexec/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/26 21:54:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('dtc-capstone') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "357ef30e",
   "metadata": {
    "code_folding": [
     4,
     24
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iremertuerk/workspace-personal/Data-Engineering-Zoomcamp-All/dtc-capstone-project/data-ingestion/venv/lib/python3.9/site-packages/airflow/configuration.py:276: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if StrictVersion(sqlite3.sqlite_version) < StrictVersion(min_sqlite_version):\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.cloud import storage\n",
    "from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator\n",
    "\n",
    "def upload_file_to_gcs(bucket, object_name, local_file):\n",
    "    \"\"\"\n",
    "    Ref: https://cloud.google.com/storage/docs/uploading-objects#storage-upload-object-python\n",
    "    :param bucket: GCS bucket name\n",
    "    :param object_name: target path & file-name\n",
    "    :param local_file: source path & file-name\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # WORKAROUND to prevent timeout for files > 6 MB on 800 kbps upload speed.\n",
    "    # (Ref: https://github.com/googleapis/python-storage/issues/74)\n",
    "    storage.blob._MAX_MULTIPART_SIZE = 5 * 1024 * 1024  # 5 MB\n",
    "    storage.blob._DEFAULT_CHUNKSIZE = 5 * 1024 * 1024  # 5 MB\n",
    "    # End of Workaround\n",
    "\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket)\n",
    "\n",
    "    blob = bucket.blob(object_name)\n",
    "    blob.upload_from_filename(local_file)\n",
    "\n",
    "def upload_directory_to_gcs(bucket_name, gcs_path, local_path):\n",
    "    \"\"\"\n",
    "    Ref: https://cloud.google.com/storage/docs/uploading-objects#storage-upload-object-python\n",
    "    :param bucket_name: GCS bucket name\n",
    "    :param gcs_path: target path & file-name\n",
    "    :param local_file: source path & directory-name\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # WORKAROUND to prevent timeout for files > 6 MB on 800 kbps upload speed.\n",
    "    # (Ref: https://github.com/googleapis/python-storage/issues/74)\n",
    "    storage.blob._MAX_MULTIPART_SIZE = 5 * 1024 * 1024  # 5 MB\n",
    "    storage.blob._DEFAULT_CHUNKSIZE = 5 * 1024 * 1024  # 5 MB\n",
    "    # End of Workaround\n",
    "    \n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "\n",
    "    assert os.path.isdir(local_path)  # Works only with local_paths that are directory\n",
    "    for local_file in glob(local_path + \"/**\", recursive=True):\n",
    "        if os.path.isfile(local_file):\n",
    "            remote_path = os.path.join(gcs_path, local_file[1 + len(local_path) :])\n",
    "            blob = bucket.blob(remote_path)\n",
    "            blob.upload_from_filename(local_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "613d2443",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Source Constants\n",
    "DATA_SOURCE_ROOT = \"../../dtc-capstone-data/slack-data\" \n",
    "COURSE_CHANNEL = \"course-data-engineering\"\n",
    "# TODO: maybe more channel like course-channel, ml-zoomcamp, etc.\n",
    "WELCOME_CHANNEL = \"welcome\"\n",
    "USERS_DATA = \"users.json\"\n",
    "# book-of-the-week\n",
    "# announcements-course-data-engineering\n",
    "# shameless-content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ce93364",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Target Constants\n",
    "PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", 'dtc-capstone-344019')\n",
    "BUCKET = os.environ.get(\"GCP_GCS_BUCKET\", 'dtc_capstone_data-lake')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d6dfcf",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40601a6d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Read And Uplaod Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecbd72ff",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/22 17:27:33 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "users = spark.read.json(f'{DATA_SOURCE_ROOT}/{USERS_DATA}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fa50d2",
   "metadata": {
    "code_folding": [],
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Cleanup Unnecessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60867321",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['profile', 'color', 'who_can_share_contact_card', 'team_id', '_corrupt_record']\n",
    "users = users.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e99d6c5f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField(\"deleted\",types.BooleanType(),True),\n",
    "    types.StructField(\"id\",types.StringType(),True),\n",
    "    types.StructField(\"is_admin\",types.BooleanType(),True),\n",
    "    types.StructField(\"is_app_user\",types.BooleanType(),True),\n",
    "    types.StructField(\"is_bot\",types.BooleanType(),True),\n",
    "    types.StructField(\"is_email_confirmed\",types.BooleanType(),True),\n",
    "    types.StructField(\"is_invited_user\",types.BooleanType(),True),\n",
    "    types.StructField(\"is_owner\",types.BooleanType(),True),\n",
    "    types.StructField(\"is_primary_owner\",types.BooleanType(),True),\n",
    "    types.StructField(\"is_restricted\",types.BooleanType(),True),\n",
    "    types.StructField(\"is_ultra_restricted\",types.BooleanType(),True),\n",
    "    types.StructField(\"name\",types.StringType(),True),\n",
    "    types.StructField(\"real_name\",types.StringType(),True),\n",
    "    types.StructField(\"tz\",types.StringType(),True),\n",
    "    types.StructField(\"tz_label\",types.StringType(),True),\n",
    "    types.StructField(\"tz_offset\",types.IntegerType(),True),\n",
    "    types.StructField(\"updated\",types.LongType(),True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7cd5aa",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Create Sub-Categories for User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6e49182",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def format_user_data(d):\n",
    "    identity = [\"name\", \"real_name\"]\n",
    "    location = [\"tz\",\"tz_label\", \"tz_offset\"]\n",
    "    status = [\"deleted\", \"is_admin\", \"is_owner\", \"is_primary_owner\",\n",
    "              \"is_restricted\",\"is_ultra_restricted\",\"is_bot\", \n",
    "              \"is_email_confirmed\"]\n",
    "    \n",
    "    user = {}\n",
    "    for key, value in {\"identifiers\": identity, \"location\":location, \"status\":status}.items():\n",
    "        user[key]={}\n",
    "        user[key][\"id\"] = d[\"id\"]\n",
    "        for v in value:\n",
    "            if v in d:\n",
    "                user[key][v]=d[v]\n",
    "            else:\n",
    "                user[key][v]=\"None\"   \n",
    "                \n",
    "    return user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a79f69d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "3. Upload Clean Data ..in 4 different versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100d0485",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "4.Create 4 External Table from the Buckets and changed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc98594c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "5.Partition Data and create new tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c68322",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Partition Data\n",
    "df.repartition(24) \\\n",
    "    .write.parquet('data/pq/fhvhv/', mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e67a70",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec13a7c4",
   "metadata": {},
   "source": [
    "## Course Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baac0d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2020-11-22', '2022-03-14')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A0. Find all related files \n",
    "files = sorted(glob(f'{DATA_SOURCE_ROOT}/{COURSE_CHANNEL}/*.json'))\n",
    "\n",
    "# A0. Analyse start and end of the data set\n",
    "start, end = files[0].split(\"/\")[-1].split(\".\")[0], files[-1].split(\"/\")[-1].split(\".\")[0]\n",
    "(start,end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ffeff7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/26 21:55:16 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9136, 32)\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the data into pyspark dataframe\n",
    "messages_cde = spark.read.json(f'{DATA_SOURCE_ROOT}/{COURSE_CHANNEL}/*.json', multiLine=True) \n",
    "# messages_cde.printSchema()\n",
    "print((messages_cde.count(), len(messages_cde.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0893181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/26 21:55:26 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "22/03/26 21:55:26 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "22/03/26 21:55:26 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "22/03/26 21:55:26 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "22/03/26 21:55:26 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "22/03/26 21:55:27 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "22/03/26 21:55:27 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "22/03/26 21:55:27 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "22/03/26 21:55:27 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write on Directory Structure\n",
    "# messages_cde.write.option(\"header\", \"true\").parquet(f\"messages_raw.parquet\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "395c31d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_columns = [\n",
    "    \"client_msg_id\",\n",
    "    \"parent_user_id\",\n",
    "    \"reply_count\",\n",
    "    \"subtype\",\n",
    "    \"text\",\n",
    "    \"thread_ts\",\n",
    "    \"ts\",\n",
    "    \"user\",\n",
    "]\n",
    "messages_cde = messages_cde.select(interest_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ec90541",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=messages_cde.toPandas()#.to_csv(header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "769190c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_msg_id</th>\n",
       "      <th>parent_user_id</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>subtype</th>\n",
       "      <th>text</th>\n",
       "      <th>thread_ts</th>\n",
       "      <th>ts</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9e77c6e5-e288-4060-a6e6-3ec9185345c2</td>\n",
       "      <td>U02TMP4GJEM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Hi &lt;@U01AXE0P5M3&gt;, I had a lot of problems whe...</td>\n",
       "      <td>1642995076.257900</td>\n",
       "      <td>1643012748.273300</td>\n",
       "      <td>U02TMP4GJEM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3bda75c8-bdf7-4794-aacd-530d430a75d8</td>\n",
       "      <td>U02B8U0QZEK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>i dont even know where the 1:latest come from</td>\n",
       "      <td>1642984070.239900</td>\n",
       "      <td>1643012833.273500</td>\n",
       "      <td>U02B8U0QZEK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e4912f0a-1d64-4699-915b-8ef2cc9413ce</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thread_broadcast</td>\n",
       "      <td>okay so terraform is still disturbing me ...wo...</td>\n",
       "      <td>1642422345.167200</td>\n",
       "      <td>1643012974.274000</td>\n",
       "      <td>U02RTJPV6TZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e9dad671-17e3-4c8b-acf3-380573fab37a</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thread_broadcast</td>\n",
       "      <td>google_bigquery_dataset.dataset: Creating...\\n...</td>\n",
       "      <td>1642422345.167200</td>\n",
       "      <td>1643013000.274300</td>\n",
       "      <td>U02RTJPV6TZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acb4f60a-26e0-4900-9c06-3a77df329487</td>\n",
       "      <td>U02T9550LTU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Unique submissions? (i.e. different emails)</td>\n",
       "      <td>1642999704.262500</td>\n",
       "      <td>1643013190.274800</td>\n",
       "      <td>U02U809EAE7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9131</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>channel_join</td>\n",
       "      <td>&lt;@U01ENFKMTU5&gt; has joined the channel</td>\n",
       "      <td>None</td>\n",
       "      <td>1607784533.011300</td>\n",
       "      <td>U01ENFKMTU5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9132</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>channel_join</td>\n",
       "      <td>&lt;@U01GL9D4H7Z&gt; has joined the channel</td>\n",
       "      <td>None</td>\n",
       "      <td>1608114154.006500</td>\n",
       "      <td>U01GL9D4H7Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9133</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>channel_join</td>\n",
       "      <td>&lt;@U01H4CB36HK&gt; has joined the channel</td>\n",
       "      <td>None</td>\n",
       "      <td>1608591242.019100</td>\n",
       "      <td>U01H4CB36HK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9134</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>channel_join</td>\n",
       "      <td>&lt;@U01HXRYSD09&gt; has joined the channel</td>\n",
       "      <td>None</td>\n",
       "      <td>1608776149.019700</td>\n",
       "      <td>U01HXRYSD09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9135</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>channel_join</td>\n",
       "      <td>&lt;@U01HGDYJZUN&gt; has joined the channel</td>\n",
       "      <td>None</td>\n",
       "      <td>1609150354.021500</td>\n",
       "      <td>U01HGDYJZUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9136 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             client_msg_id parent_user_id  reply_count  \\\n",
       "0     9e77c6e5-e288-4060-a6e6-3ec9185345c2    U02TMP4GJEM          NaN   \n",
       "1     3bda75c8-bdf7-4794-aacd-530d430a75d8    U02B8U0QZEK          NaN   \n",
       "2     e4912f0a-1d64-4699-915b-8ef2cc9413ce           None          NaN   \n",
       "3     e9dad671-17e3-4c8b-acf3-380573fab37a           None          NaN   \n",
       "4     acb4f60a-26e0-4900-9c06-3a77df329487    U02T9550LTU          NaN   \n",
       "...                                    ...            ...          ...   \n",
       "9131                                  None           None          NaN   \n",
       "9132                                  None           None          NaN   \n",
       "9133                                  None           None          NaN   \n",
       "9134                                  None           None          NaN   \n",
       "9135                                  None           None          NaN   \n",
       "\n",
       "               subtype                                               text  \\\n",
       "0                 None  Hi <@U01AXE0P5M3>, I had a lot of problems whe...   \n",
       "1                 None      i dont even know where the 1:latest come from   \n",
       "2     thread_broadcast  okay so terraform is still disturbing me ...wo...   \n",
       "3     thread_broadcast  google_bigquery_dataset.dataset: Creating...\\n...   \n",
       "4                 None        Unique submissions? (i.e. different emails)   \n",
       "...                ...                                                ...   \n",
       "9131      channel_join              <@U01ENFKMTU5> has joined the channel   \n",
       "9132      channel_join              <@U01GL9D4H7Z> has joined the channel   \n",
       "9133      channel_join              <@U01H4CB36HK> has joined the channel   \n",
       "9134      channel_join              <@U01HXRYSD09> has joined the channel   \n",
       "9135      channel_join              <@U01HGDYJZUN> has joined the channel   \n",
       "\n",
       "              thread_ts                 ts         user  \n",
       "0     1642995076.257900  1643012748.273300  U02TMP4GJEM  \n",
       "1     1642984070.239900  1643012833.273500  U02B8U0QZEK  \n",
       "2     1642422345.167200  1643012974.274000  U02RTJPV6TZ  \n",
       "3     1642422345.167200  1643013000.274300  U02RTJPV6TZ  \n",
       "4     1642999704.262500  1643013190.274800  U02U809EAE7  \n",
       "...                 ...                ...          ...  \n",
       "9131               None  1607784533.011300  U01ENFKMTU5  \n",
       "9132               None  1608114154.006500  U01GL9D4H7Z  \n",
       "9133               None  1608591242.019100  U01H4CB36HK  \n",
       "9134               None  1608776149.019700  U01HXRYSD09  \n",
       "9135               None  1609150354.021500  U01HGDYJZUN  \n",
       "\n",
       "[9136 rows x 8 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "335cc094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'client_msg_id,parent_user_id,reply_count,subtype,text,thread_ts,ts,user\\n9e77c6e5-e288-4060-a6e6-3ec9185345c2,U02TMP4GJEM,,,\"Hi <@U01AXE0P5M3>, I had a lot of problems when trying to convert the .ipynb file to a py script. I copied exactly the code in the video and I kept getting the error `No template sub-directory with name \\'script\\' can be found\\'. After a bit of googling, I found that if I uninstalled nbconvert module and then reinstalled it with a lower version, the command would work. I was close to throwing my laptop out of the window by that point! Obviously we all have different computers, OS and python versions, so mentioning the module version would be very helpful!\",1642995076.257900,1643012748.273300,U02TMP4GJEM\\n3bda75c8-bdf7-4794-aacd-530d430a75d8,U02B8U0QZEK,,,i dont even know where the 1:latest come from,1642984070.239900,1643012833.273500,U02B8U0QZEK\\ne4912f0a-1d64-4699-915b-8ef2cc9413ce,,,thread_broadcast,okay so terraform is still disturbing me ...wondering what am missing in my google account,1642422345.167200,1643012974.274000,U02RTJPV6TZ\\ne9dad671-17e3-4c8b-acf3-380573fab37a,,,thread_broadcast,\"google_bigquery_dataset.dataset: Creating...\\ngoogle_storage_bucket.data-lake-bucket: Creating...\\ngoogle_bigquery_dataset.dataset: Creation complete after 5s [id=projects/data-zoomcamp-338514/datasets/trips_data_all]\\n╷\\n│ Error: googleapi: Error 403: The project to be billed is associated with an absent billing account., accountDisabled\\n│\\n│   with google_storage_bucket.data-lake-bucket,\\n│   on <http://main.tf|main.tf> line 19, in resource \"\"google_storage_bucket\"\" \"\"data-lake-bucket\"\":\\n│   19: resource \"\"google_storage_bucket\"\" \"\"data-lake-bucket\"\" {\\n│\",1642422345.167200,1643013000.274300,U02RTJPV6TZ\\nacb4f60a-26e0-4900-9c06-3a77df329487,U02T9550LTU,,,Unique submissions? (i.e. different emails),1642999704.262500,1643013190.274800,U02U809EAE7\\ndc9a3f98-c70e-448f-abd2-aeb028c5f95b,,1.0,,\"Also <@U01AXE0P5M3>, do you have to submit homework for all the 6 weeks to be eligible for the project stage ?\",1643013272.276200,1643013272.276200,U02TMP4GJEM\\n72864621-32ce-44d9-9358-1e742c68d8c0,U02B8U0QZEK,,,\"Actually i figured it out, quotations around the -v statement is necessary\",1642984070.239900,1643013532.276500,U02B8U0QZEK\\n2ea10978-cd1f-4e2c-90ce-7214ced61715,U01AXE0P5M3,,,<@U01AXE0P5M3> for windows11 pro I have successfully used the below solution <https://github.com/dbcli/pgcli/issues/974#issuecomment-443164053>,1642511128.453300,1643013747.276700,U02UY1QTGHW\\n37aa1165-d46e-4d01-964c-9ac90ed383f4,U02TB5NK4FL,,,\"<@U02U34YJ8C8>, thanks for your answer. I have already spent so much time on week 1 tasks that I wanted to avoid doing everything all over again. Gosh this course is so hard. I will delete the containers then.\",1642975558.217600,1643013913.277300,U02TB5NK4FL\\n,USLACKBOT,1.0,tombstone,This message was deleted.,1643014045.280000,1643014045.280000,USLACKBOT\\n28d0e930-a480-482c-aab5-a24a7b30ee4d,,4.0,,\"i am getting some error in SQL query tool, i am tring to find a specific location in the \"\"zones\"\" data set but i keep getting the\\n`ERROR:  column \"\"borough\"\" does not exist`\\n`LINE 7: Borough=\\'Queens\\'`\\nhere\\'s an example that i tried\\n```SELECT \\n*\\nFROM\\nzones\\nWHERE\\nBorough=\\'Queens\\'```\",1643014096.280500,1643014096.280500,U02RREQ7MHU\\nd751f16a-ca2a-4032-b443-9322e816156b,USLACKBOT,,,Not my day,1643014045.280000,1643014230.280800,U02UEJMRB8X\\nd388e852-b7e9-4a3e-bc62-e5381ac863b6,U02RREQ7MHU,,,\"try `zones.\"\"Borough\"\"` instead of just `Borough`\",1643014096.280500,1643014235.281200,U02U34YJ8C8\\ndf80f04e-ca04-44f1-8749-8018993b5f77,U02QL21SR4J,,,<@U02RTJPV6TZ> you need to linked your GCP project to your billing account,1642422345.167200,1643014296.281400,U02QBJYQFK9\\n4c2a01da-8449-4bbe-846f-b1e71d4932fb,U02QL21SR4J,,,hey thanks <@U02QBJYQFK9> figured it out,1642422345.167200,1643014465.283000,U02RTJPV6TZ\\n506a7c5c-a1fc-4669-8d14-3034c659a4da,U02RREQ7MHU,,,\"it works, thanks <@U02U34YJ8C8>\",1643014096.280500,1643014498.284000,U02RREQ7MHU\\n30805e02-7047-4db9-a8f3-2317259370e0,,6.0,,\"Hey all, I want to ask something about home work. So, besides `yellow_taxi_data` we need to import dataset about zones right. Do I need to create python script to ingest this dataset to the database or can I just import it using sql script or using import feature in pgadmin? Thanks:raised_hands:\",1643014599.287000,1643014599.287000,U02QBJYQFK9\\nF2C9AD3A-E61C-4D83-A58D-C83CE930EE88,U02RREQ7MHU,,,Just “Boroughs” might work too (with the double quotes). Can\\'t remember the reason. Some weird thing in Postgres.  ,1643014096.280500,1643014663.287800,U02U34YJ8C8\\n780394d7-0647-499d-bbba-67cf18f3eb47,U02QL21SR4J,,,\"Got the same problem too yesterday. If you\\'ve linked the project to billing account but still received error, try to destroy all infrastructure first (`terraform destroy`) and do \"\"restart\"\" linking to billing account (disable and enabled it again). After that you can run terraform plan and terraform apply again\",1642422345.167200,1643014786.288300,U02QBJYQFK9\\n2653138d-7cd2-4b78-8fb2-ea5eba76ae45,U02QBJYQFK9,,,\"I\\'m having concern if I\\'m using the sql script, it will reduce the hw point.\",1643014599.287000,1643014864.289200,U02QBJYQFK9\\n425a4e1f-fd43-4963-9682-adc0202670c6,,6.0,,\"Week 1: seems to me that the README.md is a little confusing, if i were to follow the steps documented there, as some steps are duplicated?\\n\\nunderstand there are multiple ways to achieve the same task, but perhaps they can be marked as alternatives?\",1643014896.289500,1643014896.289500,U02QJJ30E05\\naa607d5d-55fe-45f4-9232-803b6c758c31,U02QBJYQFK9,,,I modified the data-ingest script a little bit so that it can ingest the zone data as well,1643014599.287000,1643014984.289600,U02U809EAE7\\n343c5243-a0a4-48cb-b5b1-7b7d2f266d0f,U02QBJYQFK9,,,\"hi guys, u may check out the zone details via <https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv>\\n\\nFor me, i just look up the table in the csv manually, hence no need to ingest a new table to capture zone details.\",1643014599.287000,1643015137.290000,U02ULMHKBQT\\na72b3aa0-db7b-4881-8621-7499a7c5e6f6,U02RREQ7MHU,,,\"Postgres lowercase all relation names. To preserve capitalization, you have to put it in double quotes: <https://stackoverflow.com/questions/43111996/why-postgresql-does-not-like-uppercase-table-names>\",1643014096.280500,1643015452.291000,U02U809EAE7\\nbd9a6421-b9fc-4dae-b35b-9e8a671779a2,,18.0,,\"I\\'m having this error while trying to run the data ingestion script.\\n\\n```Traceback (most recent call last):\\n  File \"\"/app/ingest_data.py\"\", line 67, in &lt;module&gt;\\n    main(args)\\n  File \"\"/app/ingest_data.py\"\", line 30, in main\\n    df = next(df_iter)\\n  File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\"\", line 1187, in __next__\\n    return self.get_chunk()\\n  File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\"\", line 1280, in get_chunk\\n    return self.read(nrows=size)\\n  File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\"\", line 1250, in read\\n    index, columns, col_dict = self._engine.read(nrows)\\n  File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\"\", line 225, in read\\n    chunks = self._reader.read_low_memory(nrows)\\n  File \"\"pandas/_libs/parsers.pyx\"\", line 817, in pandas._libs.parsers.TextReader.read_low_memory\\n  File \"\"pandas/_libs/parsers.pyx\"\", line 861, in pandas._libs.parsers.TextReader._read_rows\\n  File \"\"pandas/_libs/parsers.pyx\"\", line 847, in pandas._libs.parsers.TextReader._tokenize_rows\\n  File \"\"pandas/_libs/parsers.pyx\"\", line 1960, in pandas._libs.parsers.raise_parser_error\\npandas.errors.ParserError: Error tokenizing data. C error: Expected 1 fields in line 3, saw 2```\",1643015453.291300,1643015453.291300,U02T25ULN9W\\n15F79CD0-4862-4815-8AE1-58D18AEBF3DC,U02T25ULN9W,,,Can you post your ingestion script? And is this the full error?,1643015453.291300,1643015558.292200,U02U34YJ8C8\\nde617b00-c6d0-4471-8b26-b5e9e47830fa,U02T25ULN9W,,,\"In addition to posting your ingestion script, are you sure you had the yellow taxi data set\",1643015453.291300,1643015931.292500,U02HFP7UTFB\\nc626493e-7256-48a8-99d7-c2da0f47a1da,U02T25ULN9W,,,\"```#!/usr/bin/env python\\n# coding: utf-8\\n\\nimport os\\nimport argparse\\n\\nfrom time import time\\n\\nimport pandas as pd\\nfrom sqlalchemy import create_engine\\n\\n\\ndef main(params):\\n    user = params.user\\n    password = params.password\\n    host = params.host\\n    port = params.port\\n    db = params.db\\n    table_name = params.table_name\\n    url = params.url\\n    csv_name = \\'output.csv\\'\\n\\n    # download the csv\\n    os.system(f\"\"wget {url} -o {csv_name}\"\")\\n\\n    engine = create_engine(f\\'postgresql://{user}:{password}@{host}:{port}/{db}\\')\\n\\n    df_iter = pd.read_csv(csv_name, iterator=True, chunksize=100000, error_bad_lines=False)\\n\\n    df = next(df_iter)\\n\\n    df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\\n    df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\\n\\n\\n    df.head(n=0).to_sql(name=table_name, if_exists=\\'replace\\', con=engine)\\n\\n\\n    df.to_sql(name=table_name, if_exists=\\'append\\', con=engine)\\n\\n    while True:\\n        t_start = time()\\n        df = next(df_iter)\\n\\n        df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\\n        df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\\n\\n        df.to_sql(name=table_name, if_exists=\\'append\\', con=engine)\\n\\n        t_end = time()\\n\\n        print(\"\"Inserted another chunk..., took %.3f seconds\"\" % (t_end - t_start))\\n\\nif __name__ == \\'__main__\\':\\n    parser = argparse.ArgumentParser(description=\\'Ingesting CSV data to Postgres.\\')\\n\\n    parser.add_argument(\\'--user\\', help=\\'user name for postgres\\')\\n    parser.add_argument(\\'--password\\', help=\\'password for postgres\\')\\n    parser.add_argument(\\'--host\\', help=\\'host for postgres\\')\\n    parser.add_argument(\\'--port\\', help=\\'port for postgres\\')\\n    parser.add_argument(\\'--db\\', help=\\'data base name for postgres\\')\\n    parser.add_argument(\\'--table_name\\', help=\\'name of table we are writing the results to.\\')\\n    parser.add_argument(\\'--url\\', help=\\'url of the csv file\\')\\n\\n    args = parser.parse_args()\\n\\n    main(args)```\",1643015453.291300,1643017081.293000,U02T25ULN9W\\n95b3031c-2ce9-4099-8ab8-2deb7b2215f0,,4.0,,\"After running `docker-compose up -d`, I\\'m trying to connect to pgadmin through localhost:8080, but it\\'s just loading for too long..\",1643017107.293200,1643017107.293200,U02T9JQAX9N\\n891b6c7f-c36c-4d26-a369-bac814b6e1fe,U02T25ULN9W,,,\"<@U02HFP7UTFB> yes, I have the yellow taxi dataset\",1643015453.291300,1643017141.293500,U02T25ULN9W\\n611ede56-f753-4b9e-bef2-775da82c90a9,U02T25ULN9W,,,<@U02U34YJ8C8> That\\'s the full error.,1643015453.291300,1643017183.293700,U02T25ULN9W\\nf156e81a-09c8-4e81-889b-6e890a79d8dc,U02T9JQAX9N,,,check ther terminal for any messages. Make sure the last message (before you open the url ) is something like `[INFO] Booting worker with pid...`,1643017107.293200,1643017608.294000,U02HB9KTERJ\\ned6dd7e2-8a7a-4ccb-af3e-faf514601d26,U02T9JQAX9N,,,\"I ran it in detached mode, so I can see any messages..\",1643017107.293200,1643017659.294200,U02T9JQAX9N\\n69e08e96-0db2-45c1-b43e-484f714975ba,,2.0,,\"<@U01AXE0P5M3> Possible to highlight Central Park in the Fifth question on google forms?\\nWhat was the most popular destination for passengers picked up in *central park* on January 14? Enter the zone name (not id). If the zone name is unknown (missing), write \"\"Unknown\"\"\",1643017732.295100,1643017732.295100,U0290EYCA7Q\\n2d9130f8-24ea-43cc-b5ab-d1575c6d5636,U02TMP4GJEM,,,\"No, just the project\",1643013272.276200,1643017805.295200,U01AXE0P5M3\\n56dbe145-c85e-4dba-909f-05e79d4af0aa,U02QBJYQFK9,,,Yes you\\'ll need to either write this code yourself or take it from the notebook I used in the videos (it\\'s also committted in the repo),1643014599.287000,1643017895.295600,U01AXE0P5M3\\nc68e15d6-7863-4627-9eb8-32177a09ce16,U02T9JQAX9N,,,\"It has worked now though.. I\\'ve figured why it didn\\'t work but it\\'s confusing me.. So, I tried adding volumes to the pgadmin so I don\\'t have to keep creating a server every time\\n\\n```volumes:\\n  - ny_taxi_pgadmin_data:/var/lib/pgadmin:rw```\\nIt was when I removed this that it started working..\",1643017107.293200,1643017923.295800,U02T9JQAX9N\\n9de0d1f0-ef3c-4488-91ca-873a4b7d7e71,U02QJJ30E05,,,\"which readme are you referring to? overall, week 1 or a specific section? or all of the above? :slightly_smiling_face:\",1643014896.289500,1643017931.296000,U01AXE0P5M3\\n89b2930f-6bed-4d52-a278-d59aca9eec26,U02QJJ30E05,,,The Week 1 readme specifically,1643014896.289500,1643017968.296200,U02QJJ30E05\\n06bb6cd6-7808-4530-afb3-d523dd850223,U0290EYCA7Q,,,why it shoud be highlighted?,1643017732.295100,1643017982.296400,U01AXE0P5M3\\ne3e1684d-738e-46e0-8e19-3f6c902746dd,,,,\"Question 6: Most expensive route\\xa0*\\nWhat\\'s the pickup-dropoff pair with the largest average price for a ride (calculated based on total_amount)? Enter two zone names separated by a slashFor example:\"\"Jamaica Bay / Clinton East\"\"If any of the zone names are unknown (missing), write \"\"Unknown\"\". For example, \"\"Unknown / Clinton East\"\".\",,1643018016.296700,U02U6DR551B\\nef0227c5-eab8-484f-8e22-cd846eb1d4be,U02QJJ30E05,,,\"I see the data is being loaded in different ways, should the database be cleared in between in this case?\",1643014896.289500,1643018032.297100,U02QJJ30E05\\n5f0bb87d-a80f-4897-a28d-34c7a7e06183,,,,what column constitutes a ride in yellow_taxi_trips data,,1643018033.297300,U02U6DR551B\\n23f1f387-9f5c-474b-9864-f97af6ccb885,,1.0,,the pickup and drop location IDs?,1643018040.297600,1643018040.297600,U02U6DR551B\\ndb88aeea-f1c0-4240-9af5-a551b5d2f956,U02QJJ30E05,,,\"I think I explicitly clear the database in the videos? \\n\\nBut the order in the week 1 readme is the order you should follow\",1643014896.289500,1643018306.298000,U01AXE0P5M3\\nf13816ca-ef27-45f3-8717-8f18bc1dfbf3,U02QJJ30E05,,,\"Ok, so it\\'s not meant for someone to try out the course just based on the Git Repos, the videos are the main content source.  Understand that then\",1643014896.289500,1643018376.298200,U02QJJ30E05\\nc2a1e872-dfa2-4735-83da-588db6ad9f0b,U02TC704A3F,,,Sorry yes my bad. It does help if actually read the instructions...,1642986252.245400,1643018631.298700,U02U5SW982W\\nffef7130-d2f8-4a31-ac58-1f7ea8a30c5b,,2.0,,I found out about the this data engineering zoomcamp today from a reddit post and singed up for the course just now. However it seems like the course has already begun a few days ago. Am I too late to register?,1643018753.300500,1643018753.300500,U0308865C0H\\n1ade0883-ba49-4d37-930b-7aa8914a3bd8,U0308865C0H,,,Nope -  You can manage.,1643018753.300500,1643019177.303200,U0290EYCA7Q\\n9176084b-7dbe-4a5d-b59a-2d8182d7f9d1,U0290EYCA7Q,,,\"Actually, I missed it first time, and wrote the query for all zones. Luckily I figured it out.\",1643017732.295100,1643019363.303500,U0290EYCA7Q\\ne35ef97a-c174-412e-9b0f-7127b2a13484,U02T25ULN9W,,,\"Could be an issue with dataset. Download it again, and try.\",1643015453.291300,1643019491.304400,U0290EYCA7Q\\n29f12de9-4834-405f-ad52-ea0adb6de551,U02R09ZR6FQ,,,<@U02UY1QTGHW> I followed the advice of Alexey and used docker-compose instead and it worked. But thank you for sharing your solution!,1642891400.489400,1643019557.304600,U02R09ZR6FQ\\n951c9798-e34a-4741-9e7e-e2802c373eb2,U02T25ULN9W,,,\"```pandas.errors.ParserError: Error tokenizing data. C error: Expected 1 fields in line 3, saw 2```\\ni believe its dataset issue, cos this error means pandas read_csv method cant read the dataset supplied. check your dataset delimiter\",1643015453.291300,1643019662.305400,U02HFP7UTFB\\ne7f392f3-ca9b-46fe-a1d5-62211acc58ff,U02QJJ30E05,,,pull requests with improvements are welcome!,1643014896.289500,1643019670.305800,U01AXE0P5M3\\n437FB930-AC2E-4207-B395-4A800736DBBE,U02B8U0QZEK,,,Try using the full path,1642984070.239900,1643019672.306200,U02T2S9TY6S\\nbf6a69fe-2f04-4374-8f46-3610421fd0b4,U02T9550LTU,,,\"no, just submissions\",1642999704.262500,1643019698.306600,U01AXE0P5M3\\n896c4f50-7b10-4fa6-b095-6f280271ed94,,4.0,,\"Video 1.4.2 issue with Port forwarding. Everything is working fine up until around 33 minutes in. When I put in port forwarding it generates a local address of localhost:42197. Then when I go back into my terminal and type  `pgcli -h localhost -U root -d ny_taxi` it asks for the password which I type in as \\'root\\' and then gives the following output: `connection to server at \"\"localhost\"\" (127.0.0.1), port 5432 failed: FATAL:  password authentication failed for user \"\"root\"\"`before it exits back to a prompt. Any ideas for things I should check?\",1643019830.309300,1643019830.309300,U02U5SW982W\\nAB38C8E1-26C5-433E-A8B4-3D40C62F4FD1,U02T25ULN9W,,,\"<@U02T25ULN9W> On the wget line, make sure to use a capital O rather than lower case. May not fix the issue but looks like you\\'re using a lower case O\",1643015453.291300,1643019878.310600,U02U34YJ8C8\\n117cd1cc-4e56-42d8-8788-438ba5788540,U02T25ULN9W,,,\"Yeah.  Thanks for the correction. It didn\\'t fix it, tho\\'\",1643015453.291300,1643020009.310800,U02T25ULN9W\\n61424016-f670-4b79-92bd-b63cd8ee2e36,U02U5SW982W,,,\"it\\'s actually not very important, you can skip this\",1643019830.309300,1643020057.311000,U01AXE0P5M3\\n58034738-f300-4e48-94b6-bdf4584fc635,U02U5SW982W,,,\"as long as you can connect to it from within the VM, it\\'s good\",1643019830.309300,1643020074.311200,U01AXE0P5M3\\n355ce67d-beee-47fb-ba18-6e513aaa6e5a,U02U5SW982W,,,\"but to answer your question, you should specify the port in pgcli using the -p key (e.g. `-p 42197`)\",1643019830.309300,1643020109.311500,U01AXE0P5M3\\nd006f821-23e6-46ba-a3fa-e951d5f0d152,U02T25ULN9W,,,\"<@U02T25ULN9W> are you downloading the data straight from the s3 link, or you have downloaded it on your local machine\",1643015453.291300,1643020379.312200,U02HFP7UTFB\\n76d5d398-73a9-463d-a8ff-fbea6804e3e5,U02T25ULN9W,,,I\\'ve tried doing both. Same result.,1643015453.291300,1643020421.312400,U02T25ULN9W\\ne967697d-6066-47ba-a590-4e7f670a2c73,U0308865C0H,,,Thanks!,1643018753.300500,1643020452.312800,U0308865C0H\\n19798890-733a-4a05-8cd6-816a2c0802d8,U02T25ULN9W,,,can you paste first 10 lines?,1643015453.291300,1643020495.313000,U0290EYCA7Q\\n53f8696b-acd2-4767-9554-b7c2bbf51281,U02T25ULN9W,,,\"Try regenerating your image:\\n\\nSomething like the following if I remember right:\\n\\n`docker build -t taxi_ingest:v001 .``\\n\\nThen create the container again.\\n\\nWhen you had `-o` instead of `-O` you were actually generating a log file called `output.csv`\\n\\nThat’s why pandas was throwing an error trying to parse it.\",1643015453.291300,1643020520.313200,U02U34YJ8C8\\n7779fbd0-58e6-49d4-8f31-2c323a5e5f5b,U02T25ULN9W,,,\"You might also want to remove your current container before recreating the image.\\n\\n`docker stop &lt;container ID&gt;`\\n\\nAnd then:\\n\\n`docker rm &lt;container ID&gt;`\\n\\nAlthough you might not need to stop the container, as it should stop automatically once it’s exited the ingestion script I think.\",1643015453.291300,1643020635.313400,U02U34YJ8C8\\nde9fbc11-a4ef-48b8-90aa-4e77f04cedf0,U02QBJYQFK9,,,But it doesn\\'t have to be python right? <@U01AXE0P5M3>,1643014599.287000,1643020697.313800,U02QBJYQFK9\\nb9b3ba8c-ecaf-4287-8c27-65d7245b7604,U02T25ULN9W,,,Think <@U02T25ULN9W> trying to run ingestion script directly.,1643015453.291300,1643020805.314100,U0290EYCA7Q\\n9b71dd64-3684-4e8e-983e-ec2abd2cb902,U02V4UTSU8M,,,\"I could not follow the instruction. some are videos in you tube, also in github instruction. which one should follow? also dont know which one start by order. I create google cloud account without any project, IAm role or anything, just create. what should i do now. i joined yesterday. its really hard for me to follow along. will you please tell me where and how i can slowly move?\",1642981427.237600,1643021267.314400,U02V4UTSU8M\\nccb1eaa8-fb6a-4f75-8dda-1b80a23f2eb9,U02T25ULN9W,,,It ran directly without any error (except the StopIteration Exception) and I can see the data in Pgadmin,1643015453.291300,1643021312.314600,U02T25ULN9W\\n6dbd7a0b-8fc5-4424-9a58-ca70660ddf7c,U02T25ULN9W,,,\"<@U02U34YJ8C8>, Thanks. I\\'ll try that now\",1643015453.291300,1643021362.314800,U02T25ULN9W\\n3a7e75e9-1603-4b56-b69f-f668642fee1d,U02QBJYQFK9,,,whatever works for you,1643014599.287000,1643021413.315000,U01AXE0P5M3\\n36158be4-837f-48ea-b084-c18db39fb608,,2.0,,\"I could not follow the instruction. some are videos in you tube, also in github instruction. which one should follow? also dont know which one start by order. I create google cloud account without any project, IAm role or anything, just create. what should i do now. i joined yesterday. its really hard for me to follow along. will you please tell me where and how i can slowly move?\",1643021888.315300,1643021888.315300,U02V4UTSU8M\\nd19ee05a-a98a-40f9-982e-cd7f78c4a0d7,U02V4UTSU8M,,,Follow the order in github,1643021888.315300,1643022100.315600,U01AXE0P5M3\\n40f2ea13-3538-4942-bea2-218472f77d51,U02V4UTSU8M,,,thanks,1643021888.315300,1643022122.315800,U02V4UTSU8M\\nc9bf9520-f095-4b30-ac10-ad8ee571bf4f,,3.0,,\"is there any volunteer who has time for me to navigate the ist week set up procedure, then we can meet at google meet and show you what I am doing in my machine. because I am so weak about command line. my vs code and gitbash also looks different than alex\",1643022264.318000,1643022264.318000,U02V4UTSU8M\\n071697F4-5AA5-417E-8AAC-94522F76060B,,20.0,,\"hi! after successfully running the following: \\n `$ docker run -it \\\\\\n  -e PGADMIN_DEFAULT_EMAIL=“<mailto:admin@admin.com|admin@admin.com>” \\\\\\n-e PGADMIN_DEFAULT_PASSWORD=“root” \\\\\\n-p 8080:80 \\\\\\ndpage/pgadmin4`\\nI can’t open the localhost:8080 in my browser, it is simply not loading. what could be the problem? thanks!\",1643022678.324100,1643022678.324100,U02DY0L6PHV\\n3d85d234-b13a-432a-81ee-2c3a7f4c3484,,1.0,,\"I’m getting a password authentication error when I run the command `psql -h localhost -p 5432 -U root` . The output is `psql: error: connection to server at \"\"localhost\"\" (::1), port 5432 failed: FATAL:  password authentication failed for user \"\"root\"\"` . Any ideas on how to solve this?\",1643022702.324500,1643022702.324500,U02FQPX4SC9\\n99be6ce3-fcf1-4135-997f-ce522bfe8adf,U02DY0L6PHV,,,Have you tried it on a different browser?,1643022678.324100,1643023175.324800,U02U34YJ8C8\\nCB97113B-5A02-47D0-B7AF-0148D88CC140,U02DY0L6PHV,,,\"<@U02U34YJ8C8> I did, didn\\'t help\",1643022678.324100,1643023382.325300,U02DY0L6PHV\\n64e63091-1be2-48bf-a14a-1eb5426f2938,U02DY0L6PHV,,,\"Make the URL is \"\"*http*://...\"\" and not *https*\",1643022678.324100,1643023559.325500,U02QH3TBA11\\n337a8761-00c9-454b-a9ef-05d10e8d3cbe,U02U6DR551B,,,\"You can join this table with \"\"zone\"\" table. And use zone\\'s names\",1643018040.297600,1643023684.325800,U02QL1EG0LV\\n7878D69D-AC18-407A-AEAE-AD8223C5C97B,U02DY0L6PHV,,,<@U02QH3TBA11> sadly didn\\'t help as well,1643022678.324100,1643023693.326300,U02DY0L6PHV\\n5e790593-4106-44dc-801e-26a825e9a9a3,U02DY0L6PHV,,,\"<@U02DY0L6PHV> The command looks okay from what I can tell. Try stopping and removing your containers:\\n\\n`docker stop &lt;container id&gt;`\\n`docker container prune`\\n\\nYou can find running container ids by running `docker ps`\\n\\nThen try re-running the container.\",1643022678.324100,1643023716.326500,U02U34YJ8C8\\nb32921d8-9c31-46b8-86ce-7746d42d7983,U02FQPX4SC9,,,Try to change port and use it. It seems to me I saw this problem in video from Alexey,1643022702.324500,1643023919.326700,U02QL1EG0LV\\n6762c4b8-966a-49a5-bffc-14f53e1f79a3,U02DY0L6PHV,,,\"Also are you on Mac, Windows, or Linux?\",1643022678.324100,1643024055.326900,U02U34YJ8C8\\n8CF6642C-3938-421B-A794-E6AFAD770239,U02DY0L6PHV,,,\"<@U02U34YJ8C8> created a VM with Linux, using Mac\",1643022678.324100,1643024223.327800,U02DY0L6PHV\\n81439822-39e4-4c88-a616-9dd5d68f7233,U02DY0L6PHV,,,What happens if you run `nc -vz localhost 8080` in your terminal?,1643022678.324100,1643024280.328000,U02U34YJ8C8\\nd9f94979-19f9-4fc6-9fa5-cc3fa94ee5d6,U02DY0L6PHV,,,Maybe you are on a company\\'s vpn with a firewall?,1643022678.324100,1643024396.328800,U0308MF3KUH\\n2128076A-F598-464C-AD9A-4A4862CCD981,U02DY0L6PHV,,,<@U02U34YJ8C8> there’s `Connection to localhost port 8080 [tcp/http-alt] succeeded!`,1643022678.324100,1643024432.329700,U02DY0L6PHV\\n0F73517E-6509-47FD-AFBB-E7838546CF70,U02DY0L6PHV,,,<@U0308MF3KUH> I don’t have any enabled vpn right now,1643022678.324100,1643024500.331100,U02DY0L6PHV\\nbb765310-4700-4b7e-b55a-f94ca769d7a5,U02DY0L6PHV,,,\"Ok seems to be working. As <@U0308MF3KUH> suggests, maybe disable firewall on your machine, if it’s enabled. On Mac, this is under Security &amp; Preferences. Just disable it temporarily, then re-enable once/if pgadmin shows in your browser\",1643022678.324100,1643024613.331400,U02U34YJ8C8\\nED2E727D-AA58-4F7D-B21B-C34A2D25B732,U02DY0L6PHV,,,\"<@U02U34YJ8C8> yeah, checked this, the firewall was disabled in system preferences. after stopping, removing and re-running the container as was suggested the same issue occurs \",1643022678.324100,1643024840.333500,U02DY0L6PHV\\n44ce47a6-a75b-472f-ba72-241add845330,U02DY0L6PHV,,,\"Strange. Are you getting an error, or does it just keep loading?\",1643022678.324100,1643024971.333800,U02U34YJ8C8\\n4468ee9d-0047-4e91-83d0-0488a890f015,,10.0,,\"hey guys, after running this code, docker run -it \\\\\\n\\xa0 -e POSTGRES_USER=\"\"root\"\" \\\\\\n\\xa0 -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n\\xa0 -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n\\xa0 -v \\'/d/data_engineering/week_1_basics/docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data\\' \\\\\\n\\xa0 -p 5432:5432 \\\\\\n\\xa0 postgres:13 I am not able to get all the files in the ny_taxi_postgres_data folder, instead it creates a new folder with the same name\",1643025087.334200,1643025087.334200,U02C5MT2HQC\\n530d20f4-c2d5-4ce2-9cc3-d194cfad7103,U02DY0L6PHV,,,\"Where does the vm live? In virtual box? Can you try running this command in Mac directly, not from a VM?\",1643022678.324100,1643025231.334400,U01AXE0P5M3\\n517aa62e-af7b-4b86-b9e0-eeea5e9806ec,U02DY0L6PHV,,,Maybe you\\'ll need to configure your VM to expose port 8080,1643022678.324100,1643025254.334600,U01AXE0P5M3\\nec295980-1ed8-4fe7-b48b-7e783754c0ff,U02DY0L6PHV,,,\"Try to setup a web server with python on Linux and check if you can access it, as in the example Alexey did\",1643022678.324100,1643025262.334800,U02GVGA5F9Q\\nba1fc97b-3afd-4068-a5b1-2696de21a413,U02C5MT2HQC,,,\"can u check if this path exists?\\n```/d/data_engineering/week_1_basics/docker_sql/ny_taxi_postgres_data```\\n\",1643025087.334200,1643025280.335000,U01DFQ82AK1\\n4778ff13-e0cd-46f6-886b-ed6e9b416f3b,U02C5MT2HQC,,,\"Yeah, it is there\",1643025087.334200,1643025408.335200,U02C5MT2HQC\\nc61b722c-9bbb-41fe-b3cd-72533c83c630,U02C5MT2HQC,,,It creates a new folder but doesn\\'t import the files from postgres,1643025087.334200,1643025450.335400,U02C5MT2HQC\\n37cfcf2e-c84e-4b9d-93f4-739330b6f58e,U02C5MT2HQC,,,is that folder in d drive on windows? can you please try as d:/data_engineering/...,1643025087.334200,1643025559.335900,U02NSF7RYP4\\nbff32e95-12ae-4fad-841e-6973d33e68b2,,5.0,,\"hey guys, i got this error\\n`error checking context: \\'no permission to read from \\'/home/kenwu/Documents/GitHub/DE-Zoomcamp/Week 1 Homework/docker_sql/ny_taxi_postgres_data/pg_stat_tmp/db_0.stat\\'\\'.`\\ntrying to build the docker image using this command\\n```docker build -t taxi_ingest:v001 .```\\nas insights as to why and how can i solve it? i tried using chmod to give permission to give every file in the directory\",1643025643.337300,1643025643.337300,U02B8U0QZEK\\n4f1f6212-aab8-46a8-bb83-1f8fd213cdad,U02C5MT2HQC,,,getting error,1643025087.334200,1643025975.337600,U02C5MT2HQC\\n760094a6-756d-4641-a8dc-8d1a2685224e,U02T25ULN9W,,,\"It\\'s working now <@U02U34YJ8C8>  and <@U0290EYCA7Q> :grin:  Thanks for unblocking me.\\n\\nI removed the taxi_ingest:v001 image and started all over again.\",1643015453.291300,1643026007.337800,U02T25ULN9W\\nbe7a608c-fde5-4e74-a8b9-2a279780c2f5,,10.0,,Hallo I\\'m. Having a challenging loading the tables in postgress..I have successfully created the database but the tables are empty,1643026101.339300,1643026101.339300,U02AUCL9ZQF\\nce038456-8a58-48b8-9c84-a7846349d084,U02C5MT2HQC,,,\"don\\'t worry about that folder,  same thing happened to me, the folder got created but there was nothing inside it, just follow the video for now you wont need it , when you make a docker-compose.yaml file, you will get all the files automatically,\",1643025087.334200,1643026156.339600,U02RREQ7MHU\\n194b5c67-428f-40aa-94e4-66f9b51c91da,U02C5MT2HQC,,,as long as postgres:13 is running with port 5432:5432 you can just follow the video,1643025087.334200,1643026183.339800,U02RREQ7MHU\\n2350ecb7-e8fa-46d8-9148-2eb590f2be7c,U02B8U0QZEK,,,\"Try deleting the folder `ny_taxi_postgres_data`  the recreate it, give it the right permissions, then re-run the image:\\n\\n`rm -r ny_taxi_postgres_data`\\n`mkdir ny_taxi_postgres_data`\\n`sudo chmod a+rwx ny_taxi_postgres_data`\",1643025643.337300,1643026201.340000,U02U34YJ8C8\\nA5714A1C-6329-4152-A2E3-56C670128CA5,U02ULGHNT33,,,Had same error and this worked,1642939459.070600,1643026273.340700,U02T91CL14Y\\n14d367c0-fe9f-4924-95f2-813dc0a55c71,U02C5MT2HQC,,,Oh thanks.,1643025087.334200,1643026360.340900,U02C5MT2HQC\\n3591cc74-6b74-4a31-8113-4ad14ca8894b,U02V4UTSU8M,,,I am free to help you,1643022264.318000,1643026447.341700,U02SPLJUR42\\nd1fae936-77c0-4a73-942c-7bab6873e479,U02B8U0QZEK,,,\"Wow, worked like magic, thanks\",1643025643.337300,1643026617.341900,U02B8U0QZEK\\ncb0f3d47-92a4-4d14-8f13-8bd06415f0b3,U02B8U0QZEK,,,\"<https://youtu.be/tOr4hTsHOzU?t=453>\\n\\nalso you can check this\",1643025643.337300,1643026658.342100,U01AXE0P5M3\\nc4037df8-e14f-41d1-aa33-fced0078ccd8,U02B8U0QZEK,,,\"TLDR - create a folder \"\"data\"\" and put it to .gitignore\",1643025643.337300,1643026698.342400,U01AXE0P5M3\\na7b735a4-78e7-4d2e-9d74-950c3a96f026,U02B8U0QZEK,,,\"Hi Ken Wu\\nAlso, you can try to work in a seperate directory in which only the required files exist for building image, e.g Docker File, ingesting scripts. As far as I remember, after compose up postgres service, it created a directory with name \\'ny-taxi.. \\', owner of this directory was system-dump, something like that.. So it\\'s not good idea to change permissions of that directory. By the way, I am working on Ubuntu.\",1643025643.337300,1643026698.342600,U02QTG6A71U\\ndef90145-667d-4fe5-9951-42db27ebfa43,,2.0,,\"when you are resubmitting a google form, do you have to fill all the fields again or just the one you are replacing ?\",1643026927.343800,1643026927.343800,U02RREQ7MHU\\n0A9BF94E-5904-4E7F-9C9D-B7468F4154A7,U02V4UTSU8M,,,\"Will be ok , I will join 13.40\",1643022264.318000,1643027200.344600,U02V4UTSU8M\\n39a304b3-2b58-4450-a045-f09cdf3ee1eb,U02RREQ7MHU,,,Most of them are mandatory. It won\\'t allow you to submit again otherwise.,1643026927.343800,1643027366.344800,U0290EYCA7Q\\n2a465c48-5296-4ef5-8d1e-f38cefdf6c4e,U02RREQ7MHU,,,oh ok,1643026927.343800,1643027419.345000,U02RREQ7MHU\\n02a2cd60-3db6-4582-b459-7c043c5a0da4,U02AUCL9ZQF,,,Did you run ingestion script?,1643026101.339300,1643027448.345200,U0290EYCA7Q\\n824d38ff-b7fa-478f-b2e4-d2c827681de9,U02V4UTSU8M,,,\"i am here, let me know if your are available\",1643022264.318000,1643027978.345500,U02V4UTSU8M\\nedcaa235-0b8c-4746-9900-eed3163adf3f,,1.0,,\"hi all happy learning!!:upside_down_face:, lets go!!\",1643028084.348200,1643028084.348200,U02RTJPV6TZ\\n97f5d041-69f9-4fbf-996d-311e308797d6,,1.0,,I had a problem with setting up the postgres db and turns out the mapping was not being done successfully. The only clue that pointed me to this failure was the fact that the local folder( originally empty) did not get populated with postgres artifacts like Alexey showed . this cross checking step showed was what enabled me to debug. Thanks for that!,1643028184.349800,1643028184.349800,U02TWFZURD1\\n4a920f72-2ace-42a6-ba4e-c5efb557a2e5,U02AUCL9ZQF,,,which step are you at ? can you point to the relevant point on the video ?,1643026101.339300,1643028233.350300,U02TWFZURD1\\n28426f6b-271d-422a-8640-abe405d1e2f3,U02RTJPV6TZ,,,\"just completed some you tube lessons on how to use \"\"git hub\"\"....:smile: , we shall get there\",1643028084.348200,1643028305.351000,U02RTJPV6TZ\\n312d62ca-297f-4136-92ba-710d17c690cb,,6.0,,\"when I was building the docker network and importing the data into the docker image, I encountered this error\\n`sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name \"\"pg-database\"\" to address: Name or service not known`\\nthe command I ran is the following\\n```docker run -it \\\\\\n  --network=pg-network \\\\\\n  taxi_ingest:v001 \\\\\\n    --user=root \\\\\\n    --password=root \\\\\\n    --host=pg-database \\\\\\n    --port=5432 \\\\\\n    --db=ny_taxi \\\\\\n    --table_name=yellow_taxi_trips \\\\\\n    --url=${URL}```\",1643028416.352400,1643028416.352400,U02B8U0QZEK\\na157c562-fd82-453f-a498-3e7ce52a5a05,U02B8U0QZEK,,,\"1. Is the postgres container pg-database running?\\n2. Did you create pg-network?\\nCan you run these commands to verify?\\n\\n1. docker ps -  should list the containers running\\n2. docker network inspect pg-network : Please confirm if you see three containers inside.\",1643028416.352400,1643028760.352700,U0290EYCA7Q\\nff94fb7f-05ad-47d9-a14d-6d971c8ea4d4,U02AUCL9ZQF,,,I\\'m in the 7min and 26 seconds of the Running postgress and pgAdmin with docker.,1643026101.339300,1643028845.353000,U02AUCL9ZQF\\ndaa79e17-4ea0-4ac2-8f9b-b4cd72194eca,,1.0,,\"Has anyone tried this before?\\n\\n<https://developers.redhat.com/articles/2022/01/17/reduce-size-container-images-dockerslim#using_dockerslim>\",1643029001.353700,1643029001.353700,U0290EYCA7Q\\n0d05875a-12dc-4677-a5b1-85212a59b892,U02B8U0QZEK,,,\"If nothing <@U0290EYCA7Q>said works, it may be the name of the database \"\"pgdatabase\"\" is what has worked for most people (That\\'s how it is in the docker-compose file)\",1643028416.352400,1643029486.354000,U02TEKL21JQ\\nd0b9b5fe-c123-43d6-9e56-ff2aa6c90cc3,U02C5MT2HQC,,,\"accessing database through pgcli is not working, it just gets the password and not accessing database\",1643025087.334200,1643029668.354200,U02C5MT2HQC\\n5914ef3b-aeec-4715-8805-3c89c33127f3,U02B8U0QZEK,,,\"turn out pg-database was force shut down, but i can\\'t turn it back. it reports the following error\\n`docker: Error response from daemon: Conflict. The container name \"\"/pg-database\"\" is already in use by container \"\"b417e67d5f48e824aad68bc75cfb42552985e684574e74772407faed58a022a7\"\". You have to remove (or rename) that container to be able to reuse that name.`\\neven though there isnt another container named pg-database as shown using docker ps command\\n`(base) [kenwu@ken-latitude7480 docker_sql]$ docker ps`\\n`CONTAINER ID   IMAGE            COMMAND            CREATED          STATUS          PORTS                                            NAMES`\\n`c2c004f6dfea   dpage/pgadmin4   \"\"/entrypoint.sh\"\"   11 minutes ago   Up 11 minutes   443/tcp, 0.0.0.0:8080-&gt;80/tcp, :::8080-&gt;80/tcp   pgadmin`\",1643028416.352400,1643029929.354500,U02B8U0QZEK\\n517b8963-1ca9-4f02-9c65-f4c81d70c9f9,U0290EYCA7Q,,,\"nop, but u can try with alpine. That is a small linux distribution.\",1643029001.353700,1643030267.354700,U02TEKD2UKE\\na87124aa-2104-409a-b0fe-3c06b5601732,U02AUCL9ZQF,,,\"Can you try this ?\\n\\nSelect count(1) from &lt;table_name&gt;\",1643026101.339300,1643030316.355000,U0290EYCA7Q\\n4d919e76-e4aa-43cd-8159-0b9ee6e5953b,U02B8U0QZEK,,,\"looks like whenever i build the docker image, it creates a container that force the original pg-database to shut down, very weird\",1643028416.352400,1643030535.355300,U02B8U0QZEK\\n0d4e361c-0f20-4c8b-834d-ac36a500c5fc,U02AUCL9ZQF,,,\"And do you see the columns?\\n\\n```root@localhost:ny_taxi&gt; select column_name, data_type, character_maximum_length, column_default, is_nullable\\n from INFORMATION_SCHEMA.COLUMNS where table_name = \\'trips\\'\\n+-----------------------+-----------------------------+--------------------------+----------------+-------------+\\n| column_name           | data_type                   | character_maximum_length | column_default | is_nullable |\\n|-----------------------+-----------------------------+--------------------------+----------------+-------------|\\n| congestion_surcharge  | double precision            | &lt;null&gt;                   | &lt;null&gt;         | YES         |\\n| VendorID              | bigint                      | &lt;null&gt;                   | &lt;null&gt;         | YES         |\\n| tpep_pickup_datetime  | timestamp without time zone | &lt;null&gt;                   | &lt;null&gt;         | YES         |\\n| tpep_dropoff_datetime | timestamp without time zone | &lt;null&gt;                   | &lt;null&gt;         | YES         |\\n| passenger_count       | bigint                      | &lt;null&gt;                   | &lt;null&gt;         | YES         |\\n| trip_distance         | double precision            | &lt;null&gt;                   | &lt;null&gt;         | YES         |\\n| RatecodeID            | bigint                      | &lt;null&gt;                   | &lt;null&gt;         | YES         |\\n| index                 | bigint                      | &lt;null&gt;                   | &lt;null&gt;         | YES         |\\n| PULocationID          | bigint                      | &lt;null&gt;                   | &lt;null&gt;         | YES         |\\n| DOLocationID          | bigint                      | &lt;null&gt;                   | &lt;null&gt;         | YES         |\\n| payment_type          | bigint                      | &lt;null&gt;                   | &lt;null&gt;         | YES         |\\n| fare_amount           | double precision            | &lt;null&gt;                   | &lt;null&gt;         | YES         |\\n| extra                 | double precision            | &lt;null&gt;                   | &lt;null&gt;         | YES         |\\n| mta_tax               | double precision            | &lt;null&gt;                   | &lt;null&gt;         | YES         |\\n| tip_amount            | double precision            | &lt;null&gt;                   | &lt;null&gt;         | YES         |\\n| tolls_amount          | double precision            | &lt;null&gt;                   | &lt;null&gt;         | YES         |\\n| improvement_surcharge | double precision            | &lt;null&gt;                   | &lt;null&gt;         | YES         |\\n| total_amount          | double precision            | &lt;null&gt;                   | &lt;null&gt;         | YES         |\\n| store_and_fwd_flag    | text                        | &lt;null&gt;                   | &lt;null&gt;         | YES         |\\n+-----------------------+-----------------------------+--------------------------+----------------+-------------+```\",1643026101.339300,1643030780.355600,U0290EYCA7Q\\ne201a753-064b-4e5c-8ebd-22cb3dcc0feb,,1.0,,\"Hi all, please when is the deadline for submission\",1643031059.356300,1643031059.356300,U02T0CYNNP2\\n3f716653-c593-4830-ae48-d34bbdc87b99,U02T0CYNNP2,,,<https://datatalks-club.slack.com/archives/C02V1Q9CL8K/p1643024469240700>,1643031059.356300,1643031106.356600,U0290EYCA7Q\\na63c2b1b-db08-416d-b863-7443bd19fa5a,,2.0,,What do I do if I do not see the answer I get in the assignment options,1643031390.357000,1643031390.357000,U02T9JQAX9N\\n5e36f033-7be2-4869-91c7-b27dc8d22b22,U02AUCL9ZQF,,,\"<@U02AUCL9ZQF> sorry, haven\\'t reached this far yet\",1643026101.339300,1643031499.357100,U02TWFZURD1\\n509f6bb1-13b4-434f-86c8-1d54e1846b25,U02T9JQAX9N,,,\"you need to change the permission to the pgadmin folder\\n`sudo chown 5050:5050 data_pgadmin`\",1643017107.293200,1643031507.357300,U02HB9KTERJ\\n4c95dd77-6cee-4b80-bd06-25ca34e8ea9e,U02UAADSJ84,,,Can you try running it on Anaconda cmd?,1642655334.281200,1643032046.358100,U02UAADSJ84\\n6785b032-8af6-4a97-bfdf-f53dfa15d4e0,,2.0,,\"Hello, please what\\'s the course hashtag for social media posts?\",1643032048.358300,1643032048.358300,U02QS4BD1NF\\n74856b73-7410-4662-92f8-3d849d7081df,U02QS4BD1NF,,,\"`Learning in public links`\\nLinks to social media posts where you share your progress with others (LinkedIn, Twitter, etc). Use #dezoomcamp tag.\\n\\nIt\\'s in the form.\",1643032048.358300,1643032124.358500,U0290EYCA7Q\\n0814c6b7-f116-43a4-a3aa-52111eb79d1b,U02QS4BD1NF,,,\"Thank you.\\nI didn\\'t see it,  my bad.\",1643032048.358300,1643032160.358800,U02QS4BD1NF\\n24ded196-e38a-4d42-b61a-3f8c72819019,U02T9JQAX9N,,,Select the closest one,1643031390.357000,1643032450.359200,U01AXE0P5M3\\ndbb58c2c-6e20-4468-aed2-546ee76acae7,,7.0,,\"How can I specify the code used to get the sql solutions?\\nI have pasted them in the homework file is this enough?\",1643032759.360800,1643032759.360800,U02V90BSU1Y\\nc27efbce-e7c8-4b5d-bf63-e2df0e61ddb3,U02V90BSU1Y,,,Yes.,1643032759.360800,1643032783.360900,U0290EYCA7Q\\nc0a548bb-606c-4633-ad8a-377e600a9ad0,U02V90BSU1Y,,,Then I need to push the code to my github and share the url,1643032759.360800,1643032937.361200,U02V90BSU1Y\\n62d617a6-efdf-49ab-952e-824222ee758a,U02T9JQAX9N,,,Ok.. Thanks,1643031390.357000,1643033311.361500,U02T9JQAX9N\\n97676f0d-a485-48a8-aadb-ffeaa9de2b4c,U02V90BSU1Y,,,From my end i created a .sql file inside the week_1/docker_sql repo and store it inside.....hope it will be accessible by the grader,1643032759.360800,1643033390.361700,U02T0CYNNP2\\n5b002270-2f17-42f1-92b2-1242975cd041,U02AUCL9ZQF,,,Let me try right away,1643026101.339300,1643033406.361900,U02AUCL9ZQF\\n37fbdd1e-7a64-4cf7-82e9-8a389d778e90,U02V90BSU1Y,,,<@U01AXE0P5M3> <@U0290EYCA7Q>,1643032759.360800,1643033416.362100,U02T0CYNNP2\\n540f121c-9569-43d8-ab7a-ecc8a83a62e4,U02AUCL9ZQF,,,How do I ingest data,1643026101.339300,1643033551.362300,U02AUCL9ZQF\\n7cf03812-b5d9-4355-97ca-3eb38ee4ea48,U02AUCL9ZQF,,,Cos I\\'m still finding that the tables are empty,1643026101.339300,1643033569.362500,U02AUCL9ZQF\\n2AC09A95-48BB-4331-9C43-7FA829F13B53,U02DY0L6PHV,,,after doing all the recommended things several restarts did help and now it’s working ,1643022678.324100,1643033616.363700,U02DY0L6PHV\\nd8bd3879-d42d-43c3-b46b-26b3d2423151,U02AUCL9ZQF,,,There is a python script named ingest_data.py,1643026101.339300,1643034036.364000,U0290EYCA7Q\\n478130ac-b244-4904-9259-2944c5fad4b6,U02V90BSU1Y,,,it works,1643032759.360800,1643034503.364300,U01AXE0P5M3\\n4935d226-26f6-456e-9c81-aa0b19c8c835,,3.0,,\"Quite by accident I found <https://docs.google.com/presentation/d/1RkH-YhBz2apIjYZAxUz2Uks4Pt51-fVWVN9CcH9ckyY/edit#slide=id.g10c891aba4d_0_19|slides> in the repo for the 2nd week and one of them has the abbreviations `ETL` and `ELT` and the decoding of the letter `E` is represented as `export`. I\\'m not exactly sure, but `extract` seems to be more correct. Sorry for the tedium :blush:\",1643035603.365100,1643035603.365100,U02ULQFCXL0\\naa58e049-2a6b-4932-b583-19e0db886524,,1.0,,\"I keep getting the below error when I run \\'terraform apply\\'  how can I resolved please?\\n\\ngoogle_bigquery_dataset.dataset: Creating...\\ngoogle_storage_bucket.data-lake-bucket: Creating...\\ngoogle_storage_bucket.data-lake-bucket: Still creating... [10s elapsed]\\ngoogle_bigquery_dataset.dataset: Still creating... [10s elapsed]\\ngoogle_bigquery_dataset.dataset: Still creating... [20s elapsed]\\ngoogle_storage_bucket.data-lake-bucket: Still creating... [20s elapsed]\\ngoogle_storage_bucket.data-lake-bucket: Still creating... [30s elapsed]\\ngoogle_storage_bucket.data-lake-bucket: Still creating... [40s elapsed]\\n╷\\n│ Error: Post \"\"<https://storage.googleapis.com/storage/v1/b?alt=json&amp;prettyPrint=false&amp;project=ferrous-marking-339119>\"\": Post \"\"<https://oauth2.googleapis.com/token>\"\": dial tcp 216.58.223.234:443: connectex: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond.\\n│\",1643036355.366500,1643036355.366500,U02V90BSU1Y\\nA187BCB6-D721-48AF-ACB8-D27EC726EBE5,U02V90BSU1Y,,,\"I don\\'t think we have to provide the code, just the solutions. At least that was my understanding \\n\\nEdit: Sorry ignore me\",1643032759.360800,1643036541.367600,U02U34YJ8C8\\nb571164a-d55f-45b6-ae8f-349e740a6245,,1.0,,\"Hi, I\\'m a bit overwhelmed right now with all the new concepts that we\\'re seeing. I have experience with python as a data scientist but we\\'re covering new things and programs that I\\'m not used to. Is there any \"\"requirements list\"\" to review before starting the course? Thank you.\",1643036907.372300,1643036907.372300,U02UX01PGCC\\n12a9758e-d061-45e2-999d-c2eb31fba065,,19.0,,\"Hi :wave: I’m trying to run `terraform apply` in `data-engineering-zoomcamp/week_1_basics_n_setup/1_terraform_gcp/terraform` folder. However, I keep getting a permissions issue.\\n\\n```$ terrform apply\\n\\n...\\n\\ngoogle_storage_bucket.data-lake-bucket: Creating...\\ngoogle_bigquery_dataset.dataset: Creating...\\n╷\\n│ Error: googleapi: Error 403: <mailto:dtc-de-user@data-talks-club-de-course.iam.gserviceaccount.com|dtc-de-user@data-talks-club-de-course.iam.gserviceaccount.com> does not have storage.buckets.create access to the Google Cloud project., forbidden\\n│\\n│   with google_storage_bucket.data-lake-bucket,\\n│   on <http://main.tf|main.tf> line 19, in resource \"\"google_storage_bucket\"\" \"\"data-lake-bucket\"\":\\n│   19: resource \"\"google_storage_bucket\"\" \"\"data-lake-bucket\"\" {\\n│\\n╵\\n╷\\n│ Error: Error creating Dataset: googleapi: Error 403: Access Denied: Project disco-portal-275314: User does not have bigquery.datasets.create permission in project disco-portal-275314., accessDenied\\n│\\n│   with google_bigquery_dataset.dataset,\\n│   on <http://main.tf|main.tf> line 45, in resource \"\"google_bigquery_dataset\"\" \"\"dataset\"\":\\n│   45: resource \"\"google_bigquery_dataset\"\" \"\"dataset\"\" {\\n│\\n╵```\\nI don’t know why this would happen, as I’ve set the IAM permission on my service account to have `editor` access.\",1643036945.372700,1643036945.372700,U02TEERF0DA\\ncd01670b-f5b2-4bc5-a93b-c5f1214d061d,U02TEERF0DA,,,Here are the permissions,1643036945.372700,1643037008.373000,U02TEERF0DA\\n7a1161cb-5b14-488b-98ad-44539462a4b2,U02TEERF0DA,,,Any suggestions on why this would be the case?,1643036945.372700,1643037016.373400,U02TEERF0DA\\n180b2e95-21b7-4f4e-aca7-9a279e085db1,U02TEERF0DA,,,did you enable the access APIS?,1643036945.372700,1643037068.373600,U02S6KXPH8W\\nf04a9280-4a28-454a-98ff-6b4434ee267c,U02V90BSU1Y,,,Yes just sql is enough,1643032759.360800,1643037080.373800,U01AXE0P5M3\\nb06fd035-446b-493d-801f-c1c2e94ccd26,U02TEERF0DA,,,I believe I did … let me double check that,1643036945.372700,1643037139.374500,U02TEERF0DA\\nA507C5AB-33A2-4B18-9335-11970E2A91EE,U02TB5NK4FL,,,\"<@U02TB5NK4FL> don\\'t worry, removing containers doesn\\'t mean you need to start from scratch. In fact it\\'s recommended to remove all containers once finished with them. Did you figure it out?\",1642975558.217600,1643037178.375900,U02U34YJ8C8\\n5a04eeb1-6051-466f-87f5-3fcae85a13ff,U02TEERF0DA,,,\"Yup, I enabled the two APIs here:\\n\\n* <https://console.cloud.google.com/apis/library/iam.googleapis.com>\\n* <https://console.cloud.google.com/apis/library/iamcredentials.googleapis.com>\",1643036945.372700,1643037283.376100,U02TEERF0DA\\n93898f1e-9ad7-444c-959a-d2c0c5426186,U02TEERF0DA,,,\"mine issue was i added a space when i copied the project id ,so you might want to check that.\",1643036945.372700,1643037571.376400,U02S6KXPH8W\\n8f80c13f-db95-45b1-a356-35f5277dadf8,,1.0,,Please what is the # hashtag for public learning?,1643037730.377400,1643037730.377400,U02V90BSU1Y\\n0be4ebed-c5b4-4a6a-99aa-e2720d340ed5,U02TEERF0DA,,,\"(data-engineering-zoomcamp) % gcloud iam service-accounts list\\nDISPLAY NAME  EMAIL                                                     DISABLED\\ndtc-de-user   ***************.<http://iam.gserviceaccount.com|iam.gserviceaccount.com>  False\",1643036945.372700,1643037984.378000,U0290EYCA7Q\\n8f424bb0-567b-4aa1-bfad-9ef35f2303f0,U02V90BSU1Y,,,#dezoomcamp,1643037730.377400,1643038032.378500,U02BVP1QTQF\\n5f3bc8d9-bb62-4c48-906a-ec5584e851cf,,6.0,,Will there be a live class today?,1643038033.378700,1643038033.378700,U02QKMCV39R\\n14af4d00-fc6c-48fe-a05f-1b36e074b9a0,U02TEERF0DA,,,Do You see these permissions?,1643036945.372700,1643038107.379300,U0290EYCA7Q\\nfd582296-4af8-45bb-ae22-2a9eea427dbf,U02QKMCV39R,,,\"Yes, we\\'ll have office hours and answer your questions\",1643038033.378700,1643038120.379700,U01AXE0P5M3\\n676a3f9e-caaa-4a8f-a68c-e092943e069e,U02QKMCV39R,,,When is that happening? <@U01AXE0P5M3>,1643038033.378700,1643038266.379900,U02QMF5NCRY\\n4dbe4f15-0efd-481e-b309-e3f0553a5440,U02V90BSU1Y,,,Maybe try running it later - or from gcp VM,1643036355.366500,1643038267.380100,U01AXE0P5M3\\n4e38d28e-2532-496e-a98c-5e441b59d455,U02QKMCV39R,,,17:00 CET,1643038033.378700,1643038283.380300,U01AXE0P5M3\\n19cee623-d7e4-4e30-8528-8bc4f6b1fa01,U02QKMCV39R,,,There is a zoom link? Or it will be live on youtube?,1643038033.378700,1643038573.380600,U0308MF3KUH\\nf742e590-cc49-406c-a6c7-a0c9b6ac2127,U02C5MT2HQC,,,try with double // -- //d/data_engineering/week_1_basics/docker_sql/ny_taxi_postgres_data,1643025087.334200,1643038731.380900,U02KZNLAEAW\\n691dcebb-2940-4f58-998a-d00a67ae7fe2,U02TEERF0DA,,,Yeah I have those permissions,1643036945.372700,1643039029.381400,U02TEERF0DA\\nc0a28a3d-6b3e-4d48-8c05-2b45db08e497,U02TEERF0DA,,,And I just checked again that my credential file was linked in `<http://main.tf|main.tf>`,1643036945.372700,1643039076.381600,U02TEERF0DA\\ncd8c0c27-3c23-436e-8735-8ba400cacfab,U02QKMCV39R,,,<@U0308MF3KUH> it will be live on YouTube,1643038033.378700,1643039101.381800,U02QL1EG0LV\\nafdfc0f1-d3ae-4069-9cd4-23710f0fd382,U02TEERF0DA,,,still not working ?,1643036945.372700,1643039254.382300,U02S6KXPH8W\\n7244bdb8-98af-4e19-8457-041d6d6df9db,U02RHT0M3M5,,,\"Thank you Mausam,that is true  it worked well after doing like what you say:ok_hand:\",1642970528.200900,1643039285.382600,U02RHT0M3M5\\ncab2c71d-1fce-4a26-a97f-e662308dfd7c,U02TEERF0DA,,,\"So I ended up just deleting all the terraform files in the folder, and running `terraform init .. plan … apply` again\",1643036945.372700,1643039309.382800,U02TEERF0DA\\nd77e80db-d962-4cc8-a150-c5bee7079500,U02TEERF0DA,,,And it worked,1643036945.372700,1643039311.383000,U02TEERF0DA\\n734444dd-f9c5-4f7a-9837-33650c84b352,U02TEERF0DA,,,So … clearly I did something wrong on my end :disappointed: I just don’t know what,1643036945.372700,1643039330.383200,U02TEERF0DA\\n6ae366c0-be36-4ff1-97e4-1f675321cac1,U02TEERF0DA,,,Thanks for the help <@U02S6KXPH8W> :smile:,1643036945.372700,1643039351.383400,U02TEERF0DA\\nd77f8bc1-9ba1-4c9c-8819-c1ed23c003dd,U02TEERF0DA,,,\"no problem. destroy the infrastructure and create it again ,experiment with it . its always some minor thing that messes with us :grinning:\",1643036945.372700,1643039475.383700,U02S6KXPH8W\\n3b030e94-bdd0-4962-a536-ea22d7d2bd61,,1.0,,Where we can upload our homework?,1643039678.384700,1643039678.384700,U02U14G4W0L\\n7a2f27d8-7b2c-4f84-9811-3acca4f6ed0f,U02TEERF0DA,,,\"Just a quick question `terraform destroy` destroys everything on the sever side, but keeps the config files intact locally … right?\",1643036945.372700,1643039747.384800,U02TEERF0DA\\n22b306bb-6c78-4135-8615-115722a2a24a,U02U14G4W0L,,,\"• Form for submitting:\\xa0<https://forms.gle/yGQrkgRdVbiFs8Vd7>\\n• You can submit your homework multiple times. In this case, only the last submission will be used.\\nDeadline: 26 January (Wednesday), 22:00 CET\",1643039678.384700,1643039816.385400,U02RCQJNRCZ\\nae3a62d1-78df-43a5-9f17-d2689a3e27a4,U02QKMCV39R,,,link please for live,1643038033.378700,1643039910.385700,U02V4UTSU8M\\na190d226-e94a-4d31-a97c-8f934175de20,,,,please what’s the link for today’s session?,,1643039938.386300,U02T91CL14Y\\ne2045d76-542f-4178-ae9c-a90e66f96b82,,,,\"<https://app.sli.do/event/hQEdVHXKXTKVDUDHw6iNHM>\\n\\n<https://www.youtube.com/watch?v=Vpt1BRWT1c0>\",,1643039957.386600,U01AXE0P5M3\\nF9DE4133-8A2B-4054-A33C-3E0EE954BD82,,,,\"For github, whoever has diffucilties can download a desktop wizard, really really helpful and easy with user-friendly gui\",,1643041622.391500,U02U07906Q0\\n4e873975-7979-4971-a6f3-c6ba43a4b153,U02TEERF0DA,,,\"<@U02TEERF0DA> yes, <https://www.terraform.io/cli/commands/destroy>\",1643036945.372700,1643041660.391800,U02T675JL2Z\\naa4ca809-0e1c-44a3-af18-6997448046ce,,,,\"This is my <https://medium.com/@ahmetekiz/git-and-github-beginner-tutorial-multiple-accounts-23da033d816a|Git and GitHub Beginner Tutorial — Multiple Accounts>. For advanced commands, you check the resources in my tutorial. I hope it helps. :slightly_smiling_face:\",,1643041785.393800,U02S82E4N4S\\n992558c0-9f36-4082-96cb-ecb3e86ee332,,,,\"This is an interactive git command explorer. I hope it helps! :smile:\\n<https://gitexplorer.com/>\",,1643042543.395200,U02URV3EPA7\\n833e4a08-ca4c-42b2-8125-267643da019f,U01AXE0P5M3,,,Thanks!,1642511128.453300,1643043417.396200,U02QZN0LSBT\\nB0524EA9-1E61-4273-A999-8D0747B350F5,U02U9G1P76X,,,\"hi, did you resolve this somehow? i’m facing the same error\",1642928489.050200,1643043683.397100,U02DY0L6PHV\\n51d09536-b937-44d9-8b52-f95bd730ccc1,U02ULQFCXL0,,,<@U01DHB2HS3X> <@U01AXE0P5M3> <@U01DFQ82AK1>,1643035603.365100,1643043831.397300,U02ULQFCXL0\\nae9de5a3-ab22-4314-84a6-3e2e3065cbe0,U02ULQFCXL0,,,My bad,1643035603.365100,1643043898.397700,U01DFQ82AK1\\nd45f8679-3123-4974-aeed-9275c6427cbd,U02ULQFCXL0,,,\"Fixed in slides, I think i cannot fix that in video\",1643035603.365100,1643043964.398000,U01DFQ82AK1\\n2b20c560-9d88-4867-af95-86a88206c7aa,,,,\"Hello All,\\nengine = create_engine(\\'<postgresql://postgres:root@localhost:5432/ny_taxi>\\')\\nengine.connect()\",,1643044181.401000,U02VBG59VQ9\\n0a38b9df-b9e6-4017-9787-6c9fb2ff39ac,,2.0,,\"Hi!\\nI\\'ve got a very beginner question: how could i specify in which directory I want to work in Ubuntu?\\nI tried cd/ command but somehow it doesn\\'t work for me.\\nWhat is the problem?\\n cd: C:/Users/Aurora/PycharmProjects/SemanticScholat: No such file or directory\",1643044240.402200,1643044240.402200,U02R6JB9LKA\\na21db1fb-0230-44cc-97f3-000051f741ff,,27.0,,\"and the error i got is\\n```~\\\\anaconda3\\\\lib\\\\site-packages\\\\psycopg2\\\\__init__.py in connect(dsn, connection_factory, cursor_factory, **kwargs)\\n    120 \\n    121     dsn = _ext.make_dsn(dsn, **kwargs)\\n--&gt; 122     conn = _connect(dsn, connection_factory=connection_factory, **kwasync)\\n    123     if cursor_factory is not None:\\n    124         conn.cursor_factory = cursor_factory\\n\\nOperationalError: (psycopg2.OperationalError) connection to server at \"\"localhost\"\" (::1), port 5432 failed: FATAL:  database \"\"ny_taxi\"\" does not exist\\n\\n(Background on this error at: <http://sqlalche.me/e/e3q8>)```\\n\",1643044246.402900,1643044246.402900,U02VBG59VQ9\\n3d65f865-f2f4-4bfa-8b7e-7bdbaa76d3ba,,1.0,,\"Someone asked about version control for database -  <https://liquibase.org/> is the best tool for that, and it is an open source product.\",1643044288.403300,1643044288.403300,U0290EYCA7Q\\nd39a3857-02cc-4156-93d5-4538b510d733,U02R6JB9LKA,,,You need to go to /mnt/c if you need to access your files from windows,1643044240.402200,1643044407.405800,U01AXE0P5M3\\n9df2df83-5a24-4156-84fd-7f7338fbe6a1,U02VBG59VQ9,,,Is it running?,1643044246.402900,1643044433.406200,U01AXE0P5M3\\n5ddecaf0-c5cc-4feb-beaf-8dca08c8f44a,U02VBG59VQ9,,,<@U01AXE0P5M3> what is?,1643044246.402900,1643044600.407200,U02VBG59VQ9\\ne4793f90-310b-4ed9-bfdd-67525fa168cd,U02R6JB9LKA,,,\"Omg, now it works properly. Thanks!!\",1643044240.402200,1643044669.408800,U02R6JB9LKA\\nefa0dea6-dca3-4530-8726-b56ef2203b73,,2.0,,\"The videos `DE Zoomcamp 2.4.4 - Moving Files from AWS to GPC with Transfer Service` and `DE Zoomcamp 2.4.4 - Configuring the Transfer Service with Terraform` are using the same numbering (`2.4.4`), is this intentional?\",1643044737.409300,1643044737.409300,U02TGS5B4R1\\nc3367d13-2409-463f-a2a2-4cebdca3b705,U02VBG59VQ9,,,what does `docker ps` command show?,1643044246.402900,1643044863.409500,U0290EYCA7Q\\n2fc4f901-a2a9-42c1-93cd-7bb771b93f13,U02VBG59VQ9,,,<@U0290EYCA7Q> where?,1643044246.402900,1643044895.409700,U02VBG59VQ9\\ndc70e788-6408-4fcf-aafc-b6733cc3d2fe,U02VBG59VQ9,,,\"<@U0290EYCA7Q>\\n$ winpty docker ps\\nCONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\",1643044246.402900,1643044944.410000,U02VBG59VQ9\\n9555817f-1e55-45f0-8000-9cfdf31b1742,,3.0,,\"Hello all! I just heard about this course and just signed up for it. I know I am a week late, can I still join? If so, where do I start and how can I catch up? Thanks!\",1643044964.410600,1643044964.410600,U02V5EP18PQ\\n6581e4cb-9623-4975-ac95-10df055bb7a7,U02TGS5B4R1,,,\"No, it\\'s a mistake\",1643044737.409300,1643045153.410800,U01AXE0P5M3\\n4659e3c0-ac42-4173-80a0-06b3cc0564cf,U02TGS5B4R1,,,Thanks for letting us know!,1643044737.409300,1643045164.411000,U01AXE0P5M3\\n9c48d84e-4d2a-4a90-b1d2-ac16a16012e4,U02V5EP18PQ,,,You can! Start with the course github page,1643044964.410600,1643045189.411600,U01AXE0P5M3\\ncf5aeb42-e11c-4720-836d-828e67e09b07,U02V5EP18PQ,,,Thanks! and <https://www.youtube.com/c/DataTalksClub/videos> here? I just realized we can start watching courses too!,1643044964.410600,1643045251.412900,U02V5EP18PQ\\n20730f1f-fef8-4aad-9163-12b51ae8c822,,17.0,,\"Hello all,\\n\\nI\\'m a bit not sure, how to add-in the zones table in docker. I know whenever I run this:\\n\\n`docker run -it \\\\`\\n  `--network=week1_default \\\\`\\n  `taxi_ingest:v001 \\\\`\\n    `--user=root \\\\`\\n    `--password=root \\\\`\\n    `--host=pgdatabase \\\\`\\n    `--port=5432 \\\\`\\n    `--db=ny_taxi \\\\`\\n    `--table_name=zones \\\\`\\n    `--url=${URL}`\\n\\ndocker runs the ingest_data.py file, I wrote earlier. It\\'s not clear to me, why since I never specified that in the syntax above (this might be dumb but I do not know:face_with_head_bandage:). Would I need to write a new \"\"ingest_data.py\"\" for the zones table or what\\'s the best approach?\",1643045354.415900,1643045354.415900,U02RSAE2M4P\\n6f2ac0f2-bb1b-419d-a2b6-3c3dece8a763,,5.0,,\"Hi! I\\'m having trouble installing Docker on my computer, it\\'s too old and can\\'t run it.. is there any other way i can do the exercises without installing it directly?\",1643045400.416400,1643045400.416400,U02Q51Y4MM5\\nc368e01e-c59d-4d84-a007-3d58dc5bd2f0,U02VBG59VQ9,,,That\\'s the issue. No Docker container is running.,1643044246.402900,1643045537.416600,U0290EYCA7Q\\nc07e4b42-a83f-452d-8943-ef0a1b966bdb,U02RSAE2M4P,,,\"This is what I did. I added two url arguments, for trips and zones.\",1643045354.415900,1643045851.416900,U0290EYCA7Q\\n2e35be76-abeb-45ae-b370-a008842d7e80,U02VBG59VQ9,,,\"<@U0290EYCA7Q>\\nData page checksums are disabled.\\n\\nfixing permissions on existing directory /var/lib/postgresql/data ... ok\\ncreating subdirectories ... ok\\nselecting dynamic shared memory implementation ... posix\\nselecting default max_connections ... 20\\nselecting default shared_buffers ... 400kB\\nselecting default time zone ... Etc/UTC\\ncreating configuration files ... ok\\nrunning bootstrap script ... 2022-01-24 17:37:05.746 UTC [82] FATAL:  data direc\\ntory \"\"/var/lib/postgresql/data\"\" has invalid permissions\\n2022-01-24 17:37:05.746 UTC [82] DETAIL:  Permissions should be u=rwx (0700) or\\nu=rwx,g=rx (0750).\\nchild process exited with exit code 1\\ninitdb: removing contents of data directory \"\"/var/lib/postgresql/data\"\"\",1643044246.402900,1643045905.417100,U02VBG59VQ9\\n2FBF8E45-75CD-400F-B483-E361A8B5A826,U02VBG59VQ9,,,The postgre db docker is not running. Run it and then try again. ,1643044246.402900,1643046109.418400,U02QK4ZV4UX\\n7d5fc906-1cf9-430f-a7e1-bbf07ce37ea5,U02Q51Y4MM5,,,\"Hi <@U02Q51Y4MM5>, old computers normally can run Linux without much trouble, albeit a little slower sometimes. But they run, specially if you use a lightweight distribution (lubuntu, for example). I completed a web dev bootcamp with such a computer.\",1643045400.416400,1643046139.418600,U02GVGA5F9Q\\n9c89f0e9-fcf3-46ff-83ba-7aa78f1515fc,U02VBG59VQ9,,,I would suggest you to try docker compose. It worked for me.,1643044246.402900,1643046156.418800,U0290EYCA7Q\\nc0f90eec-79c3-4390-b8a1-7f7e5cc58d33,U02VBG59VQ9,,,<@U02QK4ZV4UX> how do i run it?,1643044246.402900,1643046156.419000,U02VBG59VQ9\\n4a93ca66-5e4e-4e03-b73a-2eba510f224c,U02VBG59VQ9,,,\"Go thru this thread. Either try docker compose, or try this -<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642939459070600>\",1643044246.402900,1643046226.419200,U0290EYCA7Q\\n8FAE1B7B-0A4D-4FC4-8FF1-06E5276EAAB7,U02VBG59VQ9,,,Are you following this <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/2_docker_sql/README.md|https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/2_docker_sql/README.md>,1643044246.402900,1643046291.420200,U02QK4ZV4UX\\n47742cd7-0711-46a8-b6a1-b04dadde167c,U02V5EP18PQ,,,We have a lot more in the channel than just the course. You\\'ll need to go to the playlists and select the course one. The github repository contains all the information,1643044964.410600,1643046608.420500,U01AXE0P5M3\\n84f7b8a6-a87e-49a9-ab0b-e33f10d4885c,,2.0,,\"1. When to use docker and when to use virtual environment like conda or virtualenv? is there a rule of thumb here ?\\n2. Alexey used a docker for pgAdmin instead of downloading it ...When to download and install and when to go for the docker container ( if available) ?\",1643046858.423200,1643046858.423200,U02TWFZURD1\\nb462d3c4-f9c1-4148-8bf9-0a99db01d2c8,U02Q51Y4MM5,,,\"<@U02Q51Y4MM5>, you don\\'t have to install anything on your computer. They created a video on how to set up an instance in GCP that you can do all the work for the course on -- your computer just needs internet and a browser. Video is <https://youtu.be/ae-CV2KfoN0|here>.\",1643045400.416400,1643046864.423300,U02U55REW1K\\n115cee8a-e723-4a63-b189-2ffe41ad2f1e,U02Q51Y4MM5,,,\"Maybe <@U01AXE0P5M3> can clarify if the $300 credit for a new GPC account is enough for this VM and all the requirements of this course. It\\'s definitively the best solution, but kind of temporary, if not costly. You have to migrate all the relevant contents to your computer before dismissing it.\",1643045400.416400,1643047104.423700,U02GVGA5F9Q\\nebb098a6-738f-43aa-9d26-ba069158e489,,2.0,,Few things I want to know guys ...I am a little bit late to this course .. today I came to know about it ..as I understood week 1 already completed ...so can u pls tell whether all course from week 1 is available in YouTube ..and when week 2 will start,1643047187.426400,1643047187.426400,U02VCCJ394K\\na2601ed5-8a63-4ba1-ba65-519649ff1640,U02TWFZURD1,,,\"A\\xa0virtual environment\\xa0is a tool for dependency management and project isolation. Some project might require different version of python. For that, we can create different environment using anaconda or virtual env. This is mostly for development.\\n\\nOnce we get it working in our local, we can ship our packages/project to production with the same version used in virtual environment using docker image.\",1643046858.423200,1643047396.426700,U0290EYCA7Q\\nc7623683-0566-480f-bfe8-fc881b5c4578,U02VCCJ394K,,,<https://github.com/DataTalksClub/data-engineering-zoomcamp>,1643047187.426400,1643047427.426900,U0290EYCA7Q\\n71af92bd-5494-48f3-a94d-9006607f6f1a,U02VCCJ394K,,,\"HW for week 1 is still due this Wednesday 22:00 CET.\\nweek2 started today and some videos are uploaded already. all videos can be found on the DE playlist <https://www.youtube.com/watch?v=bkJZDmreIpA&amp;list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb>\",1643047187.426400,1643047483.427400,U02TWFZURD1\\n3aa8715c-e650-4e77-996e-f67d46860ceb,U02VBG59VQ9,,,\"<@U0290EYCA7Q>\\n$ docker-compose -f postgres.yaml up\\nCreateFile C:\\\\Program Files\\\\Git\\\\postgres.yaml: The system cannot find the file specified.\",1643044246.402900,1643047640.428000,U02VBG59VQ9\\na3678b2d-cd47-4516-a88c-ce3cc66287ff,U02RSAE2M4P,,,\"I know this has to do with the ENTRYPOINT we specified with the dockerfile we wrote, but I don\\'t understand how.\",1643045354.415900,1643047664.428200,U02RSAE2M4P\\n959c64ae-5860-4a19-97a4-4e067c002394,U02RSAE2M4P,,,\"This is what I did. This might help you to get an idea.\\n\\n```#!/usr/bin/env python\\n# coding: utf-8\\n\\nimport os\\nimport sys\\nimport argparse\\n\\nfrom time import time\\n\\nimport pandas as pd\\nfrom sqlalchemy import create_engine\\n\\ndef download_csv(url, filename):\\n    try:\\n        print(f\\'Downloading {filename}, if not exist\\')\\n        if os.path.isfile(filename):\\n            print (f\"\"{filename} already exist\"\")\\n        else:\\n            print (\"\"Downloading ....\"\")\\n            os.system(f\"\"curl  {url} -# --output {filename}\"\")\\n            print (f\"\"Downloaded {filename}\"\")\\n        \\n        print(\"\"\\\\n----Printing the first 10 lines of {filename}----\"\")\\n        os.system(f\"\"head -10 {filename}\"\")\\n        print(\"\"-------------------------------------------------\"\")\\n    except:\\n        print(\"\"Oops!\"\", sys.exc_info()[0], \"\"occurred.\"\")\\n\\ndef load_data(csv_name,user,password,host,port,db,table_name):\\n    print(f\"\"Loading {csv_name} into table {table_name} - postgresql://{user}:{password}@{host}:{port}/{db}\"\")\\n    engine = create_engine(f\\'postgresql://{user}:{password}@{host}:{port}/{db}\\')\\n    numOfLines = int(os.popen(f\\'wc -l &lt; {csv_name}\\').read()[:-1])\\n    if numOfLines &gt; 100000:\\n        try: \\n            df_iter = pd.read_csv(csv_name, iterator=True, chunksize=100000)\\n            df = next(df_iter)\\n            df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\\n            df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\\n            df.head(n=0).to_sql(name=table_name, con=engine, if_exists=\\'replace\\')\\n            df.to_sql(name=table_name, con=engine, if_exists=\\'append\\')\\n            while True: \\n                t_start = time()\\n                df = next(df_iter)\\n                df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\\n                df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\\n                df.to_sql(name=table_name, con=engine, if_exists=\\'append\\')\\n                t_end = time()\\n                print(\\'inserted another chunk, took %.3f second\\' % (t_end - t_start))\\n        except StopIteration as err:\\n                print(\\'Batch Processing is Completed!!\\')\\n    else:\\n        try: \\n            df =pd.read_csv(csv_name)\\n            df.head(n=0).to_sql(name=table_name, con=engine, if_exists=\\'replace\\')\\n            df.to_sql(name=table_name, con=engine, if_exists=\\'append\\')\\n            print(\\'Finished inserting zones to database\\')\\n        except:\\n            print(\"\"Oops!\"\", sys.exc_info()[0], \"\"occurred.\"\")\\n\\ndef main(params):\\n    user = params.user\\n    password = params.password\\n    host = params.host \\n    port = params.port \\n    db = params.db\\n    trip_url = params.trip_url\\n    zone_url = params.zone_url\\n    try:\\n        download_csv(trip_url,\\'trips.csv\\')\\n        load_data(\\'trips.csv\\',user,password,host,port,db,\\'trips\\')\\n        download_csv(zone_url,\\'zones.csv\\')\\n        load_data(\\'zones.csv\\',user,password,host,port,db,\\'zones\\')\\n    except:\\n        print(\"\"Oops!\"\", sys.exc_info()[0], \"\"occurred.\"\")\\n\\nif __name__ == \\'__main__\\':\\n    parser = argparse.ArgumentParser(description=\\'Ingest CSV data to Postgres\\')\\n\\n    parser.add_argument(\\'--user\\', help=\\'user name for postgres\\')\\n    parser.add_argument(\\'--password\\', help=\\'password for postgres\\')\\n    parser.add_argument(\\'--host\\', help=\\'host for postgres\\')\\n    parser.add_argument(\\'--port\\', help=\\'port for postgres\\')\\n    parser.add_argument(\\'--db\\', help=\\'database name for postgres\\')\\n    parser.add_argument(\\'--trip_url\\', help=\\'trip url of the csv file\\')\\n    parser.add_argument(\\'--zone_url\\', help=\\'zone url of the csv file\\')\\n    args = parser.parse_args()\\n\\n    main(args)```\\nTo run it, after creating the image.\\n\\n```TRIP_URL=\"\"<https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>\"\"\\nZONE_URL=\"\"<https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv>\"\"\\n\\ndocker run -it \\\\\\n  --network=pg-network \\\\\\n  taxi_ingest:v001 \\\\\\n    --user=root \\\\\\n    --password=root \\\\\\n    --host=pg-database \\\\\\n    --port=5432 \\\\\\n    --db=ny_taxi \\\\\\n    --trip_url=${TRIP_URL} \\\\\\n    --zone_url=${ZONE_URL}```\",1643045354.415900,1643048381.428600,U0290EYCA7Q\\n3d965fab-effb-4b78-9600-9cfe14308a2d,U02RSAE2M4P,,,\"Hi Samson, in Docker File by defining entry point, you mean that when \\'docker run your image\\' the entry point task runs. In this case python run ingest-data.py file. If you want to create new image with a modified ingest-data.py (in order to feed zone.csv to database as well), you need re-create image with docker built command after modifying the ingest-data.py script. Then docker run this new image with the relevant parameters.\",1643045354.415900,1643048436.428800,U02QTG6A71U\\n61b5a1f9-dbc8-42d1-bc8a-966fa3272650,U02VBG59VQ9,,,No -  Watch Alexey\\'s video.,1643044246.402900,1643048607.429100,U0290EYCA7Q\\n70f15ca7-881d-4ae9-96a8-44bc3bdbf569,U02VBG59VQ9,,,<https://www.youtube.com/watch?v=hKI6PkPhpa0>,1643044246.402900,1643048656.429400,U0290EYCA7Q\\nef51b051-b790-430e-b8a3-765fd24f9e43,,19.0,,\"hi there anyone having this issue\\n``` pgcli -h localhost -p 5432 -U root -d ny_taxi\\nTraceback (most recent call last):\\n  File \"\"/opt/anaconda3/bin/pgcli\"\", line 8, in &lt;module&gt;\\n    sys.exit(cli())\\n  File \"\"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\"\", line 1128, in __call__\\n    return self.main(*args, **kwargs)\\n  File \"\"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\"\", line 1053, in main\\n    rv = self.invoke(ctx)\\n  File \"\"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\"\", line 1395, in invoke\\n    return ctx.invoke(self.callback, **ctx.params)\\n  File \"\"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\"\", line 754, in invoke\\n    return __callback(*args, **kwargs)\\n  File \"\"/opt/anaconda3/lib/python3.9/site-packages/pgcli/main.py\"\", line 880, in cli\\n    os.makedirs(config_dir)\\n  File \"\"/opt/anaconda3/lib/python3.9/os.py\"\", line 225, in makedirs\\n    mkdir(name, mode)\\nPermissionError: [Errno 13] Permission denied: \\'/Users/vray/.config/pgcli\\'```\",1643048920.431600,1643048920.431600,U0297MFTTM1\\n096c1813-86a6-4527-8629-a96ec5dee94e,U0297MFTTM1,,,Using PGadmin on my local mac works fine,1643048920.431600,1643049006.431800,U0297MFTTM1\\nF748BAB7-F9D5-4021-BC7C-FC3D0FB99B50,U02RSAE2M4P,,,\"<@U02QTG6A71U> To clarify, if we update the ingestion script, do we have to recreate the image, or can we just create a new container based on the existing image?\",1643045354.415900,1643049240.433700,U02U34YJ8C8\\nb0df42ee-78df-4883-8425-2a96f0610893,U0297MFTTM1,,,did you install pgcli with sudo?,1643048920.431600,1643049245.433900,U01AXE0P5M3\\n20c48dcf-f92a-408c-b738-70ed2d43a73b,U0297MFTTM1,,,yep,1643048920.431600,1643049275.434100,U0297MFTTM1\\nf166e285-96c0-4c49-8a68-089e410d892f,U02Q51Y4MM5,,,\"should be enough, it costs like 10 cents per hour or 15, I don\\'t remember. You won\\'t need to keep the VM running all the time\",1643045400.416400,1643049301.434300,U01AXE0P5M3\\n4f653b3b-02dd-4c9f-b028-169fbc3c4ba2,U0297MFTTM1,,,\"try one more time, but without sudo. I\\'d also recommend conda or anaconda to make sure you don\\'t interfere with system python\",1643048920.431600,1643049339.434500,U01AXE0P5M3\\n5c8ec344-d8b9-4c5f-80d2-2188a7ecc97d,U02Q51Y4MM5,,,Great! thank you all for your help :),1643045400.416400,1643049370.434700,U02Q51Y4MM5\\n56e84c87-53fa-4730-93ea-962d39da1d7c,U0297MFTTM1,,,ok,1643048920.431600,1643049411.435000,U0297MFTTM1\\n23db5b2c-47b7-47e9-b1fc-b7f8fe8d393c,U0297MFTTM1,,,\"I didi sudo pgcli .... adn then it promts me for password I enter root as per the env variables from docker run and seems like I was able to enter in the container but don´t know if this is ok  I got this\\n```Version: 1.9.0\\nChat: <https://gitter.im/dbcli/pgcli>\\nMail: <https://groups.google.com/forum/#!forum/pgcli>\\nHome: <http://pgcli.com>\\nroot@localhost:ny_taxi&gt; Exception in thread completion_refresh:\\nTraceback (most recent call last):\\n  File \"\"/opt/anaconda3/lib/python3.9/threading.py\"\", line 973, in _bootstrap_inner\\n    self.run()\\n  File \"\"/opt/anaconda3/lib/python3.9/threading.py\"\", line 910, in run\\n    self._target(*self._args, **self._kwargs)\\n  File \"\"/opt/anaconda3/lib/python3.9/site-packages/pgcli/completion_refresher.py\"\", line 68, in _bg_refresh\\n    refresher(completer, executor)\\n  File \"\"/opt/anaconda3/lib/python3.9/site-packages/pgcli/completion_refresher.py\"\", line 110, in refresh_tables\\n    completer.extend_columns(executor.table_columns(), kind=\\'tables\\')\\n  File \"\"/opt/anaconda3/lib/python3.9/site-packages/pgcli/pgcompleter.py\"\", line 204, in extend_columns\\n    for schema, relname, colname, datatype, has_default, default in column_data:\\n  File \"\"/opt/anaconda3/lib/python3.9/site-packages/pgcli/pgexecute.py\"\", line 483, in table_columns\\n    for row in self._columns(kinds=[\\'r\\']):\\n  File \"\"/opt/anaconda3/lib/python3.9/site-packages/pgcli/pgexecute.py\"\", line 478, in _columns\\n    cur.execute(sql)\\npsycopg2.errors.UndefinedColumn: column def.adsrc does not exist\\nLINE 7:                         def.adsrc as default```\",1643048920.431600,1643049637.435600,U0297MFTTM1\\nd3598e15-0198-462a-99d5-feb3c82a94ff,U02VBG59VQ9,,,\"<@U0290EYCA7Q> watched the video and followed step by step and got this error\\n$ docker ps\\nCONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\\n\\nUser1@DESKTOP-PD6UM8A MINGW64 /\\n$ docker-compose up\\nno configuration file provided: not found\",1643044246.402900,1643049676.435800,U02VBG59VQ9\\n699b7b2f-65ec-4b67-992b-7ab559ae4ec8,U02VBG59VQ9,,,,1643044246.402900,1643049818.436100,U02VBG59VQ9\\nb0c3fa91-6d30-4a2b-aaaf-5d250162c8e7,U0297MFTTM1,,,<https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/2_docker_sql/README.md#cli-for-postgres>,1643048920.431600,1643049875.436500,U01AXE0P5M3\\n5df51160-88a9-4045-9d3f-f7b5704b342e,U0297MFTTM1,,,\"```(data-engineering-zoomcamp) % pgcli -v\\nVersion: 3.3.0```\\nThat version may not be compatible with postgres13.\",1643048920.431600,1643049952.436800,U0290EYCA7Q\\nb96869d6-e3a5-4ff0-8f61-431d77467f17,U0297MFTTM1,,,yep I got that version,1643048920.431600,1643050067.437600,U0297MFTTM1\\n1fa88db9-6942-4cb9-8bdd-9715325f49e6,U0297MFTTM1,,,:(,1643048920.431600,1643050070.437800,U0297MFTTM1\\n02637020-a78c-44f4-a821-012fbcff7683,U02TEERF0DA,,,Great. Thanks for all the help everyone,1643036945.372700,1643050107.438100,U02TEERF0DA\\nb14bbb0e-64f4-452b-a6f3-f0dad2bb4578,U02VBG59VQ9,,,\"Go to the folder where docker-compose.yml is, and run docker-compose up\",1643044246.402900,1643050126.438300,U0290EYCA7Q\\n8cb851a8-ad24-414e-981f-de4719e7a484,U02UKLHDWMQ,,,i discover there is compatible issues with pgcli and prompt-toolkit. my pgcli version is 1.9.0 and it is not compatible with prompt-toolkit 3.0.24. When i try to upgrade pgcli to the latest version it does not work rather it will downgrade my prompt-toolkit which affects my notebook kernel from connecting,1642975381.215900,1643050154.438600,U02UKLHDWMQ\\n203af860-36e2-4f42-ac8f-6f6cdd35d390,U0297MFTTM1,,,\"```$ pgcli -v\\nVersion: 3.3.0```\\nInteresting, it works for me - I guess it also depends on a system and/or version of psycopg2?\\n\\n```$ pip freeze | grep psycopg2\\npsycopg2==2.9.3```\",1643048920.431600,1643050275.438900,U01AXE0P5M3\\nb2afbe87-0492-4fac-ab89-bcde3352b766,U02VBG59VQ9,,,i am not following you <@U0290EYCA7Q>,1643044246.402900,1643050302.439100,U02VBG59VQ9\\n9771f62d-1dc4-439e-b7b9-4d62dabbb0bd,U02RSAE2M4P,,,\"Yes Aaron, because the running relevant container is just an instance of the \\'old\\' image (which is built according to the instructions on Docker File) , and it copied the old version of your ingest script into the image.\",1643045354.415900,1643050365.439300,U02QTG6A71U\\nc9352879-916c-49bc-9f1b-2104620fa6bb,U02VBG59VQ9,,,i don\\'t see run on the .yml file when i right click on it,1643044246.402900,1643050368.439500,U02VBG59VQ9\\ndad7dde3-5491-4a3f-a1c4-ea7b87995c5a,U02VBG59VQ9,,,What is your present working directory? `pwd` and where is docker-compose.yml?,1643044246.402900,1643050372.439700,U0290EYCA7Q\\n7331a1b0-db18-4567-8b0f-6e9d376620e0,U02VBG59VQ9,,,C:\\\\Users\\\\User1\\\\2_docker_sql,1643044246.402900,1643050394.439900,U02VBG59VQ9\\n62075c33-b61f-4f3d-a748-72f2550eb89f,U02VBG59VQ9,,,thank you for you patience btw,1643044246.402900,1643050437.440100,U02VBG59VQ9\\n736ec5da-cd55-4d19-8cae-3f8626149dce,U0297MFTTM1,,,\"And this is the version of?\\n\\n```Version: 1.9.0\\nChat: <https://gitter.im/dbcli/pgcli>\\nMail: <https://groups.google.com/forum/#!forum/pgcli>\\nHome: <http://pgcli.com>```\\nI am using 3.3.0 only\",1643048920.431600,1643050438.440300,U0290EYCA7Q\\n2008f265-9026-417d-8da7-6502ae64a9de,U0297MFTTM1,,,1.9.0 will throw these weird exceptions but will actually work,1643048920.431600,1643050505.440600,U01AXE0P5M3\\n595141eb-8efd-4236-be4d-97948e385d13,U02VBG59VQ9,,,can you run `dir`,1643044246.402900,1643050523.441000,U0290EYCA7Q\\n3d34cc46-354a-4e79-bd2d-5f7df7355610,U0297MFTTM1,,,(This is pgcli),1643048920.431600,1643050532.441200,U01AXE0P5M3\\ndb8fd412-bde9-4306-a278-1e3d194df42d,U02VBG59VQ9,,,\"$ dir\\nLICENSE.txt        cmd  git-bash.exe  postgres.yml  unins000.dat  usr\\nReleaseNotes.html  dev  git-cmd.exe   proc          unins000.exe\\nbin                etc  mingw64       tmp           unins000.msg\",1643044246.402900,1643050561.441400,U02VBG59VQ9\\n4047aa9b-d8bc-45db-84c6-2a6966ed69dc,U02VBG59VQ9,,,,1643044246.402900,1643050675.441600,U02VBG59VQ9\\n02b0fd2e-6763-4efd-8b27-8d5d4f53b61b,U02UKLHDWMQ,,,I have version 1.9.0 and python 3.9,1642975381.215900,1643050739.442000,U02UKLHDWMQ\\n9d05be50-67d9-44a1-99ab-ffd08ed7b250,U02VBG59VQ9,,,\"I dont see docker-compose.yml there. You need to run the command inside the folder where docker-compose.yml is. It is looking for that file, but not present.\",1643044246.402900,1643050786.442200,U0290EYCA7Q\\n62dafa7e-fccf-4096-9489-7f49a365685f,,,,\"Even though it can be a bit challenging going through the videos and coding along, while resolving any errors, I must say I\\'m enjoying the course.\",,1643051372.443600,U02TMP4GJEM\\nBFD0457A-DDEA-49C1-AE87-CC153B3F01A1,U02RSAE2M4P,,,\"Ah okay thanks <@U02QTG6A71U>. So the actual image exists on our machine somewhere, once we create it?\",1643045354.415900,1643051481.445600,U02U34YJ8C8\\nb50f3b56-8275-4858-9d30-8e9840e03309,U02RSAE2M4P,,,Images on disk,1643045354.415900,1643051680.445900,U0290EYCA7Q\\naf818b85-4a32-45a7-a0ed-dd67923d5274,U02RSAE2M4P,,,Yes you can list all your images with docker images ls  - - all,1643045354.415900,1643051702.446300,U02QTG6A71U\\n5c53bb16-5002-40b6-966b-49f0956849fb,U02RSAE2M4P,,,\"I didnt create an image for ingest, so it is not showing it\",1643045354.415900,1643051714.446500,U0290EYCA7Q\\n87b84d7d-5292-4ffa-8c0c-c4b377bb9d08,U02RSAE2M4P,,,\"I dockerized all tasks: postgres, pgadmin (make use of docker compose) and data ingesting.\",1643045354.415900,1643051840.446800,U02QTG6A71U\\ndaf69c58-4480-4a38-bed7-a74d4896e9fd,U02RSAE2M4P,,,\"I actually did for hands on. Deleted it, and didn\\'t create one for the homework\",1643045354.415900,1643051911.447500,U0290EYCA7Q\\n35e53894-b04d-4374-96a2-b144c2b28fd9,U02RSAE2M4P,,,\"There is a space between two -s. :slightly_smiling_face:\\n `docker images ls\\xa0-- all`\",1643045354.415900,1643052026.447900,U0290EYCA7Q\\nec698f3b-d735-4fde-a8b0-d4641b8c80b3,U02RSAE2M4P,,,\"For this simple task, you are right, what\\'s the point of dockerized for just small python script. But I like the concept of \\'task\\', not just for dockerizing continuous services.\",1643045354.415900,1643052144.448200,U02QTG6A71U\\n08edc4b9-0844-42d3-a3d9-44133382a111,,1.0,,\"Hi All, I wanted to confirm if there is a video missing in between the 2.2.1 Introduction to workflow and 2.4.4 Moving files from AWS to GCP or am i just mistaken?\",1643052567.450500,1643052567.450500,U02TBTX45LK\\n8e60de6b-b2be-46a3-a5f4-2bacd048d9ec,U02TBTX45LK,,,I guess the remaining hasn\\'t been upload yet.,1643052567.450500,1643052640.450600,U01MFQW46BE\\n66ab3722-1b2e-4556-8e25-c64a3e63431c,,,,\"Ok..this is extremely overwhelming :sob: :sob: :sob:\\n\\nI have been really busy at work. It feels like a lot has happened here and it\\'s only been a week!. I have some catching up to do :muscle: :muscle:\",,1643053272.453500,U02SZARNXUG\\n7c4b5fd2-d306-400f-97d3-e069d412e279,U0297MFTTM1,,,I got somehting similar with loading data it could be possible to load form pgadmin4 instead I mean is valid as well?,1643048920.431600,1643053369.455500,U0297MFTTM1\\na3ebb376-e327-4cdc-9c7e-216a5b6597f8,U0297MFTTM1,,,\"```    StreamHandler.__init__(self, self._open())\\n  File \"\"/opt/anaconda3/lib/python3.9/logging/__init__.py\"\", line 1175, in _open\\n    return open(self.baseFilename, self.mode, encoding=self.encoding,\\nPermissionError: [Errno 13] Permission denied: \\'/Users/vray/.config/pgcli/log\\'```\",1643048920.431600,1643053377.456100,U0297MFTTM1\\nfd645f35-91bb-4ca2-bc28-350ecb9f2b0b,,3.0,,Can I still submit assignment? I have installed some tools prior to week one but I haven\\'t gone through the setup videos. I joined the Iive call for a while before I went off grid.,1643053383.456400,1643053383.456400,U02SZARNXUG\\n0efb7418-945b-4e5e-960f-40d999cbcc6d,,3.0,,\"Try to run `terraform plan -var=\"\"project=taxi-rides-ny\"\"`like in video 2.4.2 and get error.\",1643053425.456900,1643053425.456900,U02ULQFCXL0\\n7176a8cb-d78c-4bbf-a50b-50b1da462db4,U02ULQFCXL0,,,\"```│ Error: Error when reading or editing Google Cloud Storage Transfer service account not found: googleapi: Error 403: The caller does not have permission, forbidden\\n│ \\n│   with data.google_storage_transfer_project_service_account.default,\\n│   on <http://transfer_service.tf|transfer_service.tf> line 6, in data \"\"google_storage_transfer_project_service_account\"\" \"\"default\"\":\\n│    6: data \"\"google_storage_transfer_project_service_account\"\" \"\"default\"\" {```\",1643053425.456900,1643053441.457000,U02ULQFCXL0\\n1de93cfc-df67-4896-86a4-02b1e54bfdd1,U02ULQFCXL0,,,which value we need to use?,1643053425.456900,1643053463.457200,U02ULQFCXL0\\n4911ba65-e370-43e8-862d-483494ef3b76,U02TWFZURD1,,,\"My understanding is that is easier to run something in docker than downloading nd set it up locally. This needs a bit of exposure though. In the end since pipelines run in the cloud, it also will be easier to run your docker containers on the cloud than setting up VMs in cloud. Not an experienced person,   this is my understanding\",1643046858.423200,1643054007.457500,U0308MF3KUH\\ne7880ce0-a883-4b90-8ecb-d19f982fe127,,,thread_broadcast,HW for week 1 is still due this Wednesday 22:00 CET.,1643053383.456400,1643054310.457800,U02TWFZURD1\\n166EF4BF-945B-41A2-A791-DB6048E07A0A,,3.0,,\"General question\\nWhere is noSQL databases used in data engineering architecture?\",1643054335.458900,1643054335.458900,U02U6DR551B\\nd23ce1f7-dd89-4829-8103-b94b5523fcc7,U02U6DR551B,,,shopping carts,1643054335.458900,1643054584.459000,U02U2Q5P61Z\\n2276c283-3062-45dd-bc85-d4fd66b76c1a,U02U6DR551B,,,if the schema is likely to change there\\'s no point using RDBMSs because you\\'re going to get a bunch of nulls everywhere on the new column you add,1643054335.458900,1643054641.459200,U02U2Q5P61Z\\n7cc570d9-d063-4189-9161-78885da8eff2,U02U6DR551B,,,NoSQL allows data to be stored in a Schema-less or free-form fashion. Great for storing user session data and other real time data.,1643054335.458900,1643054647.459400,U0290EYCA7Q\\na53c86b3-7bf7-4348-8b5d-7f0ce70019ba,U02ULQFCXL0,,,your project ID on GCP,1643053425.456900,1643054866.459700,U0297MFTTM1\\n403245D0-E5C6-4CC5-B2C4-2D9AF0E6FBA4,,4.0,,\"hello, I’m stuck with the following issue: when trying to run `$ pgcli -h localhost -p 5432 -u root -d ny_taxi` I’ve got the error `could not connect to server: Connection refused`. this command was working before and i could access pgcli. could anyone give me some hints on troubleshooting this please?\",1643055068.463500,1643055068.463500,U02DY0L6PHV\\n66a76b7e-248a-4c53-be13-3ee3df5fde75,,13.0,,\"`$ pgcli -h localhost -U root -d ny_taxi`\\n\\nafter running this i got this error\\n\\n `Connection refused Is the server running on that host and accepting TCP/IP connections?`\\n\\n\\ncan someone assist\",1643055223.464900,1643055223.464900,U02QKMCV39R\\n9b59841d-0793-49d3-9e31-b413df2be3cc,U0297MFTTM1,,,What\\'s the output of ls -ltr /Users/cray/.config/pgcli/log?,1643048920.431600,1643055941.465300,U0290EYCA7Q\\n75d4799a-3b98-4c4a-b8b4-8f69ae708ddf,U02DY0L6PHV,,,maybe need to run container or establish ssh connectipn to server,1643055068.463500,1643056574.465900,U02ULQFCXL0\\nb2bb7b89-88d8-467e-b625-1849467634bb,U02QKMCV39R,,,maybe need to run container or establish ssh connectipn to server,1643055223.464900,1643056584.466100,U02ULQFCXL0\\n5B4A3BE7-DAE1-49E5-8398-C4EA2EFB94A9,U02DY0L6PHV,,,\"Hi Lina, maybe helps\\n<https://stackoverflow.com/questions/32439167/psql-could-not-connect-to-server-connection-refused-error-when-connecting-to|https://stackoverflow.com/questions/32439167/psql-could-not-connect-to-server-connection-refused-error-when-connecting-to>\",1643055068.463500,1643056667.466600,U02URBSGTV4\\n046a1af3-eb13-4817-adcb-756b5a4d1161,U02QKMCV39R,,,I’m actually doing the GCP part,1643055223.464900,1643056732.466900,U02QKMCV39R\\nA81516CF-E247-451E-AFDA-31DD97AF1460,U02RSAE2M4P,,,Thanks. So `docker images ls —all` doesn’t actually show me anything. Although `docker images -a` does. Can anyone explain?,1643045354.415900,1643056818.468600,U02U34YJ8C8\\n71e00f2a-abdb-4689-9d7e-dbc95a5f5302,U02SZARNXUG,,,Take your time and relax,1643053383.456400,1643056928.469000,U02SPLJUR42\\n6127f39c-12ff-44f3-9b8d-1af7922a5cf5,U0297MFTTM1,,,`-rw-r--r--\\xa01 root\\xa0staff\\xa09770 Jan 24 12:46 /Users/vray/.config/pgcli/log`,1643048920.431600,1643056957.469200,U0297MFTTM1\\n57ecc4d2-a7dc-4113-90fd-7f37236d164e,U02DY0L6PHV,,,is it actually running?,1643055068.463500,1643056958.469400,U01AXE0P5M3\\n76669e05-e7e2-46f0-85e0-40e7b3c0d333,U02QKMCV39R,,,\"ok, docker containers are running?\",1643055223.464900,1643057116.469800,U02ULQFCXL0\\n70caa59a-bc73-4bb8-96af-b25970cca1f4,U02QKMCV39R,,,Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get <http://%2Fvar%2Frun%2Fdocker.sock/v1.24/containers/json>: dial unix /var/run/docker.sock: connect: permission denied,1643055223.464900,1643057178.470000,U02QKMCV39R\\nbf9d4b3d-829f-422f-a1da-3b16407db9c2,U02QKMCV39R,,,It was running before but now it’s saying this,1643055223.464900,1643057193.470200,U02QKMCV39R\\n914ac923-bd55-42e8-aaaf-d8d483327bb4,U02QKMCV39R,,,<https://github.com/sindresorhus/guides/blob/main/docker-without-sudo.md|https://github.com/sindresorhus/guides/blob/main/docker-without-sudo.md>,1643055223.464900,1643057398.470400,U02ULQFCXL0\\ne558adb8-1f21-47e8-a38c-6b0ac7af120e,U02QKMCV39R,,,try to apply this :arrow_up: steps,1643055223.464900,1643057466.470700,U02ULQFCXL0\\n7e1e8fa7-053b-48ba-8382-fe6c60eab786,U02QKMCV39R,,,and don`t forget to apply things in IMPORTANT block,1643055223.464900,1643057495.470900,U02ULQFCXL0\\ncc36edb5-a067-4511-bba4-7c953b3bf148,U02QKMCV39R,,,hope this helps,1643055223.464900,1643057529.471100,U02ULQFCXL0\\n1d9781ca-bcf6-4674-9fc9-ef0178a328d4,U02QKMCV39R,,,\"I have done this steps\\nShould i re-do them\",1643055223.464900,1643057649.471300,U02QKMCV39R\\n84536c13-afa7-4d4b-a756-88dcaf77e887,,6.0,,\"I don\\'t understand this message\\n\\n```StopIteration                             Traceback (most recent call last)\\n/tmp/ipykernel_7196/1101886938.py in &lt;module&gt;\\n      4     start_time = time()\\n      5 \\n----&gt; 6     df = next(df_iter)\\n      7     df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\\n      8     df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\\n\\n~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py in __next__(self)\\n   1022     def __next__(self):\\n   1023         try:\\n-&gt; 1024             return self.get_chunk()\\n   1025         except StopIteration:\\n   1026             self.close()\\n\\n~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py in get_chunk(self, size)\\n   1072                 raise StopIteration\\n   1073             size = min(size, self.nrows - self._currow)\\n-&gt; 1074         return self.read(nrows=size)\\n   1075 \\n   1076     def __enter__(self):\\n\\n~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py in read(self, nrows)\\n   1045     def read(self, nrows=None):\\n   1046         nrows = validate_integer(\"\"nrows\"\", nrows)\\n-&gt; 1047         index, columns, col_dict = self._engine.read(nrows)\\n   1048 \\n   1049         if index is None:\\n\\n~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py in read(self, nrows)\\n    221         try:\\n    222             if self.low_memory:\\n--&gt; 223                 chunks = self._reader.read_low_memory(nrows)\\n    224                 # destructive to chunks\\n    225                 data = _concatenate_chunks(chunks)\\n\\n~/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.read_low_memory()\\n\\nStopIteration: ```\\n[ ]:\\n\\n``````\\nwhen i loaded my my data and count is showing i only have 100000 rows in my table whereas it should be more than that count\",1643057832.473700,1643057832.473700,U02UKLHDWMQ\\n679a685f-7dfc-4205-accb-540a3b274518,U02UKLHDWMQ,,,this is how iterators work in python. I was just too lazy to properly handle this exception - I hope it didn\\'t cause too much confusion,1643057832.473700,1643057871.473800,U01AXE0P5M3\\ne9fed0af-278b-427e-abfd-d1d43f33f266,U02QKMCV39R,,,Not working,1643055223.464900,1643057893.474100,U02QKMCV39R\\ned478b99-e691-4fd7-8398-8db86f7b0ddc,U02UKLHDWMQ,,,\"<https://www.w3schools.com/python/python_iterators.asp>\\n\\ncheck the last code snippet\",1643057832.473700,1643057908.474300,U01AXE0P5M3\\n39f4647d-a34c-46e3-ac97-9664add5ec0d,U02UKLHDWMQ,,,\"```while True:\\n    try:\\n        # get the next item\\n        element = next(iter_obj)\\n        # do something with element\\n    except StopIteration:\\n        # if StopIteration is raised, break from loop\\n        break```\\nThis is the proper way of handling this\\n\\n(see here - <https://www.programiz.com/python-programming/iterator_>\",1643057832.473700,1643057976.474600,U01AXE0P5M3\\n57e12b3c-33d5-4fe8-9e50-5c9a814f2d8b,U02QKMCV39R,,,check <https://stackoverflow.com/questions/48957195/how-to-fix-docker-got-permission-denied-issue|this>,1643055223.464900,1643058023.474800,U02ULQFCXL0\\nfc3e27b9-d3c5-487d-83dd-6fe5cdd6c520,U02UKLHDWMQ,,,But why is count showing just 100000 when i successfully appended more than that,1643057832.473700,1643058774.475100,U02UKLHDWMQ\\n7afc8166-e062-4f96-aefc-dc8b76ae744b,,3.0,,\"I couldn\\'t attend today\\'s webinar, how is week 2 looking so far? As crazy as week 1?\",1643060255.476900,1643060255.476900,U02UX664K5E\\n582f5d19-22cc-4de4-9e42-aee2de3b14ff,U02UX664K5E,,,I believe week 2 content release is being delayed slightly (by a day I think) and the homework deadline has been extended till Wednesday I believe.,1643060255.476900,1643060431.477000,U02SUH9N1FH\\nc219e685-1995-4623-9a66-fa5d8de61c21,U02UX664K5E,,,\"Oh nice, I will work on my homework then :smile:\",1643060255.476900,1643060450.477200,U02UX664K5E\\nf34ac8ff-b42c-451c-8bbd-a6894221351f,U02QKMCV39R,,,\"`pip install pgcli`\\n\\nCommand ‘pip’ not found, but can be installed with:\\n\\napt install python3-pip\\nPlease ask your administrator.\",1643055223.464900,1643060463.477400,U02QKMCV39R\\n4340c94a-5b5d-49ca-adbe-69d39c715435,U02UX664K5E,,,Lol that\\'s exactly what I thought. I was busy working on my masters assignment over the weekend so couldn\\'t give much time to this homework. Now I will :ok_hand:,1643060255.476900,1643060542.478300,U02SUH9N1FH\\nbc629f4b-4cd5-4ece-ba34-9ee2de07b2b7,U0297MFTTM1,,,That could be the reason. No execute permission for the owner itself. And others can\\'t write.,1643048920.431600,1643060591.478800,U0290EYCA7Q\\nc1831cd7-d378-4640-93d7-479ab5ac43e2,U02UKLHDWMQ,,,That\\'s just for demonstration.,1643057832.473700,1643060690.480500,U0290EYCA7Q\\ne0f879cd-3192-484e-8a54-193e3ae5624b,,6.0,,\"Hi, should I upload data to database before running `docker-compose up`? I ran docker-compose yaml, but there is not data in postgres…\",1643060744.482100,1643060744.482100,U02ULGHNT33\\n8695050c-3a8e-4bbd-a3cf-17b8d6f964e3,U02ULGHNT33,,,docker compose is for setting up infrastructure. You can\\'t load data before that.,1643060744.482100,1643060862.482900,U0290EYCA7Q\\n41f6d041-89e4-4626-9edd-555c9e76bce5,,,thread_broadcast,\"Hey, although `pgcli` is not 100% necessary for the Week 1 completion, but it makes writing SQL queries more pleasant, so I decided to try to reproduce errors that MAY appear while installing pgcli and / or using it.\\n\\nI\\'ve set-up my env in `WSL2 (Ubuntu-20.04)` on `Windows 10 (21H1)`, `Python 3.9.10`\\n\\nCheck your `pgcli` version (`pip list`). If it\\'s below 3.0 (should be 3.3.1) then:\\n  - you most probably had the _*\"\"Error: pg_config executable not found\"\"*_ issue while installing `pgcli`, thus `psycopg2` was not installed (couldn\\'t build from source)\\n  - but you should notice `psycopg2-binary` package installed, as well as some very very old version of `prompt-toolkit` (~1.0) [which conflicts with `jupyter` requirements btw]\\n\\nThis combination of package versions should still _work_ but you might notice frequent errors after running the `pgcli`, like:\\n```psycopg2.errors.UndefinedColumn: column def.adsrc does not exist\\nLINE 7:                         def.adsrc as default```\\nor after typing this one it will bleed as well:\\n```decamp@localhost:ny_taxi&gt; \\\\d yellow_taxi_data\\ncolumn c.relhasoids does not exist\\nLINE 2: ...                 c.relhasrules, c.relhastriggers, c.relhasoi...\\n                                                             ^```\\nFortunately there is a one-line solution which can be found on <https://github.com/dbcli/pgcli#linux|pgcli\\'s github> but one need to scroll a little bit ;)\\n\\n`$ sudo apt-get install python-dev libpq-dev libevent-dev`\\n\\nThen you need to update your Python environment:\\n```(decamp) nervuzz@DELL-7559:~/venvs$ pip install --upgrade pgcli\\n[...]\\nSuccessfully installed pendulum-2.1.2 pgcli-3.3.1 pgspecial-1.13.0 prompt-toolkit-3.0.24 psycopg2-2.9.3 pytzdata-2020.1 sqlparse-0.4.2```\\nEnjoy writing SQL queries with pgcli!\",1642975381.215900,1643060941.483200,U02UKBMGJCR\\n98049fbb-a0a2-482b-8cb7-be16560f453c,U02ULGHNT33,,,Exactly what <@U0290EYCA7Q> said. Your docker-compose yaml file is like the recipe you need to create the infrastructure before you start ingesting data.,1643060744.482100,1643060955.483500,U02SUH9N1FH\\nc75da967-189f-4363-9f10-18e6a7395a43,U02ULGHNT33,,,\"With docker compose, all containers will run in the same network. We don\\'t have to create one.\",1643060744.482100,1643060964.483700,U0290EYCA7Q\\nda6acfb1-0b84-4c20-b1dc-2540230b0b5a,U02UKLHDWMQ,,,This should definitely be pinned :ok_hand:,1642975381.215900,1643061069.484100,U02SUH9N1FH\\nb598e1d9-55a0-4ff0-8eda-197d1a53c1d0,U02UKLHDWMQ,,,<@U0297MFTTM1> this might be the solution for your issue.,1642975381.215900,1643061094.484300,U0290EYCA7Q\\n91f85cea-2cc0-493c-9d5a-a86480f4f2b0,U02UKLHDWMQ,,,\"<@U02UKBMGJCR> please add it here. They are documenting it.\\n\\n<https://datatalks-club.slack.com/archives/C02V1Q9CL8K/p1643050132266400|https://datatalks-club.slack.com/archives/C02V1Q9CL8K/p1643050132266400>\",1642975381.215900,1643061175.484500,U0290EYCA7Q\\n9463fca9-6882-42b3-8893-f9797987a117,U02UKLHDWMQ,,,<@U0290EYCA7Q> I will give a try since I also try brew but on Monterey is like a pain permissions are everywhere :disappointed:,1642975381.215900,1643061244.484800,U0297MFTTM1\\n81c560d4-32cc-4827-a7e5-d89a57d1e9ef,U02UKLHDWMQ,,,\"I am on Monterey only. All i did was creating a separate conda environment with python 3.9, and jupyter lab installed.\",1642975381.215900,1643061324.485300,U0290EYCA7Q\\n7ec64ece-7176-4afb-8f8b-3bdadd1e6685,U02T2DX4LG6,,,\"nop not yet , still getting this error: `sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name \"\"pgdatabase\"\" to address: Name or service not known`\",1642787354.152800,1643061344.485500,U02T2DX4LG6\\n5DF5220C-0456-485F-80AA-E419515DB329,U02UKLHDWMQ,,,I got the same,1642975381.215900,1643061417.486400,U0297MFTTM1\\nfa1c274c-23c2-460d-8760-4c0d26fc9420,U02ULGHNT33,,,thank you! how can I ingest data after running docker compose?,1643060744.482100,1643061539.486600,U02ULGHNT33\\n750bde4d-96a4-413c-b46b-d3940c126392,,7.0,,Appreciate if someone could help me . I am getting this error when running the docker with the network.,1643061773.488300,1643061773.488300,U02T2DX4LG6\\n3cb0b597-c1c3-4804-ae20-3bfb3d5c15c9,U02T2DX4LG6,,,\"`sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name \"\"pgdatabase\"\" to address: Name or service not known`\",1643061773.488300,1643061781.488400,U02T2DX4LG6\\nd203b52b-cc4d-456f-bb2d-4e31bc171270,,29.0,,\"I\\'ve done docker compose up and successfully got the postgres db and pgadmin running however my data wasn\\'t persisted (no tables or data).  This is my docker compose file\\n```services:```\\n  pgdatabase:\\n    image: postgres:13\\n    environment:\\n      - POSTGRES_USER=root\\n      - POSTGRES_PASSWORD=root\\n      - POSTGRES_DB=ny_taxi\\n    volumes:\\n      - \"\"./ny_taxi_postgres_data:/var/lib/postgresql/data:rw\"\"\\n    ports:\\n      - \"\"5432:5432\"\"\\n  pgadmin:\\n    image: dpage/pgadmin4\\n    environment:\\n      - PGADMIN_DEFAULT_EMAIL=<mailto:admin@admin.com|admin@admin.com>\\n      - PGADMIN_DEFAULT_PASSWORD=root\\n    ports:\\n      - \"\"8080:80\"\"\\n```and my ingest docker command is docker run -it \\\\\\n--network=pg-network \\\\\\ntaxi_ingest:v001 \\\\\\n--user=root \\\\\\n--password=root \\\\\\n--host=pgdatabase \\\\\\n--port=5432 \\\\\\n--db=ny_taxi \\\\\\n--table_name=yellow_taxi_trips \\\\\\n--url=\"\"<https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>\"\" . However my error message is \"\"Could not translate host name \"\"pgdatabase\"\" to address: Name or service not known. I\\'m on video 1.2.5 at timestamp 7.23. Any ideas on how to fix this ? ```\",1643061968.489400,1643061968.489400,U02TMP4GJEM\\n38a43cec-8468-46d1-822d-c68c576d2c1a,U02T2DX4LG6,,,Is the hostname right? pgdatabase or pg-database?,1643061773.488300,1643062002.489500,U0290EYCA7Q\\n4024b055-9236-42b5-8406-7f4affece989,U02ULGHNT33,,,\"You can either run Python script locally, or firing up ingest docker container.\",1643060744.482100,1643062122.489800,U0290EYCA7Q\\na2e680ec-ec6c-4bbf-8684-4bddee17bc58,,5.0,,\"Hi, I had to re-install the enviroment and strangely I can not loging to postgresql getting this error\\n\\n: connection to server at \"\"localhost\"\" (::1), port 5432 failed: FATAL:  password authentication failed for user \"\"root\"\"\\n\\nI have tried to change the port and tried all other options but the same error. Can someone help me here?\",1643062227.491700,1643062227.491700,U02U5Q8FK38\\n2919ca82-1c85-40de-a45e-352fdf54a354,U02ULGHNT33,,,\"If you are trying to load zone data for homework, you need to build the docker image for ingesting first to reflect rhe changes in python script.\",1643060744.482100,1643062291.491800,U0290EYCA7Q\\nb9bb42d1-8c83-4280-979d-e0007ee3a1e2,U02UKLHDWMQ,,,\"Sorry guys, I am not familiar with macOS so I cannot help you :expressionless:\",1642975381.215900,1643062349.492100,U02UKBMGJCR\\n7C7AD829-79D0-4A60-BFDA-0282A3E069B7,U02UKLHDWMQ,,,xd,1642975381.215900,1643062364.492400,U0297MFTTM1\\nfc036eba-d3eb-4702-af63-700f647ac61c,U02TMP4GJEM,,,\"It could be that you are not running docker-compose up within the directory in which \"\"ny_taxi_postgres_data\"\" is located.  Try changing directories first, and then running docker-compose up again.\",1643061968.489400,1643062425.492700,U02SUUT290F\\n61a534c6-b570-465f-8366-7445fed1ccbd,U02T2DX4LG6,,,I tried it with pg-database but I have same mistake.,1643061773.488300,1643062465.492900,U02T2DX4LG6\\n905e13f5-0b15-4e2e-8ca1-152c7926234c,U02U5Q8FK38,,,I’m getting the same error. Has anyone else had the same problem and was able to solve it?,1643062227.491700,1643062517.493100,U02FQPX4SC9\\ne0b8af7e-e2e4-42c9-ba28-04ab67920447,U02T2DX4LG6,,,\"Try using \"\"localhost\"\" and port 5432\",1643061773.488300,1643062900.494900,U02SUUT290F\\nb2c67943-9614-4422-97bd-5f3f43f76bcd,U02UKLHDWMQ,,,will try later on a linux box then :),1642975381.215900,1643063017.495300,U0297MFTTM1\\ndfcf1690-1069-4b96-9ac3-9a5b9ee4f96a,,2.0,,\"Hello all after my `docker-compose -d`\\n\\nit showed 2 containers started\\n\\nbut after i ran `docker ps`\\n\\nit only showed one container running\",1643063543.496600,1643063543.496600,U02QKMCV39R\\n1a81b7aa-32e2-42bd-ab7a-6f3361a0be2c,U02TMP4GJEM,,,\"ok , changed directory to ny_taxi_postgres_data and re ran docker compose up. Able to load the database and pg admin but when I run the ingest docker command, I still get the same error. I also shutdown the docker network as in the video, it was stated that it was required anymore because the db and pgadmin are now in the same docker compose file. Still no joy.\",1643061968.489400,1643063667.496800,U02TMP4GJEM\\nDBEC11B5-188B-47E5-A4A6-8C5F78A06401,,3.0,,Anyone setup their environment using wsl2? If so what issues did you run into? I\\'m setting environment tonight and hopefully it\\'s smooth sailing lol,1643063867.499600,1643063867.499600,U02QLLT4B7U\\n6e031139-1e93-4868-ab9d-4e290a0c219e,,3.0,,\"Hi, if I start the three containers seperately, pgadmin, db and ingest then everything works fine. However when I use docker compose to start pgadmin and db together, my data and tables are not persisted and even then when i try to start the ingest data container, it can\\'t connect to the db even if I start everything from inside the ny_taxi_postgres_database directory. Has anyone got a solution ?\",1643063932.000800,1643063932.000800,U02TMP4GJEM\\n1f889ffe-b507-422d-8c19-d2cc07c50f5e,U02TC704A3F,,,\"So if I\\'m running everything fine in my machine, I shouldn\\'t do all that Google VM setup <@U01AXE0P5M3>?\",1642986252.245400,1643063977.000900,U02TC704A3F\\n884f8e89-f4d1-47d0-a0e6-b474e31564de,,8.0,,\"Hi, can anyone recommend some SQL documentation that would help with the homework Q5 and Q6? I have no clue how one does any of this stuff in SQL.\",1643064537.002700,1643064537.002700,U02SZMRHT7Z\\nc1f3e81f-31f1-4901-97a1-dcc164318071,U02TMP4GJEM,,,Can you share the dockerfile for the taxi_ingest image?,1643061968.489400,1643064707.002800,U02SUUT290F\\n6a1c1c48-7544-42e7-8a43-2f2e81596679,U02U5Q8FK38,,,Same problem not solved..,1643062227.491700,1643064752.003000,U0308MF3KUH\\n41a92a66-4ef4-4bd8-9c69-7923443e2b39,U02U5SW982W,,,Awesome Alexey. Thanks very much for that :slightly_smiling_face:. I\\'m getting a bit OCD about the course but I guess I\\'d like to know the different approaches to getting things to work and trying to understand what\\'s going on and why. But I\\'d better get on with Week 2 now.,1643019830.309300,1643065056.003200,U02U5SW982W\\nf4c78e7e-8ec2-458c-bebe-f226177643af,U02TMP4GJEM,,,\"FROM python:3.9.1\\n\\nRUN apt-get install wget\\nRUN pip install pandas sqlalchemy psycopg2\\n\\nWORKDIR /app\\nCOPY ingest_data.py ingest_data.py\\n\\n\\nENTRYPOINT [ \"\"python\"\", \"\"ingest_data.py\"\" ]\",1643061968.489400,1643065160.003700,U02TMP4GJEM\\n0e23f7f3-fc4c-41e4-ac7d-3df7c06b34da,,2.0,,Will there be any more videos for week 2? When will the homework be posted?,1643065172.004000,1643065172.004000,U02TNEJLC84\\n7aa23c70-7993-474c-af69-0bd3ca87eb3e,U02TMP4GJEM,,,\"and the docker command to run it.\\n```docker run -it \\\\\\n--network=pg-network \\\\\\ntaxi_ingest:v001 \\\\\\n--user=root \\\\\\n--password=root \\\\\\n--host=pgdatabase \\\\\\n--port=5432 \\\\\\n--db=ny_taxi \\\\\\n--table_name=yellow_taxi_trips \\\\\\n--url=\"\"<https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>\"\"```\\n\",1643061968.489400,1643065198.004100,U02TMP4GJEM\\n95ac5f1b-b2d4-40d8-8967-f74b823c7431,U02TMP4GJEM,,,\"even if i remove the --network option, i still get the same error\",1643061968.489400,1643065229.004300,U02TMP4GJEM\\ncdc31b96-c367-461a-a04f-d5e286ee0fa4,U02SZMRHT7Z,,,\"I would google \"\"sql aggregate functions\"\" and \"\"sql joins\"\" I think that should be enough as well as the video Alexey posted in the play list.\",1643064537.002700,1643065310.004500,U02TNEJLC84\\n28abcc19-2b41-4be1-99b8-a2d48148bc03,U02SZMRHT7Z,,,<https://www.w3schools.com/sql/sql_groupby.asp>,1643064537.002700,1643065332.004700,U02TNEJLC84\\n076dbb1a-2671-4817-84f3-5eb4c54d9dfc,U02SZMRHT7Z,,,<https://www.w3schools.com/sql/sql_join.asp>,1643064537.002700,1643065350.005000,U02TNEJLC84\\nef301ece-0c91-45f8-b8ae-d7ee88a05612,U02SZMRHT7Z,,,Hi <@U02SZMRHT7Z> I believe these questions are trying to get you to think about doing `GROUP BY` and perhaps even `ORDER BY` (though I don\\'t think it\\'s totally necessary for this). You will also need to  think about the `WHERE` clause for this to limit it to the day or month but you\\'ve already done that in question 3. As <@U02TNEJLC84> states <http://w3schools.com|w3schools.com> is a good resource. I generally just type in a generic search to google and it usually brings up something pretty similar to what I want to do (usually <http://w3schools.co|w3schools.co>m is somewhere near the top of the searches).,1643064537.002700,1643065456.005300,U02U5SW982W\\ndf416fe1-154d-4142-8d6d-4eaf32860781,U02TMP4GJEM,,,Could you provide the code and the error you are getting when you try to connect?,1643063932.000800,1643065546.005500,U02TNEJLC84\\n21067131-6dd7-4df8-8ee2-3cc2afe547d9,U02QLLT4B7U,,,\"Probably more expedient to just search the Slack channel for \"\"wsl2\"\". <https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642186903450800>\",1643063867.499600,1643065597.005700,U02TNEJLC84\\n7ac48415-7fc7-4c5a-a5eb-457aca4d6d79,U02QKMCV39R,,,Run it without the -d. Do you see any errors in the output?,1643063543.496600,1643065634.006000,U02TNEJLC84\\n2637d9cd-56f9-4bf3-a755-74052db60724,U02U5Q8FK38,,,\"I\\'d search the slack for the issue. Someone yesterday had the same thing. Not sure, but it seems like this comes up a lot when people use apt or something else other than pip to install pgcli.\",1643062227.491700,1643065722.006300,U02TNEJLC84\\n74450abd-929a-4c54-bd9a-81813699d5f0,U02U5Q8FK38,,,<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642959774147300>,1643062227.491700,1643065808.006500,U02TNEJLC84\\n7ccb43a7-cb6e-4aeb-8d03-99afedbac31a,U01AXE0P5M3,,,We are supposed to create a GitHub account for these solutions? Python and SQL?,1642516682.463800,1643065850.006800,U02GVGA5F9Q\\n987ea400-6edc-4bd8-ac30-8f23b2f14297,U01AXE0P5M3,,,\"I have two slightly different scripts, because I used my Ubuntu (which had postgresql and pgadmin installed before) and my windows 11 WSL2 Ubuntu, that started clean...\",1642516682.463800,1643066012.007000,U02GVGA5F9Q\\n458d8101-a190-4a30-82fa-60ee01965f05,U02U5Q8FK38,,,\"Also, might need to destroy the files in your local mount, delete any running or stopped containers and rebuild them.\",1643062227.491700,1643066032.007300,U02TNEJLC84\\nd4c7455a-2b42-4980-aef8-679ad9fffb4d,U02DY0L6PHV,,,\"I\\'m having the same issue on Linux VM.\\n\\nI realized this sequence of errors after running docker-compose up without the -d.\",1643022678.324100,1643066280.007500,U02TC704A3F\\n42900989-72a5-4704-84c4-94c3c43049c8,U02T2DX4LG6,,,Is you docker container for Postgres running? And is this after the stage where you ran `docker compose up` ?,1643061773.488300,1643066704.007900,U02U34YJ8C8\\n58c07385-b997-4058-acc4-46c91cee18f0,U02SZMRHT7Z,,,\"Spend a day or so going through this:\\n\\n<https://www.w3schools.com/sql/>\\n\\nSQL basics don’t take too long to grasp thankfully\",1643064537.002700,1643066797.008100,U02U34YJ8C8\\nd2aa0bbc-127a-4f69-9ba4-77b441c88911,U02TMP4GJEM,,,maybe is because the are not in the same network. Try docker network ls and then in --network= set the network that was created by the dockerfile by default,1643061968.489400,1643066807.008400,U02T2JGQ8UE\\ndb51b0ad-50ba-4d9f-a476-e7c4da4fc7dd,U02TNEJLC84,,,Hoping there will be more stuff on the airflow-docker setup. Didn’t understand anything in the `Docker` and `Docker-Compose` files,1643065172.004000,1643066846.008600,U02U34YJ8C8\\na7f8fe2b-9bf6-4a89-9773-262b1c1b976f,U02TMP4GJEM,,,\"In my case was --network=2_docker_sql_default\\n```docker run -it \\\\\\n  --network=2_docker_sql_default \\\\\\n  taxi_ingest:v001 \\\\\\n    --user=root \\\\\\n    --password=root \\\\\\n    --host=pgdatabase \\\\\\n    --port=5432 \\\\\\n    --db=ny_taxi \\\\\\n    --table_name=yellow_taxi_trips \\\\\\n    --url=${URL} \\\\```\",1643061968.489400,1643066897.009200,U02T2JGQ8UE\\n1010b341-d008-4c88-a6b4-b5b519e56992,U02TMP4GJEM,,,\"It seems that there\\'s a network error that doesn\\'t allow you to connect to the database properly. I would try modifying your docker-compose.yaml file making the network connection explicit, as follows:\",1643061968.489400,1643066976.009500,U02SUUT290F\\n39a53d48-8af7-4632-b741-255d6223b95c,U02TMP4GJEM,,,\"Be careful with the ingest docker, the network it uses is not the one that is created in the docker-compose up execution. For me  I just had to change the flag\\n```--network=2_docker_sql_default```\\nTo see the networks you type\\n```  docker network ls```\\nAlso check the name of the database\\n```--host=pgdatabase```\\nThe README has pg-database which is not the appropiate service name in the docker-compose file\",1643063932.000800,1643067006.009700,U02TEKL21JQ\\n00870f2e-28e1-4398-abde-98841db4d723,U02TMP4GJEM,,,\"Your network will be wrong. As <@U02R4F43B0C> suggested, check what network docker created by default, and replace `pg-network` with that in your `docker run` command. Or try <@U02TZ71470X>’s way.\",1643061968.489400,1643067073.010000,U02U34YJ8C8\\n356d73c9-919d-4908-b2b4-8911515c0c46,U02TMP4GJEM,,,\"`services:`\\n  `pgdatabase:`\\n    `image: postgres:13`\\n    `environment:`\\n      `- POSTGRES_USER=root`\\n      `- POSTGRES_PASSWORD=root`\\n      `- POSTGRES_DB=ny_taxi`\\n    `volumes:`\\n      `- \"\"./ny_taxi_postgres_data:/var/lib/postgresql/data:rw\"\"`\\n    `ports:`\\n      `- \"\"5432:5432\"\"`\\n    `networks:`\\n      `- pg-network`\\n  `pgadmin:`\\n    `image: dpage/pgadmin4`\\n    `environment:`\\n      `- PGADMIN_DEFAULT_EMAIL=<mailto:admin@admin.com|admin@admin.com>`\\n      `- PGADMIN_DEFAULT_PASSWORD=root`\\n    `ports:`\\n      `- \"\"8080:80\"\"`\\n    `networks:`\\n      `- pg-network`\\n\\n`networks:`\\n  `pg-network:`\\n    `driver: bridge`\",1643061968.489400,1643067091.010200,U02SUUT290F\\n92f41a1b-8606-4b4b-aaae-8b0f7175e04a,U02TNEJLC84,,,\"I believe it\\'s just installing airflow. But I\\'m not sure if we are supposed to do thin on our VM in GCP or locally and by providing our credentials it connects to GCP. Since the instructions say \"\"localhost:8080\"\" and \"\"UI\"\" I\\'m leaning towards a local install, but could be mistaken and we are supposed to forward the port from GCP.\",1643065172.004000,1643067358.010600,U02TNEJLC84\\n8d14a05a-ccde-4b6c-8d30-bb237a0f8d62,U02T2DX4LG6,,,\"<@U02T2DX4LG6> <@U02UAFF1WU9> I think you need to change your network name. Run `docker network ls` to see the default network Docker will have created when you ran `docker compose up` , and replace `--network=pg-network`  with `--network=&lt;new network name&gt;`  in your `docker build` command for the taxi ingest image.\",1642787354.152800,1643067376.010800,U02U34YJ8C8\\n0b03a1fd-e9bf-48d6-aaa0-8ba31f4393b5,U02SZMRHT7Z,,,\"Hi, Thx for the links. I came across s3 before, but I dont get how aggregation can help me with the questions and how to apply those multiple filters on the data.\\nAfter three hours on the sql alone, i will leave it though.\\nI hope they will provide solutions for the questions.\",1643064537.002700,1643067381.011000,U02SZMRHT7Z\\n89585aa8-59e1-46cc-ae10-6d4441b51d97,U02SZMRHT7Z,,,On Wednesday they will. In today\\'s live stream they stated that the Homework is really just for the students benefit (and if they want to be on the leader board). We will be graded on the final project in order to receive the certificate.,1643064537.002700,1643067561.011500,U02TNEJLC84\\nbd9489a4-c7a0-42d8-8aaf-f6b28a749e07,U02QKMCV39R,,,\"One of your containers exited with status code 1. Like <@U02TNEJLC84> said, dont run in detach mode, and you will see the container not running.\",1643063543.496600,1643067682.011900,U02HFP7UTFB\\n8b116e3e-d456-4b99-ba90-77cb4a9fc8ad,U02TMP4GJEM,,,\"<@U02TEKL21JQ> Thank you, it worked! so it looks like when you use docker compose, a default network is created that spans the containers in the docker compose file and it order to connect to the spun up containers, you\\'ll need to include the name of the network too\",1643063932.000800,1643067821.012100,U02TMP4GJEM\\n5cb8a4de-e47d-4bf6-9788-186f0126af5e,U02SZMRHT7Z,,,for postgres I would recommend this one: <https://www.postgresqltutorial.com/>,1643064537.002700,1643068189.012400,U02T2JGQ8UE\\n42cb5960-1455-4502-af8f-33a73fa36ff8,U02TMP4GJEM,,,<@U02U34YJ8C8> yes that\\'s what I did.Modified my docker compose yaml file and added a network service and used that network when connecting with my data ingest container. It\\'s now working. Many thanks,1643061968.489400,1643068268.012600,U02TMP4GJEM\\nc67d48f9-15c8-4944-87ed-19b55e0686b9,,,thread_broadcast,<@U0290EYCA7Q> I still receive the same error,1642795056.206700,1643068769.012900,U02T52AQB2R\\n4f418fcc-0d08-4f48-963c-372cc7ba01b9,U02TC704A3F,,,\"Ooohh I just saw the new Office Hours <@U01AXE0P5M3>, thanks for explaning there\",1642986252.245400,1643068857.013200,U02TC704A3F\\n1d16d1bb-0a4d-4da6-8088-ed21a3aedb3a,U02T52AQB2R,,,\"I wouldn\\'t spend too much time on this. It is not necessary for the homework, you can try and login to pgadmin and check the database from there.\",1642795056.206700,1643068903.013600,U02TEKL21JQ\\n56bbecc1-7e55-4a0b-86ae-2651410477c4,,2.0,,For the airflow example this week does anyone know if we are supposed to run it locally or in the VM on GCP?,1643070190.014700,1643070190.014700,U02TNEJLC84\\nea9a43b6-e2ff-43c3-bb21-030dcf1d6cbd,U02UE7NTLUU,,,\"Hi how are things? I have the same problem along with the following error, try to do what you said and it keeps giving me the error:\\n```pgcli -h localhost -p 5432 -u root -d ny_taxi\\ncould not connect to server: Connection refused\\nIs the server running on host \"\"localhost\"\" (127.0.0.1) and accepting\\nTCP/IP connections on port 5432?```\\nand when I try on port 5431 it asks me for the root password, it does not accept any input.\",1642465146.363400,1643071101.015300,U02R4F43B0C\\n302b095a-5b86-495c-af87-40d9b6db19d1,U02QLLT4B7U,,,Running docker on wsl I think is challenging,1643063867.499600,1643071533.015800,U0308MF3KUH\\n21f77c75-75a9-42a7-89cb-ccdeee188887,,3.0,,\"any one know how to resolve this\\ndf.head(n=0).to_sql(name=\\'yellow_taxi_data\\', con=engine, if_exists=\\'replace\\')\\n(psycopg2.OperationalError) connection to server at \"\"localhost\"\" (::1), port 5432 failed: FATAL:  password authentication failed for user \"\"root\"\"\",1643073879.016600,1643073879.016600,U02VBG59VQ9\\n45ca192f-9ea0-4fae-be2c-991c48849a9b,U02VBG59VQ9,,,\"Did you launch the postgres database before running this line?\\n\\nCan you double-check if the connection setting (e.g. username, password, port, host) is correct?\",1643073879.016600,1643074761.017000,U02BRPZKV6J\\nab1efa56-dc36-4418-9e13-03dd90e07ea8,,3.0,,\"Hi All, In week 2 material, if we do a terraform plan on the folder, then the earlier resources(data lake + big query) is also added to the plan. <@U01DFQ82AK1>, are you changing the <http://main.tf|main.tf> with the <http://transfer_service.tf|transfer_service.tf> infrastructure? It would be great if you can video capture you entire terminal, as some of the commands are not visible on the video.\",1643075363.020200,1643075363.020200,U02TTSXUV2B\\nebc69547-bc79-48da-a8df-7d50c36d1c7f,U02VBG59VQ9,,,<@U02BRPZKV6J>,1643073879.016600,1643076079.020300,U02VBG59VQ9\\n1c6a8466-88d2-4a0f-a249-097fafea9d5e,U02VBG59VQ9,,,\"Based on the image you shared. Are you sure the host is called pgdatabase? Have you tried filling it with localhost or 127.0.0.1?\\nAlso It seem that the database name (Maintenance database) box is empty.\",1643073879.016600,1643079058.021200,U02U1F84FSS\\n1eefcc5f-5eb5-4d11-8fcc-3993e006b78f,,15.0,,i followed video 1.2.5 using docker-compose and got the error below in pgadmin,1643079831.022400,1643079831.022400,U02VBG59VQ9\\n77fe4c5e-eb93-4bee-967b-d044a3b124e0,U02B8U0QZEK,,,For me the same thing happened. I had to startup the containers again.,1643028416.352400,1643080683.023000,U02UB59DKDL\\n3a4d8816-4704-453a-b010-0ba18bf3c862,U02B8U0QZEK,,,do  docker container rm &lt;your_container_name&gt; to remove the container,1643028416.352400,1643081461.023400,U02T59ZAVE2\\n70ea32b8-277f-4cb3-9dc8-46a221e4a9e1,U02TNEJLC84,,,\"not sure, but i think - VM on GCP\",1643070190.014700,1643082138.023700,U02ULQFCXL0\\nD009EA1F-120A-4A40-9C9B-40B2B02B4304,,2.0,,Can I submit the assessment this week got stuck in technical issues pls let me know. I am eager to complete and submit my work. ,1643086171.025400,1643086171.025400,U02V2G3NK8U\\nf50309e2-bf9c-40b0-ae54-0e59e27524d7,U02TWFZURD1,,,\"Hi Aya, I am getting the same error, what should I do?\",1643028184.349800,1643087573.026100,U02C5MT2HQC\\nce1f13e2-8508-43e7-b3db-90c3f718101f,U02VBG59VQ9,,,Is pgdatabase running?,1643079831.022400,1643087906.026300,U0290EYCA7Q\\n9a38ccef-bd94-4f2d-957c-65faec44c09f,,5.0,,\"Using docker-compose up, I need to create server every-time in pgadmin. Is there any other way to run docker compose if server is created once?\",1643088062.028800,1643088062.028800,U02KKN0H3SR\\n5cf8faac-056c-4808-b26c-7aa59bdbe05e,U02KKN0H3SR,,,Docker compose is for setting up infrastructure. Your docker-compose yaml file is like the recipe you need to create the infrastructure.,1643088062.028800,1643088555.028900,U0290EYCA7Q\\n4d453e35-0517-40e9-a24f-51764f346633,U02KKN0H3SR,,,\"So once the infrastructure is created, that is docker compose up is running. How can I know the host and network in which it is running?\",1643088062.028800,1643088678.029100,U02KKN0H3SR\\nedeff28c-194b-4cd6-be1a-a4a4cb27611c,U02V2G3NK8U,,,The deadline is Wednesday 10 PM CET I guess. So you still have time.,1643086171.025400,1643089353.029300,U02TATJKLHG\\n2261a2a3-fdbf-426f-84ef-da8b43e7d3eb,U02VBG59VQ9,,,Hi <@U0290EYCA7Q> where?,1643079831.022400,1643089596.029500,U02VBG59VQ9\\nc574c9c2-5226-4a99-8bef-95ffd4cecdc5,U02KKN0H3SR,,,\"Use the command to start containers in detached mode:\\xa0`docker-compose up -d`\\n```(data-engineering-zoomcamp) hw % docker compose up -d\\n[+] Running 2/2\\n ⠿ Container pg-admin     Started                                                                                                                                                                      0.6s\\n ⠿ Container pg-database  Started```\\nTo view the containers use:\\xa0`docker ps`.\\n```(data-engineering-zoomcamp) hw % docker ps\\nCONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS          PORTS                           NAMES\\nfaf05090972e   postgres:13      \"\"docker-entrypoint.s…\"\"   39 seconds ago   Up 37 seconds   0.0.0.0:5432-&gt;5432/tcp          pg-database\\n6344dcecd58f   dpage/pgadmin4   \"\"/entrypoint.sh\"\"         39 seconds ago   Up 37 seconds   443/tcp, 0.0.0.0:8080-&gt;80/tcp   pg-admin```\\nTo view logs for a container:\\xa0`docker logs &lt;containerid&gt;`\\n```(data-engineering-zoomcamp) hw % docker logs faf05090972e\\n\\nPostgreSQL Database directory appears to contain a database; Skipping initialization\\n\\n2022-01-25 05:58:45.948 UTC [1] LOG:  starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv4 address \"\"0.0.0.0\"\", port 5432\\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv6 address \"\"::\"\", port 5432\\n2022-01-25 05:58:45.954 UTC [1] LOG:  listening on Unix socket \"\"/var/run/postgresql/.s.PGSQL.5432\"\"\\n2022-01-25 05:58:45.984 UTC [28] LOG:  database system was interrupted; last known up at 2022-01-24 17:48:35 UTC\\n2022-01-25 05:58:48.581 UTC [28] LOG:  database system was not properly shut down; automatic recovery in progress\\n2022-01-25 05:58:48.602 UTC [28] LOG:  redo starts at 0/872A5910\\n2022-01-25 05:59:33.726 UTC [28] LOG:  invalid record length at 0/98A3C160: wanted 24, got 0\\n2022-01-25 05:59:33.726 UTC [28] LOG:  redo done at 0/98A3C128\\n2022-01-25 05:59:48.051 UTC [1] LOG:  database system is ready to accept connections```\",1643088062.028800,1643089704.029700,U0290EYCA7Q\\n3e333e1c-a9e4-47c8-8d8e-8cfeacf028ff,U02VBG59VQ9,,,`docker ps` command,1643079831.022400,1643089814.030100,U0290EYCA7Q\\n6e85ed0b-715e-44b1-ba1a-3ce7f761080b,U02VBG59VQ9,,,<@U0290EYCA7Q>,1643079831.022400,1643090005.030400,U02VBG59VQ9\\n00fcd8a3-dc75-4851-bd4b-b34cd9b29823,U02VBG59VQ9,,,I dont see it is running,1643079831.022400,1643090164.030800,U0290EYCA7Q\\n9f34cd4a-d9a2-4698-aa3e-704483497a15,U02VBG59VQ9,,,how do i start it <@U0290EYCA7Q>,1643079831.022400,1643090189.031000,U02VBG59VQ9\\nea4cabc6-5a74-41de-9bf2-0806ea66223e,U02VBG59VQ9,,,\"You should see two containers started\\n\\n```(data-engineering-zoomcamp) hw % docker compose up -d\\n[+] Running 2/2\\n ⠿ Container pg-admin     Started                                                                                                                                                                      0.6s\\n ⠿ Container pg-database  Started```\",1643079831.022400,1643090353.031300,U0290EYCA7Q\\n93213e7a-1844-40d7-9491-074c821bfb51,U02VBG59VQ9,,,\"```(data-engineering-zoomcamp) hw % docker ps\\nCONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS          PORTS                           NAMES\\nfaf05090972e   postgres:13      \"\"docker-entrypoint.s…\"\"   39 seconds ago   Up 37 seconds   0.0.0.0:5432-&gt;5432/tcp          pg-database\\n6344dcecd58f   dpage/pgadmin4   \"\"/entrypoint.sh\"\"         39 seconds ago   Up 37 seconds   443/tcp, 0.0.0.0:8080-&gt;80/tcp   pg-admin```\",1643079831.022400,1643090388.031600,U0290EYCA7Q\\n7a478143-1778-4b33-958c-9ec7c6f63c51,U02VBG59VQ9,,,\"```(data-engineering-zoomcamp) hw % docker logs faf05090972e\\n\\nPostgreSQL Database directory appears to contain a database; Skipping initialization\\n\\n2022-01-25 05:58:45.948 UTC [1] LOG:  starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv4 address \"\"0.0.0.0\"\", port 5432\\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv6 address \"\"::\"\", port 5432\\n2022-01-25 05:58:45.954 UTC [1] LOG:  listening on Unix socket \"\"/var/run/postgresql/.s.PGSQL.5432\"\"\\n2022-01-25 05:58:45.984 UTC [28] LOG:  database system was interrupted; last known up at 2022-01-24 17:48:35 UTC\\n2022-01-25 05:58:48.581 UTC [28] LOG:  database system was not properly shut down; automatic recovery in progress\\n2022-01-25 05:58:48.602 UTC [28] LOG:  redo starts at 0/872A5910\\n2022-01-25 05:59:33.726 UTC [28] LOG:  invalid record length at 0/98A3C160: wanted 24, got 0\\n2022-01-25 05:59:33.726 UTC [28] LOG:  redo done at 0/98A3C128\\n2022-01-25 05:59:48.051 UTC [1] LOG:  database system is ready to accept connections```\",1643079831.022400,1643090427.031800,U0290EYCA7Q\\nc2ffeda5-90d0-4117-bc5f-601f8f18dc45,U02VBG59VQ9,,,,1643079831.022400,1643090587.032400,U02VBG59VQ9\\n5f9a96f6-1c14-429e-b56a-3e23bb5baa8d,U02RSAE2M4P,,,\"two are the same actually, when you use as an abbreviation just use single dash (-), for full parameter use (--), it\\'s a common practice for most of the command prompt activities, eg. - v or - - verbose. (since I use Ubuntu /Bash, I\\'m not familiar with other shells for Mac or Windows)\",1643045354.415900,1643090615.032800,U02QTG6A71U\\nd2acee67-7597-4c34-9442-a09cdf50e546,U02VBG59VQ9,,,\"Run `docker ps -a`\\nThis should show all containers, either running or stopped.\\nGet the container id for pgdatabase-1, and run `docker log &lt;container_id&gt;`\",1643079831.022400,1643090754.033100,U0290EYCA7Q\\n989d521f-b2d4-4f13-ac30-fb9fce3b003a,U02RSAE2M4P,,,\"```(data-engineering-zoomcamp) hw % docker images --help\\n\\nUsage:  docker images [OPTIONS] [REPOSITORY[:TAG]]\\n\\nList images\\n\\nOptions:\\n  -a, --all             Show all images (default hides intermediate images)\\n      --digests         Show digests\\n  -f, --filter filter   Filter output based on conditions provided\\n      --format string   Pretty-print images using a Go template\\n      --no-trunc        Don\\'t truncate output\\n  -q, --quiet           Only show image IDs```\",1643045354.415900,1643090890.033300,U0290EYCA7Q\\n577402ba-e6f6-4823-8be5-9387b8bb6d53,U02UKLHDWMQ,,,Hm that I don\\'t know. Should be more,1643057832.473700,1643091041.033500,U01AXE0P5M3\\nafe18b3c-cf5e-4367-871a-3f5af9e67d5c,U02VBG59VQ9,,,i can\\'t find the pgdatabase-1 container,1643079831.022400,1643091132.033700,U02VBG59VQ9\\n0f9bbeab-4b06-4a74-b0a5-11dcaff4aad7,U02VBG59VQ9,,,only the admin,1643079831.022400,1643091141.033900,U02VBG59VQ9\\n50416915-5a44-4054-a1fd-146a7a1c42d4,U02VBG59VQ9,,,even with `ps -a?`,1643079831.022400,1643091581.034300,U0290EYCA7Q\\n55efa74d-257d-4d3c-8a3e-3386f328e1d2,U02VBG59VQ9,,,Try to run compose without -d option. It should show the logs,1643079831.022400,1643091619.034500,U0290EYCA7Q\\nfa569b3d-f523-413b-866e-c4bf42e86f41,U02QLLT4B7U,,,You need to install docker for windows and select the wsl option there. Then you\\'ll have docker in both windows and wsl,1643063867.499600,1643091662.034700,U01AXE0P5M3\\nf64bd9e9-625f-47aa-907a-bcb815ea7522,U02TNEJLC84,,,Both should work fine,1643070190.014700,1643091854.034900,U01AXE0P5M3\\n4a73c4c0-4168-44ff-9c6c-5cd55b527856,U02KKN0H3SR,,,<https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup/2_docker_sql#docker-compose|https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup/2_docker_sql#docker-compose>,1643088062.028800,1643092005.035100,U01AXE0P5M3\\n3e333738-98db-45e7-8393-105e9ac7ce91,U02TC704A3F,,,Yes you won\\'t need a vm if everything is running smoothly,1642986252.245400,1643092116.035500,U01AXE0P5M3\\na7af2003-e480-4412-995d-7662b6ea58a4,U01AXE0P5M3,,,You can submit either of them. A github account would be helpful but you can also use bitbucker or gitlab,1642516682.463800,1643092217.035700,U01AXE0P5M3\\nd0fee5ce-c850-446d-a461-2db2b663ee96,,1.0,,\"When doing homework 1, I got this error message:\\n```Note: You didn\\'t use the -out option to save this plan, so Terraform can\\'t guarantee to take exactly these actions if you run \"\"terraform apply\"\" now.```\\n`terraform apply` still worked -- but should I append the -out option to the previous step?\",1643093658.036400,1643093658.036400,U02SQQTC5NV\\n1f010950-ce2e-4431-ba5e-a7d2f3084c12,U02KKN0H3SR,,,Thanks <@U0290EYCA7Q> &amp; <@U01AXE0P5M3>,1643088062.028800,1643094681.036900,U02KKN0H3SR\\n500ea9ac-7fe8-465c-9e9a-c3c67b546799,,5.0,,\"I have a question about hw submission q2. Which part of the output should we submit, the whole console text, or only the last line?\",1642842028.290300,1642842028.290300,U02UECC4H6U\\n2b0a1d14-6fa9-4bea-af2e-119a42a8aff9,U02UECC4H6U,,,The whole thing,1642842028.290300,1642842523.290400,U01AXE0P5M3\\n21204fac-a88d-40ab-a441-97f43a0f8e8a,U02UZBJ2Q6L,,,\"Hey, did you find an answer to this?\",1642769587.082900,1642844469.290700,U02TATJKLHG\\n8a0365f3-d9d7-4d04-87b6-525c8f408778,U02CD7E30T0,,,\"The answer for my question about alias was simple - you should take the whole thing like \"\"zones.LocaionID\"\" into quotation mark.\\n<@U01B6TH1LRL> <@U02ULMHKBQT> thanks a lot\",1642673343.329000,1642845768.291000,U02UECC4H6U\\nA0135C43-F9AA-4BDD-B287-1056C7074B2B,U02VBG59VQ9,,,Try to use two slashes: &lt;c://../|c://../&gt;/.. - it worked for me!,1642813718.259500,1642846000.293000,U02V18P84BT\\n63039dba-62b9-4ef7-bcfc-d6a3cfbfa923,,2.0,,\"hello, anybody else with this issue?\\n```python ingest_data.py --user=root --password=root --host=localhost --port=5432 --db=ny_taxi --table_name=yellow_taxi_trips --url=\"\"<https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>\"\"\\nSYSTEM_WGETRC = c:/progra~1/wget/etc/wgetrc\\nsyswgetrc = C:\\\\Program Files (x86)\\\\GnuWin32/etc/wgetrc\\n--2022-01-22 11:08:31--  <https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>\\nResolving s3.amazonaws.com... 52.216.160.77\\nConnecting to <http://s3.amazonaws.com|s3.amazonaws.com>|52.216.160.77|:443... connected.\\nERROR: cannot verify <http://s3.amazonaws.com|s3.amazonaws.com>\\'s certificate, issued by `/C=US/O=DigiCert Inc/OU=<http://www.digicert.com/CN=DigiCert|www.digicert.com/CN=DigiCert> Baltimore CA-2 G2\\':\\n  Unable to locally verify the issuer\\'s authority.\\nTo connect to <http://s3.amazonaws.com|s3.amazonaws.com> insecurely, use `--no-check-certificate\\'.\\nUnable to establish SSL connection.```\",1642846167.293400,1642846167.293400,U02UA0EEHA8\\nd4031b8e-bc06-472b-842e-185e8b29f2be,U02UECC4H6U,,,<@U01AXE0P5M3> and what should be the link to code? Was there a task to write any code apart from SQL queries?,1642842028.290300,1642847047.293800,U02UECC4H6U\\nd43feb5d-ae03-4f7e-8870-ceab7dec8957,,5.0,,\"Hey all , facing the issue of creating postgres and mapping to windows directory .\\ni am using wsl+windows+ubuntu on my machine.\\nthe command is as follows .\\ncan any body help me out the issue ?\\n*FYI i can create postgres instance if create docker volume before this  and docker volume as var/lib directory .*\",1642847257.296400,1642847257.296400,U02UEE4MBEG\\n6eac862e-c04d-43f6-975f-bcdebb1de17a,U02UA0EEHA8,,,\"Do u have curl installed? You could use curl instead of wget. I am sure git comes with curl. \\n\\ncurl url --output output.csv\",1642846167.293400,1642847371.296800,U0290EYCA7Q\\ncc3ced31-9179-4c27-aa2e-95c740548547,U02UA0EEHA8,,,\"Or you could simply download the CSV file, and set-up a simple httpserver using python. Alexis demonstrated that at the end of the video.\",1642846167.293400,1642847581.297300,U0290EYCA7Q\\n73b7f72f-e429-4407-892b-d7fccb012d7f,U02UEE4MBEG,,,\"Try to wrap the `-v` param in `\"\"\"\"`\",1642847257.296400,1642847615.297500,U02TT5SRDSB\\n0144f82e-f058-4a9a-9120-4362c9c4e4c1,U02U34YJ8C8,,,You are not alone. None of the lessons in the ML zoomcamp has been as dense as this first week of DE (the Kubernetes session comes close and the Kserve session isn’t finished yet but will also come close).,1642810460.254700,1642847705.297700,U02BVP1QTQF\\n11ff7c67-2d21-48ea-ac86-98e3476fd162,U02U34YJ8C8,,,\"I haven\\'t started it yet. My weekly plan went nuts and thus I hadn\\'t the opportunity to start this. All these posts with so many problems quite scare me, but even though I\\'m going to try to tackle it this weekend.\",1642810460.254700,1642848407.297900,U02GVGA5F9Q\\neea3ca89-08f6-431b-b120-47c8613e48c2,U02UEE4MBEG,,,i tried that as well,1642847257.296400,1642848898.300500,U02UEE4MBEG\\n673a1db9-4ef8-467b-b0c8-b79c98dbdc47,U02UEE4MBEG,,,not working if i give windows absolute path,1642847257.296400,1642848909.300700,U02UEE4MBEG\\n17e2fdfe-9868-42ef-b68a-6a2218a010e2,U02UEE4MBEG,,,\"it will work if i create docker volume and give -v \"\"docker_volumne:/var/lib etc etc\"\"\",1642847257.296400,1642848957.300900,U02UEE4MBEG\\n27a8e9ec-46b6-4750-8370-06c245770649,,5.0,,\"I got the docker-compose version of postgres and pgadmin. Then I dockerized the ingestion script and used this command first(command in thread) and got error “sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name “pgdatabase” to address: Name or service not known” then I changed the host from “pgdatabase” to “localhost” but got the error “Is the server running on host “localhost” (::1) and accepting\\n\\tTCP/IP connections on port 5432?”\\nWhen I ran the ingestion script locally I could connect fine but the dockerized script couldn’t.\",1642848984.301200,1642848984.301200,U02QQEQGTV2\\nbcb2b2aa-18e9-4460-aecd-08004cf39ce0,U02QQEQGTV2,,,\"did docker build first, the did:\\ndocker run -it \\\\\\n  taxi_ingest:v001 \\\\\\n    --user=root \\\\\\n    --password=root \\\\\\n    --host=pgdatabase \\\\\\n    --port=5432 \\\\\\n    --db=ny_taxi \\\\\\n    --table_name=yellow_taxi_trips \\\\\\n    --url=${URL}\\n\\nthen I tried:\\ndocker run -it \\\\\\n  taxi_ingest:v001 \\\\\\n    --user=root \\\\\\n    --password=root \\\\\\n    --host=localhost \\\\\\n    --port=5432 \\\\\\n    --db=ny_taxi \\\\\\n    --table_name=yellow_taxi_trips \\\\\\n    --url=${URL}\\n\\nthe docker-compose.yaml I’m using\\nservices:\\n  pgdatabase:\\n    image: postgres:13\\n    environment:\\n      - POSTGRES_USER=root\\n      - POSTGRES_PASSWORD=root\\n      - POSTGRES_DB=ny_taxi\\n    volumes:\\n      - “./ny_taxi_postgres_data:/var/lib/postgresql/data:rw”\\n    ports:\\n      - “5432:5432\"\"\\n  pgadmin:\\n    image: dpage/pgadmin4\\n    environment:\\n      - PGADMIN_DEFAULT_EMAIL=<mailto:admin@admin.com|admin@admin.com>\\n      - PGADMIN_DEFAULT_PASSWORD=root\\n    ports:\\n      - “8080:80”\",1642848984.301200,1642849027.301300,U02QQEQGTV2\\n6699e4ab-9b64-473f-abec-95cad8ed7cf4,U02U34YJ8C8,,,\"I think it’s doable in a weekend but it all depends on how you tackle it and you previous background knowledge. If you’re only watching the videos and follow along, and you already have a little experience with Linux, bash and docker, I think you can get everything done in a day. If you pause the videos constantly to take notes and you struggle with things like folder permissions because you’re unfamiliar with your particular environment, it may be a struggle. Considering the amount of issues I’ve been seeing from students using Windows and in a lesser degree Macs with Apple Silicon chips, if you fall into one of these categories I’d advise to go straight to the last video of the playlist where Alexey explains how to set up a VM in GCP in order to get a working environment to get your work done and then follow the rest of the videos.\",1642810460.254700,1642849129.301600,U02BVP1QTQF\\n68f4c901-5ffb-4e93-b7d9-12852a8ba8fe,U02U34YJ8C8,,,\"Thanks for the tips, <@U02BVP1QTQF>! Yes, I work with Microsoft and Windows systems, but built my sideways small business using only Linux and open source tools.  I\\'m betting on my Linux skills to tackle this during this weekend. Let\\'s see! Thanks!\",1642810460.254700,1642849632.302300,U02GVGA5F9Q\\nca57bae9-dda1-4eeb-ba02-0f770705238a,U02TVGE99QU,,,I had the same error and needed to install libpq-dev before installing pgcli.,1642467396.368700,1642849686.302500,U01T2HV3HNJ\\n20d18184-7021-4217-a848-a6cc092587eb,U02UX664K5E,,,\"<@U02UX664K5E>, thanks for sharing this to help me to setup wget on windows 10:grin:\",1642721639.474000,1642851357.303000,U02T697HNUD\\nbfbbe4e4-8e0a-4a3c-ac12-681d7d447dc1,U02T0CYNNP2,,,\"Solved\\nI check the available networks, so i detect the network for the container generated by the docker compose which is \\' docker_sql_default\\'\",1642786979.150600,1642851639.303200,U02T0CYNNP2\\ndc07ee27-7d68-4325-bfc3-116340843434,U02T0CYNNP2,,,\"docker run -it \\\\\\n    --network=docker_sql_default \\\\\\n    ingest_taxi_data:001 \\\\\\n    --user=mide \\\\\\n    --password=adebimpe1994 \\\\\\n    --host=pgdatabase \\\\\\n    --port=5432 \\\\\\n    --db=ny_taxi \\\\\\n    --table=yellow_taxi_trips \\\\\\n    --url=\"\"<https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv|https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>\"\"\",1642786979.150600,1642851668.303400,U02T0CYNNP2\\n71c5bed0-b7b1-4567-a1d8-13cd2f11eb8f,U02QQEQGTV2,,,\"I check the available networks, so i detect the network for the container generated by the docker compose which is \\' docker_sql_default\\'\",1642848984.301200,1642851757.303600,U02T0CYNNP2\\n570bf300-b587-44a2-ba97-46c72f274829,U02QQEQGTV2,,,\"docker run -it \\\\\\n    --network=docker_sql_default \\\\\\n    ingest_taxi_data:001 \\\\\\n    --user=mide \\\\\\n    --password=adebimpe1994 \\\\\\n    --host=pgdatabase \\\\\\n    --port=5432 \\\\\\n    --db=ny_taxi \\\\\\n    --table=yellow_taxi_trips \\\\\\n    --url=\"\"<https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv|https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>\"\"\",1642848984.301200,1642851792.303800,U02T0CYNNP2\\n13df72ca-5dcc-45e1-b535-f6d8ce308c79,U02QQEQGTV2,,,thank you that worked!,1642848984.301200,1642852308.304100,U02QQEQGTV2\\n0962bec1-4dcc-4e42-b67e-059ea75759ac,U02QQEQGTV2,,,We learn everyday,1642848984.301200,1642852540.304500,U02T0CYNNP2\\n6fc26309-e20b-4dac-9501-2d6fbcac0c55,U02RR9Z0CCV,,,Thank you Alexey!,1642781963.136000,1642852896.304800,U02RR9Z0CCV\\n20cad310-308a-4d05-a0e9-a462fa71cee0,U02BVP1QTQF,,,\"I see. Thanks for sharing. Was there anything missing from the notes/explanation here on \"\"terraform\"\" and \"\"provider\"\" sections, or something you think we should have covered more in our videos? <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/1_terraform_overview.md|https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/1_terraform_overview.md>\",1642713863.457200,1642853255.305000,U01DHB2HS3X\\n8d89dfba-3fd2-4d96-b09a-6cb5649285d4,,6.0,,\"hi I\\'m trying to create the pg-database and i am getting an error called \"\"Bind for 0.0.0.0:5431 failed: port is already allocated.\"\"\\nis it ok if i assign the database to another port, like say 5430?\",1642853448.306600,1642853448.306600,U02RREQ7MHU\\n73a26f7c-a7f2-4c9b-8202-247739ec049a,U02RREQ7MHU,,,i should mention that i  was running postgres on 5431 port,1642853448.306600,1642853526.306700,U02RREQ7MHU\\na8aef1f4-ba49-4ec0-9401-2fd5d6da7b61,,1.0,,What\\'s the deadline on 24.1.22 for submitting homework ?,1642853841.307400,1642853841.307400,U02TMP4GJEM\\nf217b411-0e6b-4869-bf1d-6d72d28dd70a,U02TMP4GJEM,,,\"Deadline: 24 January, 17:00 CET\",1642853841.307400,1642853923.307500,U02RREQ7MHU\\n990D988C-2881-4CFD-8EA8-053C476B970F,U02U34YJ8C8,,,\"<@U02BVP1QTQF>Regarding the VM - is this not an optional step, or am I missing something? At the end of that tutorial the instructor deleted the VM\",1642810460.254700,1642854160.309600,U02U34YJ8C8\\n568554d1-c576-4beb-aa3b-02c0f1f917ba,U02U34YJ8C8,,,\"<@U02U34YJ8C8>, I think that video was created with the sole purpose of walking everybody through a successful setup, to illustrate it.\",1642810460.254700,1642854339.309900,U02GVGA5F9Q\\nb0ff11b1-ed74-48b0-9583-83fe2342c460,U02RREQ7MHU,,,run  `docker ps` to see if you have any docker containers running. I bet you have. Then kill the processes.,1642853448.306600,1642854665.310100,U02QZN0LSBT\\n720e4048-be7a-4561-b8d9-20dd178ec245,U02RREQ7MHU,,,ya postgres is running should i remove it ?,1642853448.306600,1642854863.310600,U02RREQ7MHU\\n29e5ccb8-ced4-4496-ba21-83292dcef99e,U02UECC4H6U,,,Just the queries,1642842028.290300,1642854941.310800,U01AXE0P5M3\\n549575ef-b2f9-49ac-b98c-6744834ed467,,2.0,,i am not able to see anything under tables in pgadmin dashboard,1642855079.311600,1642855079.311600,U02RREQ7MHU\\n85094cab-4e0b-4bce-a6da-32555012b580,U02RREQ7MHU,,,\"No, just map it to a different port (-p 5431:5432)\",1642853448.306600,1642855150.311900,U01AXE0P5M3\\nd8f474d7-b5c2-407d-a44a-ee40a5bd736c,U02RREQ7MHU,,,may be need to create it?,1642855079.311600,1642855353.312200,U02ULQFCXL0\\n79c92b11-52e0-4c33-8eb5-fee6bdbf9563,U02RREQ7MHU,,,\"my postgres is already mapped to 5431 port , so should i map the port of postgres to something else or should i change the port for the pg-database ?\",1642853448.306600,1642855497.312500,U02RREQ7MHU\\n2bb5a3b2-91fd-4934-8a7e-419d41c3d2cb,U02RREQ7MHU,,,\"alright i was having issue with assigning ports, i just fixed it and reinjected the data into the right port in jupyter notebook, its working now\",1642855079.311600,1642855967.312700,U02RREQ7MHU\\nd90d942d-a83f-418f-a39f-d4ae7a365ed2,U02RREQ7MHU,,,alright i sorta fixed it i just assigned my pg-database to 5430 and reinjected the data to 5430 port in jupyter notebook,1642853448.306600,1642856051.313000,U02RREQ7MHU\\n690c97af-a566-447e-a328-06ca882a0031,,,,\"Hello @everyone*!* I see lots of people are having issues with connecting to #*postgres* DB running in the #*docker* container. Just try the following steps from the start\\n1. Stop running containers if there are any and delete postgres:13 image, as well as the mounted folder (ny_taxi_postgres_data)\\ndocker rmi  postgres:13\\ndelete the mounted folder\\n2. Create a new folder and assign a different name (pg_data in this case). Recreate the docker image by running the following code and bind another port to #postgres container  (in case if you have an installed version on your machine, check by running pg_isready)\\n```version: \"\"3.8\"\"\\nservices:\\n  pgdatabase: # host/address\\n    image: postgres:13\\n    environment:\\n      - POSTGRES_USER=test\\n      - POSTGRES_PASSWORD=test\\n      - POSTGRES_DB=ny_taxi \\n    volumes:\\n      - \"\"./pg_data:/var/lib/postgresql/data\"\" # mount newly created folder   \\n    ports:\\n      - \"\"7000:5432\"\" # map another port, in case if you have already installed postgres db locally\\n# test connection\\n#psql --host=localhost --port 7000 --username=test --dbname=ny_taxi```\\n3. Run docker-compose up and test the connection from CLI\\n4. If your connection is stable, it\\'s time to  populate the DB by running the python script (as some people may not have wget installed, just download the .csv file manually and comment out the url param)\\n```python ingest_data.py --user=test --password=test --host=localhost --port=7000 --db=ny_taxi --table_name=yellow_taxi_trips --url=\"\"Nope\"\"```\\n5. You can run PGAdmin in the container as well without changing the Github code. But there is a more simple way to create a convenient working env by installing PGAdmin with Postgres locally from *<https://www.enterprisedb.com/downloads/postgres-postgresql-downloads|EDB>*\\nJust add a new server specifying localhost:7000 as the connection and voila you have a stable working environment! (Just don\\'t forget to up the #postgess container before connecting to ny_taxi DB)\\nHopefully, these steps will be helpful to get you started :v:\",,1642859024.330200,U02UJGGM7K6\\nc7a102b1-2681-4da9-960b-4f6f1914e16a,,1.0,,\"Mac M1 users, Do we need to use Rosetta Terminal to install terraform?\",1642859115.330900,1642859115.330900,U0290EYCA7Q\\n9557a3f6-54ab-4e38-ab1f-4004e8677b9d,U02VBG59VQ9,,,\"Data page checksums are disabled.\\n\\nfixing permissions on existing directory /var/lib/postgresql/data ... ok\\ncreating subdirectories ... ok\\nselecting dynamic shared memory implementation ... posix\\nselecting default max_connections ... 20\\nselecting default shared_buffers ... 400kB\\nselecting default time zone ... Etc/UTC\\ncreating configuration files ... ok\\nrunning bootstrap script ... 2022-01-22 13:50:00.142 UTC [82] FATAL:  data directory \"\"/var/lib/postgresql/\\ndata\"\" has invalid permissions\\n2022-01-22 13:50:00.142 UTC [82] DETAIL:  Permissions should be u=rwx (0700) or u=rwx,g=rx (0750).\\nchild process exited with exit code 1\\ninitdb: removing contents of data directory \"\"/var/lib/postgresql/data\"\"\",1642813718.259500,1642859485.331500,U02VBG59VQ9\\nca4ce777-e0a3-4bcc-ab42-7a6fee875324,U02VBG59VQ9,,,<@U02V18P84BT>,1642813718.259500,1642859519.331700,U02VBG59VQ9\\nb549d811-a7da-429c-a84d-d79b43264cca,,,,\"Please team, how can I resolve this error:\\ndocker: invalid reference format.\\nI\\'m running a windows system.\",,1642860529.334200,U02STT0TPGX\\n11965fc0-232e-4948-8010-80f7f93d4b29,,8.0,,\"Please team, how can I resolve this error:\\ndocker: invalid reference format.\\nI\\'m running a windows system.\",1642860972.334800,1642860972.334800,U02STT0TPGX\\ne0cfedf0-2ec5-4945-9e9f-edbab3f22ad9,U02STT0TPGX,,,\"Think it is looking for the next line, because of backslash.\",1642860972.334800,1642861295.335100,U0290EYCA7Q\\n2c53b20e-a443-4b2c-b8b4-f9f5cc5c89d7,U02STT0TPGX,,,\"Hey <@U02STT0TPGX>\\n\\nI believe you need to specify what you are going to run\",1642860972.334800,1642861301.335300,U02UAFF1WU9\\nc1d7acb6-c6d0-472c-bb1d-81e040c5cb22,U02STT0TPGX,,,\"This is what i\\'m running:\\n\\ndocker run -it \\\\ \\xa0 \\xa0\\n\\xa0 -e POSTGRES_USER=\"\"root\"\" \\\\ \\xa0 \\xa0\\n\\xa0 -e POSTGRES_PASSWORD=\"\"root\"\" \\\\ \\xa0 \\xa0\\n\\xa0 -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\ \\xa0 \\xa0\\n\\xa0 -v c:\\\\Users\\\\AFEX ANALYTICS\\\\My Data Engineering\\\\2_docker_sql\\\\ny_taxi_postgres_data:/var/lib/postgresql/data \\\\ \\xa0 \\xa0\\n\\xa0 -p 5432:5432 \\\\ \\xa0 \\xa0\\n\\xa0 postgres:13\",1642860972.334800,1642861423.335900,U02STT0TPGX\\n985871af-4a3e-44be-9f78-1082230e8ce9,U02STT0TPGX,,,I am running the command on my windows cmd,1642860972.334800,1642861514.336100,U02STT0TPGX\\na2f63715-4eec-4338-8571-efe5dc8409ec,U02U5Q8FK38,,,<@U02T18VH90F> Yes this works perfectly! Thank you !,1642762426.057700,1642861525.336300,U02U5Q8FK38\\n673c9acb-9dd6-47df-afa1-dc3175e4fdb4,U02STT0TPGX,,,\"From Stack Overflow : Change the directory name to not have spaces in there, and it should work if this is the problem.\\n\\n<https://stackoverflow.com/questions/45682010/docker-invalid-reference-format>\",1642860972.334800,1642861559.336500,U0290EYCA7Q\\n11df19da-1401-46a3-ab51-8acc7fb905c4,U02STT0TPGX,,,And I would recommend you to try docker compose.,1642860972.334800,1642861810.337300,U0290EYCA7Q\\ndec2dd1d-58e3-4d88-9c89-b15c637bf446,,6.0,,\"Hi guys, I want to learn more about SQL and want to upload this file and create a database in pgadmin and postgres but it gives me error for some commands and using \"\"`\"\". Anyone has experience?\\nI\\'m following this course: <https://www.youtube.com/watch?v=7S_tz1z_5bA&amp;t=1087s>\",1642862212.339000,1642862212.339000,U030FNZC26L\\ned32de39-bd7c-4489-97d7-bbd1f7935d15,,3.0,,\"I also wrote a blog post for the first week here:\\n<https://kargarisaac.github.io/blog/data%20engineering/jupyter/2022/01/18/data-engineering-w1.html>\",1642862313.339700,1642862313.339700,U030FNZC26L\\n39e65a48-ed31-42ab-8832-e43603bddb42,U030FNZC26L,,,Replace those quotes with single quotes \\'sql...\\',1642862212.339000,1642862352.339900,U02H0GUC7ML\\n70988e68-1ff7-4990-9b8c-00591a050643,U030FNZC26L,,,it doesn\\'t work. Is there any difference between loading this file in mysql and postgres?,1642862212.339000,1642862453.340300,U030FNZC26L\\nb9fbdab0-0f10-490d-999c-e7d29f2457ce,,,,Guys,,1642862978.340800,U02RBKL48KE\\n2d55589e-5b34-4565-915a-521c07cc89bf,,,,\"docker run -it \\\\\\n  -e POSTGRES_USER: \"\"root\"\" \\\\\\n  -e POSTGRES_PASSWORD: \"\"root\"\" \\\\\\n  -e POSTGRES_DB: \"\"ny_taxi\"\" \\\\\\n  -v /home/nurdaulet/Desktop/DE Zoomcamp/ny_taxi_postgres_data: /var/lib/postgresql/data \\\\\\n  -p 5432: 5432 \\\\\\n  postgres:13\",,1642862983.341000,U02RBKL48KE\\n5e5d17ef-6f00-4537-b1fa-6cac525757d2,,,,did i any mistaker? i have errors when i execute this code to cli,,1642863047.341800,U02RBKL48KE\\nd433c434-1784-40ac-ad73-7bd44c687922,,,,Unable to find image \\'root:latest\\' locally,,1642863059.342000,U02RBKL48KE\\nf17a9899-60d7-4fe7-900f-f7ad4d331fc3,,4.0,,-p: command not found too,1642863089.342200,1642863089.342200,U02RBKL48KE\\ne09dd2f2-614a-484d-9abc-b6e9e0d40a91,U02RBKL48KE,,,\"I think your first path in -v command is wrong.\\nDE\\\\ Zoomcamp\\nor maybe put it in quotes\",1642863089.342200,1642863152.342500,U030FNZC26L\\nf9032bb4-52d8-4d51-8949-b9fdf03bab11,U02RBKL48KE,,,the space in your folder name may cause the problem,1642863089.342200,1642863187.343000,U030FNZC26L\\ndc8ff5b9-f319-439b-b317-509a21bddb96,U02STT0TPGX,,,\"I have tried all possible solutions, yet, its not working\",1642860972.334800,1642863188.343200,U02STT0TPGX\\n2b7169d8-f198-4607-8ddb-259128a44bf7,U02RBKL48KE,,,you directory cannot have spaces add _,1642863089.342200,1642863221.343800,U02RREQ7MHU\\n9ebb4a49-84db-4c20-868a-0d6609e1f770,U030FNZC26L,,,\"Try using double quotes then \\'\\'sql\\'\\'.. yes there are very minor syntactical differences between each DB engine we connect, use or query. \\n\\nBut enclosing a table name or database name is not mandatory.\\n\\nYou can use \\n\\ndrop database if exists &lt;name of database&gt;\",1642862212.339000,1642863328.344700,U02H0GUC7ML\\n62f6c9e1-fb3a-4726-b5c5-975c4966e543,U030FNZC26L,,,\"1. Firstly, i will suggest you delete those quote and use a new one.\\n2. Run the query one after the other\",1642862212.339000,1642863357.344900,U02STT0TPGX\\n7b1bc7da-5612-40e9-ac0c-d85bdc5a918c,U030FNZC26L,,,use the quote that is beside the enter button on your keyboard,1642862212.339000,1642863435.345300,U02STT0TPGX\\na376bd1b-b7ce-4b75-8102-807713982fbc,U030FNZC26L,,,thank you guys,1642862212.339000,1642863488.346200,U030FNZC26L\\n9a72e189-884f-4e1a-85e5-79d4263972ef,,6.0,,Same mistake. Please help me!,1642863505.346900,1642863505.346900,U02RBKL48KE\\n3f246836-a014-4d0d-b1bf-09bb7c782d9b,U02RBKL48KE,,,can you type the command instead of copy paste?,1642863505.346900,1642863598.347900,U030FNZC26L\\nf1d9aaef-d4a4-45f6-991d-27615f1e6b68,U02RBKL48KE,,,and also I\\'m not sure if : is correct. I think you have to use =,1642863505.346900,1642863614.348200,U030FNZC26L\\n364a58d2-4957-4024-99f7-48f8692977b7,,4.0,,\"i am getting an error when doing the python ingest_data.py\\n`File \"\"ingest_data.py\"\", line 21`\\n    `engine = create_engine(f\\'postgresql://{user}:{password}@{host}:{port}/{db}\\')`\\n                                                                               `^`\\n`IndentationError: unindent does not match any outer indentation level`\\n\\nmy python code is in reply\",1642863721.349900,1642863721.349900,U02RREQ7MHU\\n2ccd5ac8-8bcd-441d-8569-4539370287bb,U02RBKL48KE,,,\"root@NurdauletDekstop:/home/nurdaulet# docker run -it \\\\\\n&gt;   -e POSTGRES_USER: \"\"root\"\" \\\\\\n&gt;   -e POSTGRES_PASSWORD: \"\"root\"\" \\\\\\n&gt;   -e POSTGRES_DB: \"\"ny_taxi\"\" \\\\\\n&gt;   -v /home/nurdaulet/Desktop/Zoomcamp/ny_taxi_postgres_data: /var/lib/postgresql/data \\\\\\nUnable to find image \\'root:latest\\' locally\\ndocker: Error response from daemon: pull access denied for root, repository does not exist or may require \\'docker login\\': denied: requested access to the resource is denied.\\nSee \\'docker run --help\\'.\\nroot@NurdauletDekstop:/home/nurdaulet#   -p 5432: 5432 \\\\\\n&gt;   postgres:13\\n-p: command not found\",1642863505.346900,1642863727.350000,U02RBKL48KE\\n349f51af-a745-4b7b-9caa-b22c7f3ff3c8,U02RREQ7MHU,,,\"```#!/usr/bin/env python\\n# coding: utf-8\\n\\nimport os\\nimport argparse\\n\\nfrom time import time \\n\\nimport pandas as pd\\nfrom sqlalchemy import create_engine\\n\\n\\ndef main(params): \\n  user = params.user\\n  password = params.password\\n  host = params.host\\n  port = params.port\\n  db = params.db  \\n  table_name = params.table_name\\n  url = params.url\\n  csv_name = \\'output.csv\\' \\n\\n  os.system(f\"\"wget {url} -o {csv_name}\"\")\\n\\n\\n  engine = create_engine(f\\'postgresql://{user}:{password}@{host}:{port}/{db}\\')\\n\\n  df_iter = pd.read_csv(csv_name, iterator=True, chunksize = 100000)\\n\\n  df = next(df_iter)\\n\\n  df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\\n  df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\\n\\n  df.head (n=0).to_sql(name = table_name,con = engine, if_exists = \\'replace\\')\\n\\n  df.to_sql(name = table_name, con=engine, if_exists = \\'append\\')\\n\\n  while True:\\n    t_start = time()\\n    \\n    df = next(df_iter)\\n    df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\\n    df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\\n    \\n    df.to_sql(name = table_name, con = engine, if_exists = \\'append\\')\\n    \\n    t_end = time()\\n    \\n    print (\\'insterted another chunk..., took %.3f second\\' % (t_end - t_start))\\n\\n\\nif __name__ == \\'__main__\\':\\n  parser = argparse.ArgumentParser(description=\\'Ingest CSV data to Postgres\\')\\n\\n\\n  parser.add_argument(\\'--user\\', help=\\'user name for postgres\\')\\n  parser.add_argument(\\'--password\\', help=\\'password for postgres\\')\\n  parser.add_argument(\\'--host\\', help=\\'host for postgres\\')\\n  parser.add_argument(\\'--port\\', help=\\'port for postgres\\')\\n  parser.add_argument(\\'--db\\', help=\\'database name for postgres\\')\\n  parser.add_argument(\\'--table_name\\', help=\\'name of the table where we will write the results to\\')\\n  parser.add_argument(\\'--url\\', help=\\'url of the csv file\\')\\n\\n  args = parser.parse_args()\\n\\n  main(args)```\\n\",1642863721.349900,1642863740.350200,U02RREQ7MHU\\n250ae3a4-ebab-4d8e-a97b-a216a88fb0ba,U02RBKL48KE,,,\"```Try = instead of :\\n\\ndocker run -it \\\\ -\\ne POSTGRES_USER=\"\"root\"\" \\\\ \\n-e POSTGRES_PASSWORD=\"\"root\"\" \\\\ \\n-e POSTGRES_DB=\"\"ny_taxi\"\" \\\\ \\n-v ny_taxi_postgres_data:/var/lib/postgresql/data \\\\ \\n-p 5432:5432 \\\\ \\n--network=pg-network \\\\ \\n--name pg-database \\\\ \\npostgres:13```\",1642863505.346900,1642864005.351700,U0290EYCA7Q\\n801FD7DD-45C5-4C89-832B-110678E3CD3D,,2.0,,\"Just to confirm, for part of the homework, do we need to amend out ingestion script to create 2 tables and populate them both, rather than just 1?\",1642864019.352200,1642864019.352200,U02U34YJ8C8\\n8a50a00c-ab60-4cbe-994d-bb1eabbb4a72,U02RBKL48KE,,,\"I mean in the terminal, not here. It seems it doesn\\'t execute the command all to gether\",1642863505.346900,1642864108.352300,U030FNZC26L\\n7d2ab0c9-838e-49b1-920b-ac228726cac7,,9.0,,\"root@NurdauletDekstop:/home/nurdaulet/Desktop/Zoomcamp# docker run -it \\\\\\n&gt;   -e POSTGRES_USER=\"\"root\"\" \\\\\\n&gt;   -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n&gt;   -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n&gt;   -v \"\"$(pwd)\"\"/ny_taxi_postgres_data: /var/lib/postgresql/data \\\\\\ndocker: invalid reference format.\\nSee \\'docker run --help\\'.\\nroot@NurdauletDekstop:/home/nurdaulet/Desktop/Zoomcamp#   -p 5432: 5432 \\\\\\n&gt;  postgres:13\\n*what is the problem??*\",1642864233.352700,1642864233.352700,U02RBKL48KE\\n6bcae7b0-a75d-407d-b423-c11cb9f9acd5,U02RREQ7MHU,,,There might be spaces mixed in with your tabs. Try any python formatter.,1642863721.349900,1642864262.352800,U0290EYCA7Q\\nabcaf338-d8f4-4320-b6a5-57b3484392bb,U02RREQ7MHU,,,\"I guess you are using pycharm or something else, and it doesn\\'t recognize the indentation the same way as VS Code does, thus it\\'s showing you an indentation error. Try using document autoformatting to see if there are some errors and fix them\",1642863721.349900,1642864271.353000,U02UECC4H6U\\n5e9d78b7-9bb2-4aa7-b99f-9e00bea0126f,U02RBKL48KE,,,Please use the same thread. Don\\'t create new one unnecessarily.,1642864233.352700,1642864583.353300,U0290EYCA7Q\\n0995f293-0528-4815-a787-0f89f76978c0,U02RBKL48KE,,,Did you try without quotes for -v?,1642864233.352700,1642864634.354800,U0290EYCA7Q\\n082C319E-D6ED-4C30-BD60-F23D5F89294B,U02RBKL48KE,,,Sorry. It is because of another problem. It is not the same,1642864233.352700,1642864637.355100,U02RBKL48KE\\nE6D0884A-6427-46DF-8714-B95FAE7F58BC,U02RBKL48KE,,,Its my first time with docket + cli. My os is ubuntu. Could u please write how to correctly do for -v in ubuntu?,1642864233.352700,1642864689.356500,U02RBKL48KE\\nde6d9916-36c7-408d-9ea5-a37462799e91,,18.0,,\"Hi everyone. I have been battling with the following error after running\\n`pip install pgcli` and got a successful message. Bash doest recognise pgcli\",1642864750.357300,1642864750.357300,U02TMEUQ7MY\\neb57e470-6991-43c5-855f-e1e569773732,U02RBKL48KE,,,\"You dont need to put $(pwd) in quotation marks, check how it works in a terminal\",1642864233.352700,1642864782.357600,U02UECC4H6U\\n5e7cf5eb-542e-48d6-86af-2ab447096cf5,U02TMEUQ7MY,,,Can you try install it again with -U option?,1642864750.357300,1642864792.357800,U0290EYCA7Q\\n9f38efec-29d6-4cbb-a847-6293501b9d84,U02RBKL48KE,,,echo ${pwd} or echo $(pwd) - try,1642864233.352700,1642864820.358300,U0290EYCA7Q\\ne5d48cbe-52f4-4d36-a13e-0788b7d12453,U02TMEUQ7MY,,,`pip install -U pgcli`,1642864750.357300,1642864854.358500,U02TMEUQ7MY\\n65abfc1e-ee46-454a-b5f7-1ef963125ba6,U02TMEUQ7MY,,,Yes,1642864750.357300,1642864884.358700,U0290EYCA7Q\\n8d426e71-9450-4f9d-8c17-9a2c3c3fcaca,U02TMEUQ7MY,,,I have tried this earlier but still getting the same error,1642864750.357300,1642864885.358900,U02TMEUQ7MY\\n6efaff14-5891-46c4-9328-a3fc2a075778,U02TMEUQ7MY,,,\"What do you get when you run `which pgcli`?\\n\\nI don\\'t know if `which` command comes with git bash.\",1642864750.357300,1642864931.359300,U0290EYCA7Q\\nc7c9b252-c7fc-4900-a4ba-c8917c561309,U02RBKL48KE,,,\"I can suggest you to run without $(pwd), to get it working. It will create a folder under /var/lib/docker/\\n\\nYou can examine it by running `docker volume inspect &lt;volume-name&gt;`\",1642864233.352700,1642865076.359800,U0290EYCA7Q\\nafca4501-562d-4b6e-905b-ae409ebe502f,U02UECC4H6U,,,\"<@U01AXE0P5M3> sorry for bothering you so much, but does it mean that we have to create some repository, or a folder in an existing one, and put there a text file with sql queries labeled with corresponding tasks?\",1642842028.290300,1642865170.360400,U02UECC4H6U\\n53af88da-d871-43ec-8e6c-5f6268f90b20,,4.0,,\"when i run \"\"docker build -t test:pandas .\"\" am getting the following error\\n\\nGot permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post \"\"<http://%2Fvar%2Frun%2Fdocker.sock/v1.24/build?buildargs=%7B%7D&amp;cachefrom=%5B%5D&amp;cgroupparent=&amp;cpuperiod=0&amp;cpuquota=0&amp;cpusetcpus=&amp;cpusetmems=&amp;cpushares=0&amp;dockerfile=Dockerfile&amp;labels=%7B%7D&amp;memory=0&amp;memswap=0&amp;networkmode=default&amp;rm=1&amp;shmsize=0&amp;t=test%3Apandas&amp;target=&amp;ulimits=null&amp;version=1>\"\": dial unix /var/run/docker.sock: connect: permission denied\",1642865209.361200,1642865209.361200,U02UKLHDWMQ\\n268480ca-a49d-45a5-b140-d8df3121e3ec,U02UKLHDWMQ,,,\"search for \"\"how to run docker without sudo\"\" if you use ubuntu. for other OSs, you can also find the solution\",1642865209.361200,1642865360.361900,U030FNZC26L\\n9c449819-169f-4f2c-9d9c-b52820f35c7c,U02TMEUQ7MY,,,try pgcli --help,1642864750.357300,1642865592.362100,U02RREQ7MHU\\n26402196-de97-4357-ae29-6f861ee79481,U02TMEUQ7MY,,,,1642864750.357300,1642865601.362300,U02TMEUQ7MY\\nd0018844-774e-4559-804e-768b404649ae,U02TMEUQ7MY,,,,1642864750.357300,1642865652.362700,U02TMEUQ7MY\\nf5edb09e-7220-4686-9945-2bcfc005af0d,U02TMEUQ7MY,,,\"It clearly says `pgcli` is not in your path.\\n\\nWhat do you get when you run ?\\n`pip show pgcli`\",1642864750.357300,1642865777.363100,U0290EYCA7Q\\n7cfdb306-7bf2-4a06-b4ab-f719a61e4452,U02TMEUQ7MY,,,or `pip list -v` would do,1642864750.357300,1642865877.363400,U0290EYCA7Q\\ne6cf70b7-526e-4c36-8d79-cb4368d2601a,U02RBKL48KE,,,how to write it without pwd? can u please edit my script and send it back?,1642864233.352700,1642866002.363700,U02RBKL48KE\\nf0bc7192-cac2-49f9-8eda-2c9afd5e3218,U02UKLHDWMQ,,,<https://github.com/sindresorhus/guides/blob/main/docker-without-sudo.md|https://github.com/sindresorhus/guides/blob/main/docker-without-sudo.md>,1642865209.361200,1642866020.363900,U02QP6JM83U\\ncd7b77bc-5d9a-45cd-94a5-5e7039cb96e2,U02RBKL48KE,,,\"```docker run -it \\\\ \\n-e POSTGRES_USER=\"\"root\"\" \\\\ \\n-e POSTGRES_PASSWORD=\"\"root\"\" \\\\ \\n-e POSTGRES_DB=\"\"ny_taxi\"\" \\\\ \\n-v ny_taxi_postgres_data:/var/lib/postgresql/data \\\\ \\n-p 5432:5432 \\\\ \\n--name pg \\\\ \\npostgres:13```\\nThis should create a container named pg, and volume ny_taxi_postgres_data (not in the current directory).\",1642864233.352700,1642866081.364300,U0290EYCA7Q\\n2762d5cd-232c-471b-8f66-8eb25d32cbf7,U02UKLHDWMQ,,,don`t forget to apply things in IMPORTANT block in link above,1642865209.361200,1642866098.364500,U02QP6JM83U\\n99582BB2-039D-415E-B81A-B85AF387D6E7,U02U34YJ8C8,,,\"<@U02U5SW982W> you\\'re not alone. I think we\\'re all dedicating a lot of hours to this. I\\'ve found it\\'s helping to document my progress on GitHub, saying what I did, errors I got etc. It provides a good reference point and something I can read over if I\\'ve forgotten what I\\'ve done. It is slow progress for me too\",1642810460.254700,1642867863.369300,U02U34YJ8C8\\n9026dcd0-3002-445e-bfb3-3282ac5af431,,6.0,,\"Guys regarding the error returned from the while loop, I think adding try and except (with break) can help right.. See SS under thread.\",1642867882.369800,1642867882.369800,U02AX5NC5B6\\n1fd12b88-d666-4b4c-9eee-2521998d846a,U02AX5NC5B6,,,\"Didn\\'t test it yet, as it takes some time to download the data.\",1642867882.369800,1642867912.369900,U02AX5NC5B6\\nb475835e-22d0-4136-aec7-75f1d1773f7d,U02AX5NC5B6,,,\"Yes this definitely works, I just print \"\"Batch processing complete\"\" in the except block :thumbsup:\",1642867882.369800,1642867970.370300,U02SUH9N1FH\\nef086723-f877-4f7a-ba48-fbf63b998407,U02STT0TPGX,,,\"Thanks to every one that contribute to this question.\\n\\nWhat i use to solve this issue is that:\\n1. I inserted my path in a double quote \"\"C:\\\\my path location\"\"\\n2. I make sure all the command is on a strainght line.\\n3. I removed the backward slash \\'\\\\\\' from command and make sure that there is 4 space inbetween the command\\ndocker run -it    -e POSTGRES_USER=\"\"root\"\"    -e POSTGRES_PASSWORD=\"\"root\"\"    -e POSTGRES_DB=\"\"ny_taxi\"\"    -v \"\"C:\\\\Users\\\\AFEX ANALYTICS\\\\My Data Engineering\\\\2_docker_sql\\\\my_taxi_postgres_data\"\":/var/lib/postgresql/data    -p 5432:5431    postgres:13\",1642860972.334800,1642868036.370500,U02STT0TPGX\\ne66a1f64-1aec-49d7-87d5-327bc133d6aa,U02AX5NC5B6,,,You can be even more specific and say except StopIteration,1642867882.369800,1642868084.370700,U02SUH9N1FH\\n1cc9c65b-977f-48da-b160-715c1c67e687,U02UZBJ2Q6L,,,<@U02TQUYTBJA> you save me too :sweat_smile:. big thanks,1642766045.069100,1642868112.370900,U02UUT2J7GQ\\n430ec11a-430f-4aeb-9bb8-78ac8cfbd22c,U02AX5NC5B6,,,\"```    for chunk in df_iterator:\\n        df = chunk\\n        df[\"\"tpep_pickup_datetime\"\"] = pd.to_datetime(df.tpep_pickup_datetime)\\n        df[\"\"tpep_dropoff_datetime\"\"] = pd.to_datetime(df.tpep_dropoff_datetime)\\n        df.to_sql(name = table_name, con = engine ,if_exists= \\'append\\')\\n        print(\"\"...chunk is added to the database\"\")```\\nThis is how I fix the while loop issue\",1642867882.369800,1642868212.371100,U0297ANJF6F\\nb40b0463-e36d-465c-bef7-d2d35383ea7a,U02BVP1QTQF,,,\"I don’t think so. I did most of my notes using the audio-only workshop and I was listening to bits of the audio then pausing and figuring out things. To be honest I probably should watch/listen to everything first in a single sitting and then sit down and write notes, but I get impatient and start pausing and writing down things as soon as a concept becomes clear enough. That also means that I don’t end up remembering most of the video structure and end up remembering my notes mostly, so I’d have to watch the video again to see if there was something missing that I think should be added.\\n\\nI mentioned the bit about the resource block because it was probably a mix of me not listening properly or forgetting something that was said before (I listen to the videos slightly sped up most of the time and sometimes I miss small but important details) and then checking the Terraform docs but not quite understanding their explanation, so it took me much more time that I expected.\",1642713863.457200,1642868355.371600,U02BVP1QTQF\\nfc24c2be-42a4-4580-bbda-f0d7aedfe215,U02TJ69RKT5,,,I got the same error too and received the same amount of rows. It seems expected since the code is not handle exception,1642638867.256500,1642868476.371900,U02QBJYQFK9\\n4bb2cd40-9675-430b-8b7a-da0242590e7a,U02U34YJ8C8,,,<@U02U34YJ8C8> exactly what <@U02GVGA5F9Q> said: it’s completely optional and meant to make it easier to have a working environment in case difficulties showed up in your local setup.,1642810460.254700,1642868485.372100,U02BVP1QTQF\\ne755f8f0-e395-467a-bc95-767736eb5b75,U02U34YJ8C8,,,\"According to the SQL refresher video, you should have 2 tables: one for the trips and another for the zones. How you populate the second table is up to you (I personally used a modified notebook, similar to what Alexey showed in the video)\",1642864019.352200,1642868604.372400,U02BVP1QTQF\\n787e4baf-3b5e-4371-8c20-4c1a14266ae9,U02TMEUQ7MY,,,,1642864750.357300,1642868689.373300,U02TMEUQ7MY\\n6bfb46dc-d181-4e2f-9ca2-13810cca0231,U02TMEUQ7MY,,,It showed,1642864750.357300,1642868694.373800,U02TMEUQ7MY\\naf236aec-52d1-45b4-b54c-882298313b4d,,12.0,,\"getting this error when i run python ingest_data.py\\n``` python ingest_data.py \\\\\\n&gt;\\n--2022-01-22 21:50:04--  <http://none/>\\nResolving none (none)... failed: No such host is known. .\\nwget: unable to resolve host address \\'none\\'\\nTraceback (most recent call last):\\n  File \"\"ingest_data.py\"\", line 67, in &lt;module&gt;\\n    main(args)\\n  File \"\"ingest_data.py\"\", line 26, in main\\n    engine = create_engine(f\\'postgresql://{user}:{password}@{host}:{port}/{db}\\')\\n  File \"\"&lt;string&gt;\"\", line 2, in create_engine\\n  File \"\"C:\\\\Users\\\\abhis\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python36-32\\\\lib\\\\site-packages\\\\sqlalchemy\\\\util\\\\deprecations.py\"\", line 309, in warned\\n    return fn(*args, **kwargs)\\n  File \"\"C:\\\\Users\\\\abhis\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python36-32\\\\lib\\\\site-packages\\\\sqlalchemy\\\\engine\\\\create.py\"\", line 530, in create_engine\\n    u = _url.make_url(url)\\n  File \"\"C:\\\\Users\\\\abhis\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python36-32\\\\lib\\\\site-packages\\\\sqlalchemy\\\\engine\\\\url.py\"\", line 731, in make_url\\n    return _parse_rfc1738_args(name_or_url)\\n  File \"\"C:\\\\Users\\\\abhis\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python36-32\\\\lib\\\\site-packages\\\\sqlalchemy\\\\engine\\\\url.py\"\", line 787, in _parse_rfc1738_args\\n    components[\"\"port\"\"] = int(components[\"\"port\"\"])\\nValueError: invalid literal for int() with base 10: \\'None\\'```\",1642868754.374000,1642868754.374000,U02RREQ7MHU\\n7f91c984-5a8f-4698-aec3-9004e0601acb,U02RREQ7MHU,,,\"thanks for the help, the problem was a syntax error in the line  `os.system(f\"\"wget {url} -o {csv_name}\"\")`  o was supposed to be capital\",1642863721.349900,1642868862.374100,U02RREQ7MHU\\ndcfef8d0-faad-4cb0-b31b-03e0a04e7b9e,U02TMEUQ7MY,,,This also worked,1642864750.357300,1642868864.374300,U02TMEUQ7MY\\n9fb2c429-d47b-4601-bea0-e47691b34a76,U02TMEUQ7MY,,,How do I add it to my path?,1642864750.357300,1642868900.374800,U02TMEUQ7MY\\n3cd37225-0cfd-4468-bde0-d263cf508f6f,U02RREQ7MHU,,,Did you print wget string before os command??,1642868754.374000,1642869273.378700,U0290EYCA7Q\\n0139fb2b-c53a-4702-9979-a5d742cb17df,,,,\"I getting error when try to query data from pgcli. It return result but got error that make me to reconnect pgcli every command can anyone help me :pray:\\n```root@localhost:ny_taxi&gt; \\\\d yellow_taxi_data\\n+-----------------------+-----------------------------+-----------+\\n| Column                | Type                        | Modifiers |\\n|-----------------------+-----------------------------+-----------|\\n| index                 | bigint                      |           |\\n| VendorID              | bigint                      |           |\\n| tpep_pickup_datetime  | timestamp without time zone |           |\\n| tpep_dropoff_datetime | timestamp without time zone |           |\\n| passenger_count       | bigint                      |           |\\n| trip_distance         | double precision            |           |\\n| RatecodeID            | bigint                      |           |\\n| store_and_fwd_flag    | text                        |           |\\n| PULocationID          | bigint                      |           |\\n| DOLocationID          | bigint                      |           |\\n| payment_type          | bigint                      |           |\\n| fare_amount           | double precision            |           |\\n| extra                 | double precision            |           |\\n| mta_tax               | double precision            |           |\\n| tip_amount            | double precision            |           |\\n| tolls_amount          | double precision            |           |\\n| improvement_surcharge | double precision            |           |\\n| total_amount          | double precision            |           |\\n| congestion_surcharge  | double precision            |           |\\n+-----------------------+-----------------------------+-----------+\\nIndexes:\\n    \"\"ix_yellow_taxi_data_index\"\" btree (index)\\n\\nTraceback (most recent call last):\\n  File \"\"c:\\\\users\\\\jiraw\\\\anaconda3\\\\lib\\\\runpy.py\"\", line 194, in _run_module_as_main\\n    return _run_code(code, main_globals, None,\\n  File \"\"c:\\\\users\\\\jiraw\\\\anaconda3\\\\lib\\\\runpy.py\"\", line 87, in _run_code\\n    exec(code, run_globals)\\n  File \"\"C:\\\\Users\\\\jiraw\\\\anaconda3\\\\Scripts\\\\pgcli.exe\\\\__main__.py\"\", line 7, in &lt;module&gt;\\n  File \"\"c:\\\\users\\\\jiraw\\\\anaconda3\\\\lib\\\\site-packages\\\\click\\\\core.py\"\", line 1137, in __call__\\n    return self.main(*args, **kwargs)\\n  File \"\"c:\\\\users\\\\jiraw\\\\anaconda3\\\\lib\\\\site-packages\\\\click\\\\core.py\"\", line 1062, in main\\n    rv = self.invoke(ctx)\\n  File \"\"c:\\\\users\\\\jiraw\\\\anaconda3\\\\lib\\\\site-packages\\\\click\\\\core.py\"\", line 1404, in invoke\\n    return ctx.invoke(self.callback, **ctx.params)\\n  File \"\"c:\\\\users\\\\jiraw\\\\anaconda3\\\\lib\\\\site-packages\\\\click\\\\core.py\"\", line 763, in invoke\\n    return __callback(*args, **kwargs)\\n  File \"\"c:\\\\users\\\\jiraw\\\\anaconda3\\\\lib\\\\site-packages\\\\pgcli\\\\main.py\"\", line 1377, in cli\\n    pgcli.run_cli()\\n  File \"\"c:\\\\users\\\\jiraw\\\\anaconda3\\\\lib\\\\site-packages\\\\pgcli\\\\main.py\"\", line 776, in run_cli\\n    self.handle_watch_command(text)\\n  File \"\"c:\\\\users\\\\jiraw\\\\anaconda3\\\\lib\\\\site-packages\\\\pgcli\\\\main.py\"\", line 814, in handle_watch_command\\n    query = self.execute_command(text)\\n  File \"\"c:\\\\users\\\\jiraw\\\\anaconda3\\\\lib\\\\site-packages\\\\pgcli\\\\main.py\"\", line 711, in execute_command\\n    self.echo_via_pager(\"\"\\\\n\"\".join(output))\\n  File \"\"c:\\\\users\\\\jiraw\\\\anaconda3\\\\lib\\\\site-packages\\\\pgcli\\\\main.py\"\", line 1123, in echo_via_pager\\n    click.echo_via_pager(text, color=color)\\n  File \"\"c:\\\\users\\\\jiraw\\\\anaconda3\\\\lib\\\\site-packages\\\\click\\\\termui.py\"\", line 302, in echo_via_pager\\n    return pager(itertools.chain(text_generator, \"\"\\\\n\"\"), color)\\n  File \"\"c:\\\\users\\\\jiraw\\\\anaconda3\\\\lib\\\\site-packages\\\\click\\\\_termui_impl.py\"\", line 357, in pager\\n    return _tempfilepager(generator, \"\"more &lt;\"\", color)\\n  File \"\"c:\\\\users\\\\jiraw\\\\anaconda3\\\\lib\\\\site-packages\\\\click\\\\_termui_impl.py\"\", line 440, in _tempfilepager\\n    os.unlink(filename)\\nPermissionError: [WinError 32] The process cannot access the file because it is being used by another process: \\'C:\\\\\\\\Users\\\\\\\\jiraw\\\\\\\\AppData\\\\\\\\Local\\\\\\\\Temp\\\\\\\\tmp2pf_y_ep\\'```\",,1642869366.379900,U02U8SRCHU6\\nb591d3b1-0c28-4ca1-812c-9f083a3853f2,U02RREQ7MHU,,,Think u didnt assign the url.,1642868754.374000,1642869374.380000,U0290EYCA7Q\\n2a2f4731-6abb-4aef-8066-d0d38fad3e52,U02RREQ7MHU,,,\"```URL=\"\"<https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>\"\"\\n\\npython ingest_data.py \\\\\\n  --user=root \\\\\\n  --password=root \\\\\\n  --host=localhost \\\\\\n  --port=5432 \\\\\\n  --db=ny_taxi \\\\\\n  --table_name=yellow_taxi_trips \\\\\\n  --url=${URL}```\",1642868754.374000,1642869397.380600,U0290EYCA7Q\\nCA310247-FA52-4036-8F2B-18F716FD4971,U02BVP1QTQF,,,\"<@U01DHB2HS3X> I found this old <https://towardsdatascience.com/sql-in-a-nutshell-part-1-basic-real-world-scenarios-33a25ba8d220|“SQL in a Nutshell”>post on TDS from you which I thought was excellent. It covers most of what <@U01AXE0P5M3> went through on the SQL refresher 1.2.6 video. Maybe you guys want to recycle it as I haven\\'t seen any notes on GitHub for that video :grin:\\n\",1642713863.457200,1642869755.383300,U02UX664K5E\\na69750ab-eba1-4945-90c9-57a2ce20245c,U02AX5NC5B6,,,nice thanks guys!,1642867882.369800,1642869779.383600,U02AX5NC5B6\\n1432d1a3-62b6-4dd6-a93f-63b2f1585454,,12.0,,I cannot ingest data into postgres. Has anyone faced the issue?,1642869972.384700,1642869972.384700,U02QELRUA3X\\n694bc411-df24-45d2-bc68-08c4b5c30a5e,U02QELRUA3X,,,can you confirm if the container is running? `docker ps`,1642869972.384700,1642870030.385000,U0290EYCA7Q\\n1dfc4f68-c091-480d-8eaa-d3fdcf2dc0fd,U02AX5NC5B6,,,\"just tested it when Alexey showed how to download data from local machine\\'s local host, that hack is so cool!\",1642867882.369800,1642870067.385200,U02AX5NC5B6\\n9d0ade0c-a9e0-44d3-9bd1-79b7dec6574b,U02RREQ7MHU,,,\"<@U0290EYCA7Q> i did assign the URL\\n```URL=\"\"<https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>\"\"\\n\\npython ingest_data.py \\\\\\n\\n  --user = root \\\\\\n  --password = admin \\\\\\n  --host = localhost \\\\\\n  --port = 5430 \\\\\\n  --db = ny_taxi \\\\\\n  --table_name = yellow_taxi_data \\\\ \\n  --url = ${URL}```\",1642868754.374000,1642870183.385900,U02RREQ7MHU\\naad2cdb4-376f-42d9-8699-5e3d1b04b68c,U02RREQ7MHU,,,\"also here\\'s my python code\\n```#!/usr/bin/env python\\n# coding: utf-8\\n\\nimport os\\nimport argparse\\n\\nfrom time import time\\n\\nimport pandas as pd\\nfrom sqlalchemy import create_engine\\n\\n\\ndef main(params): \\n    user = params.user\\n    password = params.password\\n    host = params.host\\n    port = params.port\\n    db = params.db\\n    table_name = params.table_name\\n    url = params.url\\n    csv_name = \\'output.csv\\'\\n\\n    os.system(f\"\"wget {url} -O {csv_name}\"\")\\n\\n\\n    engine = create_engine(f\\'postgresql://{user}:{password}@{host}:{port}/{db}\\')\\n\\n    df_iter = pd.read_csv(csv_name, iterator=True, chunksize = 100000)\\n\\n    df = next(df_iter)\\n\\n    df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\\n    df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\\n\\n    df.head (n=0).to_sql(name = table_name,con = engine, if_exists = \\'replace\\')\\n\\n    df.to_sql(name = table_name, con=engine, if_exists = \\'append\\')\\n\\n    while True:\\n        t_start = time()\\n    \\n        df = next(df_iter)\\n        df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\\n        df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\\n    \\n        df.to_sql(name = table_name, con = engine, if_exists=\\'append\\')\\n    \\n        t_end = time()\\n    \\n        print (\\'insterted another chunk..., took %.3f second\\' % (t_end - t_start))\\n\\n\\nif __name__ == \\'__main__\\':\\n  parser = argparse.ArgumentParser(description=\\'Ingest CSV data to Postgres\\')\\n\\n  parser.add_argument(\\'--user\\', help=\\'user name for postgres\\')\\n  parser.add_argument(\\'--password\\', help=\\'password for postgres\\')\\n  parser.add_argument(\\'--host\\', help=\\'host for postgres\\')\\n  parser.add_argument(\\'--port\\', help=\\'port for postgres\\')\\n  parser.add_argument(\\'--db\\', help=\\'database name for postgres\\')\\n  parser.add_argument(\\'--table_name\\', help=\\'name of the table where we will write the results to\\')\\n  parser.add_argument(\\'--url\\', help=\\'url of the csv file\\')\\n\\n  args = parser.parse_args()\\n\\n  main(args)```\\n\",1642868754.374000,1642870212.386100,U02RREQ7MHU\\n5c75f413-a9ea-4145-80e0-451c3487dd40,U02QELRUA3X,,,\"<@U0290EYCA7Q> , both pgadmin and postgres are working. Moreover, pgadmin see database \"\"ny_taxi\"\"\",1642869972.384700,1642870471.387000,U02QELRUA3X\\n8b6bb41c-f3c7-41fc-91bb-83e8f3964cdd,U02QELRUA3X,,,what about the network?,1642869972.384700,1642870502.387400,U030FNZC26L\\nc3c97ef9-57d5-4229-ae0b-58a409dc0b61,U02QELRUA3X,,,\"which commands did you use for pgadmin, postgres, and your ingestion data pipeline?\",1642869972.384700,1642870534.388100,U030FNZC26L\\n258fd9d8-c705-44d2-b59a-c6a6ca764dcb,U02RREQ7MHU,,,\"Don\\'t think if this matters, but can you remove space before and after =, in the arguments?\",1642868754.374000,1642870587.389000,U0290EYCA7Q\\nb6873547-68be-4e64-ab72-12f0be892bdb,U02RREQ7MHU,,,\"In Unix, we don\\'t add space usually inside shell script.\",1642868754.374000,1642870615.389400,U0290EYCA7Q\\n3b6bdbc4-03d6-4872-9edd-8cc8eae7a726,U02QELRUA3X,,,<@U030FNZC26L> docker-compose up,1642869972.384700,1642870714.391100,U02QELRUA3X\\n6c348b88-4363-4ff4-92f4-eb29abe03f1b,,7.0,,\"I have a problem in lesson _*1.2.2 Ingesting NY Taxi Data to Postgres*_:\\n```winpty docker run -it \\\\\\n-e POSTGRES_USER: \"\"root\"\" \\\\\\n-e POSTGRES_PASSWORD: \"\"root\"\" \\\\\\n-e POSTGRES_DB: \"\"ny_taxi\"\" \\\\\\n-v f:/Omar/Data_Engineering/Zoomcamp/Week_1/docker/ny_taaxi_postgres_data: /var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\\\npostgres:13```\\nI use the above commands trying to run the postgres container following the _*Postgres command line for Docker.*_ For some reason it\\'s giving me this error: Unable to find image \\'root:latest\\' locally. When i try just:\\n```winpty docker run -it postgres:13```\\nit works fine but tells me that i didn\\'t provide user &amp; pw as env variables. Anybody know how to solve this?\",1642870717.391300,1642870717.391300,U02TPTXFVQ9\\n4c29830a-7365-4ee2-94f2-f580e4bf1fea,U02RREQ7MHU,,,\"```(data-engineering-zoomcamp)% URL=\"\"<https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>\"\"\\n\\npython ingest_data.py \\\\\\n\\n  --user = root \\\\\\n  --password = admin \\\\\\n  --host = localhost \\\\\\n  --port = 5430 \\\\\\n  --db = ny_taxi \\\\\\n  --table_name = yellow_taxi_data \\\\\\n  --url = ${URL}\\n\\n\\ncurl None --output output.csv\\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\\n                                 Dload  Upload   Total   Spent    Left  Speed\\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (6) Could not resolve host: None```\\nSpace is the issue.\",1642868754.374000,1642870724.391400,U0290EYCA7Q\\n8a807a5f-e015-48e9-bc06-25a9c7df77c8,U02QELRUA3X,,,what about your data ingestion pipeline?,1642869972.384700,1642870768.391800,U030FNZC26L\\n02057989-c9ba-4e59-984e-39f4e5c3a65c,U02QELRUA3X,,,\"<@U030FNZC26L> and python file called \"\"upload_data.py\"\"\",1642869972.384700,1642870770.392000,U02QELRUA3X\\nc16fb43f-111a-4668-b6a3-e7bf7fd89ac4,U02RREQ7MHU,,,i\\'ll try and remove all the unnecessary spaces,1642868754.374000,1642870853.392400,U02RREQ7MHU\\n2eae4501-881c-46b0-8026-a492bfe5c297,U02QELRUA3X,,,<https://kargarisaac.github.io/blog/data%20engineering/jupyter/2022/01/18/data-engineering-w1.html#:~:text=Note%20that%20the%20network%20created%20automatically%20by%20docker%20compose%20has%20changed%20(2_docker_sql_default)%20and%20also%20the%20host%20name%20(pgdatabase)>.,1642869972.384700,1642870853.392600,U030FNZC26L\\nc1945a95-3821-4e33-acbb-4b48a2f8d5a4,U02TPTXFVQ9,,,put this within space and try : f:/Omar/Data_Engineering/Zoomcamp/Week_1/docker/ny_taaxi_postgres_data,1642870717.391300,1642870874.392900,U0290EYCA7Q\\need41b07-8b12-40b3-b8da-53960571043b,U02QELRUA3X,,,can you test it with docker?,1642869972.384700,1642870890.393100,U030FNZC26L\\n6d1eb95b-2d10-4da8-8cac-0a3e8dc8d900,U02RREQ7MHU,,,\"still getting the same error with this also\\n```python ingest_data.py --user=root --password=admin --host=localhost --port=5430 --db=ny_taxi --table_name=yellow_taxi_trips --url=${url}```\",1642868754.374000,1642870953.393300,U02RREQ7MHU\\n7c5a66fa-e65d-4a9f-bfb1-7d322c59330f,U02QELRUA3X,,,\"<@U030FNZC26L> you mean using docker exec -it &lt;image_name&gt; bash, and inside docker container connect to database?\",1642869972.384700,1642871030.393500,U02QELRUA3X\\n39590c32-07ca-4bfb-902a-2cf9eb9dc69a,U02QELRUA3X,,,\"Can You try connection string like this? just trial and error\\n\\n```conn = psycopg2.connect(\"\"dbname=test user=postgres password=secret\"\")```\\n\",1642869972.384700,1642871254.393700,U0290EYCA7Q\\n8dd14180-7b3b-4e5a-970b-6cebb8bdb895,U02RREQ7MHU,,,\"<@U0290EYCA7Q> OMG thank you, putting the url in the python file in  the right spot worked\",1642868754.374000,1642871382.394100,U02RREQ7MHU\\n52b2390b-357e-4e87-8a33-ae83d46f8fe2,U02QELRUA3X,,,\"yes, and set the network. `docker run -it ...`\\nsimilar to the video.\",1642869972.384700,1642871398.394400,U030FNZC26L\\n7894b270-c683-4768-966a-84a272b18e3e,U02TPTXFVQ9,,,\"if you mean double quotes : \"\"f:/Omar/Data_Engineering/Zoomcamp/Week_1/docker/ny_taaxi_postgres_data\"\"\\nI tried that and didn\\'t work, if you mean just adding spaces before and after it, it didn\\'t work too :disappointed:\",1642870717.391300,1642871688.397900,U02TPTXFVQ9\\n8dd904e9-38b3-49cd-b632-b12e0413a256,,4.0,,\"Some useful commands that have helped me resolve some issues:\\n\\n`docker ps` - Lists running containers\\n`docker ps -a` - Lists all containers, including stopped ones\\n`docker stop &lt;container id&gt;` - Stops a running container\\n`docker rm &lt;container id&gt;` - Removes a stopped container\\n`docker rm $(docker ps --filter status=exited -q)` - Removes all stopped containers\\n`docker network ls` - Lists all running networks\\n`docker network rm &lt;network id&gt;` - Removes a network\\n\\nIt actually wasn’t until I stopped all my containers, removed them, and removed networks I had created (starting fresh) that I realised I had some issues in my code that needed fixing.\",1642871725.398200,1642871725.398200,U02U34YJ8C8\\ndca33f49-c980-47a3-aae4-2fbc7c7e0867,U02TPTXFVQ9,,,\"```docker run -it \\\\\\n  -e POSTGRES_USER=\"\"root\"\" \\\\\\n  -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n  -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n  -v ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n  -p 5432:5432 \\\\\\n  --network=pg-network \\\\\\n  --name pg-database \\\\\\n  postgres:13```\\nI used =\",1642870717.391300,1642871868.398300,U0290EYCA7Q\\nc47f4091-00fd-49d6-8606-0b9fabe9ee18,U02TPTXFVQ9,,,\"not working for me, i even tried just:\\n```winpty docker run -it \\\\\\n-e \"\"POSTGRES_USER\"\": \"\"root\"\" \\\\\\n-e \"\"POSTGRES_PASSWORD\"\": \"\"root\"\" \\\\\\n-e \"\"POSTGRES_DB\"\": \"\"ny_taxi\"\" \\\\\\npostgres:13```\\nand still have the same error.\",1642870717.391300,1642872249.401900,U02TPTXFVQ9\\n003C51A6-6BD5-4852-B8E9-E49BAE28AAC2,,8.0,,\"hi! I am stuck with running `$ docker build -t test:pandas .` command. I get the `error checking context` constantly. the dockerfile is saved in VS Code, and the directory should be the right one.. please help!\",1642872315.403500,1642872315.403500,U02DY0L6PHV\\nc536061a-cbe9-47ae-b474-80707472d569,U02TPTXFVQ9,,,\"put it in a single line, and remove all spaces. Use = instead of :\\n\\n```docker run -it -e POSTGRES_USER=\"\"root\"\" -e POSTGRES_PASSWORD=\"\"root\"\" -e POSTGRES_DB=\"\"ny_taxi\"\" -v ny_taxi_postgres_data:/var/lib/postgresql/data -p 5432:5432 --network=pg-network --name pg-database postgres:13```\\n\",1642870717.391300,1642872409.403600,U0290EYCA7Q\\n98dda8b8-e413-4424-81ee-7ea3b595830a,U02TMEUQ7MY,,,\"Please get the bin path from pip, and try `export PATH=&lt;pip path&gt;/bin:$PATH`\",1642864750.357300,1642872516.404100,U0290EYCA7Q\\neb4f1422-7c85-47b9-a27f-7d8bb5befbd3,U02DY0L6PHV,,,Are you running docker from the directory where Dockerfile resides?,1642872315.403500,1642872622.404600,U0290EYCA7Q\\n4F676F78-5A43-49D3-9297-BE6174BF2535,U02DY0L6PHV,,,I do,1642872315.403500,1642873226.405200,U02DY0L6PHV\\nd8056f7c-8d8a-495d-8945-eb05780ec66f,U02RBKL48KE,,,\"hi, guys, i still face the same problem ? do anyone solve it ? please help!\",1642863505.346900,1642873269.405400,U02RHT0M3M5\\ndce8af77-fa0f-49c5-9a51-03c26a6dcb70,,7.0,,\"Hello all\\ni created my container using docker compose\\nbut i couldn’t get the dataset on postgresql\\n\\ni get this error\\n\\nFATAL:  database “ny_taxi” does not exist\\n\\n(Background on this error at: <https://sqlalche.me/e/14/e3q8>)\",1642873331.407000,1642873331.407000,U02QKMCV39R\\nC91A6331-214E-4C6C-A930-F2200EEE4666,U02DY0L6PHV,,,Maybe post a screenshot of the whole error and command run - as well as the content of your Dockerfile ,1642872315.403500,1642873394.407600,U02U34YJ8C8\\n77a5d12d-ff76-49e3-8ac0-346a1269ae70,,,,,,1642873483.407800,U02QKMCV39R\\nbc6fa787-0774-4a54-ab7c-0890da51a58b,U02BVP1QTQF,,,Oh yes we need to link this article!,1642713863.457200,1642873515.408200,U01AXE0P5M3\\n789F28F8-6AC5-4FB2-83E0-DE010A32069C,U02QKMCV39R,,,\"What command are you running, and what command did you run to generate the postgresql container?\",1642873331.407000,1642873615.409700,U02U34YJ8C8\\n18C54687-A195-4635-8736-10A0992ABAC3,U02U34YJ8C8,,,Thanks just about to watch the video. I\\'ll probably do what you\\'ve done as well,1642864019.352200,1642873705.411400,U02U34YJ8C8\\nc2942cc2-f5ab-4cca-85ee-f62e2abd2566,,17.0,,Hello please i need help getting past this error . Mac OS M1,1642873732.412000,1642873732.412000,U02U5G0EKEH\\n60d21dc5-b22d-4911-aaed-fa570c8e534a,U030FNZC26L,,,\"Amazing, please create a PR!\",1642862313.339700,1642873750.412300,U01AXE0P5M3\\nf753957c-c3c4-44e5-81c1-70e3a0c7b827,U02RBKL48KE,,,\"Also it should be =, not colon\",1642863089.342200,1642873781.412500,U01AXE0P5M3\\n000f25ff-906c-4bbf-b831-21fa02c30260,,,,<http://main.tf|main.tf>,,1642873908.413100,U02U5G0EKEH\\n79ca89ad-4dad-4bb1-961a-b8ae007faab1,U02U5G0EKEH,,,Running on Rosetta Terminal?,1642873732.412000,1642873967.414100,U0290EYCA7Q\\n1177f0ae-6bff-4245-8563-7de5a72cb154,,14.0,,\"I still can\\'t access Postgres using pgcli, can\\'t figure out why.\\n\\nI\\'ve tried a bunch of things, commands with --password, changing password, full docker rebuild and build again, deleted everything from zero and started over, even not using a password, always the same error.\\n\\nAny ideas?\",1642874048.415600,1642874048.415600,U02TC704A3F\\na3e8c81a-3b43-48ba-98ca-8af35358f7a8,U02U5G0EKEH,,,iTerm,1642873732.412000,1642874067.415900,U02U5G0EKEH\\n7e51f2cd-ee73-4ad7-a7db-93877ffc7158,U02U5G0EKEH,,,\"I haven\\'t started terraform yet. I was able to install via rosetta terminal.  Will check, and let you know if I face the issue.\\n\\n```(data-engineering-zoomcamp)% terraform -version\\nTerraform v1.1.4\\non darwin_amd64```\\n<https://www.notion.so/Run-x86-Apps-including-homebrew-in-the-Terminal-on-Apple-Silicon-8350b43d97de4ce690f283277e958602>\",1642873732.412000,1642874211.416100,U0290EYCA7Q\\nd9ec4b7f-7c84-46e5-852b-ffb4081d1759,U02TC704A3F,,,\"Looking at the first command, I don\\'t think you\\'ve mapped the port 5432 to your host. please add `-p 5432:5432`  to your docker run command and try again.\",1642874048.415600,1642874415.416600,U02S8K9JBD0\\n820241db-bf7a-4f6e-824c-52aac303fe08,U02QKMCV39R,,,\"i used this\\npgcli -h localhost -p 5432 -u root -d ny_taxi\",1642873331.407000,1642874439.416800,U02QKMCV39R\\n70a46a45-e1e4-4e41-92bf-f10be1dc72ae,U02QKMCV39R,,,\"docker run -it \\\\  -e POSTGRES_USER=“root” \\\\\\n  -e POSTGRES_PASSWORD=“root” \\\\\\n  -e POSTGRES_DB=“ny_taxi” \\\\\\n  -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n  -p 5432:5432 \\\\\\n  postgres:13\",1642873331.407000,1642874486.417000,U02QKMCV39R\\n57203f47-c132-46db-85b5-fe1c9abdcb2e,U02QKMCV39R,,,I used the last i sent to get the container running,1642873331.407000,1642874508.417200,U02QKMCV39R\\n64477d6e-dcf0-4f1f-a983-d341adc3db33,U02TC704A3F,,,\"Already did that too. Also, the port is already 5432 in both sides. You can declare if you need it but after all my attempts I realized that even if I do not map it chooses this port automatically.\",1642874048.415600,1642874788.417600,U02TC704A3F\\n27ed3050-213f-4ad1-af8f-8b82285c343c,U02TC704A3F,,,\"setting the password as admin worked for me, also maybe try a different port like `-p 5431:5432`\",1642874048.415600,1642874875.418100,U02RREQ7MHU\\n622115e9-fb03-4e48-a0c2-e57f64bad5e9,U02TC704A3F,,,\"When you type your password using CLI, does it appears the letters being typed or it also stays hidden?\",1642874048.415600,1642874907.418300,U02TC704A3F\\n7c88a99d-d0b0-4a3a-95a8-4a29bbc63cf2,U02TC704A3F,,,hidden,1642874048.415600,1642874915.418500,U02S8K9JBD0\\nca55f468-4d0d-40a6-9b9f-dea09d6b4895,U02TC704A3F,,,This is my config. I am on a mac btw.,1642874048.415600,1642874966.418900,U02S8K9JBD0\\ndd47a5a7-9ee2-4388-ba0e-862650915700,U02U5G0EKEH,,,am able to install brew and terraform without issues so not sure if this article is relevant to the error,1642873732.412000,1642874972.419300,U02U5G0EKEH\\n08f0c150-6ab8-430d-b6e6-49b3ad736fc3,,,,\"Hello all, i’m having this issue\",,1642875019.420200,U02QKMCV39R\\nb0ccc48c-0afb-44c5-b65f-69c296811e0a,U02U5G0EKEH,,,Yes - I had to create a duplicate terminal for intel.,1642873732.412000,1642875092.421300,U0290EYCA7Q\\n2ec3bee8-0d1b-4fdb-a230-d73888cf4cfd,U02TC704A3F,,,\"can u try on different terminal, I ran some of the codes on conda command prompt\",1642874048.415600,1642875107.421500,U02AX5NC5B6\\nf1059ec1-368a-4e28-8b00-4635103bc88e,,10.0,,\"when i run the `docker run -it --network=pg-network taxi_ingest:v001 --user=root --password=admin --host=pg-database --port=5430 --db=ny_taxi --table_name=yellow_taxi_trips --url=${url}` command i\\'m getting an error\\nOperationalError: could not connect to server: Connection refused\\n        Is the server running on host \"\"pg-database\"\" (172.18.0.2) and accepting\\n        TCP/IP connections on port 5430?\",1642875158.421900,1642875158.421900,U02RREQ7MHU\\nb9694134-1426-4e8c-9987-2a5f1f31accf,U02RREQ7MHU,,,\"full code\\n```docker run -it --network=pg-network taxi_ingest:v001 --user=root --password=admin --host=pg-database --port=5430 --db=ny_taxi --table_name=yellow_taxi_trips --url=${url}\\n--2022-01-22 18:04:13--  <https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>\\nResolving <http://s3.amazonaws.com|s3.amazonaws.com> (<http://s3.amazonaws.com|s3.amazonaws.com>)... 52.216.1.75\\nConnecting to <http://s3.amazonaws.com|s3.amazonaws.com> (<http://s3.amazonaws.com|s3.amazonaws.com>)|52.216.1.75|:443... connected.\\nHTTP request sent, awaiting response... 200 OK\\nLength: 125981363 (120M) [text/csv]\\nSaving to: \\'output.csv\\'\\n\\noutput.csv                            100%[========================================================================&gt;] 120.14M  6.90MB/s    in 21s\\n\\n2022-01-22 18:04:36 (5.69 MB/s) - \\'output.csv\\' saved [125981363/125981363]\\n\\nTraceback (most recent call last):\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py\"\", line 3250, in _wrap_pool_connect\\n    return fn()\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py\"\", line 310, in connect\\n    return _ConnectionFairy._checkout(self)\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py\"\", line 868, in _checkout\\n    fairy = _ConnectionRecord.checkout(pool)\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py\"\", line 476, in checkout\\n    rec = pool._do_get()\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/impl.py\"\", line 146, in _do_get\\n    self._dec_overflow()\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py\"\", line 70, in __exit__\\n    compat.raise_(\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py\"\", line 207, in raise_\\n    raise exception\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/impl.py\"\", line 143, in _do_get\\n    return self._create_connection()\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py\"\", line 256, in _create_connection\\n    return _ConnectionRecord(self)\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py\"\", line 371, in __init__\\n    self.__connect()\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py\"\", line 666, in __connect\\n    pool.logger.debug(\"\"Error on connect(): %s\"\", e)\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py\"\", line 70, in __exit__\\n    compat.raise_(\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py\"\", line 207, in raise_\\n    raise exception\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py\"\", line 661, in __connect\\n    self.dbapi_connection = connection = pool._invoke_creator(self)\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/create.py\"\", line 590, in connect\\n    return dialect.connect(*cargs, **cparams)\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py\"\", line 597, in connect\\n    return self.dbapi.connect(*cargs, **cparams)\\n  File \"\"/usr/local/lib/python3.9/site-packages/psycopg2/__init__.py\"\", line 122, in connect\\n    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)\\npsycopg2.OperationalError: could not connect to server: Connection refused\\n        Is the server running on host \"\"pg-database\"\" (172.18.0.2) and accepting\\n        TCP/IP connections on port 5430?\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"\"/app/ingest_data.py\"\", line 70, in &lt;module&gt;\\n    main(args)\\n  File \"\"/app/ingest_data.py\"\", line 36, in main\\n    df.head(n=0).to_sql(name=table_name, con=engine, if_exists=\\'replace\\')\\n  File \"\"/usr/local/lib/python3.9/site-packages/pandas/core/generic.py\"\", line 2872, in to_sql\\n    sql.to_sql(\\n  File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/sql.py\"\", line 717, in to_sql\\n    pandas_sql.to_sql(\\n  File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/sql.py\"\", line 1751, in to_sql\\n    table = self.prep_table(\\n  File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/sql.py\"\", line 1650, in prep_table\\n    table.create()\\n  File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/sql.py\"\", line 856, in create\\n    if self.exists():\\n  File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/sql.py\"\", line 840, in exists\\n    return self.pd_sql.has_table(self.name, self.schema)\\n  File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/sql.py\"\", line 1783, in has_table\\n    insp = sa.inspect(self.connectable)\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/inspection.py\"\", line 64, in inspect\\n    ret = reg(subject)\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/reflection.py\"\", line 182, in _engine_insp\\n    return Inspector._construct(Inspector._init_engine, bind)\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/reflection.py\"\", line 117, in _construct\\n    init(self, bind)\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/reflection.py\"\", line 128, in _init_engine\\n    engine.connect().close()\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py\"\", line 3204, in connect\\n    return self._connection_cls(self, close_with_result=close_with_result)\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py\"\", line 96, in __init__\\n    else engine.raw_connection()\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py\"\", line 3283, in raw_connection\\n    return self._wrap_pool_connect(self.pool.connect, _connection)\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py\"\", line 3253, in _wrap_pool_connect\\n    Connection._handle_dbapi_exception_noconnection(\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py\"\", line 2100, in _handle_dbapi_exception_noconnection\\n    util.raise_(\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py\"\", line 207, in raise_\\n    raise exception\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py\"\", line 3250, in _wrap_pool_connect\\n    return fn()\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py\"\", line 310, in connect\\n    return _ConnectionFairy._checkout(self)\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py\"\", line 868, in _checkout\\n    fairy = _ConnectionRecord.checkout(pool)\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py\"\", line 476, in checkout\\n    rec = pool._do_get()\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/impl.py\"\", line 146, in _do_get\\n    self._dec_overflow()\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py\"\", line 70, in __exit__\\n    compat.raise_(\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py\"\", line 207, in raise_\\n    raise exception\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/impl.py\"\", line 143, in _do_get\\n    return self._create_connection()\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py\"\", line 256, in _create_connection\\n    return _ConnectionRecord(self)\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py\"\", line 371, in __init__\\n    self.__connect()\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py\"\", line 666, in __connect\\n    pool.logger.debug(\"\"Error on connect(): %s\"\", e)\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py\"\", line 70, in __exit__\\n    compat.raise_(\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py\"\", line 207, in raise_\\n    raise exception\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py\"\", line 661, in __connect\\n    self.dbapi_connection = connection = pool._invoke_creator(self)\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/create.py\"\", line 590, in connect\\n    return dialect.connect(*cargs, **cparams)\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py\"\", line 597, in connect\\n    return self.dbapi.connect(*cargs, **cparams)\\n  File \"\"/usr/local/lib/python3.9/site-packages/psycopg2/__init__.py\"\", line 122, in connect\\n    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)\\nsqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not connect to server: Connection refused\\n        Is the server running on host \"\"pg-database\"\" (172.18.0.2) and accepting\\n        TCP/IP connections on port 5430?\\n\\n(Background on this error at: <https://sqlalche.me/e/14/e3q8>)```\\n\",1642875158.421900,1642875206.422000,U02RREQ7MHU\\n013ef88f-bf0a-4a71-bc41-a2e618466bdb,U02UECC4H6U,,,\"Yes please create a repo, ideally on github\",1642842028.290300,1642875350.424900,U01AXE0P5M3\\ndf127a30-da27-4167-a134-79403092082c,U02RREQ7MHU,,,\"Isn\\'t default port 5432? We are specifying pg-database here.\\n\\nYou can confirm the port by running `docker container ls` command.\\n\\nIf you have telnet, you can do `telnet 172.18.0.2 5430` to confirm the same.\\n```bash-3.2$ docker container ls\\nCONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS          PORTS                           NAMES\\n156c96d8f6d2   dpage/pgadmin4   \"\"/entrypoint.sh\"\"         56 minutes ago   Up 56 minutes   443/tcp, 0.0.0.0:8080-&gt;80/tcp   pgadmin\\n8f1ebd2d9d05   postgres:13      \"\"docker-entrypoint.s…\"\"   58 minutes ago   Up 58 minutes   0.0.0.0:5432-&gt;5432/tcp          pg-database```\",1642875158.421900,1642875602.426200,U0290EYCA7Q\\nff565e50-71f0-4e12-a3a3-3cec21ad6896,U02RREQ7MHU,,,How do you run postgres?,1642875158.421900,1642875612.426400,U01AXE0P5M3\\ndef60d6d-e0ad-4f52-9311-435d839cb92e,U02TC704A3F,,,\"Alright, something happened here.\\n\\nI added the -d flag after docker run -it -d, thats my config now\\n\\n```docker run -it -d \\\\\\n  -e POSTGRES_USER=\"\"root\"\" \\\\\\n  -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n  -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n  -v D:/de-zoomcamp-week-1-basics-n-setup/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n  -p 7575:5432 \\\\\\n  postgres:13```\\nThen I wrongly copied my old config, as you can see in my image, I didn\\'t got the \\'Database system is ready to accept connections\\'.\\n\\nI run the code in the image, then started Powershell and pgcli.\\n\\nI tried \"\"admin\"\" as password didn\\'t worked.\\n\\nI tried \"\"root\"\" and worked ???\\n\\nJust computer things...\",1642874048.415600,1642875647.426600,U02TC704A3F\\n7a02d1ef-6a0a-489c-9f34-52c254bc0ecd,U02TC704A3F,,,\"Thanks <@U02S8K9JBD0>, <@U02AX5NC5B6> and <@U02RREQ7MHU>\",1642874048.415600,1642875684.427100,U02TC704A3F\\n0a661c8a-0396-4ca6-a21b-c7214a27b4df,U02RREQ7MHU,,,\"```docker ps\\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                           NAMES\\n4147fbc8deb7        dpage/pgadmin4      \"\"/entrypoint.sh\"\"         6 hours ago         Up 6 hours          443/tcp, 0.0.0.0:8080-&gt;80/tcp   pgadmin-2\\n5a778817ace0        postgres:13         \"\"docker-entrypoint.s…\"\"   6 hours ago         Up 6 hours          0.0.0.0:5430-&gt;5432/tcp          pg-database```\",1642875158.421900,1642875700.427400,U02RREQ7MHU\\na102e28a-612a-4752-8b91-d57414d3fa12,U02TC704A3F,,,\"I think I saw in some other thread that you need to use windows terminal, not just usual command line. Can you try it?\",1642874048.415600,1642875716.427700,U01AXE0P5M3\\n631c654f-024e-407e-965f-5a156762ad52,U02RREQ7MHU,,,postgres is running on port 5430,1642875158.421900,1642875720.427900,U02RREQ7MHU\\n2e73ca4e-dc62-4208-a20a-2f3cda572384,U02RREQ7MHU,,,\"Your are running inside the docker container, so the port should be 5432. Maybe, <@U01AXE0P5M3> can confirm.\",1642875158.421900,1642875827.428100,U0290EYCA7Q\\nac8ab35c-91aa-48a4-8102-0c4628d11bde,U02RREQ7MHU,,,\"With pgcli, we mentioned localhost. That\\'s why 5430 was working for you.\",1642875158.421900,1642875882.428300,U0290EYCA7Q\\n6acdde32-323b-4fb0-9840-e36773e07678,U02RREQ7MHU,,,oh,1642875158.421900,1642875888.428500,U02RREQ7MHU\\n8f906242-53e6-4af8-bd42-0411de4fa5ac,U02RREQ7MHU,,,yep that was it thanks <@U0290EYCA7Q>,1642875158.421900,1642875952.428900,U02RREQ7MHU\\nc04f0c1f-62c4-4d76-aff4-d6a79508ffb9,U02RREQ7MHU,,,changed the port to 5432 now it works,1642875158.421900,1642875963.429100,U02RREQ7MHU\\nac4d5107-872f-4686-9a5b-0b0695a63925,U02BVP1QTQF,,,\"<@U02UX664K5E>  This article is quite old!  (2017, I suppose?) But thanks for discovering it from the archives :sweat_smile: And glad to hear you found it interesting. :)\",1642713863.457200,1642875999.429700,U01DHB2HS3X\\n3047e052-d0b1-46f1-a85a-3cc304780349,,4.0,,\"i keep getting this error `FATAL: database \"\"ny_taxi\"\" does not exit`\\nafter running `pgcli -h localhost -p 5432 -u root -d ny_taxi`\",1642876106.431200,1642876106.431200,U02QKMCV39R\\n17a32f01-44c6-4caa-82c3-e359c9f86833,U02BVP1QTQF,,,\"<@U02BVP1QTQF>  Yes, a video-based lecture can definitely be more receptive. But glad to know you figured out the workshop and prepared your own notes based on that. Also, thanks for the feedback. This will help us prepare the rest of the course with a better approach.\",1642713863.457200,1642876209.431300,U01DHB2HS3X\\n227fda24-ca58-4bd1-b0df-d72c4eaad74c,U02U5G0EKEH,,,trying it i got this error and am confused what to do,1642873732.412000,1642876214.431500,U02U5G0EKEH\\na99fc916-7b5d-48ec-814f-2920b372f9f0,U02TC704A3F,,,\"hey <@U01AXE0P5M3>, it\\'s all working now. Don\\'t know exactly what did the trick, the last thing I did was add the \"\"-d\"\" after docker run -it. Does it makes sense? I don\\'t even know what \"\"-d\"\" does to be honest.\\n\\nI used Bash in the whole thing and had no problems.\",1642874048.415600,1642876251.432000,U02TC704A3F\\nfe99bce9-a64e-4ca2-8c2a-4efba1200f54,,4.0,,Is it necessary to set up the VM from gcloud before we can access gcloud sdk on windows,1642876309.433500,1642876309.433500,U02T0CYNNP2\\nd240a697-d04a-4004-ae9f-3ccc999850ac,U02U5G0EKEH,,,U continue with arm64. I am using /usr/local for now. Need to move to /opt/homebrew later,1642873732.412000,1642876372.433600,U0290EYCA7Q\\nd0f91579-0592-4861-b519-0a262e51bda9,U02U5G0EKEH,,,Try this? <https://stackoverflow.com/questions/66281882/how-can-i-get-terraform-init-to-run-on-my-apple-silicon-macbook-pro-for-the-go>,1642873732.412000,1642876399.433800,U0290EYCA7Q\\n1aba7172-6f7f-4f1b-aa7a-2fb137321ca8,U02T0CYNNP2,,,Nope.,1642876309.433500,1642876428.434100,U0290EYCA7Q\\nc12a0154-324c-4160-8a51-bc359f1dc242,U02TC704A3F,,,\"nice, I use conda terminal to run when i had error with some of the codes.. good that u figured out!\",1642874048.415600,1642876603.434400,U02AX5NC5B6\\ndc5b3337-a19c-4e34-9613-5e0bd68cf320,U02T0CYNNP2,,,That is to say the last 50mins video on the youtube channel is not for everybody,1642876309.433500,1642876829.434600,U02T0CYNNP2\\n46ed2eef-26c2-4b18-b232-1cdaa5ee3f3f,U02U5G0EKEH,,,yes tried it earlier no luck,1642873732.412000,1642877159.434800,U02U5G0EKEH\\n2c839e4a-a1e8-421d-8de9-e427eea81838,U02T0CYNNP2,,,\"I just answered your question. I was watching the preceding videos, where <@U01DHB2HS3X> able to use SDK for a new project, and set up service account credential. We need VM to do all stuff. VM will be part of the project.\",1642876309.433500,1642877185.435000,U0290EYCA7Q\\n1935a3a0-4dd9-416d-9918-97bf61d971c7,U02T0CYNNP2,,,\"And You can actually create VM from CLI itself. For that, You need SDK. Hope You get my point.\",1642876309.433500,1642877454.435200,U0290EYCA7Q\\n1c1e0613-8444-4ba4-b310-598c34d155d7,U02QKMCV39R,,,make sure the data base exists on docker,1642876106.431200,1642877759.435500,U02S6KXPH8W\\n559b29e1-6c55-4d81-9cae-f5fb1f9105e6,U02QKMCV39R,,,it does exist,1642876106.431200,1642878041.437800,U02QKMCV39R\\n6bf630bc-5337-4bb7-aab6-d828d15d5b59,,7.0,,\"A few questions for Mac users (I\\'m on Monterey 12.1),\\n1. which shell are you using zsh or bash?  I had been using bash for mlzoomcamp class\\n2. which version of python are you using 3.8?  \\n3. which directories are you storing apps in vs code?\\nThank you!\",1642878068.438400,1642878068.438400,U02CPBEH42W\\n349557c6-cdfb-4632-ab7d-cacd9a6bdcaf,U02U34YJ8C8,,,\"to remove stopped container easier way is\\n```docker container prune```\",1642871725.398200,1642878280.438700,U02S6KXPH8W\\n1a7609e9-de72-4a32-993a-082e41596ee6,U02TPTXFVQ9,,,\"check for extra spaces , that was why it wasn\\'t working for me\",1642870717.391300,1642878394.439000,U02S6KXPH8W\\ne5cb6a3c-dbb4-4861-8543-326c2c48fda0,U02CPBEH42W,,,\"1. I’m using fish but I’m a weirdo :upside_down_face: . I use zsh when I actually need to get things done since fish is kinda weird and I still don’t fully get it.\\n2. 3.9, because Alexey used that version in one of the first videos and I wanted to minimise differences\\n3. I have a dedicated `dev` directory inside my home directory where I put everything. I created a separate folder for my notes and homework. I’m not sure if this answers your question.\",1642878068.438400,1642878468.439200,U02BVP1QTQF\\n7edba1bd-2176-4e77-8eb1-fe39abbf8b8e,U02CPBEH42W,,,I was a little confused since one of the links mentions using 3.8 and the videos had 3.9.  I\\'ll go with 3.9,1642878068.438400,1642878702.439500,U02CPBEH42W\\n8c6b222f-5a97-4f7f-8289-720336399826,U02CPBEH42W,,,where does it mention to use 3.8?,1642878068.438400,1642878723.439700,U02BVP1QTQF\\nd402092b-0b71-401f-a39b-a01d05312370,U02CPBEH42W,,,\"It was in the docker + sql document:\\nNotes I used for preparing the videos: <https://docs.google.com/document/d/e/2PACX-1vRJUuGfzgIdbkalPgg2nQ884CnZkCg314T_OBq-_hfcowPxNIA0-z5OtMTDzuzute9VBHMjNYZFTCc1/pub|link>\",1642878068.438400,1642878749.439900,U02CPBEH42W\\n9f6a1cd8-69d7-4246-b994-439bb21f77e0,U02U5G0EKEH,,,<@U01AXE0P5M3> please any hint?,1642873732.412000,1642878809.440200,U02U5G0EKEH\\nc37f0f72-85db-48e6-ae98-fd2aa524a377,U02CPBEH42W,,,\"I\\'m just not sure how conda, pip, and brew play together either.\",1642878068.438400,1642878826.440500,U02CPBEH42W\\n2c3b74b6-66cb-4c33-95ea-a3f4a7b2247a,U02CPBEH42W,,,\"here, right? <https://docs.google.com/document/d/e/2PACX-1vRJUuGfzgIdbkalPgg2nQ884CnZkCg314T_OBq-_hfcowPxNIA0-z5OtMTDzuzute9VBHMjNYZFTCc1/pub>\\n\\nI see how it could be confusing.\\n\\nI haven’t tried running the examples with python 3.8 so I don’t know if it will work, but 3.9 works so I’d suggest sticking with it.\",1642878068.438400,1642878851.440700,U02BVP1QTQF\\n8fdbba7a-ea8e-415a-978b-2ef14a6e2cb9,U02CPBEH42W,,,\"I\\'ll update this, it was more just notes for myself. Better to use Python 3.9. It shouldn\\'t matter though\",1642878068.438400,1642878908.441000,U01AXE0P5M3\\n1df3a14c-c60a-4457-b7c5-c9011f014f84,U02QKMCV39R,,,can you send your docker run -id command?,1642876106.431200,1642878918.441200,U02TC704A3F\\ne710ee76-9161-4707-9a9b-5401047deb45,U02BVP1QTQF,,,My pleasure! And thank you very much for this course! This week has kicked my butt but I’m really enjoying it :slightly_smiling_face:,1642713863.457200,1642879021.441700,U02BVP1QTQF\\nA7FE27FC-C781-4594-8396-ABFD2DDC58EF,,4.0,,Is a service account basically just a user account for services on GCP?,1642879729.443200,1642879729.443200,U02U34YJ8C8\\n2d7b2318-4e3e-47d0-ac57-8610d9033545,,,,just to understand: is there any special reason we add the schema first to posgres? why not just pandas send the schema and first chunk of data together ?,,1642879924.445100,U02S6KXPH8W\\n4aadf97c-4832-4104-93dc-1d906992f6cb,U02U34YJ8C8,,,\"That’s what I understood from the lectures, yes. It’s an account meant for things other than users, which would be GCP services or apps, I believe\",1642879729.443200,1642879982.445200,U02BVP1QTQF\\n3bbd6890-1d23-4ed3-8e61-9c48581d16fb,U02U34YJ8C8,,,\"to remove all dangling images\\n`docker image prune`\\n\\nIf `-a` is specified, will also remove all images not referenced by any container.\",1642871725.398200,1642880750.445700,U02QP6JM83U\\n759fc27e-3244-40e0-ac9b-f3288748907b,U02DY0L6PHV,,,I\\'ve had this as well <@U02DY0L6PHV> I think the issue is around the ownership of the folder you a re working in. I resolved it by changing ownership of the folder to my user and changing the permissions to rwx. You may also solve it by running `sudo docker build -t test:pandas .` but not sure if it\\'s good practice,1642872315.403500,1642880832.445900,U02SUH9N1FH\\nac08dcc4-cac9-4eac-8f9e-40fd4254fecd,U02QKMCV39R,,,Did you get it resolved in the end <@U02QKMCV39R>?,1642873331.407000,1642880867.446200,U02SUH9N1FH\\n59BBBC06-618D-4187-B97C-AA25E3745680,U02DY0L6PHV,,,\"<@U02SUH9N1FH> running the command with `sudo` did help, thank you so much!\",1642872315.403500,1642881336.447700,U02DY0L6PHV\\n324b3c21-432f-4624-a3fc-1c215e47a28c,U02DY0L6PHV,,,Glad it worked for you :slightly_smiling_face:,1642872315.403500,1642881380.448000,U02SUH9N1FH\\na0494db5-73f0-4cf7-b9c8-f76cb5dfb55f,,2.0,,\"Hi all, I am stuck at this error. anyone seen this error? I am on Mac. Please help.\\nwhat am i doing wrong in here.\\n\\nError: Database is uninitialized and superuser password is not specified.\\n\\n\\xa0Here is the log i have\\n2_docker_sql % docker run -it \\\\\\n\\xa0-e POSTGRES_USER=\"\"root\"\" \\\\\\n\\xa0-e POSTGRES_PASSWORD:\"\"root\"\" \\\\\\n\\xa0-e POSTGRES_DB:\"\"ny_taxi\"\" \\\\\\n\\xa0-v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n\\xa0-p 5431:5432 \\\\\\n\\xa0postgres:13\\nUnable to find image \\'postgres:13\\' locally\\n13: Pulling from library/postgres\\na2abf6c4d29d: Already exists\\xa0\\ne1769f49f910: Already exists\\xa0\\n33a59cfee47c: Already exists\\xa0\\n461b2090c345: Already exists\\xa0\\n8ed8ab6290ac: Already exists\\xa0\\n495e42c822a0: Already exists\\xa0\\n18e858c71c58: Already exists\\xa0\\n594792c80d5f: Already exists\\xa0\\n8ce2d192c320: Pull complete\\xa0\\n00cfe308d793: Pull complete\\xa0\\n4c4a326c1cb3: Pull complete\\xa0\\nc9f2a6fdb326: Pull complete\\xa0\\n9c1f873a68ce: Pull complete\\xa0\\nDigest: sha256:a63b1bd5dff73a9c1851a0f97e4c593a6b6e2cde6671811b1fa9d10d7e86b658\\nStatus: Downloaded newer image for postgres:13\\nError: Database is uninitialized and superuser password is not specified.\\n\\xa0\\xa0\\xa0\\xa0You must specify POSTGRES_PASSWORD to a non-empty value for the\\n\\xa0\\xa0\\xa0\\xa0superuser. For example, \"\"-e POSTGRES_PASSWORD=password\"\" on \"\"docker run\"\".\\n\\n\\xa0\\xa0\\xa0\\xa0You may also use \"\"POSTGRES_HOST_AUTH_METHOD=trust\"\" to allow all\\n\\xa0\\xa0\\xa0\\xa0connections without a password. This is *not* recommended.\",1642881852.448700,1642881852.448700,U02SSA2SL95\\n67f190d5-a90a-4220-ab7e-cc2eab664e23,U02U34YJ8C8,,,\"Yes. take a look here: A service account is a special kind of account used by an application or compute workload, such as a Compute Engine virtual machine (VM) instance, rather than a person. Applications use service accounts to make <https://developers.google.com/identity/protocols/OAuth2ServiceAccount#authorizingrequests|authorized API calls>, authorized as either the service account itself, or as Google Workspace or Cloud Identity users through <https://developers.google.com/identity/protocols/oauth2/service-account#delegatingauthority|domain-wide delegation>.\\nFor example, a service account can be attached to a Compute Engine VM, so that applications running on that VM can authenticate as the service account. In addition, the service account can be granted IAM roles that let it access resources. The service account is used as the identity of the application, and the service account\\'s roles control which resources the application can access.\",1642879729.443200,1642881961.448800,U02T2JGQ8UE\\nd9ac7f64-a4bb-48ee-acce-6f7f99917660,U02U34YJ8C8,,,you can read more here: <https://cloud.google.com/iam/docs/service-accounts>,1642879729.443200,1642881993.449100,U02T2JGQ8UE\\n15a95417-3c63-44e7-83f1-889b09e4210d,U02SSA2SL95,,,\"Replace colon with =, and try.\",1642881852.448700,1642882042.449400,U0290EYCA7Q\\nb5db3070-e5f7-4e64-915c-d959646fb82e,U02U34YJ8C8,,,\"basically as you have roles that you set to users in GCP, you\\'ll set a service account with specific role to let your app or compute workload to make API calls or just use some services from GCP. I hope this help\",1642879729.443200,1642882166.449600,U02T2JGQ8UE\\n62bc0647-e7e3-4031-b9ee-638471251ab4,,,,\"Hi there! If you have problems with a private key on windows and created it in PuTTYgen, then try converting the file to OpenSSH format, it helps\\n\\n<https://stackoverflow.com/questions/41563973/git-clone-key-load-public-invalid-format-permission-denied-publickey>\",,1642882174.450000,U02R2PU9NLD\\nfdad91d9-9f96-462e-bb27-4ddd71cfc093,U02SSA2SL95,,,thanks that worked. :+1:,1642881852.448700,1642882353.450400,U02SSA2SL95\\nae364ec8-ebea-4896-a48a-066602242e3f,U0290EYCA7Q,,,\"No, I’ve installed without any issue. You can also download the Arm64 binary from their webpage <https://releases.hashicorp.com/terraform/1.1.4/terraform_1.1.4_darwin_arm64.zip>\",1642859115.330900,1642882508.450600,U02Q86NL971\\n9f824d82-4a88-424a-98f7-32907cc67f84,U02U5G0EKEH,,,<@U0290EYCA7Q> please how can i undo the make command? its preventing me from looking up the latest version,1642873732.412000,1642884517.451300,U02U5G0EKEH\\n89bb7cdc-085c-4be7-a0c6-654d7290978e,U02U5G0EKEH,,,\"I don\\'t have a Mac, sorry\",1642873732.412000,1642884529.451700,U01AXE0P5M3\\n696b6140-4912-4ca3-99ff-fc9b6f913d82,,4.0,,<@U01AXE0P5M3> Hii .. When is the deadline to submit every homework? I will face delay in submitting homework1 &amp; will be submitted with homework2 .. will not take any points if submitted homework solution late?,1642884820.453600,1642884820.453600,U02CA7GAAUV\\nd5cc1ae4-7157-44a6-9315-ec7762bf4b91,U02U5G0EKEH,,,<@U01DHB2HS3X> please any thoughts,1642873732.412000,1642885090.454000,U02U5G0EKEH\\nb33fbcca-7244-42ca-aea2-6bdab297b2a7,U02RREQ7MHU,,,\"Please, what do you mean by this.. I\\'m having the same issue\",1642868754.374000,1642885186.454200,U02T9JQAX9N\\nef59edb5-0f24-430f-ab05-137eee0de191,U02DY0L6PHV,,,\"I solved mine by putting the Dockerfile and `ingest_data.py` into a different folder `dockerfiles`. I think the error has to do with the `ny_taxi_postgres_data` folder. For me, Ubuntu does not give me permission to access it.\",1642872315.403500,1642885550.454400,U02T9JQAX9N\\n4cdd440a-390c-475f-a894-a56b9bb58623,U02DY0L6PHV,,,\"apparently this is a common problem.\\nwhen setting volume on your postgres data base container ,it locks the ny_taxi_postgres_data folder(perhaps for protection from different users ).\",1642872315.403500,1642885681.454800,U02S6KXPH8W\\nfef3de92-8492-4ff9-b603-edcb0b6779fe,,10.0,,\"I\\'m getting this error while trying to run the dockerised ingestion script..\\n\\n```docker run  -it \\\\\\n  --network=pg-network \\\\\\ntaxi_ingest:v001 \\\\\\n    --user=root \\\\\\n    --password=root \\\\\\n    --host=localhost \\\\\\n    --port=5432 \\\\\\n    --db=ny_taxi \\\\\\n    --table_name=yellow_taxi_trips\\n    --url=\"\"<https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>\"\"\\n--2022-01-22 20:48:43--  <http://none/>\\nResolving none (none)... failed: Temporary failure in name resolution.\\nwget: unable to resolve host address \\'none\\'\\nTraceback (most recent call last):\\n  File \"\"/app/ingest_data.py\"\", line 61, in &lt;module&gt;\\n    main(args)\\n  File \"\"/app/ingest_data.py\"\", line 25, in main\\n    df_iter = pd.read_csv(csv_name, iterator=True, chunksize=100000)\\n  File \"\"/usr/local/lib/python3.9/site-packages/pandas/util/_decorators.py\"\", line 311, in wrapper\\n    return func(*args, **kwargs)\\n  File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\"\", line 586, in read_csv\\n    return _read(filepath_or_buffer, kwds)\\n  File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\"\", line 482, in _read\\n    parser = TextFileReader(filepath_or_buffer, **kwds)\\n  File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\"\", line 811, in __init__\\n    self._engine = self._make_engine(self.engine)\\n  File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\"\", line 1040, in _make_engine\\n    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\\n  File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\"\", line 69, in __init__\\n    self._reader = parsers.TextReader(self.handles.handle, **kwds)\\n  File \"\"pandas/_libs/parsers.pyx\"\", line 549, in pandas._libs.parsers.TextReader.__cinit__\\npandas.errors.EmptyDataError: No columns to parse from file\\nbash: --url=<https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>: No such file or directory```\",1642885816.455800,1642885816.455800,U02T9JQAX9N\\nd9514b01-f7dd-4169-8cd9-6ffe09879757,U02UY1QTGHW,,,<https://stackoverflow.com/a/49965690/780405>,1642721355.469800,1642885876.456000,U02S83KSX3L\\nA75D34F2-7FB6-4C50-B8C5-BE8D5F5BE4C2,U02T9JQAX9N,,,\"I think your host needs to change, and maybe your network. What stage are you at? Have you completed the docker-compose steps?\",1642885816.455800,1642886038.457500,U02U34YJ8C8\\ne25526c9-d11f-43b6-8444-9d74bd64d9ae,U02U5G0EKEH,,,That should be problems with the versions. What is your Terraform version?,1642873732.412000,1642886086.457900,U02CD7E30T0\\n573905be-6d22-4520-999c-1d7011059592,U02T9JQAX9N,,,\"You\\'re missing a \\'\\\\\\' at the end of line\\n`--table_name=yellow_taxi_trips`\",1642885816.455800,1642886235.459300,U02QH3TBA11\\n,USLACKBOT,2.0,tombstone,This message was deleted.,1642886266.460100,1642886266.460100,USLACKBOT\\n66e5ff87-16e9-46db-89c5-5dc859f35140,U02CA7GAAUV,,,The form will not accept answers after the deadline - we are going to present the solution on Monday. But that won\\'t affect your ability to get a certificate at the end,1642884820.453600,1642886287.460200,U01AXE0P5M3\\nfd826918-17b9-405a-aa00-48370d48ce1e,USLACKBOT,,,I use windows terminal and gitbash there,1642886266.460100,1642886341.460600,U01AXE0P5M3\\nb86597ea-7f37-43db-bfde-da655313506b,,4.0,,\"<@U01AXE0P5M3> For MINGW terminal, looks like you are using some tabbed windows plugin. Can you share the plugin details?\",1642886348.461000,1642886348.461000,U02S83KSX3L\\n9a128542-dc93-4b44-a80c-3c3964268364,USLACKBOT,,,<https://www.microsoft.com/en-us/p/windows-terminal/9n0dx20hk701#activetab=pivot:overviewtab|https://www.microsoft.com/en-us/p/windows-terminal/9n0dx20hk701#activetab=pivot:overviewtab>,1642886266.460100,1642886377.461400,U01AXE0P5M3\\n125d420c-98a1-43c7-99fb-7ec9b1fbfb1e,U02T9JQAX9N,,,I remember <@U01AXE0P5M3> saying something about problems with host = localhost when using in docker,1642885816.455800,1642886641.462000,U02TC704A3F\\n9256b656-ea66-46da-860c-3c1ef29d8b73,U02TC704A3F,,,<@U02TC704A3F> -d runs the container in detached mode (in the background) so you don\\'t have to switch to another terminal,1642874048.415600,1642886642.462200,U02S8K9JBD0\\n1a8e55c2-d454-455a-9a46-05202ff7ad7e,U02TC704A3F,,,\"Weird, so it wasn\\'t exactly what solved my problem I guess, but is all working now\",1642874048.415600,1642886700.462500,U02TC704A3F\\nc84207b7-f902-4eb7-85a9-df912a0c9e14,U02T9JQAX9N,,,\"My code is\\n\\n```docker run -it \\\\\\n  --network=pg-network \\\\\\n  taxi_ingest:v001 \\\\\\n    --user=root \\\\\\n    --pwd=root \\\\\\n    --host=pg-database \\\\\\n    --port=7575 \\\\\\n    --db=ny_taxi \\\\\\n    --table_name=ny_taxi_data \\\\\\n    --url=<https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>```\",1642885816.455800,1642886843.462900,U02TC704A3F\\n5a04a802-6118-410f-8f5c-32bfcd9fcb83,U02QKMCV39R,,,\"<@U02QKMCV39R> Is your postgres container running?\\n\\nType the following into the command line:\\n\\n`docker ps`\\n\\nWhat’s the output?\",1642876106.431200,1642886864.463200,U02U34YJ8C8\\n1840c082-0d80-4c2f-9737-c5cb3cd30da8,U02T9JQAX9N,,,\"<@U02QH3TBA11> This worked.. I was able to download the csv file.. but now I\\'m having another issue, probably with postgres.. I\\'m seeing a lot of lines about port 5432\",1642885816.455800,1642886945.463400,U02T9JQAX9N\\ne2567b45-63f0-4f47-aa5a-074102b5f5e0,U02T9JQAX9N,,,<@U02U34YJ8C8> I haven\\'t watched the video on docker compose yet.. I\\'m still on docker network,1642885816.455800,1642887028.463600,U02T9JQAX9N\\nc4f78674-c064-4cae-942b-2a9a74188595,U02CA7GAAUV,,,\"<@U01AXE0P5M3> is the VM machine from gcp important for this course,and how about if we are able to perform all this on our windows ,do we still need gc VM\",1642884820.453600,1642887090.463800,U02T0CYNNP2\\n40B293C3-C4D3-4F06-9CFA-4A144393187D,U02T9JQAX9N,,,\"That\\'s fine. For your new issue, can you post the full error code? \",1642885816.455800,1642887501.464800,U02U34YJ8C8\\n31b1a490-8271-499a-8f27-e8cbd9935b72,U02S83KSX3L,,,that is the window terminal  as you can see,1642886348.461000,1642887507.465000,U02T2JGQ8UE\\n4073d9f7-8ced-41d5-8907-15128fb178fb,U02S83KSX3L,,,FYI: <https://www.microsoft.com/en-us/p/windows-terminal/9n0dx20hk701#activetab=pivot:overviewtab>,1642886348.461000,1642887544.465400,U02T2JGQ8UE\\na23347ee-af46-41f8-8576-8b50a714e839,U02T9JQAX9N,,,It has worked now.. my host was `localhost` instead of `pg_database`. Thanks,1642885816.455800,1642887553.465600,U02T9JQAX9N\\n595d437b-0cc4-48a6-b0bc-9023873ca8b2,U02T9JQAX9N,,,people are posting the same problem over and over again,1642885816.455800,1642887706.466300,U02U2Q5P61Z\\n7c118033-51c4-4819-b7eb-a179b7a88a76,U02T9JQAX9N,,,you need to change localhost for the name of the container running the database,1642885816.455800,1642887720.466500,U02U2Q5P61Z\\n5ccb3bee-afba-49e7-9521-a993046e1e30,U02U5G0EKEH,,,Arm64 has been known to have issues with some open-source technologies. But googling this has only returned me the same tested result from Stackoverflow that others posted above. A similar one on another thread has returned me this: <https://github.com/hashicorp/terraform/issues/27257#issuecomment-754777716|https://github.com/hashicorp/terraform/issues/27257#issuecomment-754777716>,1642873732.412000,1642887913.466700,U01DHB2HS3X\\nd68832cc-ffbb-4b15-b6a9-dce107be6aef,,2.0,,docker-compose --build -d,1642888178.467300,1642888178.467300,U02U6DR551B\\n639feec1-e63a-4496-9b14-69756736439d,,1.0,,does anyone know what -d means here?,1642888186.467600,1642888186.467600,U02U6DR551B\\n52e188bd-40bd-4e56-87e1-a2849a8bad3b,U02S83KSX3L,,,Got it working and here are my settings. Thanks <@U02T2JGQ8UE> <@U01AXE0P5M3>,1642886348.461000,1642888228.467700,U02S83KSX3L\\nc0773bc9-e1b9-4131-8c55-9b5f1087186a,U02U6DR551B,,,detach. Docker will be running in the background.,1642888178.467300,1642888238.468300,U0290EYCA7Q\\neb1eddc9-b62f-4831-bd04-680ea5d1bd3a,U02U6DR551B,,,<https://www.shellhacks.com/docker-compose-detached-mode-background/|https://www.shellhacks.com/docker-compose-detached-mode-background/>,1642888178.467300,1642888301.468600,U0290EYCA7Q\\nac8ccdb2-34d7-4428-ad50-56dd3eec0314,U02UKLHDWMQ,,,yeah,1642865209.361200,1642888313.468900,U02UKLHDWMQ\\n19a66197-61aa-48a0-825a-2f0c07c39f0e,U02U5G0EKEH,,,\"Also, try dropping the required_providers section (or even the terraform block) and see if that works.\",1642873732.412000,1642888509.469100,U01DHB2HS3X\\n3878ddfa-a8da-4bcc-b3bd-2355bd566730,U02U6DR551B,,,-d runs the container in detached mode (in the background) so you don\\'t have to switch to another terminal,1642888186.467600,1642888792.469800,U02S8K9JBD0\\n4393c38e-d62c-470b-9b84-b93f2100a998,,48.0,,\"good evening everyone, after pip installing pgcli and i tried to run it, i get this error\\n```bash: pgcli: command not found```\",1642888823.470600,1642888823.470600,U02RH0V5K33\\nddd22bc9-19f1-4944-804b-e00a59dbdc09,U02RH0V5K33,,,\"Try running pip list, and add bin to the path.\",1642888823.470600,1642888884.471400,U0290EYCA7Q\\n4a8f37c3-7aec-4f58-a485-ac528a3f75cf,U02RH0V5K33,,,how?,1642888823.470600,1642888916.471600,U02RH0V5K33\\n54a96b46-a5c1-4b8c-8c5c-ecfd57291843,U02RH0V5K33,,,please how do i had bin to the path?,1642888823.470600,1642888963.471800,U02RH0V5K33\\na1962a90-e337-4b1e-a7ec-0061414a26bc,U02TMEUQ7MY,,,<@U02RH0V5K33> Check this.,1642864750.357300,1642888975.472000,U0290EYCA7Q\\n2d2a676d-7541-4847-807f-80030cedbcde,U02RH0V5K33,,,I am on windows,1642888823.470600,1642888989.472200,U02RH0V5K33\\n36825ef5-7cca-47f3-b421-3c3b10cebe9f,U02RH0V5K33,,,\"maybe install with conda will solve the problem\\n`conda install pgcli`\",1642888823.470600,1642889020.472400,U02QP6JM83U\\na96168cf-2595-4b21-a002-7ed88e5769a9,U02RH0V5K33,,,\"alternatively, you can create a virtual environment within your project directory with `python3 -m venv venv` , then activate it with `source venv/bin/activate` , then install packages like `pip3 install pgcli` . When you are done with this environment you can shut it down with `deactivate` .\",1642888823.470600,1642889040.472600,U02S8K9JBD0\\n2eec866b-3e00-41c0-9ed2-4834b35f5d40,U02RH0V5K33,,,Pip3 install would be a good option.,1642888823.470600,1642889065.472800,U0290EYCA7Q\\nc77d97a1-2db6-407b-a202-80fb635fdfec,U02RH0V5K33,,,\"done that already, still getting the same issue\",1642888823.470600,1642889119.473000,U02RH0V5K33\\n0889e599-4876-478b-b9d8-3effd6583893,U02RH0V5K33,,,Do u have anaconda installed?,1642888823.470600,1642889246.473200,U0290EYCA7Q\\nb2095a82-50ee-40e8-8d6b-e924d90698c8,U02RH0V5K33,,,no i do not,1642888823.470600,1642889367.473400,U02RH0V5K33\\n7287f72c-598b-4be1-8473-5cf0f3506035,U02RH0V5K33,,,Did you run pip list -v?,1642888823.470600,1642889449.473600,U0290EYCA7Q\\n994c29f3-c5da-40ea-be0b-348b39ed56ae,U02RH0V5K33,,,can you post screenshots of your environment? from installation to run attempt? also a pip3 list would be helpful.,1642888823.470600,1642889474.473800,U02S8K9JBD0\\na839588a-b267-4a9d-8968-72a30d07698d,U02RH0V5K33,,,pip when used with virtualenv will generally install packages in the path &lt;virtualenv_name&gt;/lib/&lt;python_ver&gt;/site-packages.,1642888823.470600,1642889524.474100,U0290EYCA7Q\\n103016a4-9013-4a04-8bb3-dfc566a6c0b7,U02RH0V5K33,,,,1642888823.470600,1642889585.474300,U02RH0V5K33\\n193d39d0-f056-480f-9ba9-71ded464bb09,U02RH0V5K33,,,,1642888823.470600,1642889607.474700,U02RH0V5K33\\ne0cb80ff-fc32-4b1f-9b37-2b3cc395d30d,U02RH0V5K33,,,Please run with -v,1642888823.470600,1642889627.475100,U0290EYCA7Q\\n43c5803d-fd1a-40a8-9b6f-a9eaf64c08a9,U02S83KSX3L,,,Awesome!! have fun :slightly_smiling_face:,1642886348.461000,1642889683.475300,U02T2JGQ8UE\\n4007a0f6-9030-4fd2-9c9f-cef26b4b0a67,U02RH0V5K33,,,\"I am new to all this, when you say run with -v, how do you mean?\",1642888823.470600,1642889691.475500,U02RH0V5K33\\n1695da12-3e7b-49b5-a1cc-6ff65ed09087,U02RH0V5K33,,,run this command.  pip list -v,1642888823.470600,1642889737.475800,U0290EYCA7Q\\n94db003e-feea-45a8-a75c-271185ad1792,U02RH0V5K33,,,ok,1642888823.470600,1642889742.476000,U02RH0V5K33\\n416ee569-cc2e-40d9-9b30-b2838a2a08fa,U02RH0V5K33,,,Or pip show pgcli,1642888823.470600,1642889748.476200,U0290EYCA7Q\\n78a4ea7b-ae40-46de-82ca-2be019381415,U02RH0V5K33,,,,1642888823.470600,1642889784.476400,U02RH0V5K33\\n3d414a8c-779d-4fea-97a2-9b80268fd996,U02RH0V5K33,,,pip show pgcli,1642888823.470600,1642889825.476800,U02RH0V5K33\\n0aa5f5f1-7050-475e-a4d0-d6de67b9e985,U02RH0V5K33,,,Can yoi go to the location?,1642888823.470600,1642889856.477200,U0290EYCA7Q\\n65105ca6-4e3b-43b6-9f69-12e9a367cd5c,U02RH0V5K33,,,appdata/../site-packages,1642888823.470600,1642889886.477500,U0290EYCA7Q\\n20c1d7b2-e1c6-4d58-965a-676d356196e0,U02RH0V5K33,,,\"I am there, what next\",1642888823.470600,1642889938.477900,U02RH0V5K33\\naa0a4e83-26d0-4b03-b1ed-560d0c02c70a,U02RH0V5K33,,,U see any bin?,1642888823.470600,1642889950.478100,U0290EYCA7Q\\nc9c38e98-eb8b-4e6f-9956-2ff00a93c565,U02RH0V5K33,,,bin directory?,1642888823.470600,1642889966.478300,U0290EYCA7Q\\nce17eb5c-740c-47d2-ae47-471c74963a4e,U02RH0V5K33,,,no bin dir,1642888823.470600,1642889990.478500,U02RH0V5K33\\n5dbeb9de-1647-4a62-8002-fb424ebca361,U02RH0V5K33,,,\"Go to that directory in cmd, and try pgcli. Let\\'s see what u get\",1642888823.470600,1642890053.478900,U0290EYCA7Q\\n9a1146af-7efb-4276-b2b8-f61947551a71,U02CA7GAAUV,,,you can do all this on your windows,1642884820.453600,1642890105.479100,U01AXE0P5M3\\ne176acae-777d-47b8-9a67-b825504d1b66,U02RH0V5K33,,,\"```\\'pgcli\\' is not recognized as an internal or external command,\\noperable program or batch file.```\",1642888823.470600,1642890206.479600,U02RH0V5K33\\n7c7543c1-5201-467c-bcae-66dc066a9ce9,,2.0,,Good evening guys! Is there deadline for homework submissions:face_with_monocle: ?,1642890257.480500,1642890257.480500,U02RHT0M3M5\\nb6081610-4476-47fe-aa57-9b05f01ba13d,U02RHT0M3M5,,,\"yes, monday 17:00 CET\",1642890257.480500,1642890317.480600,U01AXE0P5M3\\n50d95752-5b60-4d33-b1d0-da10bce0263d,U02RHT0M3M5,,,\"okay, thanks alexey,\",1642890257.480500,1642890361.480800,U02RHT0M3M5\\n071ca6ce-040d-49c9-9fbd-d5e7f05deba2,,6.0,,Ritorical question. Why the name of Question 4 is `Average` while we need to find largest value of tip? :slightly_smiling_face:,1642890600.483500,1642890600.483500,U02QP6JM83U\\n99bbff6f-fd86-4fc4-8bb2-8c5af2010b95,U02RH0V5K33,,,Can u go to python38?,1642888823.470600,1642890778.483700,U0290EYCA7Q\\n28b261c5-4ba6-4cfb-8aa0-90edaeec7739,U02RH0V5K33,,,\"Inside that, there will be a Scripts directory\",1642888823.470600,1642890796.483900,U0290EYCA7Q\\n0187f122-b3eb-48c8-80cb-dd6568de7a2f,U02RH0V5K33,,,Do u see pgcli inside?,1642888823.470600,1642890805.484100,U0290EYCA7Q\\n5e61bc04-d531-4686-bfae-d3320c914827,U02RH0V5K33,,,yes,1642888823.470600,1642890923.484600,U02RH0V5K33\\n2d14b590-66d3-41dc-8c2b-23c24770b284,U02RH0V5K33,,,pgcli.exe,1642888823.470600,1642890936.484800,U02RH0V5K33\\nfebb35d7-0fc2-4703-bf02-c4cd2375bc5c,U02RH0V5K33,,,\"If u run pgcli -v, what are u getting?\",1642888823.470600,1642890960.485000,U0290EYCA7Q\\nc76cdeac-7884-4dbc-a1a3-efdc2838fb8b,U02RH0V5K33,,,\"in cmd i get\\n```\\'pgcli\\' is not recognized as an internal or external command,\\noperable program or batch file.```\",1642888823.470600,1642891053.485200,U02RH0V5K33\\nd0c1579e-3bd3-4283-8d57-9fd096d0e5ab,U02RH0V5K33,,,\"in bash i get\\n```bash: pgcli: command not found```\",1642888823.470600,1642891082.485400,U02RH0V5K33\\nd2ab483a-16f4-4f35-9ba3-33ad4fc2ab4b,U02RH0V5K33,,,Did u go to the directory in cmd?,1642888823.470600,1642891085.485600,U0290EYCA7Q\\neecf7f17-79cd-467a-b4f1-dd44231ddb91,U02QP6JM83U,,,Or we need to calc tips for whole day?,1642890600.483500,1642891089.485800,U02QP6JM83U\\nf0e30359-8dec-4474-badc-49d65cdeb612,U02RH0V5K33,,,yes,1642888823.470600,1642891106.486000,U02RH0V5K33\\n460f6c60-75da-42ba-96d0-7f689c96ac51,U02RH0V5K33,,,SS please,1642888823.470600,1642891117.486400,U0290EYCA7Q\\n311b4493-108d-4c1f-86db-72a83a428ed1,U02RH0V5K33,,,,1642888823.470600,1642891143.486600,U02RH0V5K33\\nab3287af-126c-4c97-8b13-b7e050dd7259,U02RH0V5K33,,,\"Not there.\\n\\nC\\\\Users\\\\&lt;ur_id&gt;\\\\Appdata\\\\Roaming\\\\Python\\\\Python38\\\\Scripts\",1642888823.470600,1642891223.487100,U0290EYCA7Q\\nf3966593-8fd2-43c9-a956-8fc2d8901217,U02RH0V5K33,,,\"Go upto Python 38, and cd into Scripts. There confirm if u r seeing pgcli.\",1642888823.470600,1642891262.487300,U0290EYCA7Q\\na9877151-54db-4bad-ae91-bfdf602605e6,U02RH0V5K33,,,\"after runing pgcli -v, it returned the version\",1642888823.470600,1642891337.487500,U02RH0V5K33\\nfb96258a-a920-4621-bfc6-3a41c209934a,,25.0,,\"\\'\\'\\'$ winpty docker run -it \\\\\\n&gt;   -e POSTGRES_USER=\"\"root\"\" \\\\\\n&gt;   -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n&gt;   -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n&gt;   -v C:/Users/Gustavo/Documents/data_engineering_zoomcamp/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n&gt;   -p C:/Users/Gustavo/Documents/data_engineering_zoomcamp/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n&gt;   postgres:13\\ndocker: Invalid ip address: C;C.\\nSee \\'docker run --help\\'. \\'\\'\\'\\n\\nWhat might have I done wrong?\",1642891400.489400,1642891400.489400,U02R09ZR6FQ\\nad970556-d538-49a8-a102-e57ecfd2f827,U02RH0V5K33,,,\"You can go there anr run pgcli commands there for now. \\nIt\\'s too late for me. We can set the path tomorrow.\",1642888823.470600,1642891469.489900,U0290EYCA7Q\\n2dd2613d-c29c-45a2-8326-b32769288f5d,U02RH0V5K33,,,ok thanks,1642888823.470600,1642891503.490500,U02RH0V5K33\\n4f440204-8f1c-453a-ab0d-ac348cfbdc6a,U02RH0V5K33,,,e you tomorrow,1642888823.470600,1642891527.490900,U02RH0V5K33\\n8583368b-11f8-431d-96c5-067777d2c5f0,U02R09ZR6FQ,,,\"-p is port, not path\",1642891400.489400,1642891542.491400,U01AXE0P5M3\\n088d8363-b8df-423b-9549-b2dbe2ae90c5,,2.0,,\"Here is my notes upto Docker &amp; SQL. I am working on the notes terraform and hopefully I will have some time left for homework.\\nBy the way I am also trying to add the timestamps to the videos this time around.\\n\\n<@U01AXE0P5M3> Does this count as learning in Public? I am not really active on my social media.\\n<https://itnadigital.notion.site/Week-1-Introduction-f18de7e69eb4453594175d0b1334b2f4>\",1642891597.492400,1642891597.492400,U02QZN0LSBT\\n1b147e63-6c43-458b-9e89-ac93297e2891,U02QZN0LSBT,,,It definitely does,1642891597.492400,1642891626.492600,U01AXE0P5M3\\nc6c459ec-0e55-4ac3-83a5-f6293ab9c12f,U02QZN0LSBT,,,YESSS!!! Cause I might not be able to deliver HW on time. :disappointed:,1642891597.492400,1642891698.492900,U02QZN0LSBT\\n8681ac2e-9341-4be2-bc7f-500c0cedbaac,U02CA7GAAUV,,,Thanks,1642884820.453600,1642891923.494000,U02T0CYNNP2\\n01fbc66d-195c-4a26-9ae5-a35b3368fe0a,U02R09ZR6FQ,,,Thank you!,1642891400.489400,1642892427.494400,U02R09ZR6FQ\\n2642d582-c4da-4175-8255-7e68e19b5415,U02TMQZPJQZ,,,\"I\\'ll try that, thank you! :sweat_smile:\",1642805982.244000,1642893176.495000,U02TMQZPJQZ\\nb95dc684-e36b-437f-bdd5-2808ca8cf8a2,,1.0,,Should I be bothered about this?,1642893575.496100,1642893575.496100,U02SDHJMASG\\nb566e1b0-ee34-43df-8056-3356bb3bd3aa,U02R09ZR6FQ,,,\"winpty docker run -it \\\\\\n\\xa0 -e POSTGRES_USER=\"\"root\"\" \\\\\\n\\xa0 -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n\\xa0 -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n\\xa0 -v /c/Users/Gustavo/Documents/data_engineering_zoomcamp/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n\\xa0 -p 5432:5432 \\\\\\n\\xa0 postgres:13\\n\\nBut now with this code (above) it creates an empty folder \"\"ny_taxi_postgres_data;C\"\"\",1642891400.489400,1642894669.497000,U02R09ZR6FQ\\n73e2186c-8486-4423-824f-f4671813fc49,U02SDHJMASG,,,\"Pip allows you to install third party libraries. However the libraries are not guaranteed to be compatible. Which is why using a virtual environment, is recommended as it doesn\\'t affect your global environment.\\n\\nIf this is in docker, you should be fine though.\",1642893575.496100,1642895864.498100,U02ULBM39B4\\n35a7bfb3-9ae3-4beb-9e7f-41632282bf28,U02R09ZR6FQ,,,\"<@U02R09ZR6FQ> Have you solved this problem? I can\\'t find a solution and I have the same empty folder \"\"ny_taxi_postgres_data;C\"\".\\n\\nThere are no errors in the terminal.\\nI use GitBash on Windows\\n```winpty docker run -it \\\\\\n  -e POSTGRES_USER=\"\"root\"\" \\\\\\n  -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n  -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n  -v /e/dataengineering/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n  -p 5432:5432 \\\\\\npostgres:13```\",1642891400.489400,1642896425.498500,U02QM7H8G4U\\n359bb780-fb0f-427e-8b6a-afbf65c1db13,,10.0,,\"Good afternoon, I was watching the video 1.2.2 Ingesting NY Taxi Data to Postgres and I managed to execute the command to create the container, but when I try to execute the pgcli in git bash console it keeps waiting, if I try by powershell or cmd then after the command it asks me for password and then the pgcli terminates.\",1642896501.499200,1642896501.499200,U02TXGC7WKH\\n50391498-60bb-4672-8d32-a19946a6e2d8,U02R09ZR6FQ,,,@Marina can you see the db in PGAdmin?,1642891400.489400,1642896647.000100,U02SUH9N1FH\\ne5bfe068-d52b-43a8-a0de-d57279b22523,U02R09ZR6FQ,,,<@U02QM7H8G4U> I haven\\'t solved it. I\\'m also using GitBash on Windows,1642891400.489400,1642896768.000300,U02R09ZR6FQ\\n033cd931-e87c-40da-9261-15e149a0ab37,,11.0,,\"Hi All,\\nNeed some help. I am stuck at Python Ingest_Data.py step. When I run following piece of code --&gt;\\n```URL = \"\"<https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>\"\"\\npython ingest_data.py \\\\\\n\\t--user= root \\\\\\n    --password= root \\\\\\n    --host= localhost \\\\\\n    --port= 5432 \\\\\\n    --db= ny_taxis \\\\\\n    --table_name= yellow_taxi_data \\\\\\n    --url= ${URL}```\\nI get error shown below:\\n```SyntaxError: invalid syntax\\nAMEYs-MacBook-Air:2_docker_sql amey$ URL = \"\"<https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>\"\"\\n-bash: URL: command not found\\nAMEYs-MacBook-Air:2_docker_sql amey$\\xa0\\nAMEYs-MacBook-Air:2_docker_sql amey$ python ingest_data.py \\\\\\n&gt; --user= root \\\\\\n&gt; \\xa0\\xa0--password= root \\\\\\n&gt; \\xa0\\xa0--host= localhost \\\\\\n&gt; \\xa0\\xa0--port= 5432 \\\\\\n&gt; \\xa0\\xa0--db= ny_taxis \\\\\\n&gt; \\xa0\\xa0--table_name= yellow_taxi_data \\\\\\n&gt; \\xa0\\xa0--url= ${URL}```\\nI am on Intel MAC OS. Thanks!\",1642898827.001000,1642898827.001000,U02UM74ESE5\\n8a2ca8e8-ccd0-44bd-8dce-0c9dc7838203,U02UM74ESE5,,,\"You have spaces between URL and the string, try to use\\n```URL=\"\"<https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>\"\"```\",1642898827.001000,1642899271.001100,U02TEKL21JQ\\nE88028F2-6B5F-490B-9E5D-9D6E39C9CD18,U02TXGC7WKH,,,\"When it asks for the password, are you typing the password and hitting return?\",1642896501.499200,1642899634.002100,U02U34YJ8C8\\nff93e938-6e5e-4c9a-a0e5-af6f885cc70c,U02TXGC7WKH,,,That\\'s exactly what i do,1642896501.499200,1642899676.002300,U02TXGC7WKH\\n6CA2EBB6-2C46-4AD5-9ED4-D76014FE76DB,U02TXGC7WKH,,,\"And then it just exits, without any warnings or errors?\",1642896501.499200,1642899707.003100,U02U34YJ8C8\\n757b8ebe-01ea-4fad-986c-76f41bf0b7a6,U02TXGC7WKH,,,\"yes,  that happens with powershell, or git bash console in vscode. But if I use only Git bash console, the console keeps waiting\",1642896501.499200,1642899817.003300,U02TXGC7WKH\\nf40d5f45-9773-44da-81e9-3249cf3cd973,U02TXGC7WKH,,,I would try to bash into the container and try and use pgcli inside it. Maybe is a port forwarding problem or something,1642896501.499200,1642899855.003600,U02TEKL21JQ\\n06ad85fa-7cce-42a9-80d6-74ba388dace3,U02UM74ESE5,,,\"I think the same applies to the other parameters, try removing all spaces before and after = signs\",1642898827.001000,1642900146.003800,U02SUH9N1FH\\n18736c29-5af3-4f5b-8ea4-b54204a937d5,U02UM74ESE5,,,\"Shouldn\\'t really matter for the other variables, they are arguments to the python script. The problem with URL is that in UNIX you need to remove spaces otherwise it interprets it as a command to be run and not as an assingment to a variable. If you type\\n```echo $URL```\\nand nothing comes, it means you did not assing the value properly.\",1642898827.001000,1642900386.004100,U02TEKL21JQ\\nEFF9111C-1D6F-4183-A860-19871A5DC3BB,U02UM74ESE5,,,Thanks <@U02TEKL21JQ> I will try that!,1642898827.001000,1642902989.005800,U02UM74ESE5\\n623dd6ff-67ab-4097-80c0-7dc37bc8d14e,,2.0,,<@U01AXE0P5M3> I think there\\'s a mistake <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/windows.md#terraform|here> on the terraform optional configs such as specifying the Project ID and name.,1642903195.008000,1642903195.008000,U02UX664K5E\\n9d5485d3-8c4c-4c07-aede-fd6af26c6880,U02UX664K5E,,,\"Shouldn\\'t it be:\\n```variable \"\"project\"\" {\\n  description = \"\"Project Name\"\"\\n  default = \"\"Project ID\"\"\\n  type = string\\n}```\\ninstead of:\\n```variable \"\"project\"\" {\\n  description = \"\"Your GCP Project ID\"\"\\n  default = \"\"ny-rides-alexey\"\"\\n  type = string\\n}```\\nI got this error when entering my Project ID as the value for Description:\\n`Error: Error creating Dataset: googleapi: Error 404: Project ny-taxi-rides is not found. Make sure it references valid GCP project that hasn\\'t been deleted.; Project id: ny-taxi-rides, notFound`\",1642903195.008000,1642903313.008200,U02UX664K5E\\ne6e0e7c6-59a0-4c36-a31f-83890abb75c2,U02U5G0EKEH,,,Thanks everyone. I retried the stackoverflow link again and somehow it worked all good now.,1642873732.412000,1642903569.008400,U02U5G0EKEH\\n5211e4a0-e169-45d5-b08b-067cce237c13,,2.0,,\"Hi Guys, I\\'m stuck at this. what am I doing wrong?\",1642904150.009000,1642904150.009000,U02U5G6MXJ8\\n98328617-b089-40a5-8275-b090f663a217,U02U5G6MXJ8,,,\"I could be wrong, but I think it might be your directory. It has a space in it, which can mess things up. Can you try changing \\'Git projects\\' to \\'git_projects\\' or something else that doesn\\'t have spaces?\",1642904150.009000,1642904339.009300,U02U55REW1K\\n65b73ab2-91f9-4d9f-a346-2a87c51b3e79,U02U5G6MXJ8,,,got it.. thx :slightly_smiling_face:,1642904150.009000,1642904397.009600,U02U5G6MXJ8\\nd5a03efa-14f6-4892-bf1f-4751ae1e5f68,U02TMEUQ7MY,,,This worked thanks <@U0290EYCA7Q>,1642864750.357300,1642908126.011300,U02TMEUQ7MY\\n45ed5caf-352b-4d84-88f6-70dd54679ffe,,,,\"```engine = create_engine(\\'<postgresql://postgres:root@metadatachallenge-db-1:5432/metadata_DB>\\')```\\n\",,1642908184.011800,U02U6DR551B\\nd29b453d-0f79-401a-b747-e57e43e6a443,,,,it says host name not resolved,,1642908192.012100,U02U6DR551B\\n9784bdfa-90f5-41bd-ad58-062bf9c3be7c,,,,i tried db as hostname as well,,1642908202.012400,U02U6DR551B\\nf22241f9-f8df-4fea-8e99-953500024e4d,,,,ant resolution?,,1642908208.012600,U02U6DR551B\\nc521f5f3-1b15-4bf1-8e14-b7532ba708c2,,,,i tried giving localhost as well,,1642908371.012900,U02U6DR551B\\n93a05b59-64f7-4218-89e2-16b5dfda913e,,4.0,,\"Hi all, please I need help with this\\nMy pgcli pauses after password prompt.\\n\\nI have tried changing  `-p 5432:5432 to 5431:5432` and its still the same.\",1642909987.015200,1642909987.015200,U02TMEUQ7MY\\nd61bd169-35d9-4f0a-9445-12b268441aad,U01AXE0P5M3,,,\"For MacOS, I used\\n```brew install libpq \\nbrew install pgcli\\npgcli --help```\",1642511128.453300,1642910279.015500,U02SUJRPKRV\\ne4e530bb-2c18-4b33-8878-d4e599413d47,,,,\"Hello All I need help,\\n$ winpty docker run -it \\\\\\n&gt;   -e POSTGRES_USER=\"\"root\"\" \\\\\\n&gt;   -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n&gt;   -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n&gt;   -v &lt;C://Users//User1//2_docker_sql//ny_taxi_postgres_data://var//lib//postgresql//data&gt; \\\\\\n&gt;   -p 5432:5432 \\\\\\n&gt;  postgres:13\\nThe files belonging to this database system will be owned by user \"\"postgres\"\".\\nThis user must also own the server process.\\n\\nThe database cluster will be initialized with locale \"\"en_US.utf8\"\".\\nThe default database encoding has accordingly been set to \"\"UTF8\"\".\\nThe default text search configuration will be set to \"\"english\"\".\\n\\nData page checksums are disabled.\\n\\nfixing permissions on existing directory /var/lib/postgresql/data ... ok\\ncreating subdirectories ... ok\\nselecting dynamic shared memory implementation ... posix\\nselecting default max_connections ... 20\\nselecting default shared_buffers ... 400kB\\nselecting default time zone ... Etc/UTC\\ncreating configuration files ... ok\\nrunning bootstrap script ... 2022-01-23 04:00:40.153 UTC [83] FATAL:  data dir\\nectory \"\"/var/lib/postgresql/data\"\" has invalid permissions\\n2022-01-23 04:00:40.153 UTC [83] DETAIL:  Permissions should be u=rwx (0700) o\\nr u=rwx,g=rx (0750).\\nchild process exited with exit code 1\\ninitdb: removing contents of data directory \"\"/var/lib/postgresql/data\"\"\",,1642910703.016300,U02VBG59VQ9\\na75e3c43-b6fe-4c8f-abaf-ae6e3f35241c,U02TMEUQ7MY,,,\"Can you try this?\\n\\n$ docker-machine ip &lt;my-machine&gt;\\noutput: 192.168.99.100\\n\\n$ pgcli postgresql://&lt;username&gt;@192.168.99.100:5432/&lt;my_db&gt;\",1642909987.015200,1642910714.016400,U0290EYCA7Q\\n2caa1ed3-39e1-454b-92f2-f5f5533b39d6,U02TMEUQ7MY,,,hi you can try to use powershell terminal to key in,1642909987.015200,1642910910.016600,U02U5GQK25C\\nfd367d93-6b8d-4266-a4d7-969e3171899a,U01AXE0P5M3,,,`brew install pgcli`  worked for me.,1642511128.453300,1642911253.016800,U02TF1JFK3J\\na69fa0ae-5d63-4818-a0bd-76771f15395d,U02VBG59VQ9,,,\"try to put two slashes in the first part, but remain one in the second part: -v\\xa0&lt;C://Users//User1//2_docker_sql//ny_taxi_postgres_data:/var/lib/postgresql/data&gt;\\xa0\\\\\",1642813718.259500,1642911761.017200,U02V18P84BT\\n1a616ef0-6e2d-482c-a41f-d569d4a9951b,U02VBG59VQ9,,,\"$ winpty docker run -it \\\\\\n&gt;   -e POSTGRES_USER=\"\"root\"\" \\\\\\n&gt;   -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n&gt;   -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n&gt;   -v &lt;C://Users//User1//2_docker_sql//ny_taxi_postgres_data:/var/lib/postgresql/data&gt; \\\\\\n&gt;   -p 5432:5432 \\\\\\n&gt;  postgres:13\\nThe files belonging to this database system will be owned by user \"\"postgres\"\".\\nThis user must also own the server process.\\n\\nThe database cluster will be initialized with locale \"\"en_US.utf8\"\".\\nThe default database encoding has accordingly been set to \"\"UTF8\"\".\\nThe default text search configuration will be set to \"\"english\"\".\\n\\nData page checksums are disabled.\\n\\nfixing permissions on existing directory /var/lib/postgresql/data ... ok\\ncreating subdirectories ... ok\\nselecting dynamic shared memory implementation ... posix\\nselecting default max_connections ... 20\\nselecting default shared_buffers ... 400kB\\nselecting default time zone ... Etc/UTC\\ncreating configuration files ... ok\\nrunning bootstrap script ... 2022-01-23 04:27:18.634 UTC [82] FATAL:  data directory \"\"/var/lib\\n/postgresql/data\"\" has invalid permissions\\n2022-01-23 04:27:18.634 UTC [82] DETAIL:  Permissions should be u=rwx (0700) or u=rwx,g=rx (07\\n50).\\nchild process exited with exit code 1\\ninitdb: removing contents of data directory \"\"/var/lib/postgresql/data\"\"\",1642813718.259500,1642912079.017400,U02VBG59VQ9\\ncdb8a59d-ec1c-4e32-aa23-a733f6049451,U02VBG59VQ9,,,<@U02V18P84BT>,1642813718.259500,1642912090.017600,U02VBG59VQ9\\n7c597a48-ae7e-4c22-a0a9-18b4f4524ac9,,,,\"$ winpty docker run -it \\\\\\n&gt;   -e POSTGRES_USER=\"\"root\"\" \\\\\\n&gt;   -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n&gt;   -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n&gt;   -v &lt;C://Users//User1//2_docker_sql//ny_taxi_postgres_data:/var/lib/postgresql/data&gt; \\\\\\n&gt;   -p 5432:5432 \\\\\\n&gt;  postgres:13\\nThe files belonging to this database system will be owned by user \"\"postgres\"\".\\nThis user must also own the server process.\\n\\nThe database cluster will be initialized with locale \"\"en_US.utf8\"\".\\nThe default database encoding has accordingly been set to \"\"UTF8\"\".\\nThe default text search configuration will be set to \"\"english\"\".\\n\\nData page checksums are disabled.\\n\\nfixing permissions on existing directory /var/lib/postgresql/data ... ok\\ncreating subdirectories ... ok\\nselecting dynamic shared memory implementation ... posix\\nselecting default max_connections ... 20\\nselecting default shared_buffers ... 400kB\\nselecting default time zone ... Etc/UTC\\ncreating configuration files ... ok\\nrunning bootstrap script ... 2022-01-23 04:27:18.634 UTC [82] FATAL:  data directory \"\"/var/lib\\n/postgresql/data\"\" has invalid permissions\\n2022-01-23 04:27:18.634 UTC [82] DETAIL:  Permissions should be u=rwx (0700) or u=rwx,g=rx (07\\n50).\\nchild process exited with exit code 1\\ninitdb: removing contents of data directory \"\"/var/lib/postgresql/data\"\"\",,1642912154.017900,U02VBG59VQ9\\nab072a83-347c-4236-b825-a45a1dc8aa5a,U01QGQ8B9FT,,,<@U01AXE0P5M3> I tried doing that but when I do `py -0p` it still shows that the other python(not conda ) is active i.e has * on it. Maybe I have done something wrong! pgcli also has the same problem now :smiling_face_with_tear:,1642769991.086200,1642912155.018000,U01QGQ8B9FT\\n8e02f2c7-fa63-4191-8369-641123b0601f,,,,\"`winpty docker run -it   -e POSTGRES_USER=\"\"root\"\"   -e POSTGRES_PASSWORD=\"\"root\"\"   -e POSTGRES_DB=\"\"ny_taxi\"\"   -v D:\\\\Work\\\\data-engineering-zoomcamp\\\\week1\\\\ny_taxi_postgres_data:/var/lib/postgresql/data   -p 5432:5432   postgres:13`\\n`docker: Error response from daemon: The system cannot find the file specified.`\\n`See \\'docker run --help\\'.`\\n\\nWhy i got this error? Can someone help me? Thanks before\",,1642914035.022100,U02BQEJKMR8\\nbe950984-f8ab-45bf-9067-fbc22500b1f6,,30.0,,\"Hi guys, pls help. I have been trying this for the last 2 days.\\n`&gt; docker run -it\\\\`\\n  `-e POSTGRES_USER=\"\"root\"\"\\\\`\\n  `-e POSTGRES_PASSWORD=\"\"root\"\"\\\\`\\n  `-e POSTGRES_DB=\"\"ny_taxi\"\"\\\\`\\n  `-v //d/data-engineering-zoomcamp/docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data\\\\`\\n  `-p 5432:5432\\\\`\\n  `postgres:13`\\n I used the above command to get the files, but my pgcli is just stuck after getting the password. Moreover, My volumes in the docker desktop show nothing. Is that the problem?\\n[ I am using git bash in windows]\\n<@U01AXE0P5M3> and others, where am I going wrong?\",1642914211.023000,1642914211.023000,U02UB8XDCHJ\\nadd24b87-e6b4-41a5-b724-6dbbd39a9d51,U02UY1QTGHW,,,<@U02UY1QTGHW> I too encountered same error. Did the any of the resolutions from above post worked for you?,1642721355.469800,1642914919.024100,U02UT0K6DK5\\n3ac9fb42-4c60-4976-9e1f-45514e4d11a9,U02UB8XDCHJ,,,maybe its -v what do you get when you do `pwd` in the directory?,1642914211.023000,1642915120.024300,U02ULBM39B4\\n39dac1bf-6dcd-4901-805e-35aaaa756315,U02UY1QTGHW,,,add winpty in front,1642721355.469800,1642916161.024700,U02UY1QTGHW\\na73ee636-9b15-47d0-9af2-d235597b09af,U02UB8XDCHJ,,,\"Hello, try using VS Code instead of git bash when running pgcli\",1642914211.023000,1642916580.024900,U02QPQFUU5Q\\nd0b6f7fb-5970-42d4-80ed-4d3c704b5070,U02UB8XDCHJ,,,What does it show for you guys when you open up volumes in docker desktop? Mine is empty <@U02ULBM39B4> <@U02QPQFUU5Q>,1642914211.023000,1642916636.025500,U02UB8XDCHJ\\n8772306e-a0c8-4a4f-821b-15c9ea59eac4,,2.0,,Does anyone know what is the WIndows 10 version of this linux command. `sudo chmod a+rwx ny_taxi_postgres_data`,1642916703.026200,1642916703.026200,U02UY1QTGHW\\n6ad9fd53-04a6-4e51-b927-190f62cc90bc,U02UY1QTGHW,,,\"Apparently  i need to adjust the permissions of the folder, as its blank after successfully postgres install\",1642916703.026200,1642916821.026300,U02UY1QTGHW\\n8f9d1e5e-18a9-4b5b-a93d-7f1f22d1ad56,U02UB8XDCHJ,,,can you try `docker volume ls`,1642914211.023000,1642917698.026800,U0290EYCA7Q\\n3a064b5c-9866-4939-b572-c86843c58d55,U02UB8XDCHJ,,,and try like &lt;D://data-engineering-zoomcamp&gt;,1642914211.023000,1642917758.027000,U0290EYCA7Q\\na51aec35-675e-43b8-a394-795e781d6232,U02UX664K5E,,,Is ny-taxi-rides the name of your project in GCP?,1642903195.008000,1642919440.027400,U01AXE0P5M3\\n8bba56b2-c1e6-45ed-8f88-432e6baa0409,U02UB8XDCHJ,,,<@U0290EYCA7Q> It is empty for the docker volume list command.,1642914211.023000,1642921511.027600,U02UB8XDCHJ\\ncc69e946-40fa-45db-9917-b713160d7ac4,U02UB8XDCHJ,,,\"I can see that the folder with the files are getting created in the ny_taxi_postgres_data, but the pgcli doesn\\'t work. Referring to the docker volumes shows that a volume has not been created. I don\\'t understand why <@U01AXE0P5M3> <@U0290EYCA7Q>\",1642914211.023000,1642921621.027800,U02UB8XDCHJ\\n632f1a1a-431a-47a8-a3e7-54307a5f823b,U02R09ZR6FQ,,,<@U02QM7H8G4U> were you able to solve this problem?,1642891400.489400,1642921867.028000,U02UB8XDCHJ\\n3abd26cc-b221-45f2-a6e2-fd383ac8a7a7,U02UB8XDCHJ,,,\"Can You try this?\\n\\n```python -c \"\"import psycopg2; psycopg2.connect(database=\\'mydb\\', user=\\'username\\', password=\\'secret\\', port=5432)\"\"```\",1642914211.023000,1642922242.028200,U0290EYCA7Q\\n37232498-779b-4c2a-bdb1-d6f0e505d37a,,16.0,,\"Good morning everyone, I am still having issues with pgcli, after installing  and trying to run pgcli, i get this error\\n``` bash: pgcli: command not found```\",1642923637.029500,1642923637.029500,U02RH0V5K33\\ned605318-d16f-4bbb-ab82-da160c00e14e,U02R09ZR6FQ,,,Maybe try e:/... instead of /e/. This is how I do it on windows with gitbash,1642891400.489400,1642924175.029600,U01AXE0P5M3\\n635733da-b613-46c9-a26d-ecc8de7df51d,U02RH0V5K33,,,are you using a virtual environment,1642923637.029500,1642924347.030100,U02QKMCV39R\\nfbeb1faa-6ba0-4432-8352-08773ac67f86,U02RH0V5K33,,,same as command prompt. you tried last night,1642923637.029500,1642924601.030300,U0290EYCA7Q\\n5d81ad61-e78a-4847-af02-dfc952670441,U02RH0V5K33,,,\"1. Go to the script directory.\\n2. run `pwd` and copy the result.\\n3. Add it to PATH `export PATH=&lt;COPIED&gt;:$PATH`\\n4. Run `echo $PATH` to confirm\",1642923637.029500,1642924645.030500,U0290EYCA7Q\\nc58d607c-46aa-4e53-90fb-a60f760b97a9,U02UEE4MBEG,,,You have too many :\\'s in -v,1642748652.027100,1642752490.029700,U01AXE0P5M3\\nbfb47808-b6a9-4c1c-b50a-889dc131db22,U02BVP1QTQF,,,\"This is nice, <@U02BVP1QTQF> ! Any thoughts on covering the Terraform workshop too?\",1642713863.457200,1642754557.030400,U01DHB2HS3X\\nf49ec535-60b6-4370-a14c-7352a89b6124,U02QZN0LSBT,,,Looks nice! Any thoughts on covering the Terraform part too?,1642701743.413300,1642754993.030700,U01DHB2HS3X\\n1ad799d4-1335-476a-a4ce-9c8857aa7597,,2.0,,\"Good morning everyone :) How the study is going?\\nSo far I didn\\'t do the Terraform question from the HW but the rest is done.\\nI am finally starting to understand Docker !!! :tada: :tada: :tada:\",1642755698.033900,1642755698.033900,U02CD7E30T0\\n87bc13f9-95ef-4c68-b3b7-85da0379de2c,U02BVP1QTQF,,,My notes are pretty rambly. Your notes are much precise. Thanks for sharing!,1642713863.457200,1642755716.034000,U02QZN0LSBT\\n9cdef6d6-39ee-412c-8542-6c01f2b65557,U02QZN0LSBT,,,Trying my best...pretty crammed right now hopefully I will be able to deliver.,1642701743.413300,1642755808.034200,U02QZN0LSBT\\nd7abb0c0-8251-482a-a79c-a5e16b111541,U02UGA597HS,,,\"The material is online, and mondays is the office hours\",1642722251.475100,1642756118.034600,U01B6TH1LRL\\n38a8e246-71f2-42cb-9ce3-c2b690623a04,,,,\"Hi Everyone, How\\'s the study going so far? \\nI have been stuck on setting up  gcloud on my windows machine.\\nWhen run the gcloud version on Git Bash it throws an error but when I run the same command on cmd it works fine. Any help on how I can fix it. I have attached a screenshot below.\",,1642756387.038600,U02RWK442P9\\n25eb6c01-aa49-4a39-a93c-d5573e15b981,U02AUCL9ZQF,,,I solved mine by getting a virtual card.,1642748175.026300,1642756435.039100,U02RWK442P9\\n96a3e3ea-3c67-407b-8b76-56eaa81e8b64,U02TNEJLC84,,,\"My bad, I did the total count of records :see_no_evil:\\nI updated the form already, let me know if that looks closer now. Sorry!\",1642725704.482700,1642756456.039300,U01B6TH1LRL\\n821743c7-e3db-4027-a359-20b3103a8147,U02UY1QTGHW,,,\"Hey <@U02UY1QTGHW> small suggestion to make the messages and the channel easier to read, you can first send a message with the topic and then reply to that thread with the details\",1642731532.490100,1642756544.039500,U01B6TH1LRL\\nc374d2f1-01d6-4c6c-9489-10f27f4dc2c1,U02UY1QTGHW,,,Thanks will do,1642731532.490100,1642756581.039800,U02UY1QTGHW\\n7995c02e-4729-4ceb-9fb6-0d2d8856b66d,,,,\"Please don\\'t share the screenshots with your errors - copy-paste the text instead. For me it\\'s very difficult to see the text on the screenshots, especially on mobile\",,1642757632.042600,U01AXE0P5M3\\n289D8762-CEFF-4266-A93F-7F2CA17465FD,U02BVP1QTQF,,,\"<@U01DHB2HS3X> yes, but I was waiting for the video because I have a harder time with audio-only sources, but I\\'ll probably get to it today.\\n\\n<@U02QZN0LSBT> your notes are less dense than mine and easier to parse, I think. I enjoyed looking at them\",1642713863.457200,1642757764.045700,U02BVP1QTQF\\ned111aa9-bf73-4624-8628-b2f2868f82c2,U02BVP1QTQF,,,\"<@U02BVP1QTQF> yes, unfortunately our video became distorted during editing, and I would need to do redo it.\\nBut we do have notes here instead: <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/1_terraform_overview.md#declarations|https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/1_terraform_overview.md#declarations>\",1642713863.457200,1642757968.046500,U01DHB2HS3X\\n14f7dd45-d5f3-48f9-add6-3a2413d44d71,,,thread_broadcast,\"Hello Sejal and Alexey. I don\\'t what I\\'ve done before but I just cloned the repo into the WSL and it worked :neutral_face:\\nWell... at least is done :slightly_smiling_face:\",1642693507.370300,1642758074.048000,U02CD7E30T0\\n979e5a8b-f748-4249-abf0-a28099a0381c,U02CD7E30T0,,,Glad to hear! :clap:,1642693507.370300,1642758294.049600,U01DHB2HS3X\\n1CDB209A-1C46-4A89-A3D3-97FF6B3257C6,U02BVP1QTQF,,,\"Understood, thank you\",1642713863.457200,1642758354.050300,U02BVP1QTQF\\n837b33e1-fcf6-45f2-a332-ada38e896d72,U02CD7E30T0,,,Hi. I still haven\\'t don this point :slightly_smiling_face:. I think I need to have something file for command \\'terraform init\\' but I didn\\'t see about it in playlist. I plan to deal with this on the weekend. Today I want to relax and play computer games with my friends :grin:,1642755698.033900,1642758562.051100,U02QL1EG0LV\\n24f14195-8cf6-4b46-a458-76f9e505d82b,,,,\"ok so funny thing is happening, docker is not giving me the absolute path. Does anyone know how to get the absolute path like c:\\\\.....\\nReason I am asking is because I cant see any files in my ny_taxi_postgres_data folder so I am guessing the files are being created somewhere else.\\n\\n`d751673@W00000R90XVVN1 MINGW64 /abic7/git/data-engineering-zoomcamp/week_1_basics_n_setup/2_docker_sql (master)`\\n`$ pwd`\\n`/abic7/git/data-engineering-zoomcamp/week_1_basics_n_setup/2_docker_sql`\",,1642758653.052000,U02UY1QTGHW\\nadc851ec-d948-4260-9cfc-203fcf8c921e,U02QW0M1G9J,,,<@U02QW0M1G9J> Can I find out what\\'s the path that worked for you in your docker command? Was it like $(pwd)/nyc_taxi?,1642672783.324500,1642760149.052800,U02T941CTFY\\n264140be-0187-4453-9f76-90f22fe861da,U02CD7E30T0,,,\"<@U02UECC4H6U> hello, i encountered same issue as u when i did it without quotes. double quotes will work.\\nEg. `select \"\"DOLocationID\"\" from yellow_taxi_data limit 10`\",1642673343.329000,1642760237.053100,U02ULMHKBQT\\n670b7c2c-a256-4d1d-ae74-a405a83e678f,,,,Hi! Can someone enlighten me on the difference between using *VS Code on Windows* vs *VS Code on WSL*?,,1642761061.054200,U02T941CTFY\\nA7AEE9AD-3CF0-49BB-A99F-F1D84695CED0,U02QW0M1G9J,,,\"Yes, but outside /mnt/disk folder. In usual Linux directories\",1642672783.324500,1642761202.055600,U02QW0M1G9J\\n07f9570e-d263-4626-b047-7980dd22ca6e,,4.0,,Hi! Can someone help me? When I try to connect to the database by pgcli after password prompt I enter the password and it is hanging only cursor blinking and no reaction at all,1642762426.057700,1642762426.057700,U02U5Q8FK38\\ne87c0c8f-7f44-443c-88e7-b202d958126d,U02U5Q8FK38,,,\"Yeah i\\'m also having the same issue here. Been stuck for awhile. However, the password works if you run it in cmd interestingly enough\",1642762426.057700,1642762597.058000,U02T18VH90F\\n5c8866cf-8a0c-46c5-9d09-e24dd2797222,U01AXE0P5M3,,,\"Other option, as I\\'m seeing on Udacity, would be dedicated channels for each lesson.\",1642711485.444800,1642762937.058200,U02GVGA5F9Q\\n0bfe5cfe-fcff-4c01-905d-23d540ac5586,U02U5Q8FK38,,,\"yes, I tried to run this on VS code and it works.\",1642762426.057700,1642764212.060700,U02U5Q8FK38\\n5006077a-5d87-46d3-a71a-9daaa4f66c98,,18.0,,\"Situation: I\\'m trying to run postgresql in the docker container, but nothing gets created in the nyc_taxi_postgres_data folder.\\nEnvironment: WSL on Windows\\nWhat I\\'ve tried: The given code, twice: Once to a folder on C drive, and another to a folder on $(pwd)\",1642765009.063500,1642765009.063500,U02T941CTFY\\ncf8c421e-2ec6-45ca-adb9-408c8dd35a36,U02T941CTFY,,,\"First try\\n```docker run -it \\\\\\n    -e POSTGRES_USER=\"\"root\"\" \\\\\\n    -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n    -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n    -v $(pwd)/data-engineering-zoomcamp/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data:/var/postgresql/data \\\\\\n    -p 5432:5432 \\\\\\n    postgres:13```\\nSecond try\\n```docker run -it \\\\\\n    -e POSTGRES_USER=\"\"root\"\" \\\\\\n    -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n    -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n    -v /c/users/ztmj9/data-engineering-zoomcamp/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data:/var/postgresql/data \\\\\\n    -p 5432:5432 \\\\\\n    postgres:13```\\n\",1642765009.063500,1642765185.063600,U02T941CTFY\\n1238076f-dfe1-43e2-bdc7-601a5239888f,,9.0,,\"Yesterday I finished coding-along docker-sql videos and wanted to share my feedback.\\n\\n• It took me three evenings, ca. 2h each - mostly because I decided to use Ubuntu for the course, which I barely have experience with and my sloppy typing :sweat_smile:\\n• I barely had any technical problems. There were some minor issues related either to my inexperience with Linux or typos. I also need to call all docker commands with sudo and often forget it\\n• I like that Alexey has chopped his material over relatively short videos, it\\'s easier to follow, and one is more flexible when to break the sessions\\n• Alexey is a pro typer and VSCode user - sometimes things are changing too fast and I needed to rewind the video a little bit and even slow it down to follow what was going on\\n• I don\\'t know VSCode tricks like multiline editing and autocompletion does not work for whatever reason, so it takes much longer for me to code\\n• The video titles were changed at some point, which caused some confusion - it would be helpful to have them numbered\\nOtherwise, great start. Looking forward to getting my hands dirty with GCP and Terraform!\",1642765882.066900,1642765882.066900,U02Q7JMT9P1\\nb8ee493e-e2e8-4b68-96a2-d018a3dd64a7,U02T941CTFY,,,\"I had the same issue! But for me doing it from /home on WSL worked - it did create a `nyc_taxi_postgres_data` folder with all the contents\\n\\nTo be more specific : you must be having a path like `mnt\\\\c\\\\..` , if yes then just do `cd /home` and then cd into your project directory. And try and run instructions(the `docker run -it ...`) from here\",1642765009.063500,1642766043.068900,U01QGQ8B9FT\\n806a5d0c-6973-489a-b702-0cdbd0a1ac73,,4.0,,\"Hi was running this on git bash in windows (from the running postgres in docker video)\\n```winpty docker run -it \\\\\\n\\xa0 \\xa0 -e \\xa0POSTGRES_USER =\"\"root\"\" \\\\\\n\\xa0 \\xa0 -e \\xa0POSTGRES_PASSWORD =\"\"root\"\" \\\\\\n\\xa0 \\xa0 -e \\xa0POSTGRES_DB =\"\"ny_taxi\"\" \\\\\\n\\xa0 \\xa0 -v \\xa0d:/DE_Zoom/Week 1/2_docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n\\xa0 \\xa0 -p 5432:5432\\\\\\npostgres:13```\\nGot an error stating:\\n```docker: invalid reference format. ```\\nCan someone help me as to what I am doing wrong in my format here?\",1642766045.069100,1642766045.069100,U02UZBJ2Q6L\\ndc5427cf-94af-44e6-a2c0-e25ad87247c8,U02UZBJ2Q6L,,,\"Oh thats a shame, but thanks for the  information.\",1642680003.337000,1642766077.069200,U02UZBJ2Q6L\\n4bd5e70f-c4c4-4372-9208-6adbd35846ae,U02T941CTFY,,,But had trouble with pgcli later on! So I switched from WSL to GitBash  and started from scratch - have faced no problems yet!,1642765009.063500,1642766120.069500,U01QGQ8B9FT\\n8b89005f-b69f-47fe-bdc7-522f8cc16d72,,,,Hello everyone! I am getting following error when I run the python code to download and insert in database in the dockerizing the ingestion video. *ValueError: invalid literal for int() with base 10: \\'{port}\\'*  Can anyone help me for this? Thanks,,1642766236.070900,U02TQUYTBJA\\nc338f4b0-dc61-438e-894d-1dc82d82c8ac,,,,\"<@U02UZBJ2Q6L> you should delete space before \\'=\\' for each env variable it should be POSTGRES_USER=\"\"root\"\"\",,1642766543.072800,U02TQUYTBJA\\n3955083b-a239-4b07-b058-35b8504db485,U02UZBJ2Q6L,,,\"you should delete space before \\'=\\' for each env variable it should be POSTGRES_USER=\"\"root\"\"\",1642766045.069100,1642766756.072900,U02TQUYTBJA\\n14b160dc-d355-4a44-b0d7-877e8f531580,U02Q7JMT9P1,,,\"Thank you for sharing, this is very useful :pray:\\nWe are also collecting this kind of feedback in the homework form (in case you haven\\'t seen it yet) and we are already acting on many of the comments in this channel as well. Please keep sharing :slightly_smiling_face:\",1642765882.066900,1642766968.073300,U01B6TH1LRL\\n0e8c26b6-2f00-43a1-95e3-b1b0e415130b,,3.0,,\"Why it doesn\\'t output DDL format for postgres?\\n\\n```from sqlalchemy import create_engine\\nurl=f\\'<postgresql://root:root@localhost:5432/ny_taxi>\\'\\nengine = create_engine(url, client_encoding=\\'utf8\\')\\nengine\\n\\nEngine(<postgresql://root>:***@localhost:5432/ny_taxi)```\\n```print(pd.io.sql.get_schema(df, name=\\'yellow_taxi_data\\', con=engine))\\n\\nCREATE TABLE \"\"yellow_taxi_data\"\" (\\n\"\"VendorID\"\" INTEGER,\\n  \"\"tpep_pickup_datetime\"\" TIMESTAMP,\\n  \"\"tpep_dropoff_datetime\"\" TIMESTAMP,\\n  \"\"passenger_count\"\" INTEGER,\\n  \"\"trip_distance\"\" REAL,\\n  \"\"RatecodeID\"\" INTEGER,\\n  \"\"store_and_fwd_flag\"\" TEXT,\\n  \"\"PULocationID\"\" INTEGER,\\n  \"\"DOLocationID\"\" INTEGER,\\n  \"\"payment_type\"\" INTEGER,\\n  \"\"fare_amount\"\" REAL,\\n  \"\"extra\"\" REAL,\\n  \"\"mta_tax\"\" REAL,\\n  \"\"tip_amount\"\" REAL,\\n  \"\"tolls_amount\"\" REAL,\\n  \"\"improvement_surcharge\"\" REAL,\\n  \"\"total_amount\"\" REAL,\\n  \"\"congestion_surcharge\"\" REAL\\n)```\",1642767618.075000,1642767618.075000,U0290EYCA7Q\\n891f9557-742b-4d50-bd5f-8d0b86685ab1,U02TNEJLC84,,,It looks like we should only count trips that were picked up the 15th? (not the ones only dropped off the 15th),1642725704.482700,1642767644.075100,U02UD2P3BQC\\n7fea62bf-2d59-4501-a155-ef8933f58af4,U0290EYCA7Q,,,\"Did you install sqlalchemy after you had already loaded pandas and run a few cells in your notebook? If so, restart your kernel and rerun everything.\",1642767618.075000,1642767850.075400,U02BVP1QTQF\\n2fc738ee-5f06-476a-9351-45011222fc8c,U02TNEJLC84,,,Yes!,1642725704.482700,1642768038.075600,U01B6TH1LRL\\n264bd741-c21c-4a45-acb3-2b6fa7513659,,5.0,,\"Where is `output.csv` outputted to from the line `os.system(f\"\"wget {url} -O {csv_name}\"\")`?\\n\\nI expected it in my cwd, but doesn’t show up. It is being created, and everything is working fine. I’m just wondering where this file is.\",1642768298.077200,1642768298.077200,U02U34YJ8C8\\ndad3fe80-3512-45c7-b5d8-fcc1885f49e0,U02UZBJ2Q6L,,,Oh my god completely missed something right in front of my face! Thank you it worked,1642766045.069100,1642768317.077300,U02UZBJ2Q6L\\nbe0b072d-79c0-4cb0-a76c-84c6aa22ecd9,U02Q7JMT9P1,,,\"<@U01B6TH1LRL> I feel that having a little more structure to the issues faced by the people would be greatly helpful - especially at the time of the final project!\\n\\nMaybe something like\\n\\nTag : Postgres\\nWeek : 1\\nIssue : could not initialize the connection\\nSystem : WSL(Ubuntu 20.04)\\n.\\n.\\n.\\n\\n Could be anything that can be used later to index easily by searching in the channel\",1642765882.066900,1642768326.077500,U01QGQ8B9FT\\ncfaa70f5-e5ed-4153-ae37-a7a468b6dcb8,U02U34YJ8C8,,,\"Maybe you can run ls -r | grep \"\"output.csv\"\" from the root directory.\",1642768298.077200,1642768407.078100,U0290EYCA7Q\\n3f6f7f99-cafc-407b-9fc2-29cf838aeeb6,U02U34YJ8C8,,,\"If you’re running the dockerized script rather than running it locally, the file will be downloaded inside the container and thus it will be automatically deleted once you kill the container, unless you’re mounting a volume to store the csv externally.\",1642768298.077200,1642768531.078400,U02BVP1QTQF\\n52db1b49-4565-4ca0-b4d6-a0816b5ddbd0,U0290EYCA7Q,,,\"Thanks. it worked, but why so?\",1642767618.075000,1642768540.078600,U0290EYCA7Q\\n661302c0-5c22-4f5c-bd19-3b0048ce89d9,U02U34YJ8C8,,,\"if you run the script locally, you’ll find the output.csv file in the same folder you ran the script from\",1642768298.077200,1642768588.078900,U02BVP1QTQF\\ndd96dffb-87f4-4071-99d8-1a091fb9dee3,U02U4G7U3GV,,,I am still experiencing freezing after enetering my password for the pgcli,1642523612.482900,1642769493.081000,U02T1BX1UV6\\n54086693-9104-4d3e-b9d8-6afb57685b86,U0290EYCA7Q,,,\"I’m sure that someone else can give you a better explanation because I don’t know exactly how loading libraries works, but you’re asking pandas to output DDL but in order to get the actual Postgres instructions it needs to talk to sqlalchemy. However, when you imported pandas, sqlalchemy did’t “exist” and pandas ignored it. You installed sqlalchemy afterwards but it still didn’t exist for pandas because at the moment of importing the library it didn’t exist, and it keeps “not existing” until the end of the execution.\\n\\nBy resetting the kernel and rerunning everything, when pandas loads it checks for sqlalchemy and sees that it’s there, so everything works as expected.\\n\\nThis is my understanding of this issue but I could be wrong. If anyone else knows the actual cause, please feel free to correct me.\\n\\nBut anyway, any time that you’re missing a library and need to install it, it’s always good practice to rerun everything in case there are some mysterious dependencies that you did not account for.\",1642767618.075000,1642769520.081600,U02BVP1QTQF\\nfada342e-8572-465a-8a21-8ac41c01e61a,U02UD2P3BQC,,,\"Is it OK to do write code in `pandas` instead of SQL? Unfortunately, I am not confident in SQL yet\",1642729727.485900,1642769581.082700,U02QKCUF9QU\\n4629f1cd-a9b0-4c5d-bcc3-86b31751839e,,3.0,,\"```winpty docker run -it \\\\\\n\\xa0 \\xa0 -e \\xa0POSTGRES_USER=\"\"root\"\" \\\\\\n\\xa0 \\xa0 -e \\xa0POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n\\xa0 \\xa0 -e \\xa0POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n\\xa0 \\xa0 -v \\xa0d:/de_zoom/week_1/2_docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n\\xa0 \\xa0 -p 5432:5432 \\\\\\npostgres:13```\\nThe code managed to download an image for postgres but I also gave an error :\\n```Status: Downloaded newer image for postgres:13\\ndocker: Error response from daemon: invalid mode: \\\\Program Files\\\\Git\\\\var\\\\lib\\\\pos\\ntgresql\\\\data.```\\nWhat is this error related to? A preliminary search on google mentioned creating a volume, which I did not understand\",1642769587.082900,1642769587.082900,U02UZBJ2Q6L\\nbbc19d02-5c00-4478-bb0e-2b166e27fc55,U02Q7JMT9P1,,,\"&gt;  The video titles were changed at some point, which caused some confusion - it would be helpful to have them numbered\\n+1\",1642765882.066900,1642769876.083300,U02TGS5B4R1\\n1fdc5826-f971-4eff-8c4c-6300dcf05e94,,,,\"Sejal cut off at the end of the terraform intro video before doing any of the terraform configuration, I noticed I have 1 hidden video that\\'s unavaible\",,1642769952.085200,U02TVGE99QU\\n741e2502-2ef4-455b-b0cb-a2a1b0e39051,,3.0,,was there a continuation for that?,1642769978.086000,1642769978.086000,U02TVGE99QU\\n52882f5a-1948-47b1-b186-1596d545b995,,11.0,,Problem : Stuck at the command `create_engine(\\'<postgresql://root:root@localhost:5432/ny_taxi>\\')` in jupyter nb.,1642769991.086200,1642769991.086200,U01QGQ8B9FT\\n70d808b1-62b0-4c75-93f1-31882297667c,U01QGQ8B9FT,,,\"The error is `No module named \\'psycopg2\\'` , Hence I tried `pip install psycopg2` \\n\\nGetting this :\\n`Defaulting to user installation because normal site-packages is not writeable`\\n`Requirement already satisfied: psycopg2 in c:\\\\users\\\\mistr\\\\appdata\\\\roaming\\\\python\\\\python38\\\\site-packages`\\n\\nAlso saw in one of the thread to go for `pip install psycopg2-binary`:\\nGetting this :\\n\\n`Defaulting to user installation because normal site-packages is not writeable`\\n`Collecting psycopg2-binary`\\n  `Downloading psycopg2_binary-2.9.3-cp38-cp38-win_amd64.whl (1.1 MB)`\\n     `|████████████████████████████████| 1.1 MB 136 kB/s`\\n`Installing collected packages: psycopg2-binary`\\n`ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: \\'C:\\\\\\\\Users\\\\\\\\mistr\\\\\\\\AppData\\\\\\\\Roaming\\\\\\\\Python\\\\\\\\Python38\\\\\\\\site-packages\\\\\\\\psycopg2\\\\\\\\_psycopg.cp38-win_amd64.pyd\\'`\\n`Check the permissions.` \",1642769991.086200,1642770246.086900,U01QGQ8B9FT\\n826319db-7d97-46cc-9632-aa271ae2fc94,U02Q7JMT9P1,,,\"<@U02Q7JMT9P1> If you want to run docker without sudo, you have to add your user to the docker group..\",1642765882.066900,1642771050.087200,U02T9JQAX9N\\n8f142cbe-c0aa-484a-97f4-799ed4258b08,U02Q7JMT9P1,,,\"<https://docs.docker.com/engine/install/linux-postinstall/>\\n\\nThis should help you with that\",1642765882.066900,1642771071.087400,U02T9JQAX9N\\n9af1b27a-af02-4e17-97bf-09763bf35860,U02TVGE99QU,,,\"Seems there is only an audio file available because the video was corrupted somehow. The visuals seem pretty important for this part though so not sure the audio is very useful :smile:\\n\\n<https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup#gcp--terraform>\",1642769978.086000,1642771286.088000,U02TGS5B4R1\\n55db01ca-d43e-4d65-8759-58b1be7a07b3,U02Q7JMT9P1,,,Thank you! We\\'ll add numbers back,1642765882.066900,1642771494.088800,U01AXE0P5M3\\n94b5996e-8f46-41c7-be5a-72dda5416d40,U02UD2P3BQC,,,In this case I\\'d strongly encourage you not to use Pandas because for other weeks you\\'ll need to use SQL,1642729727.485900,1642772563.089400,U01AXE0P5M3\\n17d8c8ac-f585-4585-a9cf-856a44bfae19,U01AXE0P5M3,,,\"interesting idea, but that would be too many channels :slightly_smiling_face: I\\'m creating the one for announcements now\",1642711485.444800,1642772633.089800,U01AXE0P5M3\\n687102dd-b075-486b-9f7c-0e66ed4a06f3,U02Q7JMT9P1,,,Updated the titles - now they have the numbres,1642765882.066900,1642772731.090200,U01AXE0P5M3\\n018f3253-7ee2-40b3-94b5-4358597a712f,U02TNEJLC84,,,I\\'ve updated the instructions - hopefully it makes it less ambiguous,1642725704.482700,1642772853.092300,U01AXE0P5M3\\n3a9ea23b-8679-4506-9f64-7222dcbd7c1d,,,,\"When I tried to run the container again, i\\'m getting the below shared error response. I solved it by renaming the name in --name parameter but  I don\\'t understand why I have to do this every time when I run the container. (Also I killed all the running container before doing this)\\n```docker run -it \\\\\\n  -e PGADMIN_DEFAULT_EMAIL=\"\"<mailto:admin@admin.com|admin@admin.com>\"\" \\\\\\n  -e PGADMIN_DEFAULT_PASSWORD=\"\"root\"\" \\\\\\n  -p 8080:80 \\\\\\n  --network=pg_network \\\\\\n  --name pgadmin \\\\\\n  dpage/pgadmin4```\\n`docker: Error response from daemon: Conflict. The container name \"\"/pgadmin\"\" is already in use by containe`\\n`r \"\"b5acb37bc5fd1b09adcf4cbe2da557a34215f825f15202b89524bf173d85b226\"\". You have to remove (or rename) that`\\n `container to be able to reuse that name.`\",,1642772932.093800,U0297ANJF6F\\n,,,channel_join,<@U01FB212DCL> has joined the channel,,1642772983.094700,U01FB212DCL\\nffd96b97-5007-4693-95b2-205971778484,,,,\"Hi, one question. On git bash, I cannot see the python environment (base), so the python commands like pip cannot work. I fixed it using the anaconda prompt, but it does not seem very accurate, since I am switching all time from one terminal to other. Any idea? Thank you!\",,1642773100.096300,U02UX01PGCC\\n5a7f1eac-e3f4-414d-ad20-e19a15bd6e15,,,,\"That was unanimous, so I\\'ve just created <#C02V1Q9CL8K|course-data-engineering-announcements>. Everyone who\\'s currently in this channel will be automatically added there, but those who join later will need to join manually.\\n\\nI\\'ll forward the announcements here just in case\",,1642773203.098000,U01AXE0P5M3\\n741c3290-4268-4586-b1ad-23dfee3184df,U02TVGE99QU,,,\"I believe the video will be updated soon\\n<@U01AXE0P5M3>  Kindly confirm.\",1642769978.086000,1642773380.098500,U02S9JS3D2R\\n,,,,The announcements will be shared like this,,1642773423.098700,U01AXE0P5M3\\nf431c895-d36b-4602-9c5f-e9e9653b6df0,U02TVGE99QU,,,\"yes, we just uploaded the video and I\\'ll adjust the github page right now\",1642769978.086000,1642773461.098800,U01AXE0P5M3\\nf54df9a7-0fca-49c6-b7b6-93056e2c7917,,1.0,,\"Hi everyone, i have a question about the data ingestion.\\n\\nWe have downloaded the csv file from an S3 bucket to a file and then we used pandas to create the schema.\\nAfter that, we insert into the db in chunks of 100_000 rows.\\n\\nWould it be better (thinking it would be faster) to use SQL to copy from the file we downloaded and do a bulk insert to the db?\\nLike this `\\\\copy yellow_taxi_trips from \\'yellow_tripdata_2021-01.csv\\' delimiter \\',\\' csv header;` or using the `.copy_from()` from `psycopg` lib.\\nIn these both strategies we need to save the csv to disk.\\n\\nWe could also do the bulk insert with pandas. When is it reasonable to do bulk insert vs batch insert?\\n\\nThank you!\",1642774404.103600,1642774404.103600,U02TT5SRDSB\\n9452ad7e-123e-4045-88ee-5a6ec614c751,U02T941CTFY,,,\"Was it like this (after cd /home):\\n```docker run -it \\\\\\n    -e POSTGRES_USER=\"\"root\"\" \\\\\\n    -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n    -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n    -v /ny_taxi_postgres_data:/var/postgresql/data \\\\\\n    -p 5432:5432 \\\\\\n    postgres:13```\",1642765009.063500,1642774638.103800,U02T941CTFY\\nfece89b5-0b0f-483e-8c9b-b18065e88d66,,2.0,,\"hi everyone! It is not really an error, but when trying to connect to the db I cannot get past this point (idling forever at the password). Anybody else with this issue?\",1642774729.104900,1642774729.104900,U02UA0EEHA8\\ne22c6f38-208b-4783-962d-63971882feef,U02U5Q8FK38,,,\"I just got an answer from my friend. So apparently GIT bash is unable to show the graphical elements. So you should try to use other terminals to do this instead. For me, i went to the microsoft store and just downloaded terminal (search terminal, its the first result) then just configure your bash to run in the windows terminal and it works there\",1642762426.057700,1642774816.105200,U02T18VH90F\\nc10f4d77-fde6-4dab-ad74-998a43807862,U02UA0EEHA8,,,\"<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642774816105200?thread_ts=1642762426.057700&amp;cid=C01FABYF2RG>\\nTry doing this. I had the same issue previously\",1642774729.104900,1642774920.105400,U02T18VH90F\\nc5fbf9eb-6574-4818-9a4e-986d3f9321ba,,10.0,,\"Hello everyone,\\nPlease I\\'m having an issue with the ingest_data.py...\\n\\nWhen I run the script in bash, the output that shows the download progress of the file is saved to `output.csv` on disk and then in the \"\"output.csv\"\" file it says `Saving to: \\'yellow_tripdata_2021-01.csv.1\\'` and this causes the script to fail because its expecting the data to be in `output.csv` .\\n\\nPlease how can I fix this?\\n\\n<@U01AXE0P5M3>\",1642775602.109300,1642775602.109300,U02QS4BD1NF\\na741f803-dea3-4b66-b979-dc0f59e24298,U02UA0EEHA8,,,thank you so much Gerald :slightly_smiling_face: it works,1642774729.104900,1642775647.109400,U02UA0EEHA8\\n074b7b59-0383-4b51-8f0e-553d22c75637,U02TKFAL22G,,,\"How are you installing it? I decided to install through conda, which seemed much easier\\n\\n```$ conda update conda &amp;&amp; conda update --all\\n$ conda install -c conda-forge pgcli```\",1642595209.139000,1642776005.109800,U02TEERF0DA\\n841593dc-008c-419a-b184-6c98d015cb49,U02TKFAL22G,,,This should work :thumbsup:,1642595209.139000,1642776040.110100,U02TEERF0DA\\n54c06b9c-40eb-46cf-9d07-26f912699545,,,thread_broadcast,I\\'m still stuck at this. Please leg me know if anyone has overcome this. Can\\'t believe I\\'m having this much trouble in the first week lmao,1642765009.063500,1642776137.111500,U02T941CTFY\\n96a1c276-66ab-4ca8-abcb-2c5926148e7a,U02Q7JMT9P1,,,\"Thank you <@U01AXE0P5M3>\\nIs there any article or a blog post to learn those VSCode tricks?\",1642765882.066900,1642776407.111900,U02Q7JMT9P1\\ndc24251f-14f9-41fa-ae08-0897b4c27deb,U02TT5SRDSB,,,\"Yes you can totally use SQL for this example\\n\\nbut imagine that your postgres is not running locally and your job is running in a kubernetes cluster.\\n\\nThen probably you won\\'t be able to use this way of uploading data\",1642774404.103600,1642776602.112400,U01AXE0P5M3\\n50a4b30c-5c5a-4841-bd18-f8358514f329,U02T941CTFY,,,\"pwd is your current directory\\n\\nso `cd` to `data-engineering-zoomcamp/week_1_basics_n_setup/2_docker_sql`\\n\\nand run\\n\\n```docker run -it \\\\\\n    -e POSTGRES_USER=\"\"root\"\" \\\\\\n    -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n    -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n    -v $(pwd)/ny_taxi_postgres_data:/var/postgresql/data \\\\\\n    -p 5432:5432 \\\\\\n    postgres:13```\\nFor WSL it\\'s better to remove the `ny_taxi_postgres_data` folder before running for the first time\",1642765009.063500,1642776687.112800,U01AXE0P5M3\\nb4b2aa9f-bb6c-412a-ab8c-3b287292f67f,U02QS4BD1NF,,,can you show how you save it? do you use `&gt;` ?,1642775602.109300,1642776768.113300,U01AXE0P5M3\\n6e12bf28-09ad-4a73-bd9b-e5b85b04d5fc,U02QS4BD1NF,,,or you just run the script and you haven\\'t changed anything there?,1642775602.109300,1642776794.113500,U01AXE0P5M3\\ne586e198-701e-4421-9d5a-b97e1efed052,U02QS4BD1NF,,,\"make sure it\\'s capital O\\n\\n<https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/2_docker_sql/ingest_data.py#L23>\",1642775602.109300,1642776823.113700,U01AXE0P5M3\\n1f1c184a-e184-4f1b-8bee-474e55be3957,U02Q7JMT9P1,,,\"I don\\'t know, but I mostly use just one - multiple select. Select a word and press CTRL+D  multiple times\",1642765882.066900,1642776884.114000,U01AXE0P5M3\\nbc5fe1e7-9121-432c-8362-d392bec0c357,U02QGA57GRY,,,\"<@U02QGA57GRY> Hello, I have the same issue. Did you manage to figure this out?\",1642611614.176500,1642777498.116200,U02Q9P0A0NA\\n3f314069-5e1c-4789-87e8-fc1ab6e07c86,U02QS4BD1NF,,,\"<@U01AXE0P5M3> mine also doesn\\'t looks like ingesting correctly on Google VM. `docker-compose up`  in 2_docker_sql folder will just create pgdatabase and pgadmin container, but it doesn\\'t ingest the data to database?\",1642775602.109300,1642778001.116500,U02UBV4EC8J\\n170c9b3c-8450-4f05-ac69-6bd2dc57b960,U02QS4BD1NF,,,\"docker compose up will not ingest the data, you need to run a script for that\",1642775602.109300,1642778219.118200,U01AXE0P5M3\\n9c388ebb-3a29-4b1c-a8fc-fec4857ecf49,U02QS4BD1NF,,,\"sure, make sense now\",1642775602.109300,1642778429.119400,U02UBV4EC8J\\n5734e67b-5907-447c-adc3-a80b35f490f4,,1.0,,\"Has anyone faced this? This is on M1.\\nWhen I tried without $(pwd)/&lt;folder_name&gt;, It works. I don\\'t see the files inside &lt;folder_name&gt; though. And I can\\'t locate the folder. I followed the instruction from ReadME Page.\\n<@U02U34YJ8C8> <@U01AXE0P5M3>\\n```performing post-bootstrap initialization ... 2022-01-21 15:14:40.937 UTC [44] FATAL:\\xa0data directory \"\"/var/lib/postgresql/data\"\" has invalid permissions```\\nThis works, but I don\\'t know where the folder is\\n\\n```(data-engineering-zoomcamp) % docker run -it \\\\                      \\n  -e POSTGRES_USER=\"\"root\"\" \\\\\\n  -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n  -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n  -v ny_taxi_postgres_data:/var/lib/postgresql/data \\\\ \\n  -p 5432:5432 \\\\\\n  --name pg \\\\\\n  postgres:13\\n\\nPostgreSQL Database directory appears to contain a database; Skipping initialization\\n\\n2022-01-21 15:40:22.028 UTC [1] LOG:  starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\\n2022-01-21 15:40:22.028 UTC [1] LOG:  listening on IPv4 address \"\"0.0.0.0\"\", port 5432\\n2022-01-21 15:40:22.028 UTC [1] LOG:  listening on IPv6 address \"\"::\"\", port 5432\\n2022-01-21 15:40:22.030 UTC [1] LOG:  listening on Unix socket \"\"/var/run/postgresql/.s.PGSQL.5432\"\"\\n2022-01-21 15:40:22.035 UTC [27] LOG:  database system was shut down at 2022-01-21 15:03:13 UTC\\n2022-01-21 15:40:22.037 UTC [1] LOG:  database system is ready to accept connections\\n\\n(data-engineering-zoomcamp) % ls -lrt\\ntotal 262320\\n-rw-r--r--  1 ******  staff        112 Jan 21 12:51 pipeline.py\\n-rw-r--r--  1 ******  staff        121 Jan 21 12:52 Dockerfile\\n-rw-r--r--  1 ******  staff  125981363 Jan 21 18:04 yellow_tripdata_2021-01.csv\\n-rw-r--r--  1 ******  staff      15434 Jan 21 20:29 Untitled.ipynb```\\n\",1642778637.120100,1642778637.120100,U0290EYCA7Q\\n0c33737e-9836-48a6-8f50-f3ca6a82f71e,,5.0,,\"```docker run -it \\\\ \\n  -e POSTGRES_USER=\"\"root\"\" \\\\\\n  -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n  -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n  -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n  -p 5432:5432 \\\\\\n  postgres:13```\\nHey guys , can somebody help me find the error in this command.  i get -e command not found but when i copy it from the repository code it works . it seems identical to me but apparently not\",1642779083.123300,1642779083.123300,U02S6KXPH8W\\nbc34f645-30b3-49d8-98cf-18d0d319dca5,U02S6KXPH8W,,,\"I think you have an extra colon in there.\\nTry:\\n```docker run -it \\\\ \\n  -e POSTGRES_USER=\"\"root\"\" \\\\\\n  -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n  -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n  -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n  -p 5432:5432 \\\\\\n  postgres:13```\",1642779083.123300,1642779133.123500,U02T941CTFY\\nd080708a-1fc6-4ee0-923a-a7c57bb9561a,U02S6KXPH8W,,,still the same,1642779083.123300,1642779193.123800,U02S6KXPH8W\\n30b1143c-e69e-4e4c-946b-ca667fbe5e8c,U02S6KXPH8W,,,\"```docker run -it \\\\ \\n  -e POSTGRES_USER=\"\"root\"\" \\\\\\n  -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n  -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n  -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n  -p 5432:5432 \\\\\\n  postgres:13```\",1642779083.123300,1642779240.124000,U02S6KXPH8W\\n3c41061a-96e5-46e5-935f-a0128c265419,U02T941CTFY,,,\"<@U01AXE0P5M3> I tried that, and there are no errors in the terminal, but still no pg files in the directory. I `cd` to the folder and `ls`  and it came up empty...\",1642765009.063500,1642779341.124400,U02T941CTFY\\ne48ed106-b2ec-4b0b-acc6-6c1c511c5068,U02S6KXPH8W,,,\",I don\\'t know what the difference is . its really bothering me :grin:\\nit says\\n```docker: invalid reference format.```\",1642779083.123300,1642779482.124600,U02S6KXPH8W\\ndcbc09b8-cf58-4888-87be-42c5b071be6d,U0290EYCA7Q,,,Can see the volume here. Any one knows the folder for it?,1642778637.120100,1642780334.125700,U0290EYCA7Q\\n394664c2-b78c-4100-a1df-cd399ac55de9,U02S6KXPH8W,,,solved: removed extra space after -it \\\\,1642779083.123300,1642780907.126100,U02S6KXPH8W\\n57790a61-bdb7-4609-afba-a27dece69d6f,U02QS4BD1NF,,,It was lowercase `o` let me change it to capital in the script,1642775602.109300,1642781108.126300,U02QS4BD1NF\\n70a7282c-4d58-460c-879f-e5fcdb2d1398,U02QS4BD1NF,,,<@U01AXE0P5M3> this second wget <https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv> will be under a separate table?,1642775602.109300,1642781673.129600,U02UBV4EC8J\\n6216237a-3199-4bb2-b58c-fd89c67181dc,U02QS4BD1NF,,,\"<@U01AXE0P5M3> Thanks.....It has been solved....lower case \"\"-o\"\" is to output the log to the specified file\",1642775602.109300,1642781686.130000,U02QS4BD1NF\\n33257438-5620-4f27-831b-3ce78cb0634a,,4.0,,\"Just wanted to give some additional help to people who may not be using vscode, and are doing more through the command line.\\n\\nIn the video <https://www.youtube.com/watch?v=ae-CV2KfoN0&amp;list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&amp;index=7|Setting up the environment on Google Cloud (Cloud VM + SSH access)> there is a section where port forwarding is set up through vscode. There is a way to do this only through the command line.\\n\\nIf you have already set up your VM and attached a public key to it, all you’ll need to do is make sure the docker containers with postgres and pgadmin are up within the container.\\n\\nTo do this, go into `~/data-engineering-zoomcamp/week_1_basics_n_setup/2_docker_sql` and run `$ docker-compose up`\\n\\nIn a different terminal window run:\\n\\n```$ ssh -i ~/.ssh/private_gcs_vm_key -L 8080:localhost:8080 -N -T user_name@vm_ip```\\nOnce this has run, you can use your local browser and navigate to `localhost:8080` and you’ll be forwarded to the vm, and into pg-admin.\\n\\nHope that helps!\",1642781895.135700,1642781895.135700,U02TEERF0DA\\n60049f5d-6960-430c-9709-b473384c3817,,6.0,,\"Hello guys, on Setting up the Environment (32:57) Alexey adds port 5432 and automatically gets localhost:5432. I get localhost:5433. Will it cause any troubles in future? Thank you.\",1642781963.136000,1642781963.136000,U02RR9Z0CCV\\na7bd2bb6-334c-475e-87c8-c2d0a7f3f798,U02TEERF0DA,,,The `-N` and `-T` flags are for disabling pseudo-[T]ty allocation and executio[N] of remote commands.,1642781895.135700,1642782017.136100,U02TEERF0DA\\n608a53fb-a70d-4d69-a333-b29101d28390,,2.0,,\"Hi all.\\n\\nAfter authenticating my local environment to work with the cloud environment, when I got to IAM - I only have two permission set: a deleted one, and my personal email address. Whereas in the video, there was the deleted one, personal one, and a new one -which is where roles were added\\n\\nAny thoughts?\",1642782185.137400,1642782185.137400,U02U34YJ8C8\\ncfde3168-835d-4d68-a757-9805a556cf2f,U02U34YJ8C8,,,\"<@U02BVP1QTQF> Thanks. That makes sense. Do you happen to know where files for the container are stored on an m1\\nmac?\",1642768298.077200,1642782240.137500,U02U34YJ8C8\\nd7d4b4a1-f646-4f17-8823-f0d474cfe82f,U02T941CTFY,,,\"Don’t you have to provide the full path on windows, rather than using {pwd}?\",1642765009.063500,1642782389.137700,U02U34YJ8C8\\n8ef6ab67-9232-493c-b2a2-454d350b70ac,U02U34YJ8C8,,,\"You mean the built images? I’m not 100% sure, to be honest. Since Docker is for Linux containers, any non-Linux OS has to run a VM and then run Docker from within that VM. According to <https://www.freecodecamp.org/news/where-are-docker-images-stored-docker-container-paths-explained/>, you can find the VM image at ~/Library/Containers/com.docker.docker/Data/vms/0 (I’ve got a `data` folder with a 64GB file inside) and within that VM the Docker images could be stored.\",1642768298.077200,1642782769.138000,U02BVP1QTQF\\nd7b4037e-b394-4a52-a4f1-836fa45d6358,U02U34YJ8C8,,,So this is all I’m seeing.,1642782185.137400,1642782800.138300,U02U34YJ8C8\\nf2c091c6-19bb-4c84-b387-fa660e75149f,U02TEERF0DA,,,\"This is great, worked for me.\",1642781895.135700,1642783036.138700,U02UBV4EC8J\\n9f44585c-3793-4907-ae4d-f2149023788b,U02U34YJ8C8,,,Restarting the whole process (deleting my project and creating a new one) seems to have resolved things for now - although unsure why.,1642782185.137400,1642783997.139500,U02U34YJ8C8\\n38ba0ad6-c558-4abf-a87e-c14c8f51042d,,17.0,,\"Hi, I\\'m having a problem running the \"\"`docker build -t test:pandas .`\"\" command from the Zoomcamp 1.2.1 - Introduction to Docker video (minute = 18). It returns two things, \"\"`ending build context to Docker daemon 1.536kB`\"\" and the error it returns is \"\"`Error response from daemon: the Dockerfile (Dockerfile) cannot be empty`\"\". I have everything set up like Alexey does in the video. thanks\",1642784127.140200,1642784127.140200,U02R4F43B0C\\na36a4903-9ad1-41f6-9877-035ec8ed41f1,U02R4F43B0C,,,\"i think u missed \"\".\"\" to refer the current directory.\\n\\ndocker build -t test:pandas *.*\",1642784127.140200,1642784623.140700,U0290EYCA7Q\\na5439f16-3b5a-4e1e-8493-660b802c242e,,33.0,,\"what am i missing?\\nC:\\\\Users\\\\User1&gt;terraform -version\\nTerraform v1.1.4\\non windows_amd64\\n\\nC:\\\\Users\\\\User1&gt;terraform init\\nTerraform initialized in an empty directory!\\n\\nThe directory has no Terraform configuration files. You may begin working\\nwith Terraform immediately by creating Terraform configuration files.\",1642784624.140900,1642784624.140900,U02VBG59VQ9\\ndac0dbf4-45d3-4dbb-a656-d060aecc0008,U02VBG59VQ9,,,you would need to run it from `data-engineering-zoomcamp/week_1_basics_n_setup/1_terraform_gcp/terraform`,1642784624.140900,1642784813.141100,U02UBV4EC8J\\n1d17782f-d5d8-49d4-815e-ac652ec6adda,U02VBG59VQ9,,,i am confused,1642731166.489700,1642784897.141400,U02VBG59VQ9\\n95d992f9-4737-4004-ad98-c2d65dfceb82,U02VBG59VQ9,,,\"hi kenny, what do you mean?\",1642784624.140900,1642784930.141600,U02VBG59VQ9\\n884d2a90-4653-4fab-bf35-ed00aea23de4,U02RR9Z0CCV,,,Probably no as long as you remember to use this specific port,1642781963.136000,1642785023.141900,U01AXE0P5M3\\ne25d3258-722e-48e6-8fe1-0e4d3571895f,U02VBG59VQ9,,,is there a video?,1642784624.140900,1642785057.142100,U02VBG59VQ9\\n2a4571a8-ecae-4c15-9737-ddd8b133803a,U02VBG59VQ9,,,<@U02UBV4EC8J>,1642784624.140900,1642785281.142400,U02VBG59VQ9\\n14cc4ade-bbfb-4f16-a2de-619317f30a1a,U02QGA57GRY,,,<@U02Q9P0A0NA> Nopee...,1642611614.176500,1642785545.142700,U02QGA57GRY\\nd588e101-c7c9-4388-acb5-c26023e4c1a4,U02R4F43B0C,,,\"sorry, my mistake when writing here in slack, I put it in the code\",1642784127.140200,1642785590.143000,U02R4F43B0C\\n902d9acc-ff91-4275-b95c-1b8ebeb7d3f4,U02R4F43B0C,,,are you still getting an error?,1642784127.140200,1642785694.143300,U02UBV4EC8J\\n582588ec-9647-42d3-9e83-087632160d2f,U02R4F43B0C,,,yes,1642784127.140200,1642785707.143500,U02R4F43B0C\\n89eb8f09-6c1a-4683-9fd2-b5bc7152cc89,U02R4F43B0C,,,what is the error?,1642784127.140200,1642785714.143700,U02UBV4EC8J\\nad2dc8f9-a6ea-4542-b1d1-2832270a9d6a,U02R4F43B0C,,,Here is a photo to see more easily,1642784127.140200,1642785863.144000,U02R4F43B0C\\n38184516-2784-42f6-865c-8c98d29a228f,U02VBG59VQ9,,,That\\'s the Git repository we gotta pull down.  I am attempting to do the same right now <@U02VBG59VQ9>,1642784624.140900,1642785907.144400,U02USF4NYG4\\n120a1d0c-f74b-402e-8b67-ed9a554550e3,U02UEE4MBEG,,,what is the correct format windows WSL?,1642748652.027100,1642785943.144600,U02UEE4MBEG\\nba58930b-8f7c-419c-8593-355e4f23f722,U02RR9Z0CCV,,,Thank you very much Alexey. Shall I modify something in the command that follows next: `pgcli -h localhost -U root -d ny_taxi`,1642781963.136000,1642786188.144800,U02RR9Z0CCV\\nf7f908d5-f754-40b3-b55d-02077296f7ee,U02R4F43B0C,,,have tried to restart docker daemon?,1642784127.140200,1642786230.145000,U02UBV4EC8J\\nab602b4c-4a53-4c30-9b6b-e0bae2aef4d0,U02VBG59VQ9,,,<@U02USF4NYG4> i am stuck oh this for the past 2 days. please let me know if it worked for you,1642784624.140900,1642786237.145200,U02VBG59VQ9\\n53342fa6-27ed-4030-a5d1-f088c7575b23,U02VBG59VQ9,,,Yes it worked just now,1642784624.140900,1642786252.145400,U02USF4NYG4\\nbef571ed-2760-4cdf-9b37-0988d553936e,U02R4F43B0C,,,are in the same location where the Dockerfile is?,1642784127.140200,1642786263.145600,U02UBV4EC8J\\ne3e04af2-b663-49fc-b733-d9623c8316e6,U02VBG59VQ9,,,:+1:,1642784624.140900,1642786325.145800,U02UBV4EC8J\\na5cffdde-e023-4250-b325-c32c22cbf075,U02VBG59VQ9,,,<@U02VBG59VQ9> you would need to clone this repo: <https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup/1_terraform_gcp/terraform>,1642784624.140900,1642786361.146000,U02UBV4EC8J\\n8289130b-5f77-4c9c-b652-406d881fe906,U02R4F43B0C,,,Are directory names case insensitive? Can you run *ls* or *dir* and paste the output?,1642784127.140200,1642786522.146400,U0290EYCA7Q\\naafa38ba-00dd-474f-8a65-5017d5072315,U02T2TX1GS2,,,\"interestingly, pip install pgcli with anaconda env activated (base) and bin directory added to path it does not work.\\npip install pgcli  without conda env works. if anybody has any ideas why , i would like to know.\",1642700805.403700,1642786689.146800,U02S6KXPH8W\\n9aa7589f-9965-4137-9c55-aeb8be7bba6b,U02R4F43B0C,,,yes it is,1642784127.140200,1642786732.147100,U02UBV4EC8J\\naf4fe8cc-f55e-4887-acef-6b110646826b,U02VBG59VQ9,,,<@U02USF4NYG4> would you be kind and walk me through please,1642784624.140900,1642786773.147300,U02VBG59VQ9\\nba4a8f5d-ac3c-4e11-b4ce-983cc4bf9084,U02R4F43B0C,,,accordingt o an error looks like you are in a correct directory but the docker daemon is complaining,1642784127.140200,1642786780.147500,U02UBV4EC8J\\n8923b47a-8e5c-4fd8-a744-7718e16aa8d4,U02R4F43B0C,,,have you tried restart the docker daemon?,1642784127.140200,1642786797.147700,U02UBV4EC8J\\n5f12f9a9-3d12-4698-b022-32c967ff7e87,,25.0,,\"Hello all, after successfully setting up my containers using docker compose, i tried running the the taxi_ingest image to populate my database, but am getting this error\\n(psycopg2.operationalerror) could not translate host name \"\"\",1642786979.150600,1642786979.150600,U02T0CYNNP2\\nfb71ca52-156d-4d56-ae80-e7626f2df061,U02R4F43B0C,,,\"Did you try recreating dockerfile again, and run build command?\",1642784127.140200,1642786990.150700,U0290EYCA7Q\\n17f4f4b2-c1f3-473d-9cfb-fc0e9851affb,U02VBG59VQ9,,,\"<@U02UBV4EC8J> this is what i am getting C:\\\\Users\\\\User1&gt;git clone <https://github.com/Sisaygir/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup/1_terraform_gcp/terraform>\\nCloning into \\'terraform\\'...\\nfatal: repository \\'<https://github.com/Sisaygir/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup/1_terraform_gcp/terraform/>\\' not found\",1642784624.140900,1642787087.150900,U02VBG59VQ9\\nc4c06e6f-43ad-4f1c-b4dc-ef72af94ea03,U02T0CYNNP2,,,Me too ! I really don\\'t  know what else to try,1642786979.150600,1642787123.151200,U02UY5FUCEL\\nf32c0b6d-20c6-4309-a494-257dce449be2,U02VBG59VQ9,,,\"<@U02VBG59VQ9> run this :\\ngit clone <https://github.com/DataTalksClub/data-engineering-zoomcamp.git>\",1642784624.140900,1642787139.151400,U02UBV4EC8J\\nfbdec180-064d-4de1-bc6c-cda45167191f,U02VBG59VQ9,,,I\\'m using Git Hub Desktop cause I am kind of a Git noob,1642784624.140900,1642787149.151700,U02USF4NYG4\\n5f2f3b20-765a-40bd-8a63-18beec2977d7,U02VBG59VQ9,,,ok i did that <@U02UBV4EC8J>,1642784624.140900,1642787270.152000,U02VBG59VQ9\\nd4d3a4db-3672-48d4-b4f4-7427953dcfec,U02VBG59VQ9,,,I think the command Kenny linked about is what you need so you are getting the full git repository.  I dropped my Terraform.exe in there and set the path to that Terraform folder,1642784624.140900,1642787287.152200,U02USF4NYG4\\n5c85c9e0-1093-4fd2-baa2-944ea8075c10,,11.0,,\"Hi! After docker-compose up, I get this error :disappointed: . Any idea what is going on?\",1642787354.152800,1642787354.152800,U02T2DX4LG6\\n61e86047-9b72-45d9-a017-0cdcf584f5bd,U02VBG59VQ9,,,<@U02UBV4EC8J> what is next,1642784624.140900,1642787410.153100,U02VBG59VQ9\\n0e15878f-11b6-4e8b-9a9d-5e2a1c2af842,U02VBG59VQ9,,,\"<@U02UBV4EC8J> <@U02USF4NYG4> i got this. what is the next step C:\\\\Users\\\\User1&gt;git clone <https://github.com/DataTalksClub/data-engineering-zoomcamp.git>\\nCloning into \\'data-engineering-zoomcamp\\'...\\nremote: Enumerating objects: 786, done.\\nremote: Counting objects: 100% (786/786), done.\\nremote: Compressing objects: 100% (492/492), done.\\nremote: Total 786 (delta 431), reused 508 (delta 243), pack-reused 0\\nReceiving objects: 100% (786/786), 456.60 KiB | 6.81 MiB/s, done.\\nResolving deltas: 100% (431/431), done.\",1642784624.140900,1642787468.153300,U02VBG59VQ9\\n28478d80-099c-4463-9388-cf257b3e77cf,U02T2DX4LG6,,,Can i see your docker compose file,1642787354.152800,1642787532.153600,U02T0CYNNP2\\ncad760c9-bab0-4fc9-a437-06e3fde53cdb,U02R4F43B0C,,,you could try *docker build -t &lt;image&gt; -f ./Dockerfile*,1642784127.140200,1642787544.153800,U0290EYCA7Q\\nD830C5FE-C19F-45E4-8762-9AB286957100,U02R4F43B0C,,,The screen shot shows probably you have not saved the file in vs code ,1642784127.140200,1642787587.154800,U02AGF1S0TY\\n27b48a8c-779b-4d62-87cc-eb65652a7aa8,U02VBG59VQ9,,,\"you would need your google service account, cred, keys setup to be able to initialize terraform and create dataset\",1642784624.140900,1642787685.155000,U02UBV4EC8J\\nf4aa4d3c-efde-42c4-a81d-6f7a0e4f3420,U02T2DX4LG6,,,Can you post your Yaml file?,1642787354.152800,1642787709.155200,U02U34YJ8C8\\nf10bb300-c344-410a-8370-a06ac7d1191e,U02T0CYNNP2,,,on your local or VM in GCP?,1642786979.150600,1642787714.155400,U02UBV4EC8J\\n9e3b1833-87cc-4540-b579-45304f37cb2d,U02T0CYNNP2,,,\"Local, i have not done anything with gcp\",1642786979.150600,1642787760.156800,U02T0CYNNP2\\n08ba71ea-2351-4098-88ad-977351e0ee84,U02T0CYNNP2,,,\"can you run\\n`pip list | grep psycopg2`\",1642786979.150600,1642787765.157100,U02UBV4EC8J\\nf7b34785-412d-4fbb-9f78-640700d8700f,U02T0CYNNP2,,,mac or windows?,1642786979.150600,1642787772.157300,U02UBV4EC8J\\na9944c53-5e0d-40c0-9675-e241a0ca3103,U02VBG59VQ9,,,<@U02UBV4EC8J> you have been very helpful and wants to say thank you. I have set that up yesterday. What command should i run next,1642784624.140900,1642787827.157600,U02VBG59VQ9\\n75208149-49ba-4f61-8666-351d01d85505,U02T0CYNNP2,,,Windows,1642786979.150600,1642787830.157800,U02T0CYNNP2\\n97a76494-db9b-468b-a632-9efd4568cc09,U02T0CYNNP2,,,gitbash or WSL?,1642786979.150600,1642787856.158000,U02UBV4EC8J\\n290057fe-5393-4194-bd49-b0f2179a5752,U02T0CYNNP2,,,Seems the host: on the docker run for that image is creating the issue,1642786979.150600,1642787869.158200,U02T0CYNNP2\\ndbfc96ac-2bfe-49b0-b167-9577c54805b7,U02T0CYNNP2,,,Gitbash,1642786979.150600,1642787875.158400,U02T0CYNNP2\\n964047f4-1ed4-4d04-ab2f-7675f730bf6f,U02T0CYNNP2,,,\"can you run and see what version you have\\n$ `pip list | grep psycopg2`\\npsycopg2        2.9.3\",1642786979.150600,1642787948.158600,U02UBV4EC8J\\nb2d63350-e7cb-4985-9703-35698491ac3f,U02T0CYNNP2,,,I think this is related to that python moule,1642786979.150600,1642787963.158800,U02UBV4EC8J\\n2a0849d0-2286-4599-ae12-95bd86a8d257,U02T0CYNNP2,,,\"you can try to install this:\\n```pip install psycopg2-binary```\\n\",1642786979.150600,1642788012.159000,U02UBV4EC8J\\nac7762d2-2603-4972-8443-4984959777ca,U02T0CYNNP2,,,\"Ok, i will try it out\",1642786979.150600,1642788033.159200,U02T0CYNNP2\\n7f9fa2b2-cd9d-40e1-ae0b-3c5c901bc7d2,U02T0CYNNP2,,,Thanks,1642786979.150600,1642788051.159400,U02T0CYNNP2\\nfeea210c-08c1-4046-b0d9-6e953f572ece,U02R4F43B0C,,,haha. Yeah.. It still shows big dot.,1642784127.140200,1642788052.159600,U0290EYCA7Q\\n9116b5ba-1feb-4ad1-9a59-5d1b2f851706,U02T0CYNNP2,,,\"Can you post the full `docker run` command you run to create the container, along with the full error?\\n\\nAnd also post the code you ran to generate the postgres 13 container?\",1642786979.150600,1642788109.160200,U02U34YJ8C8\\n4ee65ac6-6bbb-48ee-9188-d50b89818e27,U02VBG59VQ9,,,\"you would need to setup your google stuff, have you done that\",1642784624.140900,1642788130.160400,U02UBV4EC8J\\nda48e581-268d-4310-b0a6-b8caf66decb2,U02T0CYNNP2,,,Ok..i will do that,1642786979.150600,1642788135.160600,U02T0CYNNP2\\n8e0c0cff-f616-4b18-af12-796bb55f1db2,U02VBG59VQ9,,,\"<@U02UBV4EC8J> yes\\nThese credentials will be used by any library that requests Application Default Credentials (ADC).\\n\\nQuota project \"\"dezoomcamp22\"\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\",1642784624.140900,1642788235.160900,U02VBG59VQ9\\n65eee2ea-9ff4-40c4-b8e9-a971b96c6652,U02VBG59VQ9,,,\"if so copy over your key  json file and run this two:\\n```export GOOGLE_APPLICATION_CREDENTIALS=~/your _json_file_here\\ngcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS\\n\\nterraform init\\nterraform plan (here you would need to pass the ProjectID, get it from google console)\\nterraform apply (type \"\"yes\"\" when prompted)```\\n\",1642784624.140900,1642788250.161100,U02UBV4EC8J\\n3d1c007a-a4dd-4642-aa08-b023c96a77e8,U02VBG59VQ9,,,\"<@U02UBV4EC8J>\\nC:\\\\Users\\\\User1&gt;export GOOGLE_APPLICATION_CREDENTIALS=~/C:\\\\Users\\\\User1\\\\AppData\\\\Roaming\\\\gcloud\\\\application_default_credentials.json\\n\\'export\\' is not recognized as an internal or external command,\\noperable program or batch file.\",1642784624.140900,1642788424.161300,U02VBG59VQ9\\n3044650c-a919-471a-ad5a-8a192a2e79f6,U02RR9Z0CCV,,,\"Yes,  add -p there\",1642781963.136000,1642788628.162300,U01AXE0P5M3\\nf1b4fcb5-e626-4731-934e-9306aaac55e9,,6.0,,\"Hello <@U01AXE0P5M3>\\nfor question 4: which of the two dates should be considered i.e tpep_pickup_datetime or tpep_dropoff_datetime?\\ntpep_dropoff_datetime? bcoz at this time was the meter disengaged\",1642788692.163300,1642788692.163300,U029DM0GQHJ\\nc2a01b25-b5fb-4706-bb8c-2c5c3deb2548,U02VBG59VQ9,,,<@U02USF4NYG4> how\\'s it going so far. did you make it to this step?,1642784624.140900,1642788796.163400,U02VBG59VQ9\\n7024AC49-0939-4EAD-8EA1-CD412BB1E616,U02U34YJ8C8,,,\"Yeah think so. I think generally it\\'s a good idea to always stop and remove containers after using them, although I\\'m still learning all this.\",1642630680.235000,1642788856.164800,U02U34YJ8C8\\n4a022c71-8e17-4a06-9f76-ddc5713d7b6c,U029DM0GQHJ,,,did you create a new table for the second dataset?,1642788692.163300,1642788862.165000,U02UBV4EC8J\\nfb3dcc50-452d-4079-af3e-96ff7a28c81d,U029DM0GQHJ,,,for this one:  <https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv>,1642788692.163300,1642788885.165200,U02UBV4EC8J\\ncde19cd9-61ee-4b80-a6fa-e968e169a891,,13.0,,\"I have this weird syntax error, I can not resolve, can someone help me. I even tried copy pasting, from the github. and it is not working.\\n\\n\\n```File \"\"ingest_data.py\"\", line 20\\n    os.system(f\"\"wget {url} -O {csv_name}\"\")\\n                                        ^\\nSyntaxError: invalid syntax```\\n\\ningest_data.py\\n```import os\\nimport argparse\\n\\nfrom time import time\\n\\nimport pandas as pd\\nfrom sqlalchemy import create_engine\\n\\n\\ndef main(params):\\n    user = params.user\\n    password = params.password\\n    host = params.host\\n    port = params.port\\n    db = params.db\\n    table_name = params.table_name\\n    url = params.url\\n    csv_name = \\'output.csv\\'\\n\\n    os.system(f\"\"wget {url} -O {csv_name}\"\")\\n\\n    engine = create_engine(f\\'postgresql://{user}:{password}@{host}:{port}/{db}\\')\\n\\n    df_iter = pd.read_csv(csv_name, iterator=True, chunksize=100000)\\n\\n    df = next(df_iter)\\n\\n    df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\\n    df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\\n\\n    df.head(n=0).to_sql(name=table_name, con=engine, if_exists=\\'replace\\')\\n\\n    df.to_sql(name=table_name, con=engine, if_exists=\\'append\\')\\n\\n    while True:\\n        t_start = time()\\n\\n        df = next(df_iter)\\n\\n        df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\\n        df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\\n\\n        df.to_sql(name=table_name, con=engine, if_exists=\\'append\\')\\n\\n        t_end = time()\\n\\n        print(\\'inserted another chunk, took %.3f second\\' % (t_end - t_start))\\n\\n\\nif __name__ == \\'__main__\\':\\n    parser = argparse.ArgumentParser(description=\\'Ingest CSV data to Postgres\\')\\n\\n    parser.add_argument(\\'--user\\', help=\\'user name for postgres\\')\\n    parser.add_argument(\\'--password\\', help=\\'password for postgres\\')\\n    parser.add_argument(\\'--host\\', help=\\'host for postgres\\')\\n    parser.add_argument(\\'--port\\', help=\\'port for postgres\\')\\n    parser.add_argument(\\'--db\\', help=\\'database name for postgres\\')\\n    parser.add_argument(\\'--table_name\\', help=\\'name of the table where we will write the results to\\')\\n    parser.add_argument(\\'--url\\', help=\\'url of the csv file\\')\\n\\n    args = parser.parse_args()\\n\\n    main(args)```\\n\\ni tried this given command to run the for the docker\\n```URL=\"\"<https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>\"\"\\n\\npython ingest_data.py \\\\\\n  --user=root \\\\\\n  --password=root \\\\\\n  --host=localhost \\\\\\n  --port=5432 \\\\\\n  --db=ny_taxi \\\\\\n  --table_name=yellow_taxi_trips \\\\\\n  --url=${URL}```\",1642788935.165500,1642788935.165500,U02U85FB8TU\\ndb93ca6d-4f67-4476-8844-8792f00084b7,U029DM0GQHJ,,,Pickup. I updated the instructions - I hope it\\'s less ambiguous now,1642788692.163300,1642788947.165600,U01AXE0P5M3\\nfb65cddd-8a02-4610-8158-bbe39d2ff377,U02UEE4MBEG,,,<https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup/2_docker_sql#linux-and-macos|https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup/2_docker_sql#linux-and-macos>,1642748652.027100,1642788995.165900,U01AXE0P5M3\\n2db0fef4-35ef-4586-9e71-0e6ab351997b,U02QS4BD1NF,,,\"Separate table, yes\",1642775602.109300,1642789128.166700,U01AXE0P5M3\\n59db95d7-d1a4-4eae-936f-5c1e26fcaf0c,U02T2DX4LG6,,,\"`services:`\\n  `pgdatabase:`\\n    `image: postgres:latest`\\n    `environment:`\\n      `- POSTGRES_USER=root`\\n      `- POSTGRES_PASSWORD=root`\\n      `- POSTGRES_DB=ny_taxi`\\n    `volumes:`\\n      `- \"\"./ny_taxi_postgres_data:/var/lib/postgresql/data:rw\"\"`\\n    `ports:`\\n      `- \"\"5432:5432\"\"`\\n  `pgadmin:`\\n    `image: dpage/pgadmin4`\\n    `environment:`\\n      `- PGADMIN_DEFAULT_EMAIL=<mailto:admin@admin.com|admin@admin.com>`\\n      `- PGADMIN_DEFAULT_PASSWORD=root`\\n    `ports:`\\n      `- \"\"8080:80\"\"`\",1642787354.152800,1642789159.167100,U02T2DX4LG6\\n2da2cdde-90e0-4826-bbb4-b43e1d30dec7,,16.0,,\"```ssh: connect to host *.*.*.* port 22: Operation timed out```\\nbut why? :man-shrugging:\",1642789168.167400,1642789168.167400,U02ULQFCXL0\\n3bd017e9-8b08-4017-8a89-86f341e5bd88,U02U85FB8TU,,,Please for the next message put the code inside a thread,1642788935.165500,1642789204.167500,U01AXE0P5M3\\n7d6ac0d9-0be9-4b6a-b5c8-a8d9200fd5f7,U02ULQFCXL0,,,trying to connect GCP via ssh,1642789168.167400,1642789209.167700,U02ULQFCXL0\\n9107a0d5-bd86-4e4c-89ab-e4febf44ba9b,U01QGQ8B9FT,,,\"I know I am doing something silly and wrong but can\\'t pin point the problem\\n<@U01AXE0P5M3> how could I solve this?\",1642769991.086200,1642789267.167900,U01QGQ8B9FT\\na340f156-9907-44af-8606-5913d55831b6,U02U85FB8TU,,,\"I think I saw someone else having this problem,  please check other threads in this channel\",1642788935.165500,1642789283.168100,U01AXE0P5M3\\n0f323431-4b58-4558-ad38-8e1d6132facc,U01QGQ8B9FT,,,Try using anaconda,1642769991.086200,1642789350.168500,U01AXE0P5M3\\nf4635c26-199c-431c-8000-fdce773851c0,U02ULQFCXL0,,,Did you update the IP? It changes when you stop and restart the VM.,1642789168.167400,1642789393.168700,U02TNEJLC84\\nac6995e1-33de-4e01-a5de-24ca3e509fd1,U02ULQFCXL0,,,i just create instance and copy ip from page with VM instances,1642789168.167400,1642789526.168900,U02ULQFCXL0\\n5a8d723c-ee9b-4824-b4ad-1f2b21b90ba4,U01QGQ8B9FT,,,\"<@U01AXE0P5M3> You meant annaconda cmd right? If yes, Gives the same thing again : `Defaulting to user installation because normal site-packages is not writeable`\\n`Requirement already satisfied: psycopg2 in c:\\\\users\\\\mistr\\\\appdata\\\\roaming\\\\python\\\\python38\\\\site-packages (2.9.3)`\",1642769991.086200,1642789580.169100,U01QGQ8B9FT\\n9da3adaa-1bba-4e81-aba7-d4ae0c9c2d42,U02U85FB8TU,,,\"thinking python related issue, what version python are you using?\",1642788935.165500,1642789586.169300,U02UBV4EC8J\\n22ee87f4-bd36-459a-aa14-ea6580af9a4c,U02U85FB8TU,,,\"bash-3.2$ python --version\\nPython 2.7.16\",1642788935.165500,1642789690.169700,U02U85FB8TU\\n5a252a75-d706-43cf-8e4e-e4126dfaca87,U02U85FB8TU,,,\"i did other installation using python 3\\n\\nlike using ``pip3``\",1642788935.165500,1642789733.170400,U02U85FB8TU\\nf01aac18-ddbe-4a9e-bd63-3bb4443edc61,U02ULQFCXL0,,,check firewall,1642789168.167400,1642789736.170600,U02U2Q5P61Z\\n0783a938-0b1b-4513-aa79-9eba85843b86,U02ULQFCXL0,,,\"of the instance, the rules\",1642789168.167400,1642789750.171100,U02U2Q5P61Z\\n1ef351aa-91fa-4e2d-a4ad-a05d941bc3c4,U02ULQFCXL0,,,you need to allow port 22 on GCP,1642789168.167400,1642789760.171400,U02U2Q5P61Z\\nb48091b7-1478-4bda-8f1a-03f995ff9906,,2.0,,\"Hi I am trying to install pgcli  and getting the following error\\n\\n```ERROR: Failed building wheel for setproctitle\\nFailed to build setproctitle\\nERROR: Could not build wheels for setproctitle, which is required to install pyproject.toml-based projects```\\nI am on windows, anyone had the same issue? I tried to search thorugh the chat but looks like only one person was able to solve it and it was on Mac (not win).\",1642789792.172700,1642789792.172700,U02TCMEDTUL\\n8d6f8b17-edee-4766-a276-d244e50335a5,U02TCMEDTUL,,,can you try conda\\'s way?,1642789792.172700,1642789837.173400,U029DM0GQHJ\\nC43DD9CB-DBF5-4F6D-973D-C703EC102B25,U02U85FB8TU,,,\"I don\\'t think f strings were around for Python 2.7. Could that be it? Try writing as \\'os.system(“wget {} -O {}”.format(url, csv_name))’\",1642788935.165500,1642789933.175300,U02U34YJ8C8\\n8dd74aae-7042-45ba-ab24-6afb925f5d7e,U02U85FB8TU,,,\"yeap it is f string looks like, you would need python3.6 and up\",1642788935.165500,1642789968.175500,U02UBV4EC8J\\nbc882b49-77e5-436a-914d-9ece46ba64e0,U02U85FB8TU,,,then you would need to point to that python3,1642788935.165500,1642790016.175700,U02UBV4EC8J\\n52523c80-4974-4bbe-bfd3-98dfceddafe2,U029DM0GQHJ,,,<@U02UBV4EC8J> yes,1642788692.163300,1642790116.175900,U029DM0GQHJ\\n8f1d4af3-9053-472f-a05f-fa2ab3919e82,U02ULQFCXL0,,,Did you add your public key to the server?,1642789168.167400,1642790220.176100,U02TNEJLC84\\n1ee78f73-3e59-4f5c-a158-07cdfd21e575,U02ULQFCXL0,,,yes,1642789168.167400,1642790227.176400,U02ULQFCXL0\\n0d752b64-088e-4eab-bd52-1a7162c29b9b,U02ULQFCXL0,,,\"can connect only with this command\\n```gcloud beta compute ssh --zone \"\"europe-north1-a\"\" \"\"&lt;VM_name&gt;\"\"  --project \"\"&lt;project_ID&gt;\"\"```\\nmaybe help to anyone\",1642789168.167400,1642790248.176600,U02ULQFCXL0\\n908852f9-ff8d-41dc-9436-1948ba3e50d9,U02ULQFCXL0,,,And you\\'re using the -i flag when you\\'re trying to connect through the command line? and is your private key in your .ssh directory?,1642789168.167400,1642790288.176800,U02TNEJLC84\\n932793be-b999-460a-ab46-537a6b73e52d,U02ULQFCXL0,,,yes to all,1642789168.167400,1642790339.177600,U02ULQFCXL0\\nd77871e1-7f78-4777-b711-5b0287915f1b,U02R4F43B0C,,,\"Thanks, I didn\\'t save the file. that prevented executing the command. really thank you very much\",1642784127.140200,1642790344.177900,U02R4F43B0C\\n78d00088-ed2a-4787-8e38-248c4e6dd392,,5.0,,Are we using #mlzoomcamp tag for social media or is there an updated one for this course?,1642790374.178400,1642790374.178400,U02TNEJLC84\\nf5e58853-714d-4982-b92b-4e4ddf7f248d,U02ULQFCXL0,,,\"``` ssh -i &lt;path to your private key&gt; &lt;Your GCP User&gt;@&lt;Your server\\'s IP&gt;```\\nright?\",1642789168.167400,1642790487.178500,U02TNEJLC84\\n63348edd-1eba-4796-90cb-42516536ad91,U02VBG59VQ9,,,are you using GitBash or plain windows commands?,1642784624.140900,1642790543.178900,U02UBV4EC8J\\n31d86ae0-866d-481c-9b3e-b350ee0242ae,U02VBG59VQ9,,,<@U02UBV4EC8J> both,1642784624.140900,1642790877.179200,U02VBG59VQ9\\naffb18e6-20cf-432d-ad31-ef617eddc9ae,U02U85FB8TU,,,\"maybe try :\\n```python3 ingest_data.py \\\\\\n  --user=root \\\\\\n  --password=root \\\\\\n  --host=localhost \\\\\\n  --port=5432 \\\\\\n  --db=ny_taxi \\\\\\n  --table_name=yellow_taxi_trips \\\\\\n  --url=${URL}```\\n\",1642788935.165500,1642790948.179400,U02T2DX4LG6\\n52ec0513-ed2c-4e51-bda4-1303762949c5,U02ULQFCXL0,,,right,1642789168.167400,1642790951.179600,U02ULQFCXL0\\n62682a88-5b5f-4924-919f-c4cddc2019ff,U02ULQFCXL0,,,\"i found my bug, i use internal IP while i need to use external IP\",1642789168.167400,1642791001.179800,U02ULQFCXL0\\nbf7af754-9d2a-4f40-b87f-db7b90c78962,U02ULQFCXL0,,,\"thanks to all, and sorry :smiley:\",1642789168.167400,1642791017.180000,U02ULQFCXL0\\n136ab72b-1f21-4f38-a701-2294bd9eb6c3,U02T2DX4LG6,,,\"now when trying to run the docker image again , I get this error `URL=\"\"<https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>\"\" docker run -it \\\\ --network=pg-network \\\\ taxi_ingest:v001 \\\\ --user=root \\\\ --password=root \\\\ --host=pg-database \\\\ --port=5432 \\\\ --db=ny_taxi \\\\ --table_name=yellow_taxi_trips \\\\ --url=${URL}`\",1642787354.152800,1642791055.180300,U02T2DX4LG6\\ncf4ef73d-43df-4fb4-93f3-4cea48542606,U02U85FB8TU,,,thank you <@U02T2DX4LG6> it worked.,1642788935.165500,1642791111.181000,U02U85FB8TU\\n046bb0e7-54b3-4450-b4a6-ea779321dd0e,U02TNEJLC84,,,\"also wondering this, i’m assuming there’s a new hashtag for this zoomcamp\",1642790374.178400,1642791117.181200,U02UGA597HS\\n1342729d-9c18-41c3-a932-e5ec9660a5a2,U02U85FB8TU,,,Thank you <@U02U34YJ8C8> <@U02UBV4EC8J> problem has been resolved,1642788935.165500,1642791139.181400,U02U85FB8TU\\n0ce3653e-b5b4-49f9-9f15-d4bc4a50eed6,U02TNEJLC84,,,I would think so. :thinking_face:,1642790374.178400,1642791214.181700,U02TNEJLC84\\n934d3c9a-daff-4449-b841-f8e5d5f4b619,U02VBG59VQ9,,,it gitbash it is not recognized?,1642784624.140900,1642791259.181900,U02UBV4EC8J\\n444ac792-71e4-41d0-8418-99c01ba42ac8,U02ULQFCXL0,,,No worries. Sometimes we all need to Rubber Duck Debug stuff. Glad you got it working!,1642789168.167400,1642791271.182100,U02TNEJLC84\\ne07a9d13-e1a7-4453-a3b6-52de70ef0a5d,U02VBG59VQ9,,,are able to run any other linux commands?,1642784624.140900,1642791272.182300,U02UBV4EC8J\\n6c948443-86b2-4ef1-80b7-8046a0cdb57b,U01AXE0P5M3,,,I had the same issue with installing psycopg2 using pip in linux. Using a different Postgresql dialect (pg8000) worked for me,1642511128.453300,1642791511.182700,U02ULP8PA0Z\\na1131d99-d19c-4f6c-93de-5e4933efeca2,U02VBG59VQ9,,,\"yes. This is what i got\\nUser1@DESKTOP-PD6UM8A MINGW64 /\\n$ export GOOGLE_APPLICATION_CREDENTIALS=~/C:\\\\Users\\\\User1\\\\Downloads.json\\n\\nUser1@DESKTOP-PD6UM8A MINGW64 /\\n$\",1642784624.140900,1642791630.182900,U02VBG59VQ9\\ne44a461d-025b-42b0-a84a-313e75f65d74,U02T941CTFY,,,\"For me.. I didn\\'t see anything when I checked on VSCode.. But when i checked the folder through my file manager, it was different.. and i still couldn\\'t access it\",1642765009.063500,1642792102.183100,U02T9JQAX9N\\n1a7fd3ee-236e-469f-8775-6d6d584e27fd,U02T941CTFY,,,I use Ubuntu,1642765009.063500,1642792128.183500,U02T9JQAX9N\\n8bb5e1ce-1761-44c3-b37f-591907ba6666,U02VBG59VQ9,,,\"<@U02UBV4EC8J> User1@DESKTOP-PD6UM8A MINGW64 /\\n$ gcloud auth activate-service-account <mailto:girmaysisay@gmail.com|girmaysisay@gmail.com> --key-file=/C:\\\\Users\\\\User1\\\\Downloads.json --project=dezoomcamp22\\nPython was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings &gt; Manage App Execution Aliases.\\nERROR: (gcloud.auth.activate-service-account) Unable to read file [C:UsersUser1Downloads.json]: [Errno 2] No such file or directory: \\'C:UsersUser1Downloads.json\\'\",1642784624.140900,1642792203.183700,U02VBG59VQ9\\na8e11639-f69d-402d-b8b7-d5b493e10383,U029DM0GQHJ,,,Thanks <@U029DM0GQHJ>,1642788692.163300,1642792829.184100,U02UBV4EC8J\\n6fb0ba85-be1b-4684-943a-0a6791e9d752,U029DM0GQHJ,,,<@U029DM0GQHJ> for homework question 3 I am getting a number that is not in the list,1642788692.163300,1642792935.184300,U02UBV4EC8J\\n208d00bc-d184-429c-b851-a5c57a70b6f3,,17.0,,\"Two silly question guys:\\n1. Where is the host name `pgdatabase` coming from when loggin on pgAdmin? I haven\\'t seen this configured in the docker yaml nor on previous container builds. \\n2. Is there a way to create the sever on pgAdmin after running the docker composer without having to type the information every time we run the composer `up`\",1642793587.189800,1642793587.189800,U02UX664K5E\\nFAF80C78-22D3-44AF-B0AC-7F967C8E9940,U02T2DX4LG6,,,\"So for the second error, in order to run that container (taxi_ingest) you also need to have the Postgres 13 container running as well. The name you set in the Postgres container is the hostname you\\'ve set for the taxi_ingest container.\\n\\nSo I\\'d try creating your Postgres 13 container in another command window or terminal, then, try again\\n\\n\",1642787354.152800,1642794137.194500,U02U34YJ8C8\\n94885945-71a9-433a-9347-bdcd4d51a0c9,U02RR9Z0CCV,,,\"Thank you! Just really confused: (1) `oleg@de-zoomcamp:~$ pgcli -h localhost -U root -d ny_taxi`  works fine. (2)`oleg@de-zoomcamp:~/data-engineering-zoomcamp/week_1_basics_n_setup/2_docker_sql$ docker ps`  works fine, exactly as on the video. (3) When run `mclieor2@DESKTOP-CNBIO0D:~$ pgcli -h localhost -U root -d ny_taxi -p 5432`  I get an error: could not connect to server: Connection refused. Is the server running on host \"\"localhost\"\" (127.0.0.1) and accepting TCP/IP connections on port 5432? And no password for root. (4) VSC offers only Port 5432 and Local Address: localhost:5433\",1642781963.136000,1642794159.195300,U02RR9Z0CCV\\nd93dbfa6-f24f-4e2d-8e14-ee8db64a6097,U02UX664K5E,,,Create a volume for the pgadmin,1642793587.189800,1642794310.196000,U02T0CYNNP2\\n4f0536f7-ce7e-42a3-a931-db94cf8e861a,U02CD7E30T0,,,I’m also missing the Terraform part. I’m really loving this course and starting to get more comfortable with the command line.,1642755698.033900,1642794311.196200,U02CPH3FR33\\n49212F7E-B70D-485D-AE67-89A7AE697A9A,U02T2DX4LG6,,,You can check all your currently running containers with the command \\'docker ps’,1642787354.152800,1642794315.196400,U02U34YJ8C8\\naa74688b-f010-4bad-82ef-29908cbaf8c6,U02T0CYNNP2,,,\"docker run -it \\\\\\n    --network=pg-network \\\\\\n    ingest_taxi_data:001 \\\\\\n    --user=mide \\\\\\n    --password=adebimpe1994 \\\\\\n    --host=pgdatabase_1 \\\\\\n    --port=5432 \\\\\\n    --db=ny_taxi \\\\\\n    --table=yellow_taxi_trips \\\\\\n    --url=\"\"<https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv|https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>\"\"\",1642786979.150600,1642794359.197100,U02T0CYNNP2\\nff4a1621-5a58-489e-a82a-1bc057d7e480,U02TCMEDTUL,,,\"This one helped <https://stackoverflow.com/questions/64261546/python-cant-install-packages>\\n\\n( UPDATE: December 28, 2020 from the most upvoted answer)\",1642789792.172700,1642794432.197300,U02TCMEDTUL\\n8322b2f8-8006-4c9e-994d-26f39a9eb788,U02TNEJLC84,,,dezoomcamp would do :smiley:,1642790374.178400,1642794438.197800,U01AXE0P5M3\\n5e56d358-7ceb-4b26-a0db-7f8350219c1e,U02T0CYNNP2,,,\"File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py\"\", line 597, in connect\\n    return self.dbapi.connect(*cargs, **cparams)\\n  File \"\"/usr/local/lib/python3.9/site-packages/psycopg2/__init__.py\"\", line 122, in connect\\n    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)\\npsycopg2.OperationalError: could not translate host name \"\"pgdatabase_1\"\" to address: Temporary failure in name resolution\",1642786979.150600,1642794482.198300,U02T0CYNNP2\\n615df2e4-973e-4e95-863a-95e499304306,U02TNEJLC84,,,updated the form,1642790374.178400,1642794499.198600,U01AXE0P5M3\\ndde87cf8-ae19-43a3-bcac-cfb75acf7b64,U02UX664K5E,,,1. It\\'s the service name,1642793587.189800,1642794500.198800,U02TNEJLC84\\n55BA1388-57A7-4E84-860F-D393B2F42C4C,U02UX664K5E,,,\"1. It\\'s specified in the Yaml file. Should be the second line \\n2. Probably but I don\\'t know how\",1642793587.189800,1642794516.199400,U02U34YJ8C8\\n56bcd6c5-7ce8-4125-bf45-b38f9d3b3f4c,U02U85FB8TU,,,wow - I didn\\'t know python 2 was still alive!,1642788935.165500,1642794555.199800,U01AXE0P5M3\\n4af7b87a-50e8-4f74-abec-8966e2869820,U02T0CYNNP2,,,\"services:\\n  pgdatabase:\\n    image: postgres:13\\n    environment:\\n      - POSTGRES_USER=mide\\n      - POSTGRES_PASSWORD=adebimpe1994\\n      - POSTGRES_DB=ny_taxi\\n    volumes:\\n      - \"\"./postgresql_data:/var/lib/postgresql/data:rw\"\"\\n    ports:\\n      - \"\"5432:5432\"\"\\n  pgadmin:\\n    image: dpage/pgadmin4\\n    environment:\\n      - PGADMIN_DEFAULT_EMAIL=adesoba5@gmail.com\\n      - PGADMIN_DEFAULT_PASSWORD=adebimpe1994\\n    volumes:\\n      - \"\"./pgadmin_data:/var/lib/pgadmin:rw\"\"\\n    ports:\\n      - \"\"8080:80\"\"\",1642786979.150600,1642794562.200000,U02T0CYNNP2\\ndbcbae91-2a62-4a19-8ee5-cf8bd41cfcd8,U02UX664K5E,,,\"pgadmin:\\n    image: dpage/pgadmin4\\n    environment:\\n      - PGADMIN_DEFAULT_EMAIL=adesoba5@gmail.com\\n      - PGADMIN_DEFAULT_PASSWORD=adebimpe1994\\n    volumes:\\n      - \"\"./pgadmin_data:/var/lib/pgadmin:rw\"\"\\n    ports:\\n      - \"\"8080:80\"\"\",1642793587.189800,1642794609.200200,U02T0CYNNP2\\n7afc9ce5-08a0-46ee-94a4-79b4c0304151,U02UX664K5E,,,\"You can do something like this\\n```version: \\'3.8\\'\\nservices:\\n  db:\\n    container_name: pg_container\\n    image: postgres\\n    restart: always\\n    volumes:\\n        - ./postgres-data:/var/lib/postgresql/data\\n    environment:\\n      POSTGRES_USER: root\\n      POSTGRES_PASSWORD: test\\n      POSTGRES_DB: test_db\\n    ports:\\n      - \"\"5432:5432\"\"\\n\\n\\n  pgadmin:\\n    container_name: pgadmin4_container\\n    image: dpage/pgadmin4\\n    restart: always\\n    volumes:\\n      - ./data/pgadmin:/var/lib/pgadmin\\n    environment:\\n      PGADMIN_DEFAULT_EMAIL: <mailto:admin@admin.com|admin@admin.com>\\n      PGADMIN_DEFAULT_PASSWORD: root\\n    ports:\\n      - \"\"5050:80\"\"\\n    links:\\n      - \"\"db:psql-server\"\"```\\nBut you\\'ll need to modify the rights on the /data directory.\",1642793587.189800,1642794634.200400,U02TNEJLC84\\n602e720c-a7d1-4f27-bda7-c7a9736762ef,U02UX664K5E,,,\"for 2. just add more environment variables, so\\nyou don\\'t have to type them\",1642793587.189800,1642794704.200800,U02U2Q5P61Z\\n9dec4755-107e-4c96-b7bd-ce10886ff9ce,U02TNEJLC84,,,Awesome! Thank you Alexey!,1642790374.178400,1642794709.201100,U02TNEJLC84\\nd8363ef1-8609-4e2d-9d34-4da6d174444d,U02UX664K5E,,,\"thanks, I\\'ll mention that in the github repo\",1642793587.189800,1642794967.204800,U01AXE0P5M3\\n24757ad3-3d91-463d-877e-8a01f8a57e87,,6.0,,\"Hello I am getting an error when running pgcli after creating the docker container as shown below:\\nRunning this successfully:\\ndocker run -it \\\\\\n\\xa0\\xa0-e POSTGRES_USER=\"\"root\"\" \\\\\\n\\xa0\\xa0-e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n\\xa0\\xa0-e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n\\xa0\\xa0-v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n\\xa0\\xa0-p 5432:5432 \\\\\\npostgres:13\\n\\nThen running this: pgcli -h localhost -p 5432 -u root -d ny_taxi\\n\\nYields this error: connection to server at \"\"localhost\"\" (127.0.0.1), port 5432 failed: FATAL:\\xa0database \"\"ny_taxi\"\" does not exist\\n\\nShouldn\\'t this database already be created from the docker run? I\\'ve worked with postgres before but not confidently and it is possible I may have messed something up with the users and roles and databases. I\\'ve gone through other threads and haven\\'t been able to solve the issue. Any help would be greatly appreciated.\\n\\nEdit: This is on an M1 mac btw\\n\\nThanks!\",1642795056.206700,1642795056.206700,U02T52AQB2R\\n5d635d72-0b5f-453c-a107-ca4ae66a0f39,U02T52AQB2R,,,in whih directory are you running this command?,1642795056.206700,1642795186.206900,U02U2Q5P61Z\\n234a433a-994c-48e7-8479-04a291f28f15,U02T52AQB2R,,,the docker command,1642795056.206700,1642795215.207300,U02U2Q5P61Z\\n1f8c15e5-1fca-44b4-a7c4-8f50e0f57c73,U02T52AQB2R,,,I\\'m running it in the 2_docker_sql folder within the course directory,1642795056.206700,1642795269.207500,U02T52AQB2R\\n74867251-6a0a-4d68-9bdd-f5d4e862eefc,U02UX664K5E,,,\"docker run -it \\\\\\n    --network=pg-network \\\\\\n    ingest_taxi_data:001 \\\\\\n    --user=mide \\\\\\n    --password=adebimpe1994 \\\\\\n    --host=pgdatabase_1 \\\\\\n    --port=5432 \\\\\\n    --db=ny_taxi \\\\\\n    --table=yellow_taxi_trips \\\\\\n    --url=\"\"<https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv|https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>\"\"\\n\\n\\nAm having issues with running this image after successfully creating my containers with docker compose\",1642793587.189800,1642795390.208000,U02T0CYNNP2\\nf15c29f5-74ad-4fe4-91de-3d8105eef46e,U02T2DX4LG6,,,<@U02T2DX4LG6> any luck on this error? I was getting the same last night,1642787354.152800,1642796000.209000,U02UAFF1WU9\\n53ddcad3-8068-4363-b6c0-b5b2b39c9011,U02UX664K5E,,,\"I haven\\'t tried that, but in principle, you can get the list of available networks (`docker network ls`) and then use the network created by docker compose to run the ingestion script\",1642793587.189800,1642796171.211400,U01AXE0P5M3\\nfbdcc526-bb49-444e-9d35-6b30d2abe5e7,U02U85FB8TU,,,\"<@U01AXE0P5M3> default for mac os is python 2. They say, some applications still depend on it.\",1642788935.165500,1642796241.212600,U02U85FB8TU\\n777db341-5023-4572-97cb-8bfcf4961130,,28.0,,\"Dear DE, I have tried VScode, bash, terminal to initiate terraform. I was the video by <@U01DHB2HS3X> both parts  but still was not able to do it. I am spending so much time on this since yesterday.  I got some help from <@U02UBV4EC8J>. I would appreciate it if someone can walk me through it. I am located in EST and going to keep working on to figure out. Thank you\",1642796264.212900,1642796264.212900,U02VBG59VQ9\\nd929ef62-26c6-4bf5-8a54-a56def4095d1,U02RR9Z0CCV,,,\"so if you\\'re on the VM, use the 5432 port. but if you want to access it from your local env for some reasons, then you\\'ll need to use the 5433 port\\n\\nFor checking if pg is running, you\\'ll actually be fine with staying inside the VM - so don\\'t worry about port forwarding for postgres\",1642781963.136000,1642796339.213000,U01AXE0P5M3\\nd73adf5e-0d80-48bc-a357-1034dcb217df,U02T941CTFY,,,in Linux this directory is managed by root and not by your user. You can do `sudo ls &lt;dir name&gt;` to see what\\'s inside,1642765009.063500,1642796425.213300,U01AXE0P5M3\\n44c11d1d-6251-48f2-b330-fe9386df73a4,U01QGQ8B9FT,,,I mean make sure that `python` points to anaconda and not the other python you have in your system (it appears you have another one),1642769991.086200,1642796526.213500,U01AXE0P5M3\\n3d0dcd35-e90e-4e69-8c74-c2913bdc95f8,U01QGQ8B9FT,,,\"to be honest I\\'m not totally sure where your python comes from, but mine - which I installed with anaconda - lives in my home directory (c:\\\\users\\\\alexe\\\\Anaconda3)\",1642769991.086200,1642796611.213700,U01AXE0P5M3\\n2197cf72-d141-4b76-b041-89d30b1b0606,U02VBG59VQ9,,,can you share more details? what happens when you go to the directory with terraform files and run `terraform init`?,1642796264.212900,1642796681.213900,U01AXE0P5M3\\nd8659b9f-dc71-4bd9-947c-0ca96ce6799e,U02VBG59VQ9,,,\"Make sure you are using the PROJECT ID from your dashboard when prompted by terraform. That\\'s what got me. Otherwise, what is the exact error message that you\\'re getting?\",1642796264.212900,1642796684.214100,U02TNEJLC84\\na1a2d487-1bf3-478e-bdfb-e73206d30d15,U02VBG59VQ9,,,\"<@U01AXE0P5M3>\\nThe directory has no Terraform configuration files. You may begin working\\nwith Terraform immediately by creating Terraform configuration files.\",1642796264.212900,1642796778.214300,U02VBG59VQ9\\nd8ac7ce8-4643-4976-a246-745e70b225e0,U02VBG59VQ9,,,Ah. You need to cd and make sure you are in the directory where you see <http://main.tr|main.tr>.,1642796264.212900,1642796804.214500,U02TNEJLC84\\n67866b2c-5097-4b6a-b9fe-90020f416e98,U02VBG59VQ9,,,<https://github.com/MichaelShoemaker/data-engineering-zoomcamp|data-engineering-zoomcamp>/<https://github.com/MichaelShoemaker/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup|week_1_basics_n_setup>/<https://github.com/MichaelShoemaker/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup/1_terraform_gcp|1_terraform_gcp>/terraform/,1642796264.212900,1642796831.214700,U02TNEJLC84\\ncde8fe11-9dbb-445d-bf6a-a712e4579264,U02UX664K5E,,,\"Thanks it worked, i forgot to change my network\",1642793587.189800,1642796835.214900,U02T0CYNNP2\\n77ffae11-fed1-4fb5-9dc2-37b096bd6552,U02UX664K5E,,,\"Never knew docker compose created a new network, thanks very much\",1642793587.189800,1642796871.215100,U02T0CYNNP2\\n634f8319-94d6-44d3-b088-07d1c0283f98,U02VBG59VQ9,,,you mean clone it <@U02TNEJLC84> mike?,1642796264.212900,1642797025.215400,U02VBG59VQ9\\n6c04fd25-cb71-4626-9b14-9de49697fd85,U02VBG59VQ9,,,Yes.,1642796264.212900,1642797057.215600,U02TNEJLC84\\n43b9a24d-6658-46a2-8a50-f26f608fced6,U02VBG59VQ9,,,\"<@U02TNEJLC84> this is what i got\\nC:\\\\Users\\\\User1&gt;clone git <https://github.com/Sisaygir/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/terraform/main.tf>\\n\\'clone\\' is not recognized as an internal or external command,\\noperable program or batch file.\",1642796264.212900,1642797111.215800,U02VBG59VQ9\\ncb4523b6-21c5-4743-9403-beda5e81e57c,U02VBG59VQ9,,,Look at about timestamp 20:30 in the video. Here. <https://www.youtube.com/watch?v=ae-CV2KfoN0&amp;list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&amp;index=11> Once you have it cloned onto your VM in GCP you need to navigate to the correct directory before you try to initialize terraform.,1642796264.212900,1642797270.216100,U02TNEJLC84\\n0f996e39-07a8-4e2b-a637-c1fd136f6f73,U02VBG59VQ9,,,\"ok i clone it and here i am now <@U01AXE0P5M3> and mike\\nC:\\\\Users\\\\User1\\\\data-engineering-zoomcamp&gt;terraform init\\nTerraform initialized in an empty directory!\\n\\nThe directory has no Terraform configuration files. You may begin working\\nwith Terraform immediately by creating Terraform configuration files.\",1642796264.212900,1642797873.216600,U02VBG59VQ9\\n5b50c55d-ad24-4d63-b32a-a32906c5f5c0,U02VBG59VQ9,,,\"<@U02TNEJLC84>\\nC:\\\\Users\\\\User1\\\\data-engineering-zoomcamp&gt;terraform init\\nTerraform initialized in an empty directory!\\n\\nThe directory has no Terraform configuration files. You may begin working\\nwith Terraform immediately by creating Terraform configuration files.\",1642796264.212900,1642797977.216800,U02VBG59VQ9\\n6f1a96a8-27d6-4d7e-9e01-27b503b0525b,U02VBG59VQ9,,,\"you would need to chnage to following directories:\\nweek_1_basics_n_setup/1_terraform_gcp\",1642796264.212900,1642798078.217000,U02UBV4EC8J\\n4b281a91-c005-46bf-9265-295fa7981d3f,U02UX664K5E,,,how did the command look like for you? I\\'d like to add a comment to readme with this,1642793587.189800,1642798083.217200,U01AXE0P5M3\\n2353c449-7c84-4d8e-a5f3-45f79689bafa,U02VBG59VQ9,,,\"in there you will see terraform folder, cd to that\",1642796264.212900,1642798092.217400,U02UBV4EC8J\\nf49b182c-01f1-422d-95f1-f9be9fbc5725,U02VBG59VQ9,,,What <@U02UBV4EC8J> said.,1642796264.212900,1642798094.217600,U02TNEJLC84\\n468cec76-c764-4a40-b810-1bed1b40557c,U02VBG59VQ9,,,than run `terrform init` ,1642796264.212900,1642798105.217900,U02UBV4EC8J\\n01a383ce-0891-456f-8184-1ba65cc827de,U02VBG59VQ9,,,\"<@U02UBV4EC8J>\\nC:\\\\Users\\\\User1\\\\data-engineering-zoomcamp&gt;cd week_1_basics_n_setup/1_terraform_gcp\\n\\nC:\\\\Users\\\\User1\\\\data-engineering-zoomcamp\\\\week_1_basics_n_setup\\\\1_terraform_gcp&gt;terraform init\\nTerraform initialized in an empty directory!\\n\\nThe directory has no Terraform configuration files. You may begin working\\nwith Terraform immediately by creating Terraform configuration files.\",1642796264.212900,1642798198.218200,U02VBG59VQ9\\n8b08d9f6-3aa9-4beb-960e-a26ce81a4838,U02VBG59VQ9,,,\"Hi all, I have some problems with terraform too.\\nDo I need to clone files to my local machine and run terraform init here?\",1642796264.212900,1642798298.218400,U02Q7466F2T\\nfc5e2cfb-1aab-4139-a6f2-702ec2d00995,U02VBG59VQ9,,,<@U02UBV4EC8J> <@U02TNEJLC84> hallelujah,1642796264.212900,1642798458.218600,U02VBG59VQ9\\n0e841485-73bf-4e1a-bd79-b7130be7ff0b,U02VBG59VQ9,,,<@U02VBG59VQ9> You are SOOOO close. Just cd into the terraform directory and you\\'re home free.,1642796264.212900,1642798461.218800,U02TNEJLC84\\ndd897706-611f-43ec-b3b9-81750610540d,U02BVP1QTQF,,,\"Hi <@U02BVP1QTQF>, just had a chance to look at your notes. Thank you for covering the workshop part too, and especially the additional details. I really like it when someone takes the extra step of reading further documentation on understanding a technology, and noting what could be useful in future. Kudos! :clap:\",1642713863.457200,1642798494.219100,U01DHB2HS3X\\nfd65f457-7118-4560-b634-2d74b66c968e,U02VBG59VQ9,,,NNNNIIIIIICCCCCCEEEE!!!! <@U02VBG59VQ9>. Glad you got it friend.,1642796264.212900,1642798495.219300,U02TNEJLC84\\ncca9e1fc-40ac-4b4a-a8d9-126273f521f4,U02VBG59VQ9,,,mike 2 days man finally to the next one,1642796264.212900,1642798534.219700,U02VBG59VQ9\\n84567299-8230-4e42-a9ef-fe05eb0f4059,U02VBG59VQ9,,,<@U02Q7466F2T> Did you watch the videos yet? See above in this thread. They do a walk through in that video of how to set it up.,1642796264.212900,1642798536.219900,U02TNEJLC84\\n3e67e1ee-e497-4b56-ba95-910c37d2bed7,U02VBG59VQ9,,,awesome,1642796264.212900,1642798571.220200,U02UBV4EC8J\\n86af9163-7d01-42db-8cd2-31c6d0e78bd6,U02VBG59VQ9,,,\"<@U02Q7466F2T> yes clone it first, then cd data-engineering-zoomcamp\\\\week_1_basics_n_setup\\\\1_terraform_gcp\\\\terraform\",1642796264.212900,1642798691.220400,U02VBG59VQ9\\nf6b98180-191a-4716-9e2a-e1bef1d872b8,U02VBG59VQ9,,,<@U02UBV4EC8J> ken greatly appreciated man,1642796264.212900,1642798729.220600,U02VBG59VQ9\\n56ae6bf3-4ac2-42bc-ada2-0e98768902e6,U02VBG59VQ9,,,<@U02VBG59VQ9> invoice is on your way ))),1642796264.212900,1642798826.220900,U02UBV4EC8J\\n95bbfac6-0fea-4258-9ba9-1fbda9d82075,U02VBG59VQ9,,,Apply complete! Thanks guys!,1642796264.212900,1642798840.221100,U02Q7466F2T\\n0cd15110-53c0-45e8-87e3-26091cdd7cb4,U02VBG59VQ9,,,Well done <@U02Q7466F2T>!!!,1642796264.212900,1642799118.221400,U02TNEJLC84\\n077a577f-d414-4fb3-9f57-aea18e6db7ce,,4.0,,\"Error creating Dataset: googleapi: Error 400: The project dezoomcamp22 has not enabled BigQuery., invalid\\n│\\n│   with google_bigquery_dataset.dataset,\\n│   on <http://main.tf|main.tf> line 45, in resource \"\"google_bigquery_dataset\"\" \"\"dataset\"\":\\n│   45: resource \"\"google_bigquery_dataset\"\" \"\"dataset\"\" {\",1642799592.221900,1642799592.221900,U02VBG59VQ9\\n56163e31-4cfb-49a3-85ff-99bdf77792cb,U02VBG59VQ9,,,\"have you added these three roles to a service account:\\n1. Storage Admin\\n2. Storage Object Admin\\n3. BigQuery Admin\",1642799592.221900,1642799686.222000,U02UBV4EC8J\\n71dbdab8-0767-471d-b5e5-06c2be916b5a,U02VBG59VQ9,,,\"need to enable BigQuery, like this:\",1642799592.221900,1642799824.222200,U02QP6JM83U\\n63c25f46-3e1b-47d0-b883-94c240a4ea20,U02VBG59VQ9,,,,1642799592.221900,1642799832.222400,U02QP6JM83U\\n00cf799c-7033-4f5c-8d45-e865c3f28601,,15.0,,\"Hey All,\\n\\nHaving an issue connecting to PG admin after creating the docker network. More details in the thread\",1642799868.223500,1642799868.223500,U02UAFF1WU9\\n55779af4-2b1d-4612-b848-e8fa868281b9,U02UAFF1WU9,,,\"I created the network and both containers are inside the network\\n\\n```[\\n    {\\n        \"\"Name\"\": \"\"pg-network\"\",\\n        \"\"Id\"\": \"\"3d26b997a8719a9dcc2b81cb3eca4347a5f21b77f297e128b426892e2149c563\"\",\\n        \"\"Created\"\": \"\"2022-01-21T04:08:59.3588392Z\"\",\\n        \"\"Scope\"\": \"\"local\"\",\\n        \"\"Driver\"\": \"\"bridge\"\",\\n        \"\"EnableIPv6\"\": false,\\n        \"\"IPAM\"\": {\\n            \"\"Driver\"\": \"\"default\"\",\\n            \"\"Options\"\": {},\\n            \"\"Config\"\": [\\n                {\\n                    \"\"Subnet\"\": \"\"172.18.0.0/16\"\",\\n                    \"\"Gateway\"\": \"\"172.18.0.1\"\"\\n                }\\n            ]\\n        },\\n        \"\"Internal\"\": false,\\n        \"\"Attachable\"\": false,\\n        \"\"Ingress\"\": false,\\n        \"\"ConfigFrom\"\": {\\n            \"\"Network\"\": \"\"\"\"\\n        },\\n        \"\"ConfigOnly\"\": false,\\n        \"\"Containers\"\": {\\n            \"\"3e868933ecfad668b7b41d1dcf39da365d7f912a3665aeed5d4b53f58edebf5f\"\": {\\n                \"\"Name\"\": \"\"flamboyant_wright\"\",\\n                \"\"EndpointID\"\": \"\"7cf579471e6b4fc7708dc4196adca1cbb4539d70d622ba708fe1d6ea536c00ff\"\",\\n                \"\"MacAddress\"\": \"\"02:42:ac:12:00:03\"\",\\n                \"\"IPv4Address\"\": \"\"172.18.0.3/16\"\",\\n                \"\"IPv6Address\"\": \"\"\"\"\\n            },\\n            \"\"cf2fe7e1821435db294e30279133cb16a416732f2973d862d8e81c35a7a75355\"\": {\\n                \"\"Name\"\": \"\"keen_yalow\"\",\\n                \"\"EndpointID\"\": \"\"d3ccb25d5f82001268bf54849c7abbb67f7fb71f1a8224186ff65c426ca97a00\"\",\\n                \"\"MacAddress\"\": \"\"02:42:ac:12:00:02\"\",\\n                \"\"IPv4Address\"\": \"\"172.18.0.2/16\"\",\\n                \"\"IPv6Address\"\": \"\"\"\"\\n            }\\n        },\\n        \"\"Options\"\": {},\\n        \"\"Labels\"\": {}\\n    }\\n]```\\ncontainer `flamboyant_wright` is the postgres instance running on port 8080\\n\\ncontainer `keen_yalow` is running on port 5431 as the and should be the name passed to pgadmin, correct?\\n\\nI am getting the response: `unable to connect to server could not translate host name`\",1642799868.223500,1642800045.223600,U02UAFF1WU9\\n3b5081da-2ff6-4eaa-bc25-9c3ff6b70164,U02UAFF1WU9,,,\"This is prior to the docker compose video, I couldn\\'t move past video 1.2.3 before getting this connected\",1642799868.223500,1642800098.223800,U02UAFF1WU9\\n968aedf5-7e9e-4b37-bc4d-8ebc1272d0cc,U02VBG59VQ9,,,<@U02UBV4EC8J> yes <@U02QP6JM83U> solution worked. Thank you both,1642799592.221900,1642800206.224000,U02VBG59VQ9\\ne55cc681-7fb1-49f6-b0e3-b3894efaafd6,U02UAFF1WU9,,,is this a format for your docket-compose?,1642799868.223500,1642800315.224200,U02UBV4EC8J\\n8ffe759b-2359-46ca-966a-22c6bdf5a7c4,U02UAFF1WU9,,,yea you are right on db network being used y pgadmin.,1642799868.223500,1642800345.224400,U02UBV4EC8J\\ne52dc284-cf43-4128-9551-539b3fab988a,U02UAFF1WU9,,,\"<@U02UAFF1WU9> What commands are you running? If you\\'re on linux this should do the trick if you just make it an executable.\\n```!#/bin/bash\\n\\n#Make Postgres\\ndocker network create pg-network\\n\\ndocker run -it \\\\\\n    -e POSTGRES_USER=\"\"root\"\" \\\\\\n    -e POSTGRES_PASSWORD=\"\"root\"\"    \\\\\\n    -e POSTGRES_DB=\"\"ny_taxi\"\"    \\\\\\n    -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n    -p 5432:5432 \\\\\\n    --network=pg-network \\\\\\n    --name pg-database \\\\\\n        postgres:13\\n\\ndocker run -it \\\\\\n-e PGADMIN_DEFAULT_EMAIL=\"\"<mailto:admin@admin.com|admin@admin.com>\"\" \\\\\\n-e PGADMIN_DEFAULT_PASSWORD=\"\"root\"\" \\\\\\n-p 8080:80 \\\\\\n--network=pg-network \\\\\\n--name pgadmin \\\\\\ndpage/pgadmin4```\\n\",1642799868.223500,1642800821.224600,U02TNEJLC84\\n17ae8f57-f3bf-45c2-aa86-6720379c5e71,U02UAFF1WU9,,,\"<@U02UBV4EC8J> No its not docker compose, I didn\\'t get to that video yet. It is two seperate docker run commands.\\n\\n<@U02TNEJLC84> I will give that a shot and compare to what i was running. Thanks for the help both\",1642799868.223500,1642800906.224800,U02UAFF1WU9\\nc35d5655-6d53-476e-8f49-08ad9271f9a6,U02UAFF1WU9,,,\"yea file you paste above looked like josn list, I though you are doing it a different way\",1642799868.223500,1642801050.225100,U02UBV4EC8J\\nb768f32f-9cff-48b3-bf33-6fe3ad050e1e,U02UAFF1WU9,,,<@U02TNEJLC84> your commands worked. Thank you,1642799868.223500,1642801239.228100,U02UAFF1WU9\\n694B6463-FD9D-40EC-8F9D-72042692DE23,U02UAFF1WU9,,,\"<@U02UAFF1WU9> as you have not specified the name of container during  run time and you are trying to connect to database using default name provided by docker , I understand this not feasible . U may like to stop the container and rename .or run another container with —name specified \",1642799868.223500,1642801318.229600,U02AGF1S0TY\\nd24091b1-061e-429d-b228-67df3c4be05f,U02UAFF1WU9,,,<@U02AGF1S0TY> Thanks for the explanation! I didn\\'t realize it wasn\\'t possible to connect to the datbase with the default name provided by docker,1642799868.223500,1642801372.229800,U02UAFF1WU9\\n635C0147-2811-4ABA-BE82-B66320EE0AC1,U02UAFF1WU9,,,<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642733568495200|https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642733568495200>,1642799868.223500,1642801391.230000,U02AGF1S0TY\\nc0c84f84-36ce-4710-93b4-33e8fde2b4c9,U02UAFF1WU9,,,<@U02UAFF1WU9> No problem. Glad it worked!,1642799868.223500,1642801515.230400,U02TNEJLC84\\n0cf8aba5-9c5f-4c52-9514-31999de53fff,U02UAFF1WU9,,,\"<@U02AGF1S0TY> I tried assigning the ip address as well and that didnt work. Weird issue, but from now on i will specify the name of the container explicitly\",1642799868.223500,1642801559.230600,U02UAFF1WU9\\n66F4DFEB-1665-434B-8797-51CE08432C74,U02UAFF1WU9,,,Everyday some new learning for all of us :+1::skin-tone-2:,1642799868.223500,1642801624.231600,U02AGF1S0TY\\nd7ca95bc-83b2-4dbc-b2d0-8d4cc7c3d463,U02UX664K5E,,,According to this article (<https://runnable.com/docker/docker-compose-networking>) Docker compose sets up a default network and adds all the containers defined in the file to the networks,1642793587.189800,1642803382.234200,U02U809EAE7\\n03aad22f-276f-4b5b-9221-da6392444847,U02UZBJ2Q6L,,,no worries:),1642766045.069100,1642803701.236200,U02TQUYTBJA\\neeb650cf-0268-4b4f-bae6-d05f42d978b0,,,,\"Here is a hint for speeding up  `<http://pd.to|pd.to>_sql()` : use the ‘method’ parameter\\n```method=\\'multi\\', # Pass multiple values in a single INSERT clause\\nchunksize=10000 # Specify the number of rows in each batch to be written at a time```\\nThis sped up my inserts by 10x. Your milage may vary.\",,1642803739.236700,U02U1812MRV\\na07445dd-8a44-44a9-8ab3-4bb981cac721,U02UX664K5E,,,\"Looking at the references (<https://docs.docker.com/compose/compose-file/compose-file-v3/#networks>), it seems we can explicitly define a network and add our containers to it.\\n\\nAdding to the current `docker-compose.yaml`, I can create like so.\\n\\n```version: \\'3.8\\'\\nservices:\\n  pgdatabase:\\n    image: postgres:13\\n    environment:\\n      - POSTGRES_USER=root\\n      - POSTGRES_PASSWORD=root\\n      - POSTGRES_DB=ny_taxi\\n    volumes:\\n      - \"\"./ny_taxi_postgres_data:/var/lib/postgresql/data:rw\"\"\\n    ports:\\n      - \"\"5432:5432\"\"\\n    networks:\\n      - pg-network\\n\\n  pgadmin:\\n    image: dpage/pgadmin4\\n    environment:\\n      - PGADMIN_DEFAULT_EMAIL=admin@admin.com\\n      - PGADMIN_DEFAULT_PASSWORD=root\\n    ports:\\n      - \"\"8080:80\"\"\\n    networks:\\n      - pg-network\\n\\nnetworks:\\n  pg-network:\\n    name: pg-network```\\n\",1642793587.189800,1642804080.236900,U02U809EAE7\\n16d851bb-0e02-400d-bd5a-633f3a85c8a6,U02UX664K5E,,,<https://docs.docker.com/compose/networking/|Docker’s networking reference> is also quite good at explaining and giving example,1642793587.189800,1642804128.237400,U02U809EAE7\\n322c24f4-3485-41d6-b34e-7313586c3345,U02UX664K5E,,,\"I check the available networks, so i detect the network for the container generated by the docker compose which is \\' docker_sql_default\\'\",1642793587.189800,1642804302.237800,U02T0CYNNP2\\n8cb1a801-29a8-485d-93e1-03df85b62d91,U02UX664K5E,,,\"docker run -it \\\\\\n    --network=docker_sql_default \\\\\\n    ingest_taxi_data:001 \\\\\\n    --user=mide \\\\\\n    --password=adebimpe1994 \\\\\\n    --host=pgdatabase \\\\\\n    --port=5432 \\\\\\n    --db=ny_taxi \\\\\\n    --table=yellow_taxi_trips \\\\\\n    --url=\"\"<https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv|https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>\"\"\",1642793587.189800,1642804352.238000,U02T0CYNNP2\\n7243733c-b6e4-48b3-b46c-74a933c2f0d3,U02BVP1QTQF,,,\"Thank you! I kind of need to know how the “big picture” of things work for me to understand them, so I try to fill in the gaps when there are things I’m not getting. I actually spent like 30 minutes or so trying to understand the 2 strings right before the resource block…\",1642713863.457200,1642805134.238400,U02BVP1QTQF\\n1671251d-e74b-48dd-8fde-99ec9c0efbc3,,4.0,,Does pgadmin not work for people working with m1 macbooks(using m1 macbook air)? I thought it would work fine since we are using docker/docker-compose. The docker-compose file runs fine but when I go to localhost:8080 I can’t load the pgadmin page. Although I could use pgcli if admin doesn’t work.,1642805936.243500,1642805936.243500,U02QQEQGTV2\\n03f67500-5423-4ec7-9285-b229f1ac331b,,2.0,,\"Hello! :wave: A quick question: How much in total will The Google Cloud Platform cost? I\\'ve already used my credit for different projects so I\\'d like to know how to organize myself? Thank you in advance, good people! :hugging_face:\",1642805982.244000,1642805982.244000,U02TMQZPJQZ\\n2926FF47-1445-43FD-98FB-8677D0E649CE,U02QQEQGTV2,,,Works fine for me. Was it working before you got the docker-compose step?,1642805936.243500,1642806212.245200,U02U34YJ8C8\\nbefee06c-825d-4e65-8e0e-4d6a1dd94a69,U02QQEQGTV2,,,I went straight to the docker-compose set up so idk. I could try it the slower method but it doesn’t seem smart to keep doing stuff the slower way.,1642805936.243500,1642806522.245500,U02QQEQGTV2\\n724f1acc-360a-454e-95bc-ef3ed4476f52,U02VBG59VQ9,,,yea i have all these done now,1642784624.140900,1642806968.245900,U02USF4NYG4\\n189609d2-8451-4510-9d82-d1be145a53b9,U02VBG59VQ9,,,\"i think you have a bad path for your key file, looks like it can\\'t find it\",1642784624.140900,1642806987.246100,U02USF4NYG4\\n687b4392-838d-4bc2-b583-f5f8ed46b2f9,U02VBG59VQ9,,,and you also need to setup your Python if its not installed,1642784624.140900,1642806996.246300,U02USF4NYG4\\ne3914985-55b6-4179-afce-abe6cda83531,U02TMQZPJQZ,,,I had the same issue. Make another Google Account and they will still give you the credits with that one even though both are linked to your phone number. <https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642457024345400>,1642805982.244000,1642807897.246500,U02TNEJLC84\\n025a1531-799d-4b4f-a6a6-c23b06cbac0c,U02QQEQGTV2,,,Docker-compose setting on my M1 Macbook Pro works fine,1642805936.243500,1642808517.247100,U02SE1AFBFE\\n3f97a8ae-e34a-4a78-ab7a-84f7198ecc66,U02VBG59VQ9,,,\"there is a terraform directory which has been set up on the github with all of the configurations. You have to clone the repository, change to the terraform directory using terminal or cmd or gitbash and then run the commands\",1642731166.489700,1642809569.247500,U02T9550LTU\\na8cd5334-807e-43cc-9400-1c29771a360e,U02T941CTFY,,,I\\'m facing the same problem,1642765009.063500,1642809601.247700,U02TC704A3F\\n0e362460-8c4d-404a-a25e-6caecf878953,U02T941CTFY,,,\"Im running this code in my Bash in the directory\\n```docker run -it \\\\\\n\\xa0 -e POSTGRES_USER=\"\"root\"\" \\\\\\n\\xa0 -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n\\xa0 -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n\\xa0 -v D:\\\\de-zoomcamp-week-1-basics-n-setup\\\\ny_taxi_postgres_data \\\\\\n\\xa0 -p 5432:5432 \\\\\\n\\xa0 postgres:13```\",1642765009.063500,1642809647.247900,U02TC704A3F\\n8b733eab-2b61-45a7-91c7-508934ebd26e,,,,\"Regarding the question how to  persistent state for pgadmin in docker-compose,  I have found a stackoverflow discussion which really helped me to understand here is the link <https://stackoverflow.com/questions/57174830/pgadmin-creates-new-random-volume-with-each-docker-compose-upthat>. Basically, adding specific volume name fixed the issue for me.\",,1642810153.252600,U02DD97G6D6\\nC6220C55-8BC9-4D00-AF42-145BF0BD80BA,,14.0,,\"Really enjoying the Zooomcamp so far :slightly_smiling_face:\\n\\nAlthough I have put in waaay more than a few hours this first week. So much to take in! \",1642810460.254700,1642810460.254700,U02U34YJ8C8\\na6a4ce87-ecea-4a00-bea9-249243b23d60,U02UAFF1WU9,,,\"I had a similar issue, and resolved by using the container IP I was able to connect to the postgres in the container. I created a container with postgres and then did updates to it by adding to a network etc by stopping the container, updating its property and restarting and it worked for me.\",1642799868.223500,1642811114.255100,U02TTSXUV2B\\ne9aeeba1-3b7e-43f9-ba6a-773842b2a31f,U02T941CTFY,,,\"Its weird, no data in my computer exists in that folder after the snippet executes completely with no problem. What am I loosing here?\\n\\nI feel like it shouldn\\'t be in my computer anyway, just in the docker image, right?\",1642765009.063500,1642811232.255300,U02TC704A3F\\n1abd7cde-de67-4c06-9d4c-90f4a61ceea5,U02U34YJ8C8,,,\"Same here, definitely more hours than I had anticipated and enjoying it.. I need to block more time for the coming weeks!\",1642810460.254700,1642811375.255700,U02TTSXUV2B\\n063f4c5b-19c4-4c84-9a85-55572f5b5740,U02U34YJ8C8,,,\"I still in Class 1.2.2 and is taking way more than 2h, but I guess it is necessary\",1642810460.254700,1642812422.255900,U02TC704A3F\\n5a3de7c4-e735-4d38-b71f-958ea655d3c8,,,,\"Hello Team, If anyone of you is using the `zsh` console on `Linux distribution`, then you have to change it to `bash`, so `google_cloud_sdk` commands  will start working.\\nI was facing this problem, might help someone.\",,1642812866.259200,U0297ANJF6F\\na91a1af9-f155-465f-b7be-48bf7d87f837,,6.0,,\"$ winpty docker run -it \\\\\\n&gt;   -e POSTGRES_USER=\"\"root\"\" \\\\\\n&gt;   -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n&gt;   -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n&gt;   -v C:\\\\Users\\\\User1\\\\2_docker_sql\\\\ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n&gt;   -p 5432:5432 \\\\\\n&gt;  postgres:13\\ndocker: Error response from daemon: mkdir C:UsersUser12_docker_sqlny_taxi_postgres_data: Access is denied.\",1642813718.259500,1642813718.259500,U02VBG59VQ9\\n0a4cb6a1-747f-4900-a368-37442d25af10,U02U34YJ8C8,,,A few hours you say? I have spent at least 15 hours on this and my head is now dockerized! :smiley:,1642810460.254700,1642814407.259800,U02UX664K5E\\n700894af-d793-4b7c-9617-14c3c3efb4f1,,2.0,,\"After finishing the ingestion of `yellow_taxi_data` into Postgres from the second video, I found a slight difference in the number of rows..\\nOn running `SELECT COUNT(1) FROM yellow_taxi_data;`, the number of rows was 1369765, but it was 1369766 when we ran `wc -l yellow_tripdata_2021-01.csv`  initially..\\n\\nJust wanted to know if it\\'s something to worry about\",1642816668.265700,1642816668.265700,U02T9JQAX9N\\ndeabaf93-af39-45d1-97ad-99fd4ab1d97e,,2.0,,\"When I run docker-compose on Google Cloud VM, postgres changes the ownership of database folder (ny_taxi_postgres_data) to Systemd-coredump, and it creates permission issue.. I managed to get around the issue by adding this to ~/.bashrc\\n```export UID=$(id -u)\\nexport GID=$(id -g)```\\nand then modifying my `docker-compose.yaml` so that it includes `--user: \"\"${UID}:${GID}\"\"` for postgresql. Does anyone know why this happens and how I can run it in a “proper” way? (in other words, without the “getting around” method?)\",1642817173.267800,1642817173.267800,U02SE1AFBFE\\n93ec8e69-b437-4da9-92a0-758b12f190ef,U02T9JQAX9N,,,Might be because `wc -l data` is counting the header row (column names) while postgres isn’t.,1642816668.265700,1642817226.267900,U02SQ1X29GE\\n9b1687ce-c5cb-455e-92f9-db525ece8599,U02SE1AFBFE,,,Or is this the “proper” way to handle this issue??,1642817173.267800,1642817420.268300,U02SE1AFBFE\\nd5a519f2-dabc-4c95-a66b-dae529ad2fc6,U02T9JQAX9N,,,Yess.. this has to be it. Thanks,1642816668.265700,1642818557.268800,U02T9JQAX9N\\n74785809-16ED-47D9-8857-A1302BC1EB66,U02U34YJ8C8,,,My brain is fried. Did way too much today. Maybe going to take a break tomorrow. It\\'s cool to be learning this stuff though ,1642810460.254700,1642820835.270600,U02U34YJ8C8\\n3f6684bf-60f6-4440-8ab4-58c8804302d7,U02QQEQGTV2,,,For some reason the issue resolved itself and it works now. I am so confused.,1642805936.243500,1642825489.271200,U02QQEQGTV2\\nb0bbeba0-881a-4602-9158-41be60bedc76,U01QGQ8B9FT,,,\"Yes you were right!!\\n\\n`$ py -0p`\\n`Installed Pythons found by C:\\\\windows\\\\py.exe Launcher for Windows`\\n `-3.8-64        C:\\\\Program Files\\\\Python38\\\\python.exe *`\\n `-3.7-64        C:\\\\Users\\\\mistr\\\\Anaconda3\\\\python.exe`\",1642769991.086200,1642825663.271400,U01QGQ8B9FT\\nebab12a3-6b5e-4c6e-a113-011f2ca2f7b5,U01QGQ8B9FT,,,I\\'ll change the PATH in the env variables. Thank you so much!,1642769991.086200,1642825759.271600,U01QGQ8B9FT\\na7f332ae-b7ed-45cf-a684-f0ff02849db7,U02T0CYNNP2,,,\"Hi <@U02T0CYNNP2>, <@U02UY5FUCEL> I too am getting this exact same error. I\\'m on Ubuntu 20.04. When you actually go to the address that it tells you to in the error  &lt;(Background on this error at: <https://sqlalche.me/e/14/e3q8>) &gt; it seems to suggest that there is something going wrong with the database - it\\'s a DBAPI error - and for an operational error \\'related to the database\\'s operation and not necessarily under the control of the programmer\\'. Hmmm... doesn\\'t seem to tell me much except for that it *might* be out of my control - not terribly helpful . If I look further at the output it states that this error was the direct cause of the following exception: `Traceback (most recent call last):`\\n  `File \"\"/app/ingest_data.py\"\", line 68, in &lt;module&gt;`\\n    `main(args)`\\n  `File \"\"/app/ingest_data.py\"\", line 36, in main`\\n    `df.head(n=0).to_sql(name=table_name, con=engine, if_exists=\\'replace\\')`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/pandas/core/generic.py\"\", line 2872, in to_sql`\\n    `<http://sql.to|sql.to>_sql(`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/sql.py\"\", line 717, in to_sql`\\n    `<http://pandas_sql.to|pandas_sql.to>_sql(`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/sql.py\"\", line 1751, in to_sql`\\n    `table = self.prep_table(`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/sql.py\"\", line 1650, in prep_table`\\n    `table.create()`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/sql.py\"\", line 856, in create`\\n    `if self.exists():`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/sql.py\"\", line 840, in exists`\\n    `return self.pd_sql.has_table(self.name, self.schema)`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/sql.py\"\", line 1783, in has_table`\\n    `insp = sa.inspect(self.connectable)`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/inspection.py\"\", line 64, in inspect`\\n    `ret = reg(subject)`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/reflection.py\"\", line 182, in _engine_insp`\\n    `return Inspector._construct(Inspector._init_engine, bind)`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/reflection.py\"\", line 117, in _construct`\\n    `init(self, bind)`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/reflection.py\"\", line 128, in _init_engine`\\n    `engine.connect().close()`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py\"\", line 3204, in connect`\\n    `return self._connection_cls(self, close_with_result=close_with_result)`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py\"\", line 96, in __init__`\\n    `else engine.raw_connection()`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py\"\", line 3283, in raw_connection`\\n    `return self._wrap_pool_connect(self.pool.connect, _connection)`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py\"\", line 3253, in _wrap_pool_connect`\\n    `Connection._handle_dbapi_exception_noconnection(`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py\"\", line 2100, in _handle_dbapi_exception_noconnection`\\n    `util.raise_(`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py\"\", line 207, in raise_`\\n    `raise exception`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py\"\", line 3250, in _wrap_pool_connect`\\n    `return fn()`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py\"\", line 310, in connect`\\n    `return _ConnectionFairy._checkout(self)`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py\"\", line 868, in _checkout`\\n    `fairy = _ConnectionRecord.checkout(pool)`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py\"\", line 476, in checkout`\\n    `rec = pool._do_get()`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/impl.py\"\", line 146, in _do_get`\\n    `self._dec_overflow()`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py\"\", line 70, in __exit__`\\n    `compat.raise_(`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py\"\", line 207, in raise_`\\n    `raise exception`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/impl.py\"\", line 143, in _do_get`\\n    `return self._create_connection()`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py\"\", line 256, in _create_connection`\\n    `return _ConnectionRecord(self)`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py\"\", line 371, in __init__`\\n    `self.__connect()`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py\"\", line 666, in __connect`\\n    `pool.logger.debug(\"\"Error on connect(): %s\"\", e)`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py\"\", line 70, in __exit__`\\n    `compat.raise_(`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py\"\", line 207, in raise_`\\n    `raise exception`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py\"\", line 661, in __connect`\\n    `self.dbapi_connection = connection = pool._invoke_creator(self)`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/create.py\"\", line 590, in connect`\\n    `return dialect.connect(*cargs, **cparams)`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py\"\", line 597, in connect`\\n    `return self.dbapi.connect(*cargs, **cparams)`\\n  `File \"\"/usr/local/lib/python3.9/site-packages/psycopg2/__init__.py\"\", line 122, in connect`\\n    `conn = _connect(dsn, connection_factory=connection_factory, **kwasync)`  Not sure if this is similar to output others are getting? Does it mean it\\'s just having trouble connecting to the database for some reason? I did look on stackoverflow and found this <https://stackoverflow.com/questions/68510271/psycopg2-operationalerror-could-not-translate-host-name-joao-to-address-temp> but it isn\\'t specific to docker but it is the exact same error. Did anyone find out anything more on what this might mean or how you might solve it?\",1642786979.150600,1642828994.272400,U02U5SW982W\\n42ff93a3-57b2-484a-bccc-2fbb88cdce7b,U02T941CTFY,,,\"I have the same problem. There are no errors in the terminal, but the folder (ny_taxi_postgres_data) is empty.\\n\\nI use GitBash on Windows\",1642765009.063500,1642829437.272800,U02QM7H8G4U\\n6355f3a4-279a-46db-8d02-6c0d7c38a2d2,U02T941CTFY,,,I GOT IT,1642765009.063500,1642829572.273000,U02TC704A3F\\n34f0aed8-26db-48b1-b086-1e54d4997bce,U02T941CTFY,,,\"<https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/2_docker_sql/README.md>\\n\\nRight here.\\n\\nIn the -v parameter you have to add :/var/lib/postgresql/data after the path.\\n\\nThat\\'s the Bash command I used\\n\\nThis may help you <@U02T941CTFY> and <@U02QM7H8G4U>\\n\\n```docker run -it \\\\\\n  -e POSTGRES_USER=\"\"root\"\" \\\\\\n  -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n  -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n  -v D:/de-zoomcamp-week-1-basics-n-setup/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n  -p 5432:5432 \\\\\\n  postgres:13```\",1642765009.063500,1642829698.273200,U02TC704A3F\\n933f818d-6241-462b-9016-68bb1053e90f,U02T0CYNNP2,,,This does seem to be a somewhat temporary error - I tried running the docker command again and now it gives me the following error:,1642786979.150600,1642829716.273600,U02U5SW982W\\n1010bd1d-8266-47fa-b20d-3928625c6f42,U02T0CYNNP2,,,\"```sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not connect to server: Connection refused\\n\\tIs the server running on host \"\"pg-database\"\" (172.19.0.2) and accepting\\n\\tTCP/IP connections on port 5431?```\\n\",1642786979.150600,1642829758.273800,U02U5SW982W\\n8740e5ff-d793-4927-96f0-b19eda54f0ef,U02T941CTFY,,,\"This post in Stack Overflow may help anyone understand better <https://stackoverflow.com/questions/32269810/understanding-docker-v-command#:~:text=The%20%2Dv%20(or%20%2D%2Dvolume,it%20at%20the%20given%20location>.\",1642765009.063500,1642829799.274200,U02TC704A3F\\n8d5dd747-2230-46fe-840b-c41c4651fc2e,,14.0,,\"I get this error\\n```docker: invalid reference format.\\nSee \\'docker run --help\\'.\\nzsh: command not found: --name```\\nwhen i try to run this docker image\\n`docker run -it \\\\`\\n  `-e POSTGRES_USER=\\'root\\' \\\\`\\n  `-e POSTGRES_PASSWORD=\\'root\\' \\\\`\\n  `-e POSTGRES_DB=\\'ny_taxi\\' \\\\`\\n  `-v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\`\\n  `-p 5431:5432 \\\\`\\n  `--network=pg-network \\\\` \\n  `--name pg-database \\\\`\\n  `postgres:13`\",1642829823.274700,1642829823.274700,U02TZ1JCVEC\\nb73ec720-87bd-423f-87cb-fafff4b400ba,U02TZ1JCVEC,,,any help please,1642829823.274700,1642829961.275300,U02TZ1JCVEC\\nb02ceeed-a0fa-417b-bbab-62f182527fbb,U02TZ1JCVEC,,,\"try to check this:\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642800821224600?thread_ts=1642799868.223500&amp;cid=C01FABYF2RG|https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642800821224600?thread_ts=1642799868.223500&amp;cid=C01FABYF2RG>\",1642829823.274700,1642830036.275500,U02QP6JM83U\\ne5644666-b103-46fa-8039-9833f1223cbd,U02T0CYNNP2,,,Ok - found my problem - it has to be port 5432. Then it is happy. I get confused because I have had to use port 5431 elsewhere but in this case it obviously has to be 5432. But in terms of the error it does seem to be a temporary one. Anyone having any luck running their docker script now?,1642786979.150600,1642830218.275800,U02U5SW982W\\n6ad59810-feee-46ea-a24e-bba1b828c8b1,U02TZ1JCVEC,,,i think you need to add 4 spaces at start of last line,1642829823.274700,1642830218.276000,U02QP6JM83U\\n130f61f1-a4bd-4c41-8564-7d08c5d49733,U02U34YJ8C8,,,\"Hi <@U02U34YJ8C8>, <@U02TTSXUV2B>, <@U02TC704A3F>, <@U02UX664K5E> et al. - thanks for sharing your experience. I thought I was a bit \\'special\\' because I\\'ve spent a few hours every day since Monday just trying to get through the content of Week 1. I\\'m only up to video 1.2.5 :cry:. So another couple of hours there to go just in videos. It takes me a lot longer than the length of the time in videos just to go through them. I follow along with it, then have to rewind it a bit because I miss something and then of course I make some stupid typo along the way in my own code and spend the next of what seems an eternity trying to find my blunder. It is a fantastic course though :raised_hands: - just feeling that I\\'m going way too slow that\\'s all.\",1642810460.254700,1642831033.276400,U02U5SW982W\\ned7afbde-2da9-4b4c-9e59-7b33e3a64c58,U02TZ1JCVEC,,,\"okay, I would try that out now\",1642829823.274700,1642831126.277600,U02TZ1JCVEC\\n770c8474-9603-405b-b9b2-f07dd78e0c22,,8.0,,\"I\\'m trying login after created my database. I\\'m using this command:\\n```pgcli -h localhost -p 5432 -u root -d ny_taxi```\\nI\\'m using the right password but I receive *connection to server at \"\"localhost\"\" (::1), port 5432 failed: FATAL:  password authentication failed for user \"\"root\"\"*\\n\\nAny help on this?\",1642831177.278600,1642831177.278600,U02TC704A3F\\n162e8ee0-10cc-4d83-b956-d84738bb5904,U02TC704A3F,,,may be password is wrong?,1642831177.278600,1642831290.278800,U02QP6JM83U\\n346dced9-cfb6-40d7-8212-bf3150cd985a,U02T52AQB2R,,,\"Can You try different port? 5431:5432, may be?\",1642795056.206700,1642831304.279000,U0290EYCA7Q\\n6dae07da-ddb1-4c43-9433-99b0ca552413,U02TZ1JCVEC,,,<@U02QP6JM83U> it worked! thank you,1642829823.274700,1642831383.279200,U02TZ1JCVEC\\na3a84289-6819-4e4d-8857-e93ef047e008,U02TC704A3F,,,\"I\\'ve changed and tried out other passwords too. I\\'ve used \\'root\\', just like the tutorial, tried \\'123\\', \\'123123\\', \\'pass\\', \\'pwd\\'. Don\\'t know what\\'s happening, I\\'m not copy+pasting the password, just tipying\",1642831177.278600,1642831418.279400,U02TC704A3F\\n312e98a7-541a-457e-84a5-3b1dcecb29e9,U02TC704A3F,,,\"try this:\\n```pgcli -h localhost -U root -W 123 -d ny_taxi```\",1642831177.278600,1642831645.279800,U02QP6JM83U\\nb3330145-e6f0-4b50-a236-08fa9911a328,U02TC704A3F,,,where 123 - your password,1642831177.278600,1642831662.280300,U02QP6JM83U\\n331676b4-8e3a-4a62-90bf-07cebe9f9a66,U02TZ1JCVEC,,,<@U02QP6JM83U>,1642829823.274700,1642831719.280700,U02TZ1JCVEC\\ne4e3a099-2bac-4f6e-b37d-b76d32066534,U02TC704A3F,,,don`t forget to use strong password :male-detective:,1642831177.278600,1642831733.281100,U02QP6JM83U\\n2610570f-12e8-4ab6-93f4-40bb5ac983bd,U02TZ1JCVEC,,,containers are running?,1642829823.274700,1642831790.281300,U02QP6JM83U\\n01205d03-369b-4a1c-a256-27e9c2518b9f,U02TC704A3F,,,\"Thanks\\n\\nI tried out, new error now\\n\\n*connection to server at \"\"localhost\"\" (::1), port 5432 failed: fe_sendauth: no password supplied*\\n\\nI will google it for a moment\",1642831177.278600,1642831821.281500,U02TC704A3F\\n1f68dd28-32e6-4d63-9a06-b54eb2cfcc29,U02TZ1JCVEC,,,yes,1642829823.274700,1642831908.281700,U02TZ1JCVEC\\n9c44407d-97ed-45a7-a2fa-64e3570a7c00,U02TZ1JCVEC,,,may be need to set up network for containers,1642829823.274700,1642832089.282000,U02QP6JM83U\\ncfbb52e4-e3ca-4da9-b348-30c1a51af107,U02TZ1JCVEC,,,how,1642829823.274700,1642832233.282200,U02TZ1JCVEC\\nc7a6effc-2278-443b-97c7-076ef02ed95c,U02TC704A3F,,,\"if password is not set uped, may be this help:\\n```pgcli -h localhost -U root -d ny_taxi```\",1642831177.278600,1642832258.282400,U02QP6JM83U\\n7e5e0864-1253-40b0-b15d-33414bf35782,U02TZ1JCVEC,,,\"can you check course videos, please?\",1642829823.274700,1642832427.282700,U02QP6JM83U\\ndd790ba4-aa12-423a-8e6c-209092684ac9,U02TZ1JCVEC,,,\"would do, thnaks\",1642829823.274700,1642832462.282900,U02TZ1JCVEC\\nfe4de59a-d971-4ab1-a28e-b4e4d5da3110,U02TZ1JCVEC,,,\"another way - check link above or try to use search in slack, it`s really helpfull\",1642829823.274700,1642832536.283100,U02QP6JM83U\\n80a59c05-c255-4d6f-b36b-535cd7f298af,,1.0,,\"@Alexey Grigorev Sir which are the prerequisite for this data Engineering course and I watch your second video title \"\"Ingesting NY Taxi Data to Postgres\"\" I\\'m confused in compos.yml file.\",1642832929.283400,1642832929.283400,U02C1H2P3HQ\\nf6b2ab32-b039-4eb8-a1b8-88f98ec44b2c,U02TC704A3F,,,\"Ok, I tried a bunch of things and nothing worked.\\n\\nAnother password, no password and -w parameter, all this stuff. Looked into some Stack Overflow posts too and they are talking about changes in the .conf file that I do not fully understand yet.\\n\\nI will try more things tomorrow, is almost 4am here, hopefuly it will all work.\\n\\nThanks for the help <@U02QP6JM83U>\",1642831177.278600,1642833131.283500,U02TC704A3F\\n6fd83f07-6aa7-4c7e-a76b-41643dcca1d4,U02C1H2P3HQ,,,He explains docker compose in the video titled *DE Zoomcamp 1.2.5 - Running Postgres and pgAdmin with Docker-Compose.*,1642832929.283400,1642833730.283800,U0290EYCA7Q\\n73a5945d-fb9f-461a-ab3e-a416367aec49,,2.0,,\"Hello all\\nI’m getting this error when i try to run my pgadmin and docker\",1642833980.284800,1642833980.284800,U02QKMCV39R\\nb0d60d00-a4c3-45ef-869d-a905cb3ccdfb,,2.0,,,1642834005.284900,1642834005.284900,U02QKMCV39R\\n79a1585e-5150-4a6d-846d-abd108f57c73,U02TZ1JCVEC,,,<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642833730283800?thread_ts=1642832929.283400&amp;cid=C01FABYF2RG|https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642833730283800?thread_ts=1642832929.283400&amp;cid=C01FABYF2RG>,1642829823.274700,1642834176.285200,U02QP6JM83U\\n85dd397d-264d-44a6-9b6e-b7f9ba98dab3,U02QKMCV39R,,,\"please, try to use text, not screenshots, pity our eyes :slightly_smiling_face:\",1642833980.284800,1642834274.285500,U02QP6JM83U\\nd410c14f-7562-48cb-857b-04d2d86cc95f,U02QKMCV39R,,,\"to save code formatting you can use 3 symbols ` at start of codeblock, and another 3 at end of code block and looks like this:\\n```import pandas as pd```\",1642833980.284800,1642834373.285700,U02QP6JM83U\\nb1938cff-ae71-4054-8bf7-9c2b549df2ff,U02QKMCV39R,,,\"Give it a different name. Or stop the container running, and start it again.\\n\\n```bash-3.2$ docker container ls\\nCONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS          PORTS                           NAMES\\n156c96d8f6d2   dpage/pgadmin4   \"\"/entrypoint.sh\"\"         56 minutes ago   Up 56 minutes   443/tcp, 0.0.0.0:8080-&gt;80/tcp   pgadmin\\n8f1ebd2d9d05   postgres:13      \"\"docker-entrypoint.s…\"\"   58 minutes ago   Up 58 minutes   0.0.0.0:5432-&gt;5432/tcp          pg-database\\n\\ndocker container stop &lt;container-id&gt;```\",1642834005.284900,1642834602.285900,U0290EYCA7Q\\nf0b90a7a-49d6-4237-b237-aece0feafb2b,U02QKMCV39R,,,Thanks,1642834005.284900,1642834852.286200,U02QKMCV39R\\n4f40ea91-097d-43c4-90a3-920d86c5e1f8,U02SE1AFBFE,,,\"It\\'s a good way. But on the other hand, maybe you\\'re not supposed to look at this files, so maybe you don\\'t have to change the user\\n\\nBut as long as it works, it\\'s okay\",1642817173.267800,1642836915.287100,U01AXE0P5M3\\n855a828d-4c26-4e7c-bbef-05b3f6168bac,,4.0,,\"Ok so my Local_to_gcs_task has finished but its failing at big query with the below message.\\nIt looks like the trips_data_all table is missing. Do i need to create it or its auto created as part of the DAG? Did I miss anything again ?\\n\\n`File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/_http.py\"\", line 484, in api_request`\\n    `raise exceptions.from_http_response(response)`\\n`google.api_core.exceptions.NotFound: 404 POST <https://bigquery.googleapis.com/bigquery/v2/projects/dtc-de-course-338623/datasets/trips_data_all/tables?prettyPrint=false>: Not found: Dataset dtc-de-course-338623:trips_data_all`\",1643789176.129609,1643789176.129609,U02UY1QTGHW\\n5f34615b-88a1-42e0-a15e-8e29545ca9b9,,11.0,,\"I have an issue in the `Ingesting Data to GCP using Airflow` part.. My DAG run is failing at the `local_to_gcs_task` .. Looking at the log.. I see a line saying\\n```google.auth.exceptions.DefaultCredentialsError: File /.google/credentials/google_credentials.json was not found.```\\nBut I have the credentials file in the required place\",1643789830.773229,1643789830.773229,U02T9JQAX9N\\nfd7f24b8-82f1-484d-be19-ea19d44b6034,U02T9JQAX9N,,,Have I missed something along the line..,1643789830.773229,1643789852.314349,U02T9JQAX9N\\n31d3af09-5a2f-475c-b6ac-e5c030047e18,U02T9JQAX9N,,,Check the video I shared a few days ago in <#C02V1Q9CL8K|announcements-course-data-engineering>. It\\'ll show you what you should do,1643789830.773229,1643789995.677159,U01AXE0P5M3\\n283a70d5-6638-4c08-ad09-13b2f6072b59,U02T9JQAX9N,,,In your screenshot you show a file on your host file system. But the problem happens in the container. It\\'s likely that it can\\'t map one to another and you\\'ll need to use the absolute path in docker compose,1643789830.773229,1643790095.136949,U01AXE0P5M3\\n821f6423-bf05-497c-833e-64dc976f5a92,,23.0,,\"still struggling with how to use <@U01AXE0P5M3> \"\"ingest_script.py \"\" to load data into my gcp bucket? because the \"\"data_ingestion_gcs_dag.py\"\" seems to load only for one month as set\",1643790128.790069,1643790128.790069,U02RTJPV6TZ\\n998bcf19-a138-4814-baea-a03aca48d0ab,U02TATJKLHG,,,I think so. Big query just hides all the Internals from you unlike spark,1643781517.371389,1643790282.314569,U01AXE0P5M3\\n9fdd0bd1-e51f-466c-9fd0-6a891993a8f0,U02T697HNUD,,,\"Code - yes. Screenshots are not required, but you can add them as well\",1643786895.379309,1643790334.573719,U01AXE0P5M3\\n6c60db3f-a8e9-4076-b58f-b474a2b8459f,U02UY1QTGHW,,,\"Maybe you missed the terraform apply step? But you can remove this part, it\\'s not important right now\",1643789176.129609,1643790415.211989,U01AXE0P5M3\\n2e44e736-258e-444f-a80b-0c28b58426d2,U02T9JQAX9N,,,\"This is how it is in the docker-compose file\\n```/.google/credentials/:/.google/credentials:ro```\\nWhen you say absolute.. do you mean something like this:\\n```~/.google/credentials/:/.google/credentials:ro```\",1643789830.773229,1643790420.964149,U02T9JQAX9N\\nd983f04d-0f34-4bcf-a584-514f8ac82a20,U02T697HNUD,,,Alright thanks for the clarification Alexey :smile:,1643786895.379309,1643790530.370379,U02T697HNUD\\nbf384239-6e23-4a8d-ad01-e9faf2fc48ff,U02T9JQAX9N,,,Check the video please. I\\'m from my phone now and can\\'t type properly,1643789830.773229,1643790756.328249,U01AXE0P5M3\\n5981401d-b434-4fcf-83f7-c0fa177c1689,U02UY1QTGHW,,,\"here is the log file .... i\\'ll have a look at the terraform part\\n\\n`[2022-02-02 08:09:00,023] {taskinstance.py:1032} INFO - Dependencies all met for &lt;TaskInstance: data_ingestion_gcs_dag.bigquery_external_table_task scheduled__2022-02-01T00:00:00+00:00 [queued]&gt;`\\n`[2022-02-02 08:09:00,034] {taskinstance.py:1032} INFO - Dependencies all met for &lt;TaskInstance: data_ingestion_gcs_dag.bigquery_external_table_task scheduled__2022-02-01T00:00:00+00:00 [queued]&gt;`\\n`[2022-02-02 08:09:00,035] {taskinstance.py:1238} INFO -` \\n`--------------------------------------------------------------------------------`\\n`[2022-02-02 08:09:00,035] {taskinstance.py:1239} INFO - Starting attempt 1 of 2`\\n`[2022-02-02 08:09:00,036] {taskinstance.py:1240} INFO -` \\n`--------------------------------------------------------------------------------`\\n`[2022-02-02 08:09:00,049] {taskinstance.py:1259} INFO - Executing &lt;Task(BigQueryCreateExternalTableOperator): bigquery_external_table_task&gt; on 2022-02-01 00:00:00+00:00`\\n`[2022-02-02 08:09:00,054] {standard_task_runner.py:52} INFO - Started process 606 to run task`\\n`[2022-02-02 08:09:00,057] {standard_task_runner.py:76} INFO - Running: [\\'***\\', \\'tasks\\', \\'run\\', \\'data_ingestion_gcs_dag\\', \\'bigquery_external_table_task\\', \\'scheduled__2022-02-01T00:00:00+00:00\\', \\'--job-id\\', \\'24\\', \\'--raw\\', \\'--subdir\\', \\'DAGS_FOLDER/data_ingestion_gcs_dag.py\\', \\'--cfg-path\\', \\'/tmp/tmpmdeesb1a\\', \\'--error-file\\', \\'/tmp/tmpzfnp92t7\\']`\\n`[2022-02-02 08:09:00,059] {standard_task_runner.py:77} INFO - Job 24: Subtask bigquery_external_table_task`\\n`[2022-02-02 08:09:00,113] {logging_mixin.py:109} INFO - Running &lt;TaskInstance: data_ingestion_gcs_dag.bigquery_external_table_task scheduled__2022-02-01T00:00:00+00:00 [running]&gt; on host 6dcc5f5b8c58`\\n`[2022-02-02 08:09:00,195] {taskinstance.py:1426} INFO - Exporting the following env vars:`\\n`AIRFLOW_CTX_DAG_OWNER=***`\\n`AIRFLOW_CTX_DAG_ID=data_ingestion_gcs_dag`\\n`AIRFLOW_CTX_TASK_ID=bigquery_external_table_task`\\n`AIRFLOW_CTX_EXECUTION_DATE=2022-02-01T00:00:00+00:00`\\n`AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-02-01T00:00:00+00:00`\\n`[2022-02-02 08:09:00,197] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.`\\n`[2022-02-02 08:09:01,889] {taskinstance.py:1700} ERROR - Task failed with exception`\\n`Traceback (most recent call last):`\\n  `File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1329, in _run_raw_task    self._execute_task_with_callbacks(context)`\\n  `File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1455, in _execute_task_with_callbacks    result = self._execute_task(context, self.task)`\\n  `File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1511, in _execute_task    result = execute_callable(context=context)`\\n  `File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py\"\", line 1196, in execute    table_resource=self.table_resource,`\\n  `File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/common/hooks/base_google.py\"\", line 430, in inner_wrapper    return func(self, *args, **kwargs)`\\n  `File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/hooks/bigquery.py\"\", line 408, in create_empty_table    table=table, exists_ok=exists_ok, retry=retry`\\n  `File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py\"\", line 755, in create_table    timeout=timeout,`\\n  `File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py\"\", line 782, in _call_api    return call()`\\n  `File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/api_core/retry.py\"\", line 291, in retry_wrapped_func    on_error=on_error,`\\n  `File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/api_core/retry.py\"\", line 189, in retry_target    return target()`\\n  `File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/_http.py\"\", line 484, in api_request    raise exceptions.from_http_response(response)`\\n`google.api_core.exceptions.NotFound: 404 POST <https://bigquery.googleapis.com/bigquery/v2/projects/dtc-de-course-338623/datasets/trips_data_all/tables?prettyPrint=false>: Not found: Dataset dtc-de-course-338623:trips_data_all`\\n`[2022-02-02 08:09:01,904] {taskinstance.py:1277} INFO - Marking task as UP_FOR_RETRY. dag_id=data_ingestion_gcs_dag, task_id=bigquery_external_table_task, execution_date=20220201T000000, start_date=20220202T080900, end_date=20220202T080901`\\n`[2022-02-02 08:09:01,918] {standard_task_runner.py:92} ERROR - Failed to execute job 24 for task bigquery_external_table_task`\\n`Traceback (most recent call last):`\\n  `File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/task/task_runner/standard_task_runner.py\"\", line 85, in _start_by_fork    args.func(args, dag=self.dag)`\\n  `File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/cli_parser.py\"\", line 48, in command    return func(*args, **kwargs)`\\n  `File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/cli.py\"\", line 92, in wrapper    return f(*args, **kwargs)`\\n  `File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py\"\", line 298, in task_run    _run_task_by_selected_method(args, dag, ti)`\\n  `File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py\"\", line 107, in _run_task_by_selected_method    _run_raw_task(args, ti)`\\n  `File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py\"\", line 184, in _run_raw_task    error_file=args.error_file,`\\n  `File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py\"\", line 70, in wrapper    return func(*args, session=session, **kwargs)`\\n  `File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1329, in _run_raw_task    self._execute_task_with_callbacks(context)`\\n  `File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1455, in _execute_task_with_callbacks    result = self._execute_task(context, self.task)`\\n  `File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1511, in _execute_task    result = execute_callable(context=context)`\\n  `File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py\"\", line 1196, in execute    table_resource=self.table_resource,`\\n  `File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/common/hooks/base_google.py\"\", line 430, in inner_wrapper    return func(self, *args, **kwargs)`\\n  `File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/hooks/bigquery.py\"\", line 408, in create_empty_table    table=table, exists_ok=exists_ok, retry=retry`\\n  `File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py\"\", line 755, in create_table    timeout=timeout,`\\n  `File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py\"\", line 782, in _call_api    return call()`\\n  `File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/api_core/retry.py\"\", line 291, in retry_wrapped_func    on_error=on_error,`\\n  `File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/api_core/retry.py\"\", line 189, in retry_target    return target()`\\n  `File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/_http.py\"\", line 484, in api_request    raise exceptions.from_http_response(response)`\\n`google.api_core.exceptions.NotFound: 404 POST <https://bigquery.googleapis.com/bigquery/v2/projects/dtc-de-course-338623/datasets/trips_data_all/tables?prettyPrint=false>: Not found: Dataset dtc-de-course-338623:trips_data_all`\\n`[2022-02-02 08:09:01,962] {local_task_job.py:154} INFO - Task exited with return code 1`\\n`[2022-02-02 08:09:01,991] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check`\",1643789176.129609,1643791022.172559,U02UY1QTGHW\\ncc0e316e-955c-4598-9d93-52727e25f78e,U02T9JQAX9N,,,But yes that would be an absolute path. However docker compose seems to have problems with ~,1643789830.773229,1643791082.218369,U01AXE0P5M3\\n61f38a4e-4ad2-4105-a007-1645bd89a02d,U02T9JQAX9N,,,Since you\\'re on Ubuntu you can try $HOME,1643789830.773229,1643791128.598759,U01AXE0P5M3\\n33cf6936-2609-4016-a0e5-75d9c51d1519,U02T9JQAX9N,,,\"And if any this helps, can you please update the FAQ with your question?\",1643789830.773229,1643791153.433729,U01AXE0P5M3\\n359d0d23-372f-487d-bf0f-5092e08b4436,U02RTJPV6TZ,,,That\\'s what you need to figure out as a part of your homework =) but there were hints during office hours this week,1643790128.790069,1643793664.859129,U01AXE0P5M3\\n7e25493a-9acb-4ad0-821d-47120d2676fc,U02T9JQAX9N,,,\"I also had this problem. Check the volumes, i added absolute path in my VM - `/home/natalia/.google/credentials/:/.google/credentials:ro` and the path at variable  declaration `GOOGLE_APPLICATION_CREDENTIALS: /.google/credentials/google_credentials.json`\\n    `AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT: \\'google-cloud-platform://?extra__google_cloud_platform__key_path=/.google/credentials/google_credentials.json\\'`\",1643789830.773229,1643794153.731609,U021RS6DVUZ\\n3a1e8b7a-978d-4a31-9c7c-fcf39a9289f6,,3.0,,\"For Question 4 in week 3, I don\\'t seem to get the answer that\\'s mentioned in the options. Has anyone tried and found the same or were they able to find the answer within the options?\",1643795559.794799,1643795559.794799,U02TATJKLHG\\n49863a4e-691f-4735-a213-b517d39459f5,U02UY1QTGHW,,,\"<@U02UY1QTGHW> yep, had the same issue, check if you created a dataset called trips_data_all inside the project\",1643789176.129609,1643795572.829409,U02UJGGM7K6\\n7c52a2f7-6429-4e6e-8c55-185c0f27c01c,U02UY1QTGHW,,,More on this here: <https://stackoverflow.com/questions/51127361/bq-load-bigquery-error-in-load-operation-not-found-project>,1643789176.129609,1643795599.305179,U02UJGGM7K6\\n0ac709d3-2096-4b64-84b1-acc6539fd970,,7.0,,\"Hi everyone,\\n\\nDoes anyone get the error same as below when trying to run dag to fetch the taxi_zone file?\\n```*** Log file does not exist: /opt/airflow/logs/taxi_zone_dag/download_dataset_task/2022-02-02T09:39:17.124318+00:00/6.log\\n*** Fetching from: http://:8793/log/taxi_zone_dag/download_dataset_task/2022-02-02T09:39:17.124318+00:00/6.log\\n*** Failed to fetch log file from worker. Request URL missing either an \\'http://\\' or \\'https://\\' protocol.```\",1643795839.822289,1643795839.822289,U02BRPZKV6J\\nfe4e273a-e7cf-42a6-82aa-da4ce7b8f7dd,U02BRPZKV6J,,,\"Check -\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1643464059787079>\",1643795839.822289,1643795903.999719,U02TATJKLHG\\n73124075-90a1-4ea0-9587-cece90e560f6,U02BRPZKV6J,,,try restart docker-compose,1643795839.822289,1643796224.577939,U02RA8F3LQY\\n95C07D2B-02B7-4F38-AB75-D46CE152AE5A,U02BRPZKV6J,,,\"<@U02BRPZKV6J> Check your code properly. The log shows you were trying to fetch with wrong URL.\\n\\nThe URL in the log is &lt;http://:8793/log|http://:8793/log&gt;…… \\n\\nCheck you dag and update accordingly\",1643795839.822289,1643796768.279049,U02TBCXNZ60\\nc7c7854c-db0f-45ac-8180-719e58f54760,U02TATJKLHG,,,\"I think Big query can be directly compared to Apache Hive of Impala as a distributed SQL engine. Although Spark also has SQL API, in general, it goes beyond querying distributed stores.\",1643781517.371389,1643796900.096089,U02Q7JMT9P1\\n22bea88e-a90d-4948-b811-d530cf1ceab3,U02BRPZKV6J,,,<@U02TBCXNZ60> I don\\'t think this has something to do with the DAG. Tim Becker got the same issue. He mentioned that it could be around permissions or something. Best is to check that thread and see if anything works for you <@U02BRPZKV6J>,1643795839.822289,1643797088.465939,U02TATJKLHG\\n2c3fe0a9-feaa-4c62-911f-49db06da7791,U02TATJKLHG,,,\"Agreed, Spark is more than just a query processing engine. However, correct me if I am wrong, doesn\\'t Hive store the intermediate data in disk unlike Spark and Big Query?\",1643781517.371389,1643797215.584919,U02TATJKLHG\\n2510dd72-86d5-4b68-aa3e-789cba204f83,U02UKLHDWMQ,,,\"You can use a SSH public key wherever you configured the corresponding private key. These key pairs aren\\'t normally used in more than one server, for security reasons, but for the scope of this course I see no problem in that.\",1643762358.242459,1643797450.306229,U02GVGA5F9Q\\nbaf6e7b6-f7ec-4a3f-be8a-a9515325333c,U02BRPZKV6J,,,\"I tried this line of codes after looking at the thread from Tim Becker as recommended by <@U02TATJKLHG>.\\n\\nAfter that, I also removed a line in my code \"\"from datetime import date\"\". And then, I docker-compose up again and it works.\\n\\nThanks all for the help <@U02TATJKLHG>,<@U02RA8F3LQY>, <@U02TBCXNZ60>\\n```docker-compose down -v --rmi all --remove-orphans```\",1643795839.822289,1643797608.416079,U02BRPZKV6J\\n38ea62c4-5a42-4845-8450-a63eae861c55,U01AXE0P5M3,,,I have used pandas with sqlachemy and it has worked now,1642511128.453300,1643797956.035619,U02UR9XTSMV\\n62e345b8-b700-4171-83c8-59a171213185,U01AXE0P5M3,,,Thank you very much <@U01AXE0P5M3>,1642511128.453300,1643797966.041999,U02UR9XTSMV\\n9c1a8f0c-ac4a-48ba-9191-57cab24654ff,,3.0,,\"Hi\\nkeep getting this error:\\n```Error while reading table: zones, error message: Input file is not in Parquet format.```\\nbut i can see the file in my bucket:\",1643798254.764779,1643798254.764779,U029DM0GQHJ\\n3a3110c9-f693-482e-a5d1-7d811045ecdb,,3.0,,,1643798450.820729,1643798450.820729,U029DM0GQHJ\\ndf1060d3-55a2-40df-9530-0653f3aabd79,,,,for zones.parquet,,1643798549.150119,U029DM0GQHJ\\n7a441654-8736-4e0e-802c-e3b11a3d6d0d,U029DM0GQHJ,,,are you sure you converted it in parquet?,1643798254.764779,1643799887.312209,U01AXE0P5M3\\nb4d349cf-27f2-41e9-aad2-da7e6f7aaeec,U029DM0GQHJ,,,\"On the right hand column, you can see that the format of the file is actually `csv/txt` and not `parquet` . Did you actually convert it to parquet?\",1643798450.820729,1643799908.600009,U02TATJKLHG\\nfad7c7d0-7834-45e3-b0f2-9800af1059be,U02BRPZKV6J,,,Can you please add this to FAQ? <https://docs.google.com/document/d/19bnYs80DwuUimHM65UV3sylsCn2j1vziPOwzBwQrebw/edit?usp=sharing>,1643795839.822289,1643799942.179109,U01AXE0P5M3\\n79949d20-6e01-48a9-8506-9949cb62f9d8,U029DM0GQHJ,,,yes i did,1643798254.764779,1643800453.993279,U029DM0GQHJ\\nb4a069be-6282-411c-a5e3-b99d89a083e4,U029DM0GQHJ,,,\"because the screenshot you posted below says it\\'s csv. you can try opening it with a text editor. if you can see it, it\\'s csv\",1643798254.764779,1643800529.597729,U01AXE0P5M3\\n2c7da2b6-4f47-461b-9084-2bbd0c9b1354,U02RTJPV6TZ,,,\"I have an idea but code is failing, someone help me correct this code section\\n\\n dataset_url = \\'<https://s3.amazonaws.com/nyc-tlc/trip+data>\\'\\nurl_template = dataset_url + \\'/yellow_tripdata_{{ execution_date.strftime(\\\\\\'%Y-%m\\\\\\') }}.csv\\'\\noutput_file_template = path_to_local_home + \\'/src_file_{{ execution_date.strftime(\\\\\\'%Y-%m\\\\\\') }}.csv\\'\\nparquet_file = output_file_template.replace(\\'.csv\\', \\'.parquet\\')\\nBIGQUERY_DATASET = os.environ.get(\"\"BIGQUERY_DATASET\"\", \\'trips_data_all\\')\\n\\n\\n\\n\\ndef format_to_parquet(src_file):\\n\\xa0 \\xa0 if not src_file_{{ execution_date.strftime(\\\\\\'%Y-%m\\\\\\') }}.endswith(\\'.csv\\'):\\n\\xa0 \\xa0 \\xa0 \\xa0 logging.error(\"\"Can only accept source files in CSV format, for the moment\"\")\\n\\xa0 \\xa0 \\xa0 \\xa0 return\\n\\xa0 \\xa0 table = pv.read_csv(src_file_{{ execution_date.strftime(\\\\\\'%Y-%m\\\\\\') }})\\n\\xa0 \\xa0 pq.write_table(table, src_file_{{ execution_date.strftime(\\\\\\'%Y-%m\\\\\\') }}.replace(\\'.csv\\', \\'.parquet\\'))\",1643790128.790069,1643801278.784469,U02RTJPV6TZ\\n5c97dadf-0599-4957-a8ff-f67b4ca33de7,U02RTJPV6TZ,,,,1643790128.790069,1643801386.887829,U02RTJPV6TZ\\neb560149-e5a4-4f02-b2fb-b0112b7b7108,U02RTJPV6TZ,,,you need to put quotes around strings,1643790128.790069,1643801423.020549,U01AXE0P5M3\\nb6334098-be76-4f06-9fd9-fc6e09151ff1,U02RTJPV6TZ,,,sorry:pray: wish you would help me with an example in this code av pasted,1643790128.790069,1643801585.153749,U02RTJPV6TZ\\n55d25873-7df6-49ac-b1ef-af8d7b0fdc35,U02RTJPV6TZ,,,I would leave the format_to_parquet function untouched - you don\\'t need jinja templates there,1643790128.790069,1643801810.571299,U01AXE0P5M3\\nb0e785b7-37ac-4d64-a487-2853ae3a93d7,U02RTJPV6TZ,,,ok thnx lemme try that,1643790128.790069,1643801847.480249,U02RTJPV6TZ\\n76e6c447-2fa0-418a-9c43-3556afb3368d,U02RTJPV6TZ,,,\"if i leave out the jinja templates, my main challenge is how to link this\\noutput_file_template = path_to_local_home + \\'/src_file_{{ execution_date.strftime(\\\\\\'%Y-%m\\\\\\') }}.csv\\'\\n\\nto this bellow\\n\\ndef format_to_parquet(src_file):\\n\\xa0 \\xa0 if not src_file.endswith(\\'.csv\\'):\\n\\xa0 \\xa0 \\xa0 \\xa0 logging.error(\"\"Can only accept source files in CSV format, for the moment\"\")\\n\\xa0 \\xa0 \\xa0 \\xa0 return\\n\\xa0 \\xa0 table = pv.read_csv(src_file)\\n\\xa0 \\xa0 pq.write_table(table, src_file.replace(\\'.csv\\', \\'.parquet\\'))\\n\\nso that the \"\"src_file\"\" doesnt complain\",1643790128.790069,1643802481.790519,U02RTJPV6TZ\\n60746642-f14e-4dc2-a73b-963d952cd501,U02T9JQAX9N,,,\"Yeah it worked.. I just got `success`..\\n<@U01AXE0P5M3> When you mean video, do you mean the `Optional-Lightweight Local Setup for Airflow`\",1643789830.773229,1643802813.546749,U02T9JQAX9N\\n81b02148-d597-41f4-b0a1-52e49eb78f01,U029DM0GQHJ,,,i think it has something to do with your format_to_parquet code,1643798450.820729,1643802930.376049,U02RA8F3LQY\\nefede5d1-5c00-4fd2-a8f3-973d78d994d1,U029DM0GQHJ,,,\"i am using this piece of code only:\\n\\n```def format_to_parquet(src_file):\\n    if not src_file.endswith(\\'.csv\\'):\\n        logging.error(\"\"Can only accept source files in CSV format, for the moment\"\")\\n        return\\n    table = pv.read_csv(src_file)\\n    pq.write_table(table, src_file.replace(\\'.csv\\', \\'.parquet\\'))```\",1643798450.820729,1643803041.244329,U029DM0GQHJ\\ncf3b23d7-23a7-4c5a-978b-2f74bbd44c70,U02T9JQAX9N,,,\"Yes, in the beginning I show what you need to change in the compose file- before stripping down the components to make it lightweight\",1643789830.773229,1643803603.260039,U01AXE0P5M3\\n59474921-50aa-461e-b6f2-14c64f72220c,U02RTJPV6TZ,,,You now can pass these things as params to this function,1643790128.790069,1643803685.560569,U01AXE0P5M3\\n5bcf5f46-433d-467c-accc-bcda510678ba,U02RTJPV6TZ,,,Like we did in the video about local ingest,1643790128.790069,1643803701.423639,U01AXE0P5M3\\ne8ee5afd-e5ee-4f4a-a91a-33ee8cf8b489,U02SXQ9L0FJ,,,It resolved into a different issue...thank you for the help,1643761778.793289,1643805221.457529,U02SXQ9L0FJ\\nc5c4d0ed-998a-4fe6-ba96-dae639235cd3,U02RTJPV6TZ,,,\"```AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-02-01T00:00:00+00:00\\n[2022-02-02, 12:29:46 UTC] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.\\n[2022-02-02, 12:29:46 UTC] {taskinstance.py:1700} ERROR - Task failed with exception\\n File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/_http.py\"\", line 484, in api_request\\n    raise exceptions.from_http_response(response)\\ngoogle.api_core.exceptions.BadRequest: 400 POST <https://bigquery.googleapis.com/bigquery/v2/projects/data-zoomcamp-338514/datasets/trips_data_all/tables?prettyPrint=false>: Error while reading table: external_table, error message: Failed to expand table external_table with file pattern <gs://dtc_data_lake_data-zoomcamp-338514/raw//opt/airflow/src_file_2022-02.parquet>: matched no files.\\n[2022-02-02, 12:29:46 UTC] {taskinstance.py:1277} INFO - Marking task as UP_FOR_RETRY. dag_id=data_ingestion_gcs_dag, task_id=bigquery_external_table_task, execution_date=20220201T000000, start_date=20220202T122945, end_date=20220202T122946\\n[2022-02-02, 12:29:46 UTC] {standard_task_runner.py:92} ERROR - Failed to execute job 114 for task bigquery_external_table_task\\nTraceback (most recent call last):\\n    raise exceptions.from_http_response(response)\\ngoogle.api_core.exceptions.BadRequest: 400 POST <https://bigquery.googleapis.com/bigquery/v2/projects/data-zoomcamp-338514/datasets/trips_data_all/tables?prettyPrint=false>: Error while reading table: external_table, error message: Failed to expand table external_table with file pattern <gs://dtc_data_lake_data-zoomcamp-338514/raw//opt/airflow/src_file_2022-02.parquet>: matched no files.\\n[2022-02-02, 12:29:46 UTC] {local_task_job.py:154} INFO - Task exited with return code 1\\n[2022-02-02, 12:29:46 UTC] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check```\\n*failing at the last stage*\",1643790128.790069,1643805673.713459,U02RTJPV6TZ\\nCE7579C4-A068-4EB7-BBF4-42F5D54BB63D,U02U5L97S6T,,,\"The setting has to be done within dag file . The setting which Alexey used is similar to the one used in cron jobs for Linux .\\n\",1643777598.302299,1643805894.997109,U02AGF1S0TY\\nc128f1e3-14bd-48c9-964e-00d42a1895ec,,4.0,,\"When attempting to mount the docker files (from week 1) I typed everything out and received the following issue:\\n\\ndocker run -it\\\\-e POSTGRES_USER=\"\"root\"\"\\\\-e POSTGRES_PASSWORD=\"\"root\"\"\\\\-e POSTGRES_DB=\"\"ny_taxi\"\"\\\\-v \"\"ny_taxi_postgres_&lt;data://c&gt;\\\\users\\\\primaryuser\\\\desktop\\\\datacampwk1\\\\ny_taxi_postgres_data\"\"\\\\-p \"\"5432:5432\"\"\\\\postgres:13\\nunknown shorthand flag: \\'\\\\\\\\\\' in -\\\\-e\\nSee \\'docker run --help\\'.\\n\\n***Note: I used //c instead of c: at the suggestion of another user here who it worked for\\n\\n\\nWhen trying to alleviate the above issue I tried again by removing the \\\\ between -it and -e. This got me the following message:\\n\\nocker run -it -e POSTGRES_USER=\"\"root\"\"\\\\-e POSTGRES_PASSWORD=\"\"root\"\"\\\\-e POSTGRES_DB=\"\"ny_taxi\"\"\\\\-v \"\"ny_taxi_postgres_&lt;data://c&gt;\\\\users\\\\primaryuser\\\\desktop\\\\datacampwk1\\\\ny_taxi_postgres_data\"\"\\\\-p \"\"5432:5432\"\"\\\\postgres:13\\ndocker: invalid reference format: repository name must be lowercase.\\nSee \\'docker run --help\\'.\\n\\nLooking up the repository lowercase issue has not been helpful. If anyone has come across these issues then I would appreciate some insight on how to solve this, or if you know where I have gone wrong.\\n\\nThank you\\n\\n<@U01AXE0P5M3>\",1643806211.418879,1643806211.418879,U02SXQ9L0FJ\\n7004fd3a-9601-4137-91fc-5775b661da5d,U02QZN0LSBT,,,\"Thanks <@U01AXE0P5M3> — and <@U02QZN0LSBT> <@U02BVP1QTQF> thanks for these notes, very helpful for me especially catching up on week 2 materials.\",1642548032.081900,1643806803.801079,U02U5DPET47\\n3d228b81-ec76-481a-8abb-fd58a189e069,,15.0,,\"having some trouble when I\\'m running `docker-compose up`   from DE Zoomcamp 2.3.1 video at around 14:40\\ncode in reply\",1643807951.898489,1643807951.898489,U02RREQ7MHU\\n471c307f-0727-457d-862c-51a107e45bf4,U02RREQ7MHU,,,\"```airflow-worker_1     | [tasks]\\nairflow-worker_1     |   . airflow.executors.celery_executor.execute_command\\nairflow-worker_1     |\\nairflow-webserver_1  | [2022-02-02 13:01:33 +0000] [41] [INFO] Starting gunicorn 20.1.0\\nairflow-webserver_1  | [2022-02-02 13:01:33 +0000] [41] [INFO] Listening at: <http://0.0.0.0:8080> (41)\\nairflow-webserver_1  | [2022-02-02 13:01:33 +0000] [41] [INFO] Using worker: sync\\nairflow-webserver_1  | [2022-02-02 13:01:33 +0000] [44] [INFO] Booting worker with pid: 44\\nairflow-webserver_1  | [2022-02-02 13:01:33 +0000] [45] [INFO] Booting worker with pid: 45\\nairflow-webserver_1  | [2022-02-02 13:01:33 +0000] [46] [INFO] Booting worker with pid: 46\\nairflow-webserver_1  | [2022-02-02 13:01:33 +0000] [47] [INFO] Booting worker with pid: 47\\nairflow-worker_1     | [2022-02-02 13:01:34,548: INFO/MainProcess] Connected to <redis://redis:6379/0>\\nairflow-worker_1     | [2022-02-02 13:01:34,562: INFO/MainProcess] mingle: searching for neighbors\\nairflow-worker_1     | [2022-02-02 13:01:35,585: INFO/MainProcess] mingle: all alone\\nairflow-worker_1     | [2022-02-02 13:01:35,599: INFO/MainProcess] celery@cfae4b9e006b ready.\\nairflow-webserver_1  | [2022-02-02 13:01:36,488] {manager.py:512} WARNING - Refused to delete permission view, assoc with role exists DAG Runs.can_create Admin\\nairflow-webserver_1  | [2022-02-02 13:01:36,507] {manager.py:512} WARNING - Refused to delete permission view, assoc with role exists DAG Runs.can_create Admin\\nairflow-webserver_1  | [2022-02-02 13:01:36,528] {manager.py:512} WARNING - Refused to delete permission view, assoc with role exists DAG Runs.can_create Admin\\nairflow-webserver_1  | [2022-02-02 13:01:36,542] {manager.py:512} WARNING - Refused to delete permission view, assoc with role exists DAG Runs.can_create Admin\\nairflow-worker_1     | [2022-02-02 13:01:39,401: INFO/MainProcess] Events of group {task} enabled by remote.\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:01:42 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:01:52 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:02:02 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:02:12 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:02:22 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nairflow-triggerer_1  | [2022-02-02 13:02:28,645] {triggerer_job.py:251} INFO - 0 triggers currently running\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:02:32 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:02:42 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:02:52 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:03:02 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:03:13 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:03:23 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nairflow-triggerer_1  | [2022-02-02 13:03:28,699] {triggerer_job.py:251} INFO - 0 triggers currently running\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:03:33 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:03:43 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:03:53 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:04:03 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:04:13 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:04:23 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nairflow-triggerer_1  | [2022-02-02 13:04:28,755] {triggerer_job.py:251} INFO - 0 triggers currently running\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:04:33 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:04:44 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:04:54 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:05:04 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:05:14 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:05:24 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nairflow-triggerer_1  | [2022-02-02 13:05:28,806] {triggerer_job.py:251} INFO - 0 triggers currently running\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:05:34 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:05:44 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:05:54 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nredis_1              | 1:M 02 Feb 2022 13:05:56.723 * 100 changes in 300 seconds. Saving...\\nredis_1              | 1:M 02 Feb 2022 13:05:56.724 * Background saving started by pid 489\\nredis_1              | 489:C 02 Feb 2022 13:05:56.728 * DB saved on disk\\nredis_1              | 489:C 02 Feb 2022 13:05:56.729 * RDB: 0 MB of memory used by copy-on-write\\nredis_1              | 1:M 02 Feb 2022 13:05:56.825 * Background saving terminated with success\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:06:04 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:06:14 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nairflow-triggerer_1  | [2022-02-02 13:06:22,023] {triggerer_job.py:345} ERROR - Triggerer\\'s async thread was blocked for 0.47 seconds, likely by a badly-written trigger. Set PYTHONASYNCIODEBUG=1 to get more information on overrunning coroutines.\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:06:25 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nairflow-triggerer_1  | [2022-02-02 13:06:29,030] {triggerer_job.py:251} INFO - 0 triggers currently running\\nairflow-scheduler_1  | [2022-02-02 13:06:30,954] {scheduler_job.py:1114} INFO - Resetting orphaned tasks for active dag runs\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:06:35 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"\\nairflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:13:06:45 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"```\",1643807951.898489,1643808012.434989,U02RREQ7MHU\\nfbbed1e0-8179-4a77-9a3b-17925d448671,U02TATJKLHG,,,Hey <@U01DFQ82AK1> sorry for tagging you but I tried a lot of things and don\\'t seem to get an answer that matches the option.,1643795559.794799,1643808016.169609,U02TATJKLHG\\n70eccd44-91fe-4ebf-b1f1-89612b058149,U02SXQ9L0FJ,,,\"play with spaces, might be why you\\'re getting an error messages\",1643806211.418879,1643808671.970209,U02U2Q5P61Z\\n97bfeb25-2642-47be-8c44-9b6596c992d6,,2.0,,\"OMG, this is such a great course so far! Can\\'t wait to study MLOps from DataTalks team after this one:muscle:\",1643809045.426009,1643809045.426009,U02U5L97S6T\\nfeeb32e0-280a-4593-8695-5b05843bab0d,U02U5L97S6T,,,\"Yep, same. looking forward definitely since housekeeping a machine learning model are the things that still complicated in my company :sweat_smile:\",1643809045.426009,1643809291.058849,U02V4FG07HU\\n42aca607-0e4f-48b5-b008-e51ace603cbd,,8.0,,\"Just need to clarify one thing, If we have modified the DAG file or created a new DAG do we need to do `docker-compose airflow init` and create new docker containers everytime ?\",1643809369.994519,1643809369.994519,U02RW07CVTJ\\n0d1d14be-01ef-4109-b172-fa6a7df7445a,U02RTJPV6TZ,,,\"Don\\'t think i will make it past this last task, its still  failing at \"\"bigquery task\"\"\",1643790128.790069,1643809903.854689,U02RTJPV6TZ\\n6ecc8cb0-eff4-44b8-8022-fb5fd94df0b5,U02RW07CVTJ,,,\"Hi <@U02RW07CVTJ>, No, any changes made on the DAG file are automatically picked without the need of re-running `docker-compose airflow init`\",1643809369.994519,1643810938.579179,U02SEH4PPQB\\n9378adfa-c66f-407d-ba87-d037ad834d99,U02RW07CVTJ,,,I only run `docker-compose airflow init` at the first time. After that I just run `docker-compose up` command to start the container,1643809369.994519,1643811385.037979,U02T697HNUD\\nB99FD57A-E01E-48BF-B0B8-6640A0BD9E41,U02UNQNMH7B,,,<@U02UNQNMH7B> How were you able to solve the troubles? I\\'m still having lots of setup issues on Mac and have spent most of my time trying to troubleshoot :woozy_face:,1643733210.051579,1643812687.760489,U02BTB7H2Q3\\n7460004c-5009-444f-b50a-6f86fdbe6bec,U02UNQNMH7B,,,\"yeah same\\nso for the postgres-is-already-installed i stopped my local postgres and restarted the computer … that somehow did the trick\\nfor the python script i replaced wget with curl (also smaller case -o for mac)\",1643733210.051579,1643812779.967709,U02UNQNMH7B\\n287f21d9-9c99-4d0a-9af5-98ed4e919dc1,U02UNQNMH7B,,,but there was a lot more trouble shooting involved so maybe if you drop me your specific errors .. i might have seen them :slightly_smiling_face:,1643733210.051579,1643812825.308849,U02UNQNMH7B\\n6092C5C2-AB0D-4FEA-A211-C5BA6B212206,U02UNQNMH7B,,,\"I\\'m going to try restarting my computer now :sweat_smile: I also had Postgres already installed and pgAdmin, and had a lot of issues. I can\\'t get past the ingest_data.py script. I\\'ll try replacing wget and the small -o like you suggested and see if that helps. I\\'m trying not to give up, but this setup is so frustrating \",1643733210.051579,1643813256.258639,U02BTB7H2Q3\\n69918857-34a0-44fb-b6fd-45323471994e,U02RW07CVTJ,,,\"for me it didn’t work, i had to restart docker. Airflow didn’t see new files with dags…\",1643809369.994519,1643813537.080229,U021RS6DVUZ\\n492F29A3-6346-496C-941A-747D9877A28B,U02UNQNMH7B,,,I think I\\'ll just completely restart from scratch :upside_down_face:,1643733210.051579,1643813614.800389,U02BTB7H2Q3\\n19029d08-eba6-476f-bd61-b55840f1dd8d,U02UNQNMH7B,,,\"yeah it was a real pain\\nin the homework we need to ingest another table as well and i just did that in a notebook\",1643733210.051579,1643813770.503009,U02UNQNMH7B\\n3DA53750-8CE8-409C-A9F7-88E1533F2C0A,U02RREQ7MHU,,,It says running on 8080 ..confirm by docker ps as well ..what is the problem are you facing ? ,1643807951.898489,1643813781.164909,U02AGF1S0TY\\n234f89f8-ef29-4e1f-b563-b6203f0be759,U02UNQNMH7B,,,because there you see if it worked .. would me my suggestion … don’t do it perfect .. just get it done somehow :slightly_smiling_face: elegance can come later :slightly_smiling_face:,1643733210.051579,1643813811.171509,U02UNQNMH7B\\nee40000e-4b43-4902-adcf-cd852e34729b,U02RW07CVTJ,,,\"<@U021RS6DVUZ> did you run docker-compose on the same folder where your volumen were mapped?\\n\\nYou have to run docker-compose on the same folder, due to docker-compose file\\n\\n```# local_folder:container_folder\\n./dags:/opt/airflow/dags```\\n\",1643809369.994519,1643813850.454869,U02CNNC070C\\n4f052447-225c-4f83-ac78-37e976212ee9,U02RW07CVTJ,,,\"also, you must set AIRFLOW_UID, try list files from .dag folder with sudo\\nsudo ls dags\",1643809369.994519,1643813901.886629,U02CNNC070C\\n2220530e-a868-409c-bf33-672871538493,U02RW07CVTJ,,,\"My VM is disconnected now, but i will check it. Basically if i make changes in files with running dags i could see them. But if i add new file to .dag folder, airflow didn’t scan it. So i had to restart containers\",1643809369.994519,1643814220.399899,U021RS6DVUZ\\n5b76131e-a8f3-4a7f-9610-b40ba512ad00,U02TEERF0DA,,,\"Ah, I did not check there. Thanks :slightly_smiling_face: and I take a look!\",1643766679.185449,1643814560.444919,U02TEERF0DA\\nc1106297-b960-41ce-999d-ffb1317e10da,,2.0,,\"Hi <@U01B6TH1LRL>! Firstly, thank your for your contribution in this course!!! :slightly_smiling_face:\\n\\nA couple of quick questions.\\n1. I want to use Docker for dbt, should we use docker-core or is docker-bigquery enough (I guess the same goes for pip install)?\\n2. Since I\\'m in GCP, would it be better to create a profile.yaml wich includes the following (screenshot)?\\nMany thanks!\",1643814997.502659,1643814997.502659,U02UMV78PL0\\n267bbb0a-669e-4003-97c6-4dd776c1b17e,U02TEERF0DA,,,\"```❯ cat .env\\n   1 COMPOSE_PROJECT_NAME=dtc-de\\n   2 GOOGLE_APPLICATION_CREDENTIALS=/.google/credentials/google_credentials.json\\n   3 AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT=google-cloud-platform://?extra__google_cloud_platform__key_path=/.google/credentials/google_credentials.json\\n   4 AIRFLOW_UID=502\\n   ...```\\nAh it’s the `COMPOSE_PROJECT_NAME` thanks!\",1643766679.185449,1643815065.494979,U02TEERF0DA\\n95ab3a51-2b62-4ebe-8533-dd1d0dfe25bd,U02RW07CVTJ,,,\"maybe the AIRFLOW_UID that is being used in container is not the same as in your env.\\n\\nYou must create .env file with AIRFLOW_UID user.\\n\\nBut, I don\\'t know if it is just the reason.\",1643809369.994519,1643816118.452739,U02CNNC070C\\n1d6f4ed3-6528-41f7-99a5-ad46fcfeff11,U02RREQ7MHU,,,<@U02AGF1S0TY> it says everything is running but i cant connect to 0.0.0.0:8080,1643807951.898489,1643816655.599499,U02RREQ7MHU\\n0cfde64c-b98a-4010-9283-5537a4f6888d,U02RREQ7MHU,,,,1643807951.898489,1643816665.280709,U02RREQ7MHU\\n9efbf7bd-e94e-496a-bbce-f4a0520a8ecc,,23.0,,\"Hi! I\\'m currently in DE Zoomcamp 2.3.1 - Setup Airflow Environment with Docker-Compose\\nAlthough the google_credentials.json file is in the airflow worker:\\ndefault@5a9307a248b8:/opt/airflow$ ls -lh /.google/credentials/\\ntotal 2.5K\\n-rwxr-xr-x 1 default root 2.3K Jan 26 15:19 google_credentials.json\\n\\nThe DAG stops at the task local_to_gcs_task with the error:\\n```google.auth.exceptions.DefaultCredentialsError: File C:/Users/Gustavo/.google/credentials/google_credentials.json was not found.```\\n\",1643816806.173399,1643816806.173399,U02R09ZR6FQ\\nfa57f7c1-bc0e-4cfe-b7c4-87f78c51555e,U02R09ZR6FQ,,,Any idea of what might be the problem?,1643816806.173399,1643817010.633319,U02R09ZR6FQ\\nffbd8b54-b629-42b2-a114-a0c9420b3b85,U02R09ZR6FQ,,,it seems it\\'s pointing at your local folder. check the paths in your docker-compose.yaml,1643816806.173399,1643817219.388259,U02UMV78PL0\\n5ad50dfc-25eb-4252-8b48-856fadec6e74,U02R09ZR6FQ,,,\"I checked it now and the path is correct, it is this one that appears in the error and the file is there in the folder..:sweat_smile:\",1643816806.173399,1643817593.283549,U02R09ZR6FQ\\nd6fcc5ef-5b78-4fbb-8e7d-2a11186e8d87,U02R09ZR6FQ,,,\"There are basically two places you\\'ll have to configure the path -\\n1. Set the `GOOGLE_APPLICATION_CREDENTIALS` like this `GOOGLE_APPLICATION_CREDENTIALS: /.google/credentials/google_credentials.json`\\n2. Then mount the volume like this `~/.google/credentials/:/.google/credentials:ro` \\nCan you check if both these places are setup correctly?\",1643816806.173399,1643817845.383809,U02TATJKLHG\\n5a8e64b9-af3e-4375-afee-96da38cc8694,U02R09ZR6FQ,,,\"and just in case\\n```AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT```\",1643816806.173399,1643817944.391589,U02UMV78PL0\\nfe1af20f-db97-4b4c-aea1-2fbb3b336d9c,U02R09ZR6FQ,,,\"Yes, those three places are setup correctly, even with the local path to make sure there is no error\",1643816806.173399,1643818078.045339,U02R09ZR6FQ\\nC59CD205-286E-4145-85A4-674A138CE966,U02RREQ7MHU,,,\"Oh ok ..Did u try localhost:8080 ? May be on a different browser ..\\n\",1643807951.898489,1643818089.380199,U02AGF1S0TY\\n535f9922-e260-45e2-ab7b-25cd2c375358,U02R09ZR6FQ,,,I will rerun docker-compose from the beginning again,1643816806.173399,1643818123.884629,U02R09ZR6FQ\\nd8b486ba-06b7-4934-a87e-a8621e4d0cb0,U02R09ZR6FQ,,,\"also check the first few minutes of the video I recently published in <#C02V1Q9CL8K|announcements-course-data-engineering>. I show what you need to change to run it with docker-compose\\n\\nyou can skip the rest of the video where I make airlfow light weight\",1643816806.173399,1643818235.960849,U01AXE0P5M3\\n2d313fab-b880-40b6-93db-a08b1de3933f,U02R09ZR6FQ,,,TLDR - on windows (and on linux as well) it\\'s better to use absolute path and avoid ~,1643816806.173399,1643818275.008889,U01AXE0P5M3\\ncb6fb3aa-25a6-41f7-a365-bcf1d45fe1b6,U02U5L97S6T,,,\"thank you, glad to hear it :slightly_smiling_face:\",1643809045.426009,1643818302.131149,U01AXE0P5M3\\n19ed8838-eecc-4716-967f-c1133026875c,U02RREQ7MHU,,,\"ya that works but it says \"\"Invalid login. Please try again.\"\"\",1643807951.898489,1643818313.570239,U02RREQ7MHU\\ne0dd1ee1-7f09-40a0-a2d4-36fba67a0d4c,U02RREQ7MHU,,,user - airflow and password - airflow,1643807951.898489,1643818333.942929,U02RREQ7MHU\\n40E50D23-4D08-49B1-8B59-F9C771AE4904,U02RREQ7MHU,,,Yes un and pwd is right ,1643807951.898489,1643818394.577889,U02AGF1S0TY\\nbd5ef584-cbe3-46eb-b980-2936a9028c56,U02RREQ7MHU,,,should i maybe try and change the username and password?,1643807951.898489,1643818496.034049,U02RREQ7MHU\\n0EE46F1F-6FB9-42CE-B612-BF8520D8D075,U02RREQ7MHU,,,\"Not sure ..un and pwd should work ..\\nMay be stop everything and rebuild ..do docker-compose down —volumes —rmi all \\nThis will remove everything but then u need to build once again \",1643807951.898489,1643818527.492549,U02AGF1S0TY\\n9496c980-affd-43ac-bc9a-6b886ecea882,U02RREQ7MHU,,,ok will try this now,1643807951.898489,1643818635.616359,U02RREQ7MHU\\nc8ecb211-b3e3-467e-9401-6bbd8fab7368,U01AXE0P5M3,,,\":wave: Hi all - Am just starting week2 work today — hoping to catch up!  Working on OSX with M1, week 1 went quite smoothly, but wanted to get advice from others — best continue on my local M1 laptop or set up a VM?  :thinking_face:\\n…  <@U01AXE0P5M3> <@U030G7PJ6LQ> <@U02TBKWL7DJ> see each of you has posted on related items so flagging you, but appreciate hearing from anyone who has an opinion to share.\\nThanks!!!\",1642544154.050000,1643818704.255269,U02U5DPET47\\n72a7cc91-46a4-4c0c-bd0c-edbc545858ee,U01AXE0P5M3,,,\"<@U02U5DPET47>\\n\\nTo avoid future local dependencies issues I\\'m developing on the VM going forward which has been helpful for me.\",1642544154.050000,1643818805.865999,U02TBKWL7DJ\\n18e26cac-6282-4a5e-a84b-72da95ca561e,U02R09ZR6FQ,,,<@U01AXE0P5M3> I\\'ve seen that video but I actually think my computer runs the full docker-compose.yaml without changes. But thank you!,1643816806.173399,1643818822.723679,U02R09ZR6FQ\\n0b4dea60-609e-4079-bc32-bbd129e8dfe4,,9.0,,\"Hi all for the week 2 homework, are we ingesting all the data from 2019 down to 2020 in one table in our bigquery or different tables based on month in a single table id\",1643819006.282099,1643819006.282099,U02T0CYNNP2\\n0faf6034-e6bd-4c4d-9d03-caa4d57e32e7,U02T0CYNNP2,,,We just add files to Google Cloud Storage. Creating tables in BQ will be a part of Week 3.,1643819006.282099,1643819154.578239,U02TATJKLHG\\n86010bf8-f99f-4d2f-9043-a03e24a2beaa,U02T0CYNNP2,,,Ok so we should have 24 external tables with their respective month and year in database,1643819006.282099,1643819433.277069,U02T0CYNNP2\\nBA944C98-285D-4946-905E-02DD549FC08D,U02UNQNMH7B,,,\"Thanks for the encouragement! :pleading_face: as I\\'m restarting, I’ll let you know if I encounter more errors if you can help me :slightly_smiling_face: \",1643733210.051579,1643819598.005869,U02BTB7H2Q3\\n8cf804df-2988-4e8c-8c35-11b0dcea2fb5,U02T0CYNNP2,,,\"No external tables, really. Just the files in the Google Cloud Storage. You can skip the step where we create the external tables for the homework. It\\'s not needed.\",1643819006.282099,1643819902.385339,U02TATJKLHG\\n51b94c8c-ca14-440e-975e-6bdf04a5d32b,U02UNQNMH7B,,,\"Bri, also if you already have another postgres running, you can use a different port, e.g. 5431\\n\\nUse the `-p` key for that:\\n\\n`-p 5431:5432`\",1643733210.051579,1643820221.421359,U01AXE0P5M3\\n2a2b93d3-3ade-439d-981f-d487aa3cba36,U02TEERF0DA,,,\"yep, exactly :slightly_smiling_face:\",1643766679.185449,1643820285.469699,U01AXE0P5M3\\na5bb4449-51ad-4ef4-a605-5c11992d61f7,U02R09ZR6FQ,,,\"yea you need to change a few lines there - mostly in the volume mapping part.\\n\\ncheck it from 1:20 to 3:35\\n\\n<https://youtu.be/A1p5LQ0zzaQ?t=80>\",1643816806.173399,1643820389.353989,U01AXE0P5M3\\ndab494cf-7057-4c9d-bfce-3fdf9b95db60,,2.0,,\":wave: Looking for context on Parquet, thought this was interesting, so sharing FYI for others.  <https://towardsdatascience.com/stop-using-csvs-for-storage-here-are-the-top-5-alternatives-e3a7c9018de0>\",1643820619.553589,1643820619.553589,U02U5DPET47\\n2e5666f5-1494-4d0a-a15d-4bf8a655d0a9,U02R09ZR6FQ,,,\"is your file name called `google_credentials`  or `google_credentials.json` ? I had the same issue because I saved the file with the latter name (as a JSON), after removing `.json`  from the name it worked\",1643816806.173399,1643820644.470589,U02UA0EEHA8\\n794378f5-3480-429f-956a-cac4deceb426,U02RREQ7MHU,,,\"thanks <@U02AGF1S0TY> the code docker-compose down —volumes —rmi all helped a lot, airflow is working now but the cli keeps running this code `airflow-webserver_1  | 127.0.0.1 - - [02/Feb/2022:16:48:00 +0000] \"\"GET /health HTTP/1.1\"\" 200 187 \"\"-\"\" \"\"curl/7.64.0\"\"`\\n like the 1st screenshot, so should i just close the tab as airflow is now working properly ?\",1643807951.898489,1643820681.500549,U02RREQ7MHU\\nc4a7b484-670b-4ece-842b-ec7e5fe16f84,U02U5DPET47,,,\"…And would be curios to hear if anyone has experience or opinions on the choices, how they align or differ from that post!\",1643820619.553589,1643820683.448929,U02U5DPET47\\nbacb11f2-a01d-44b4-a842-099aa0d045c8,U02R09ZR6FQ,,,hm - I think you still need the .json part!,1643816806.173399,1643820729.167919,U01AXE0P5M3\\n4ebcd9d6-25b2-4626-b86b-ae92ec3f4050,U02R09ZR6FQ,,,\"Mine is just \"\"google_credentials\"\"\",1643816806.173399,1643820782.958289,U02R09ZR6FQ\\n8f42a01c-11cc-4fc3-883b-aa97d48cee41,U02R09ZR6FQ,,,\"And I will check the video again to see if i missed something, but the google_credentials.json file is in the airflow worker so I really don\\'t have a clue why the error appears in the log when running the dag\",1643816806.173399,1643820902.218339,U02R09ZR6FQ\\n7185ba51-e10c-4834-8985-1f9bbee6cc25,U02R09ZR6FQ,,,mine was like the one below in the beginning and didn\\'t work. Sorry it was not helpful Gustavo,1643816806.173399,1643820909.884869,U02UA0EEHA8\\n81a79378-8e14-4e94-b129-3a4c9489de59,U02R09ZR6FQ,,,\"No problem, thank you anyways! <@U02UA0EEHA8>\",1643816806.173399,1643820953.263629,U02R09ZR6FQ\\n6CA7B466-20B1-40DA-AED9-8E8A1FEDF5D6,U02RREQ7MHU,,,\"No ..let the docker-compose run ..it is what keeping ur airflow up \\nOnce u finish with airflow you can docker-compose down and stop the containers \",1643807951.898489,1643821012.645019,U02AGF1S0TY\\nb56f4e59-35f4-4ee0-b969-78ce869c1cb8,U02R09ZR6FQ,,,Could it have something to do with permissions of the project inside gcp? I think they are correct tho,1643816806.173399,1643821512.767689,U02R09ZR6FQ\\n2dea8110-01c0-4786-859c-64e7ae7e4084,U02RREQ7MHU,,,oh ok then thank you for your help,1643807951.898489,1643821745.455909,U02RREQ7MHU\\nB1F1E588-6C25-4FFA-B247-4B4C586897D4,U02RREQ7MHU,,,:+1::skin-tone-2:,1643807951.898489,1643821872.853339,U02AGF1S0TY\\n26529077-25ce-4163-8a63-eeeff4743281,U02T0CYNNP2,,,\"Oh, just bucket, no bigquery\",1643819006.282099,1643822302.378989,U02T0CYNNP2\\nb5e8bd48-aeff-4448-a3a0-945852d60a4d,U02T0CYNNP2,,,\"Just saw it now thanks, so we are having 24 parquet files in our gcs/raw\",1643819006.282099,1643822398.701589,U02T0CYNNP2\\n6ebee174-001d-4c92-8260-b95fa718e4a2,U01AXE0P5M3,,,In addition - you\\'ll learn a few other concepts when taking this route as well. It was nice to see <@U01AXE0P5M3> execute on the fly and see his thought process to quickly do workarounds :slightly_smiling_face:,1642544154.050000,1643822453.085859,U030G7PJ6LQ\\n266e4cce-b982-4e23-a666-7adf3a6aa3a7,U02T0CYNNP2,,,Yeah however you want to structure it. I had them in a separate folder named _parquet_,1643819006.282099,1643822623.049579,U02TATJKLHG\\nd2b3f22f-ea8c-4862-bfcd-d5f4b31e0401,,2.0,,\"Hello all,\\nI am getting this error.\\n\\nBroken DAG: [/opt/airflow/dags/2019.py] Traceback (most recent call last):\\n  File \"\"&lt;frozen importlib._bootstrap&gt;\"\", line 219, in _call_with_frames_removed\\n  File \"\"/opt/airflow/dags/2019.py\"\", line 76, in &lt;module&gt;\\n    bash_command=f\"\"curl -sS {dataset_url} &gt; {path_to_local_home}/{dataset_file}\"\"\\nNameError: name \\'path_to_local_home\\' is not defined\\n\\n--------------------------------------------------------------------\\nhere is my line 76\\ndownload_dataset_task = BashOperator(\\n\\xa0 \\xa0 \\xa0 \\xa0 task_id=\"\"download_dataset_task_19\"\",\\n\\xa0 \\xa0 \\xa0 \\xa0 bash_command=f\"\"curl -sS {dataset_url} &gt; {path_to_local_home}/{dataset_file}\"\"\\n\\xa0 \\xa0 )\",1643823045.842639,1643823045.842639,U02VBG59VQ9\\na6310f5f-c636-495f-ab0f-158989f5c05c,,6.0,,\"I have a question, sorry if it has been asked before.\\n\\nAfter logging into the airflow websever there was no dag there, but on creation of the python dag file, it automatically showed in the webserver without refreshing it or running any command.\\n\\nHow did this happen?\\n<@U01DHB2HS3X>\",1643823280.315089,1643823280.315089,U02QS4BD1NF\\nabfc331b-b8ae-4e38-812f-f5d55e1dc800,U02VBG59VQ9,,,Have you defined the `path_to_local_home` variable in your code?,1643823045.842639,1643823616.944569,U02TATJKLHG\\nf14e7653-6cb9-4231-95ef-6a1b733da8e0,,,,\"A good video explaining the upsides of Terraform and why you would want to use it.\\n\\n<https://youtu.be/OPDJXicUBuo>\",,1643823757.408409,U02TATJKLHG\\n65402ba6-5b3b-4872-a571-eeda4521d85a,U02VBG59VQ9,,,<@U02TATJKLHG> yes and worked now. Thank you,1643823045.842639,1643824000.828189,U02VBG59VQ9\\n35469c36-d860-4a41-ae47-2fa87326bc56,U02QS4BD1NF,,,\"Hi Damiete,\\n\\nThis is how the default Airflow setup is configured. If you go into your webserver container, something like `docker exec -it dtc-de_webserver_1 bash`\\n\\nand then look into the contents of\\n`airflow.cfg` file, you’ll find the below block somewhere:\\n```# Number of seconds after which a DAG file is parsed. The DAG file is parsed every\\n# ``min_file_process_interval`` number of seconds. Updates to DAGs are reflected after\\n# this interval. Keeping this number low will increase CPU usage.\\nmin_file_process_interval = 30\\n\\n# How often (in seconds) to scan the DAGs directory for new files. Default to 5 minutes.\\ndag_dir_list_interval = 300```\\nBasically, this config checks for new DAG files or changes in existing DAG files, every few seconds.\",1643823280.315089,1643824380.706879,U01DHB2HS3X\\n0d028271-8925-4acc-bb5e-4c1ab1ab9b4c,U02SXQ9L0FJ,,,it was a while ago but i think i fixed this by putting quotes around my path because mine had uppercase and spaces in the directory.  also i think you should be using forward slashes not backslashes. because it\\'s not matching,1643806211.418879,1643824386.669259,U02T9550LTU\\n849a287b-c5c0-4e2f-8dbe-6710e24e9621,U02UMV78PL0,,,\"Hey, we are actually going to be using dbt cloud for development and will show local installation for the people that was not able to setup GCP.\\nBut in any case, you can always use it locally for dev development and cloud for production runs.\\nThe profiles.yml format looks correct, at least what I can see, and regarding docker you will need bigquery only.\",1643814997.502659,1643824392.007489,U01B6TH1LRL\\n3FC22F5D-3359-4D9B-8CA3-B02B0C75C7EC,,16.0,,I have a problem in uploading parquet  to data lake part ,1643824705.077949,1643824705.077949,U02AGF1S0TY\\n052c27ab-fd69-4220-af52-815fcff3927f,U02QS4BD1NF,,,\"Thanks Sejal, good to know! Quite a few people had problems with dag discovery and had to restart compose. Now I understand why!\",1643823280.315089,1643824760.582439,U01AXE0P5M3\\n4a11ae5e-c84d-4fc1-af65-57af3439285b,U02QS4BD1NF,,,I see! then I’ll also add this to the Best Practices (&amp; extra info) doc,1643823280.315089,1643824806.750809,U01DHB2HS3X\\nDA5F8028-A276-4107-A473-EAAA44B02136,U02AGF1S0TY,,,I am able to run my dag without error and csv files gets downloaded and gets converted to parquet .the task on uploading happened without error but folder in bucket is empty ..Any leads ? ,1643824705.077949,1643824808.000569,U02AGF1S0TY\\n2F682008-5AD9-443C-9E94-1F5293C9DF91,U02AGF1S0TY,,,,1643824705.077949,1643824838.287089,U02AGF1S0TY\\n8553C00F-17EF-401E-93F1-D2F76D1081FA,U02AGF1S0TY,,,,1643824705.077949,1643824876.593399,U02AGF1S0TY\\n7a182b41-2d96-435f-a8f0-e9dc26c4d524,U02RW07CVTJ,,,\"Also check this thread \\n\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1643823280315089?thread_ts=1643823280.315089&amp;cid=C01FABYF2RG|https://datatalks-club.slack.com/archives/C01FABYF2RG/p1643823280315089?thread_ts=1643823280.315089&amp;cid=C01FABYF2RG>\",1643809369.994519,1643824916.974279,U01AXE0P5M3\\n3083aa76-e1a7-420b-9712-fb00d813ae7f,U02QS4BD1NF,,,\"Okay....so it checks the DAG folder for new dag files?\\n\\nAlso, do I have to run this `docker exec -it dtc-de_webserver_1 bash`\\nin cli to see the `airflow.cfg` file?\",1643823280.315089,1643825090.548299,U02QS4BD1NF\\n92f81d50-a6c6-427b-8043-1af3f98ab5d7,U02QS4BD1NF,,,\"That was an example of how to get inside your container (I’m not exactly sure what your container is called but you can find out with `docker-compose ps`  or `docker ps` )  and then yes, look within the file\",1643823280.315089,1643825239.749249,U01DHB2HS3X\\n9feb3826-6bda-416c-a958-9ee15c23fa91,U02QS4BD1NF,,,\"Alright...thank you.\\nI will check it\",1643823280.315089,1643825447.264179,U02QS4BD1NF\\nB163B282-611B-48A4-84A8-5371E5E413EC,U02AGF1S0TY,,,What\\'s the last photo of. Is that your bucket?,1643824705.077949,1643825946.294599,U02U34YJ8C8\\nFB4EB4CC-F2F1-4287-8D62-F30AB384BCDD,U02AGF1S0TY,,,Yes that is correct ,1643824705.077949,1643826008.365319,U02AGF1S0TY\\nd2473680-7612-4a24-959f-19508d0c3e9b,U02AGF1S0TY,,,\"It looks like it, but where I have folders like yellow_tripdata_2019-01.parquet, you have an underscore before parquet and a slash after.\",1643824705.077949,1643826010.836429,U02U3E6HVNC\\n4ab33970-3878-461a-bb77-0b2f5047ec8f,U02AGF1S0TY,,,Not sure how that happened?,1643824705.077949,1643826027.875169,U02U3E6HVNC\\n3BFFC829-5D98-44F7-A080-C5DE5C40A488,,2.0,,\"Is there a simple way to connect to Airflow Metadatabase, to run SQL queries and see what the data looks like inside?\",1643826029.595979,1643826029.595979,U02U34YJ8C8\\nc02d2821-facc-47d8-a51b-9f5898845986,U02AGF1S0TY,,,Maybe you created a subfolder for each parquet file?  Is there any file in each folder?,1643824705.077949,1643826099.430539,U02LQMEAREX\\n94288FA6-3D7A-4FED-9E95-365EE7AEEBD5,U02AGF1S0TY,,,Yes I created subfolders for each parquet file,1643824705.077949,1643826133.083869,U02AGF1S0TY\\n49F975F0-7B07-4470-A6EF-A493BD034F72,U02AGF1S0TY,,,\"My object_name is f”raw/rawfile.replace (“.csv”,”,parquet “)_parquet /“\",1643824705.077949,1643826276.215559,U02AGF1S0TY\\nCF791989-691F-4CEA-B94D-E0BF530977A1,U02AGF1S0TY,,,The sub folders are empty as shown in figure ,1643824705.077949,1643826316.613609,U02AGF1S0TY\\nBD7E7A46-AD6A-4B9A-8226-FA271B8571F4,U02AGF1S0TY,,,\"But I go into my container and check both csv and parquet files ( both are not empty as shown $ ls -shR \\nSo conversion is happening ok but not getting uploaded \\nTarget object ( folder ) is also getting created \",1643824705.077949,1643826508.779269,U02AGF1S0TY\\n534147fd-ba86-410b-902a-8b3d97954fab,U02AGF1S0TY,,,You can print the object name and check in the log of the task. Also maybe you missed some {}?  Example of using f\\': bash_command=f\\'curl -sSL {URL_TEMPLATE} &gt; {OUTPUT_FILE_TEMPLATE}\\',1643824705.077949,1643826698.879439,U02LQMEAREX\\ncf20bcf4-8dd8-4db2-a2e5-b0b08de2ad52,U02UMV78PL0,,,\"great, thanks!\",1643814997.502659,1643826840.216639,U02UMV78PL0\\n9B844C49-F130-4A36-9D06-38F9971B87B7,U02AGF1S0TY,,,<@U02LQMEAREX> I will try that ..looks like some parametrization error ,1643824705.077949,1643826967.226859,U02AGF1S0TY\\n8a464c4d-f58c-4682-a7a0-9ef8cf64a6aa,U02U07906Q0,,,\"In docker compose file check values for GCP_PROJECT_ID and GCP_GCS_BUCKET. See if they are matching your project id and bucket, Most likely not\",1643639767.914879,1643827533.617609,U02S9SSURMH\\n29668969-d6af-467e-8f50-c2e5aa79b6b8,U02T0CYNNP2,,,\"Good to know, thanks <@U02TATJKLHG> &amp; <@U02T0CYNNP2> for the answers, I was wondering the same thing.\",1643819006.282099,1643827611.395129,U02SUH9N1FH\\n0a4fcb12-f287-46f8-9783-f031cd4a69b3,U02U34YJ8C8,,,\"Yes, it\\'s postgres of the same version we used in week 1. You\\'ll only need to adjust the compose file to map the 5432 port from the container to some port on your host machine\",1643826029.595979,1643827671.140609,U01AXE0P5M3\\n37c5470d-cda7-43ab-a6d4-fd5cd73f6688,U02T0CYNNP2,,,\"<@U02T0CYNNP2> the way how I understood, we should have 37 files: 24 yellow taxi (2019/2020), 12 FHV (2019 only) and 1 zones. Im my console looks like this:\",1643819006.282099,1643828153.462319,U02U5L97S6T\\n8830535f-cf7f-4b64-af35-83e2a8caae3e,U02BVP1QTQF,,,\"For some reason, the download task was taking forever for me. Just for one dag run, it took an hour. But when I run the same download file command, files downloaded fast.  Here is what I did which resulted in running all dags in 30 mins.\\n1. Downloaded the files manually to local folder.\\n2. Mounted the local folder in docker compose files.\\n3. Skipped download_dataset_task.\\n4. In format_to_parquet_task, read file from mounted volume.\",1643661114.567919,1643828303.554009,U02S9SSURMH\\nb8973170-6d54-41a0-a0c3-f3223592ad27,U02T0CYNNP2,,,For sure <@U02U5L97S6T> about to start ingesting fwh and zone,1643819006.282099,1643828328.360539,U02T0CYNNP2\\n125940d8-9dc8-4c25-87c2-f7bfc8056ec9,,4.0,,\"Please what is happening can\\'t connect to my VM\\n\\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\\n@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @\\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\\nIT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\\nSomeone could be eavesdropping on you right now (man-in-the-middle attack)!\\nIt is also possible that a host key has just been changed.\\nThe fingerprint for the ED25519 key sent by the remote host is\\nSHA256:m3Rz8VM4uc/0Rmo08HVSh2USklOQWElnfvXCvhxbfzg.\\nPlease contact your system administrator.\\nAdd correct host key in /home/simeon/.ssh/known_hosts to get rid of this message.\\nOffending ECDSA key in /home/simeon/.ssh/known_hosts:2\\n  remove with:\\n  ssh-keygen -f \"\"/home/simeon/.ssh/known_hosts\"\" -R \"\"34.77.67.221\"\"\\nED25519 host key for 34.77.67.221 has changed and you have requested strict checking.\\nHost key verification failed.\",1643830758.453779,1643830758.453779,U02UKLHDWMQ\\n8b6aa828-2a78-4027-a203-fa79241c321d,,6.0,,Where are the Terraform files to transfer files form AWS to GCP? I couldn\\'t find them.,1643831310.423529,1643831310.423529,U02UX664K5E\\n68e11d4a-0f89-47fc-a610-ca533d52f0ca,U02UX664K5E,,,They are in a branch,1643831310.423529,1643831333.550529,U01AXE0P5M3\\n90e11490-903e-4175-bba9-de80b0ec1aaf,U02UKLHDWMQ,,,Have you tried googling it?,1643830758.453779,1643831368.732939,U01AXE0P5M3\\na6a792ad-fad6-4418-9623-bfc33aeb2a8f,U02UKLHDWMQ,,,Or following the suggestion from the message you posted?,1643830758.453779,1643831394.318249,U01AXE0P5M3\\n6a313532-66f4-4985-be9a-ddfd310b4019,U030FNZC26L,,,Hey <@U01DHB2HS3X>; Why actually “nofrills” version is not compatible with Windows/WSL?  It\\'s your expecatation or general assumption basing on other users feedback?,1643646196.837139,1643832622.073349,U02UKBMGJCR\\n96d9854d-fb83-4547-9927-d67f950a977b,U030FNZC26L,,,\"It seems it\\'s not specific to Windows and WSL, many people with windows reported that it works\",1643646196.837139,1643832744.182349,U01AXE0P5M3\\n0646a167-9068-4412-b21b-db7d4af5ee19,U030FNZC26L,,,\"At the time of my writing, it was my assumption initially, as there are actual reported issues on the Internet with that error, specifically for Windows/WSL. However, others with Ubuntu also reported something similar for this workshop. I use MacOS, and I didn\\'t receive this error (even after clearing the entire Docker history), plus I have used this custom setup in other workshops before (with Linux &amp; MacOS devs) and somehow it has worked before.\\n\\nSo in conclusion, it is weird, and we do not have enough data points yet to explain. The main difference is the custom entrypoint script that replaces the official airflow-init setup, so if you\\'re curious, maybe you can start there\",1643646196.837139,1643832863.920159,U01DHB2HS3X\\n70f1e1cf-6865-442d-a3c7-35db17b7b73f,U02UKLHDWMQ,,,The problem was that when i restarted my VM from google cloud it assigned a new external IP. I had remove the old IP,1643830758.453779,1643833910.408389,U02UKLHDWMQ\\nab15c721-8da7-43af-b91f-88c8bbd1c420,U02UKLHDWMQ,,,Thanks,1643830758.453779,1643833917.920919,U02UKLHDWMQ\\n3ef03c0e-f491-41f2-aaf1-2f76b5e7d8dd,,8.0,,\"So in week 2, I modified the GCP DAG to add the `start_date=datetime(2021, 1, 1)` and made sure to import datetime module `from datetime import datetime` . Now when I login to Airflow, I see error `Broken DAG: [/opt/airflow/dags/data_ingestion_gcs_dag_v02.py] Invalid timetable expression: Exactly 5 or 6 columns has to be specified for iterator expression.`. Anyone can help?\",1643834050.976189,1643834050.976189,U02UX664K5E\\n7594726c-c2d0-449e-8258-db4b586b54d9,U02UX664K5E,,,Found it?,1643831310.423529,1643834056.511959,U01AXE0P5M3\\n7b8842f2-94c0-491f-84b5-955b865c653e,U02UX664K5E,,,\"Yes, he did mention it in the video , I need to pay more attention :smile:\",1643831310.423529,1643834085.718939,U02UX664K5E\\nb99605fe-42b0-459a-ac72-654c03a3d436,U02AGF1S0TY,,,\"Also take care of commas and quotes... Just an example\\n```sometext =\"\"this should work\"\"\\nprint(f\"\"Maybe {sometext.replace(\\'should\\',\\'will\\')}\"\")```\\n\",1643824705.077949,1643834174.972829,U02LQMEAREX\\n3d3d6cfd-1577-4761-abf9-f31a4e30051b,U02UX664K5E,,,Do I need to rebuild the image after editing a DAG?,1643834050.976189,1643834248.423969,U02UX664K5E\\na37be718-fd96-430d-b047-c643e31be840,U02UX664K5E,,,I did a quick googling and it seems it\\'s related to cron expressions. can you show all the DAG params?,1643834050.976189,1643834278.409899,U01AXE0P5M3\\n640bae2a-1df7-4f63-86b4-07c600b51631,U02UX664K5E,,,\"we first wanted to merge it to the main branch, but then realizes that many people might not have a AWS account - so we ended up with leaving it on a branch\",1643831310.423529,1643834348.023559,U01AXE0P5M3\\n4e73e915-59b5-45ea-9990-e152993813b6,U02UX664K5E,,,\"<@U01AXE0P5M3> These are my parameters:\\n```with DAG(\\n    dag_id=\"\"data_ingestion_gcs_dag\"\",\\n    schedule_interval=\"\"0 6 2 **\"\",\\n    start_date=datetime(2021, 1, 1),\\n    default_args=default_args,\\n    catchup=True,\\n    max_active_runs=3,\\n    tags=[\\'dtc-de\\'],\\n)```\",1643834050.976189,1643834405.915029,U02UX664K5E\\neb519c41-c5ef-4879-b3b1-e4fb3dca05ea,U02UX664K5E,,,you have two stars together - try adding a space between them,1643834050.976189,1643834588.639619,U01AXE0P5M3\\nb29044df-0961-425d-a436-f7fa22569e67,U02UX664K5E,,,\"Correct, now it is loading the DAG properly! Thanks!\",1643834050.976189,1643834644.917549,U02UX664K5E\\nfea326dd-a2af-4f92-8335-fd75bf6de5f7,U02UX664K5E,,,\"<@U01AXE0P5M3> I see that `default_args` is also date related and has a `start_date`. What is exactly doing for our DAG and do we need it if we are already using `start_date=datetime(2021, 1, 1)` ?\\n```default_args = {\\n    \"\"owner\"\": \"\"airflow\"\",\\n    \"\"start_date\"\": days_ago(1),\\n    \"\"depends_on_past\"\": False,\\n    \"\"retries\"\": 1,\\n}```\",1643834050.976189,1643835159.285779,U02UX664K5E\\n50e66eb2-2611-445b-870b-aefffb979e00,,6.0,,\"<@U01AXE0P5M3>  and all … Starting week2 work, but realized that my GCP project name is very basic, i.e. it’s not a good choice if there’s risk that someone else would be allowed to choose the same project name + that’d lead to conflicts….\\nIs that OK (e.g. does project number provide uniqueness protection), or, should I go back and rename my project, gear up to create resources again?\",1643836670.890039,1643836670.890039,U02U5DPET47\\n1eb611fd-27b8-4c92-b78b-68b7a3bab921,,18.0,,\"Hello All,\\n```  File \"\"/opt/airflow/dags/data_ingestion_gcs_dag_19.py\"\", line 66, in upload_to_gcs\\n    blob.upload_from_filename(local_file)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py\"\", line 2720, in upload_from_filename\\n    with open(filename, \"\"rb\"\") as file_obj:\\nNotADirectoryError: [Errno 20] Not a directory: \\'/opt/airflow/output_2022-02.csv//opt/airflow/output_2022-02.parquet\\'\\n[2022-02-02, 21:33:20 UTC] {local_task_job.py:154} INFO - Task exited with return code 1\\n[2022-02-02, 21:33:20 UTC] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check```\\n\",1643837787.472119,1643837787.472119,U02VBG59VQ9\\n952db46c-7baa-4684-8f93-b054752d5b86,U02UX664K5E,,,\"From the Airflow documentation\\n<https://airflow.apache.org/docs/apache-airflow/stable/concepts/dags.html?highlight=default%20parameters#default-arguments> I guess that specific DAG parameters will override default parameters.\",1643834050.976189,1643838018.992319,U02LQMEAREX\\nFA5CD55E-E024-40DB-9361-8F044E6ECEA3,,13.0,,\"Hi,\\nJust a suggestion. The purpose of this boot camp is to learn. And you guys have done a great job in this regard. There shouldn\\'t be any score on marketing the boot camp on social media as it defies the purpose.\",1643838239.194739,1643838239.194739,U02U6DR551B\\nc51dfafc-6cbf-4537-83d1-8ce2adf51503,U02U5DPET47,,,\"What matters more is your project ID, it\\'s made of two words and 6-digit number so you should be ok :thumbsup:\",1643836670.890039,1643838331.353539,U02SUH9N1FH\\n50d2f9d7-5eef-454a-a2e6-2639ccebed22,U02U6DR551B,,,\"Why not? The course is free, if we promote it, then more people will benefit from it at no cost. I don\\'t see any issue with spreading goodwill :slightly_smiling_face:\",1643838239.194739,1643838416.443369,U02UX664K5E\\n6011301A-7739-4CBC-B8CB-66720249DD38,U02U6DR551B,,,Why there is a score on goodwill then?,1643838239.194739,1643838460.789849,U02U6DR551B\\n771a5cec-c2c0-49cc-8162-9a0d7ff42414,U02U6DR551B,,,As an incentive to spread the good word about the course I guess.,1643838239.194739,1643838509.845809,U02SUH9N1FH\\nff103c93-0fe9-4bad-b295-f846d4f5c436,U02VBG59VQ9,,,Looks like your code is trying to open a directory instead of a file. Double check the path and filename you are passing in perhaps.,1643837787.472119,1643838584.550329,U02SUH9N1FH\\na79accd7-0d40-4752-a79d-aea13b4c0385,U02UX664K5E,,,\"Thanks <@U02LQMEAREX>, makes sense!\",1643834050.976189,1643838749.959149,U02UX664K5E\\n24107a31-ce6e-4305-9ab2-05e4d8bce54a,U02U34YJ8C8,,,Ok thanks I’ll give that a shot,1643826029.595979,1643839400.841449,U02U34YJ8C8\\n80f94ab9-397b-4a81-84cd-3f89c1169b4b,U02VBG59VQ9,,,\"<@U02SUH9N1FH>\\npath_to_local_home = os.environ.get(\"\"AIRFLOW_HOME\"\", \"\"/opt/airflow/\"\")\\nURL_PREFIX = \\'<https://s3.amazonaws.com/nyc-tlc/trip+data>\\'\\nURL_TEMPLATE = URL_PREFIX + \\'/yellow_tripdata_{{ execution_date.strftime(\\\\\\'%Y-%m\\\\\\') }}.csv\\'\\nOUTPUT_FILE_TEMPLATE = path_to_local_home + \\'/src_file_{{ execution_date.strftime(\\\\\\'%Y-%m\\\\\\') }}.csv\\'\\nparquet_file = OUTPUT_FILE_TEMPLATE.replace(\\'.csv\\', \\'.parquet\\')\\nTABLE_NAME_TEMPLATE = \\'yellow_taxi_{{ execution_date.strftime(\\\\\\'%Y_%m\\\\\\') }}\\'\\nBIGQUERY_DATASET = os.environ.get(\"\"BIGQUERY_DATASET\"\", \\'trips_data_all\\')\",1643837787.472119,1643839437.649909,U02VBG59VQ9\\n2a50cc89-9599-46f8-897f-6df37fa58b48,U02U34YJ8C8,,,\"Haha yea it sometimes bothers me when I don’t know these things. I think you’re right though. I think my Macs, they do a weird thing with memory, where if you’ve got the memory to spare, they’ll allocate that memory out to running applications - which might be what’s happening.\",1643726441.258549,1643839451.133209,U02U34YJ8C8\\n2c0a444b-627f-4dc2-bbae-cca8a060f81d,U02VBG59VQ9,,,<@U02UX664K5E> any input?,1643837787.472119,1643839519.575139,U02VBG59VQ9\\nc2f563c9-f9b1-4978-ba62-1ea1f346ecff,U02SXQ9L0FJ,,,\"I had problems with git-bash and Windows paths. After playing with backslashes, quotes and with the //c or /c fix, docker/postgres run with no error, but did not create the volume correctly (it created a folder called \"\"ny_taxi_postgres_data;C\"\" which is very strange). Finally I used windows command and  worked:\\n```docker run -it -e POSTGRES_USER=\"\"root\"\" -e POSTGRES_PASSWORD=\"\"root\"\" -e POSTGRES_DB=\"\"ny_taxi\"\" -v /D/L/DataTalks/data_engineering_zoomcamp/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data -p 5432:5432 \\xa0 postgres:13```\\n\",1643806211.418879,1643839585.452589,U02LQMEAREX\\nE971EDE2-114E-4D34-AB7E-2483F0F440C4,U02U6DR551B,,,\"I think the idea behind it is, yes, to share the course with others, but also sharing what you learn with people is often the best way to solidify the learnings. \\n\\nPlus the points don\\'t matter for anything :sweat_smile:\",1643838239.194739,1643839938.185229,U02T9GHG20J\\n478b3d25-ae05-4fcf-a0d1-32ad82692abf,U02VBG59VQ9,,,I do not see anything wrong in your code. I would check the code that uses those variables (the airflow operators and callables).,1643837787.472119,1643839995.582909,U02LQMEAREX\\n1892e508-58f8-4d4d-bb65-3c56f81afbc6,,2.0,,\"For Mac users who are interested,\\n\\nOn Mac, Docker uses a Linux Virtual Machine to store and run containers, as well as store things like names or anonymous volumes. To access this VM, you can run this command:\\n\\n```docker run -it --privileged --pid=host justincormack/nsenter1```\\nProbably not of any use to anyone, but I was trying to find the `/var/lib/docker/volumes` folder on my Mac after creating an anonymous volume (just experimenting) and realised after a while it wasn’t actually stored on my Mac!\",1643840028.483199,1643840028.483199,U02U34YJ8C8\\n541b10fa-ac66-4ae2-8610-9972f98c3985,U02VBG59VQ9,,,\"<@U02LQMEAREX>\\ndownload_dataset_task = BashOperator(\\n\\xa0 \\xa0 \\xa0 \\xa0 task_id=\"\"download_dataset_task_19\"\",\\n\\xa0 \\xa0 \\xa0 \\xa0 bash_command=f\"\"curl -sS {URL_TEMPLATE} &gt; {OUTPUT_FILE_TEMPLATE}\"\"\\n\\xa0 \\xa0 )\\n\\n\\xa0 \\xa0 format_to_parquet_task = PythonOperator(\\n\\xa0 \\xa0 \\xa0 \\xa0 task_id=\"\"format_to_parquet_task_19\"\",\\n\\xa0 \\xa0 \\xa0 \\xa0 python_callable=format_to_parquet,\\n\\xa0 \\xa0 \\xa0 \\xa0 op_kwargs={\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \"\"src_file\"\": f\"\"{OUTPUT_FILE_TEMPLATE}\"\",\\n\\xa0 \\xa0 \\xa0 \\xa0 },\\n\\xa0 \\xa0 )\\n\\n\\xa0 \\xa0 # TODO: Homework - research and try XCOM to communicate output values between 2 tasks/operators\\n\\xa0 \\xa0 local_to_gcs_task = PythonOperator(\\n\\xa0 \\xa0 \\xa0 \\xa0 task_id=\"\"local_to_gcs_task_19\"\",\\n\\xa0 \\xa0 \\xa0 \\xa0 python_callable=upload_to_gcs,\\n\\xa0 \\xa0 \\xa0 \\xa0 op_kwargs={\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \"\"bucket\"\": BUCKET,\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \"\"object_name\"\": f\"\"raw/{parquet_file}\"\",\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \"\"local_file\"\": f\"\"{OUTPUT_FILE_TEMPLATE}/{parquet_file}\"\",\\n\\xa0 \\xa0 \\xa0 \\xa0 },\\n\\xa0 \\xa0 )\\n\\n\\xa0 \\xa0 bigquery_external_table_task = BigQueryCreateExternalTableOperator(\\n\\xa0 \\xa0 \\xa0 \\xa0 task_id=\"\"bigquery_external_table_task_19\"\",\\n\\xa0 \\xa0 \\xa0 \\xa0 table_resource={\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \"\"tableReference\"\": {\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \"\"projectId\"\": PROJECT_ID,\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \"\"datasetId\"\": BIGQUERY_DATASET,\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \"\"tableId\"\": \"\"external_table_19\"\",\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 },\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \"\"externalDataConfiguration\"\": {\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \"\"sourceFormat\"\": \"\"PARQUET\"\",\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \"\"sourceUris\"\": [f\"\"gs://{BUCKET}/raw/{parquet_file}\"\"],\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 },\\n\\xa0 \\xa0 \\xa0 \\xa0 },\",1643837787.472119,1643840164.198359,U02VBG59VQ9\\n8f96a200-342e-4459-80be-e8ff7bbb33d0,U02VBG59VQ9,,,<@U02LQMEAREX> thank you for looking into it,1643837787.472119,1643840204.162229,U02VBG59VQ9\\nbfa16414-d42a-4a5b-9394-6b75c8ed1a20,U02VBG59VQ9,,,I think the error is in `{OUTPUT_FILE_TEMPLATE}/{parquet_file}`. Try `{parquet_file}` instead,1643837787.472119,1643840382.696839,U02SUH9N1FH\\n71023734-f59e-4fc8-86ce-c9c20f765bce,U02VBG59VQ9,,,This command `{OUTPUT_FILE_TEMPLATE}/{parquet_file}` is returning this output `opt/airflow/output_2022-02.csv//opt/airflow/output_2022-02.parquet` which is consequently throwing an error,1643837787.472119,1643840452.063509,U02SUH9N1FH\\n5ef0b935-74f8-4ff4-a283-935a2610a8f8,U02VBG59VQ9,,,I\\'ve also noticed that the way you\\'ve defined the variable `parquet_file` will lead to your datalake structure following the same path as your local machine. Files in you bucket will be stored in `raw/opt/airflow/output_20xx-xx.parquet`. Just something to bear in mind :thumbsup:,1643837787.472119,1643841175.571609,U02SUH9N1FH\\n4b97bc47-d7bd-49a7-abce-fc47809a157c,U0316CVHMPT,,,\"So I had tried all means to install WSL , thereby cant use Docker. My Laptop at the moment can\\'t install it due to some restriction on it.\\n\\nIs there a way I can continue the course without  these installations?\",1643622530.303689,1643841237.001869,U0316CVHMPT\\n0a4c713d-9929-4643-b9f4-f000e6217a42,U02VBG59VQ9,,,do you think that is causing the issue?,1643837787.472119,1643841301.270109,U02VBG59VQ9\\n64d8bcc4-b759-4d05-96f5-96f30b19813f,U02VBG59VQ9,,,\"download_dataset_task = BashOperator(\\n\\xa0 \\xa0 \\xa0 \\xa0 task_id=\"\"download_dataset_task_19\"\",\\n\\xa0 \\xa0 \\xa0 \\xa0 bash_command=f\"\"curl -sS {URL_TEMPLATE} &gt; {OUTPUT_FILE_TEMPLATE}\"\"\\n\\xa0 \\xa0 )\\n\\n\\xa0 \\xa0 format_to_parquet_task = PythonOperator(\\n\\xa0 \\xa0 \\xa0 \\xa0 task_id=\"\"format_to_parquet_task_19\"\",\\n\\xa0 \\xa0 \\xa0 \\xa0 python_callable=format_to_parquet,\\n\\xa0 \\xa0 \\xa0 \\xa0 op_kwargs={\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \"\"src_file\"\": f\"\"{OUTPUT_FILE_TEMPLATE}\"\",\\n\\xa0 \\xa0 \\xa0 \\xa0 },\\n\\xa0 \\xa0 )\\n\\n\\xa0 \\xa0 # TODO: Homework - research and try XCOM to communicate output values between 2 tasks/operators\\n\\xa0 \\xa0 local_to_gcs_task = PythonOperator(\\n\\xa0 \\xa0 \\xa0 \\xa0 task_id=\"\"local_to_gcs_task_19\"\",\\n\\xa0 \\xa0 \\xa0 \\xa0 python_callable=upload_to_gcs,\\n\\xa0 \\xa0 \\xa0 \\xa0 op_kwargs={\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \"\"bucket\"\": BUCKET,\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \"\"object_name\"\": f\"\"raw/{parquet_file}\"\",\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \"\"local_file\"\": f\"\"{parquet_file}\"\",\\n\\xa0 \\xa0 \\xa0 \\xa0 },\",1643837787.472119,1643841335.999799,U02VBG59VQ9\\nf7f1e7d8-6922-429f-891d-e8aa7d405781,U02VBG59VQ9,,,i updated it with your recommendation and still got that /opt error <@U02SUH9N1FH>,1643837787.472119,1643841381.580369,U02VBG59VQ9\\n856b46d5-9f36-4a39-a389-1791f82a30ea,U02V4412XFA,,,\"Hi Anudeep,\\nI was wondering if you solved the problem. I have exactly the same.\",1643189014.245800,1643841495.099979,U02UJDDU2CB\\n3202487d-d1e5-4adc-8c46-16e520ba0a86,U02VBG59VQ9,,,\"Ok here\\'s what I\\'d recommend, try defining `parquet_file` as just the source file name with `.parquet` extension then point to it like this `f\"\"{path_to_local_home}/{parquet_file}` . This should output the following path: `/opt/airflow/output_20xx-xx.parquet`  and the object name for your GCS will be `raw/output_20xx-xx.parquet`\",1643837787.472119,1643841714.899249,U02SUH9N1FH\\ne9f1438f-b568-4fc2-ae57-26119a99b1b4,,11.0,,I\\'m trying to run my DAG to ingest all datasets from 2019 to 2020 to GCP and I want someone to validate my approach. I deleted all references to BigQuery from the DAG and made changes to the URL so it downloads all the CSV files based on the naming. My code is in the thread.,1643841774.642539,1643841774.642539,U02UX664K5E\\n378c953f-dee5-4e4f-beb6-b3f6d7545472,U02VBG59VQ9,,,\"Here\\'s how I did it: `dataset_file = \\'yellow_tripdata_{{ logical_date.strftime(\"\"%Y-%m\"\") }}.csv\\'`\",1643837787.472119,1643841822.754899,U02SUH9N1FH\\nf2913e5a-3008-4471-ba18-3412dc09a82e,U02UX664K5E,,,\"This is what I have for the URL parameters\\n```dataset_file = \"\"yellow_tripdata_{{ execution_date.strftime(\\\\\\'%Y-%m\\\\\\') }}.csv\"\"\\ndataset_url = f\"\"<https://s3.amazonaws.com/nyc-tlc/trip+data/{dataset_file}>\"\"```\\nAnd this is what I have for my DAG parameters:\\n```with DAG(\\n    dag_id=\"\"data_ingestion_gcs_dag_v3\"\",\\n    schedule_interval=\"\"0 6 2 * *\"\",\\n    start_date=datetime(2019, 2, 2),\\n    end_date=datetime(2019, 3, 2),\\n    catchup=True,\\n    max_active_runs=1,\\n    tags=[\\'dtc-de\\'],\\n)```\",1643841774.642539,1643841837.273769,U02UX664K5E\\na700d8a2-da6e-4ccd-997d-2dfc97ea7d32,U02VBG59VQ9,,,\"`parquet_file = dataset_file.replace(\\'.csv\\', \\'.parquet\\')`\",1643837787.472119,1643841843.614559,U02SUH9N1FH\\ned8ffd1b-851e-413f-9532-e0d7de7d0020,U02VBG59VQ9,,,\"`\"\"local_file\"\": f\"\"{path_to_local_home}/{parquet_file}\"\"`\",1643837787.472119,1643841878.296139,U02SUH9N1FH\\nf056ef0c-52c7-4549-a3f0-0cc54d589105,U02VBG59VQ9,,,<@U02VBG59VQ9> hope you find this helpful mate :thumbsup:,1643837787.472119,1643841897.451299,U02SUH9N1FH\\nb404b666-b30c-4316-8b55-b5ca58ce9dc7,,4.0,,How is public learning is graded? I only got one point even though I spent many extra hours documenting my learning process on my blog.,1643841939.618359,1643841939.618359,U02UBQJBYHZ\\nf985ad8f-8e25-4897-8918-f1059d767725,U02VBG59VQ9,,,<@U02SUH9N1FH> Would you mind taking a look at my approach and whether it\\'s right? <https://datatalks-club.slack.com/archives/C01FABYF2RG/p1643841774642539>,1643837787.472119,1643842069.294629,U02UX664K5E\\n12e28314-6b1e-431d-a0f3-c484ab0ea6b5,U02UX664K5E,,,Is your first run completing successfully <@U02UX664K5E>?,1643841774.642539,1643842069.513369,U02SUH9N1FH\\nda751260-7ea9-4e6e-a032-b800133ad0f8,U02UX664K5E,,,\"You are using these start_dates and end_dates just for testing, right?\\n\\nOverall, looks good, should work with no problems.\",1643841774.642539,1643842135.158189,U02TC704A3F\\n507865de-73b9-43d6-a472-43997ac87a6b,U02UX664K5E,,,\"<@U02SUH9N1FH> It was downloading the file but uploading to GCS bucket returned an error that looked like `upstream fail` and then my computer started going crazy even though I have 8 cores and 16 threads! I\\'ll restart the process now and make it run one at a time.\\n\\n<@U02TC704A3F> Yes, only for testing purposes.\",1643841774.642539,1643842209.430669,U02UX664K5E\\ne98cece7-d91c-49be-a965-6f3ee0dd5306,U02UX664K5E,,,\"Your variables and DAG definitions look correct to me, not sure what\\'s happening with the upload to GCS. Have you checked the run logs?\",1643841774.642539,1643842368.309489,U02SUH9N1FH\\n113C7EA6-AA98-4F04-9667-903C7C8F3ED0,,4.0,,I cannot find my email in the score sheet. Anyone in the same boat?,1643842392.246149,1643842392.246149,U02U6DR551B\\nc4a093ab-781b-4e5b-a496-250ef3255b7b,U02UX664K5E,,,I got caught by not having google credentials in the right directory,1643841774.642539,1643842400.967109,U02SUH9N1FH\\nb97e0c8e-3af2-4452-ae67-26024c035488,U02UX664K5E,,,Ooohh so you should share with us the function that is sending the file to GCS and maybe the docker-compose file because it has some GCP variables there.,1643841774.642539,1643842413.420389,U02TC704A3F\\ndb740d9f-2944-4dcb-8a22-409f6663139b,U02UX664K5E,,,<@U02SUH9N1FH> <@U02TC704A3F> Ok now it succeeded! But why is it running a task for Feb 2022 when I specifically set the end date to Jan 2021.,1643841774.642539,1643843361.465179,U02UX664K5E\\na58a30d7-6307-436b-bf90-1e6a4994b4e0,U02UX664K5E,,,\"Do you know guys which dates we need for week 3, I don\\'t want to run this for more than what I need as it requires too much resources.\",1643841774.642539,1643843432.052889,U02UX664K5E\\n89A3AE96-BA52-4442-83A5-C56E446818CE,U02AGF1S0TY,,,\"<@U02LQMEAREX> Thanks,  I got it going ..parametrization of object_name was the issue \",1643824705.077949,1643843832.748519,U02AGF1S0TY\\nf7e7a3e1-7464-4787-9828-0557a60b0838,U02UX664K5E,,,\"I had the same problem, task had one month added after the end_date parameter, I guess it has something related to the interval.\\n\\nFor week 3, you will need FHV (For-hire vehicles) for each month of 2019.\\n\\nHowever, you do not need to use Airflow for all the dates, as it was said in <https://www.youtube.com/watch?v=yZ-d7nItpB8&amp;list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&amp;index=24&amp;ab_channel=DataTalksClub|Week #3 Office Hours> in minute 4~5, you just need to show a working DAG and you can do the rest using Google Transfer Service, like in <https://www.youtube.com/watch?v=rFOFTfD1uGk&amp;list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&amp;index=22&amp;ab_channel=DataTalksClub|this video>.\\n\\nMy tip is do not use January data as pratice DAG for FHV data, its a huge file, something like 1.2 Gb, it takes a long long time, february was the fastest for me.\",1643841774.642539,1643843893.882019,U02TC704A3F\\nfb65d66d-0905-44e6-ab9b-2cbd6c09b305,U02UX664K5E,,,\"Thanks for the tip <@U02TC704A3F>, I\\'ll use the transfer service instead.\",1643841774.642539,1643844180.528239,U02UX664K5E\\n6c7c59da-8e1f-4bac-8cc4-570c5838cb97,U02TEERF0DA,,,\"I watched this video, but can\\'t seem to locate where data is added to an existing table rather than creating a table. Is there a particular time stamp you could direct me to?\",1643723713.558069,1643844278.757189,U02TNEJLC84\\n93f6317c-cec8-4a4c-9061-5ce75a9a0179,U02U6DR551B,,,\"it\\'s a way to support the creators and instructors at DataTalks club, they provided us with entirely free course, the least we could do is support them by helping their community, youtube, and audience grow through engagement and social media interaction\",1643838239.194739,1643844577.088689,U02T9550LTU\\n,,10.0,,\"Does anyone know if there is an alternative way of \"\"writing\"\" paths in .yaml files? I\\'ve checked everything and it still doesn\\'t find the credentials file\",1643844632.683509,1643844632.683509,U02R09ZR6FQ\\n7f97c4a4-eced-45e5-8d8e-814e5ac2cf8c,U02R09ZR6FQ,,,\"Hey Gustavo.\\n\\nCan you share the \"\"volumes\"\" part of your yaml file? Or maybe the entire yaml file.\\n\\nI had a similar problem but was easily fixed with reinitializing the docker-compose.\",1643844632.683509,1643845561.747869,U02TC704A3F\\n50236569-9893-49A7-8C13-236A466AF32E,U02U5DPET47,,,Thanks for sharing. I would like to try them out myself. I personally had problems with big csv files at work so my team decided to try h5. It wasn’t the easiest transition but loading time was reduced significantly.,1643820619.553589,1643845760.584239,U0308865C0H\\nf84fd608-e666-4b3f-9a05-4faaded91f8d,U02R09ZR6FQ,,,\"```# Licensed to the Apache Software Foundation (ASF) under one\\n# or more contributor license agreements.  See the NOTICE file\\n# distributed with this work for additional information\\n# regarding copyright ownership.  The ASF licenses this file\\n# to you under the Apache License, Version 2.0 (the\\n# \"\"License\"\"); you may not use this file except in compliance\\n# with the License.  You may obtain a copy of the License at\\n#\\n#   <http://www.apache.org/licenses/LICENSE-2.0>\\n#\\n# Unless required by applicable law or agreed to in writing,\\n# software distributed under the License is distributed on an\\n# \"\"AS IS\"\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\n# KIND, either express or implied.  See the License for the\\n# specific language governing permissions and limitations\\n# under the License.\\n#\\n\\n# Basic Airflow cluster configuration for CeleryExecutor with Redis and PostgreSQL.\\n#\\n# WARNING: This configuration is for local development. Do not use it in a production deployment.\\n#\\n# This configuration supports basic configuration using environment variables or an .env file\\n# The following variables are supported:\\n#\\n# AIRFLOW_IMAGE_NAME           - Docker image name used to run Airflow.\\n#                                Default: apache/airflow:2.2.3\\n# AIRFLOW_UID                  - User ID in Airflow containers\\n#                                Default: 50000\\n# Those configurations are useful mostly in case of standalone testing/running Airflow in test/try-out mode\\n#\\n# _AIRFLOW_WWW_USER_USERNAME   - Username for the administrator account (if requested).\\n#                                Default: airflow\\n# _AIRFLOW_WWW_USER_PASSWORD   - Password for the administrator account (if requested).\\n#                                Default: airflow\\n# _PIP_ADDITIONAL_REQUIREMENTS - Additional PIP requirements to add when starting all containers.\\n#                                Default: \\'\\'\\n#\\n# Feel free to modify this file to suit your needs.\\n---\\nversion: \\'3\\'\\nx-airflow-common:\\n  &amp;airflow-common\\n  # In order to add custom dependencies or upgrade provider packages you can use your extended image.\\n  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml\\n  # and uncomment the \"\"build\"\" line below, Then run `docker-compose build` to build the images.\\n  build:\\n    context: .\\n    dockerfile: ./Dockerfile\\n  environment:\\n    &amp;airflow-common-env\\n    AIRFLOW__CORE__EXECUTOR: LocalExecutor\\n    AIRFLOW__CORE__SQL_ALCHEMY_CONN: <postgresql+psycopg2://airflow:airflow@postgres/airflow>\\n    #AIRFLOW__CELERY__RESULT_BACKEND: <db+postgresql://airflow:airflow@postgres/airflow>\\n    #AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0\\n    AIRFLOW__CORE__FERNET_KEY: \\'\\'\\n    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: \\'true\\'\\n    AIRFLOW__CORE__LOAD_EXAMPLES: \\'false\\'\\n    AIRFLOW__API__AUTH_BACKEND: \\'airflow.api.auth.backend.basic_auth\\'\\n    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}\\n    GOOGLE_APPLICATION_CREDENTIALS: C:/Users/Gustavo/.google/credentials/google_credentials.json\\n    AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT: \\'google-cloud-platform://?extra__google_cloud_platform__key_path=C:/Users/Gustavo/.google/credentials/google_credentials.json\\'\\n    GCP_PROJECT_ID: \\'mystical-nimbus-338219\\'\\n    GCP_GCS_BUCKET: \"\"dtc_data_lake_mystical-nimbus-338219\"\"\\n\\n  volumes:\\n    - ./dags:/opt/airflow/dags\\n    - ./logs:/opt/airflow/logs\\n    - ./plugins:/opt/airflow/plugins\\n    - C:/Users/Gustavo/.google/credentials/:/.google/credentials:ro\\n\\n  user: \"\"${AIRFLOW_UID:-50000}:0\"\"\\n  depends_on:\\n    &amp;airflow-common-depends-on\\n    postgres:\\n      condition: service_healthy\\n\\nservices:\\n  postgres:\\n    image: postgres:13\\n    environment:\\n      POSTGRES_USER: airflow\\n      POSTGRES_PASSWORD: airflow\\n      POSTGRES_DB: airflow\\n    volumes:\\n      - postgres-db-volume:/var/lib/postgresql/data\\n    healthcheck:\\n      test: [\"\"CMD\"\", \"\"pg_isready\"\", \"\"-U\"\", \"\"airflow\"\"]\\n      interval: 5s\\n      retries: 5\\n    restart: always\\n\\n  airflow-webserver:\\n    &lt;&lt;: *airflow-common\\n    command: webserver\\n    ports:\\n      - 8080:8080\\n    healthcheck:\\n      test: [\"\"CMD\"\", \"\"curl\"\", \"\"--fail\"\", \"\"<http://localhost:8080/health>\"\"]\\n      interval: 10s\\n      timeout: 10s\\n      retries: 5\\n    restart: always\\n    depends_on:\\n      &lt;&lt;: *airflow-common-depends-on\\n      airflow-init:\\n        condition: service_completed_successfully\\n\\n  airflow-scheduler:\\n    &lt;&lt;: *airflow-common\\n    command: scheduler\\n    healthcheck:\\n      test: [\"\"CMD-SHELL\"\", \\'airflow jobs check --job-type SchedulerJob --hostname \"\"$${HOSTNAME}\"\"\\']\\n      interval: 10s\\n      timeout: 10s\\n      retries: 5\\n    restart: always\\n    depends_on:\\n      &lt;&lt;: *airflow-common-depends-on\\n      airflow-init:\\n        condition: service_completed_successfully\\n\\n  airflow-init:\\n    &lt;&lt;: *airflow-common\\n    entrypoint: /bin/bash\\n    # yamllint disable rule:line-length\\n    command:\\n      - -c\\n      - |\\n        function ver() {\\n          printf \"\"%04d%04d%04d%04d\"\" $${1//./ }\\n        }\\n        airflow_version=$$(gosu airflow airflow version)\\n        airflow_version_comparable=$$(ver $${airflow_version})\\n        min_airflow_version=2.2.0\\n        min_airflow_version_comparable=$$(ver $${min_airflow_version})\\n        if (( airflow_version_comparable &lt; min_airflow_version_comparable )); then\\n          echo\\n          echo -e \"\"\\\\033[1;31mERROR!!!: Too old Airflow version $${airflow_version}!\\\\e[0m\"\"\\n          echo \"\"The minimum Airflow version supported: $${min_airflow_version}. Only use this or higher!\"\"\\n          echo\\n          exit 1\\n        fi\\n        if [[ -z \"\"${AIRFLOW_UID}\"\" ]]; then\\n          echo\\n          echo -e \"\"\\\\033[1;33mWARNING!!!: AIRFLOW_UID not set!\\\\e[0m\"\"\\n          echo \"\"If you are on Linux, you SHOULD follow the instructions below to set \"\"\\n          echo \"\"AIRFLOW_UID environment variable, otherwise files will be owned by root.\"\"\\n          echo \"\"For other operating systems you can get rid of the warning with manually created .env file:\"\"\\n          echo \"\"    See: <https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html#setting-the-right-airflow-user>\"\"\\n          echo\\n        fi\\n        one_meg=1048576\\n        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))\\n        cpus_available=$$(grep -cE \\'cpu[0-9]+\\' /proc/stat)\\n        disk_available=$$(df / | tail -1 | awk \\'{print $$4}\\')\\n        warning_resources=\"\"false\"\"\\n        if (( mem_available &lt; 4000 )) ; then\\n          echo\\n          echo -e \"\"\\\\033[1;33mWARNING!!!: Not enough memory available for Docker.\\\\e[0m\"\"\\n          echo \"\"At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))\"\"\\n          echo\\n          warning_resources=\"\"true\"\"\\n        fi\\n        if (( cpus_available &lt; 2 )); then\\n          echo\\n          echo -e \"\"\\\\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\\\\e[0m\"\"\\n          echo \"\"At least 2 CPUs recommended. You have $${cpus_available}\"\"\\n          echo\\n          warning_resources=\"\"true\"\"\\n        fi\\n        if (( disk_available &lt; one_meg * 10 )); then\\n          echo\\n          echo -e \"\"\\\\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\\\\e[0m\"\"\\n          echo \"\"At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))\"\"\\n          echo\\n          warning_resources=\"\"true\"\"\\n        fi\\n        if [[ $${warning_resources} == \"\"true\"\" ]]; then\\n          echo\\n          echo -e \"\"\\\\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\\\\e[0m\"\"\\n          echo \"\"Please follow the instructions to increase amount of resources available:\"\"\\n          echo \"\"   <https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html#before-you-begin>\"\"\\n          echo\\n        fi\\n        mkdir -p /sources/logs /sources/dags /sources/plugins\\n        chown -R \"\"${AIRFLOW_UID}:0\"\" /sources/{logs,dags,plugins}\\n        exec /entrypoint airflow version\\n    # yamllint enable rule:line-length\\n    environment:\\n      &lt;&lt;: *airflow-common-env\\n      _AIRFLOW_DB_UPGRADE: \\'true\\'\\n      _AIRFLOW_WWW_USER_CREATE: \\'true\\'\\n      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}\\n      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}\\n    user: \"\"0:0\"\"\\n    volumes:\\n      - .:/sources\\n\\n  airflow-cli:\\n    &lt;&lt;: *airflow-common\\n    profiles:\\n      - debug\\n    environment:\\n      &lt;&lt;: *airflow-common-env\\n      CONNECTION_CHECK_MAX_COUNT: \"\"0\"\"\\n    # Workaround for entrypoint issue. See: <https://github.com/apache/airflow/issues/16252>\\n    command:\\n      - bash\\n      - -c\\n      - airflow\\n\\nvolumes:\\n  postgres-db-volume:```\",1643844632.683509,1643845838.478939,U02R09ZR6FQ\\n48290a8c-c672-4857-b821-3190cdb121ba,U02R09ZR6FQ,,,\"I have \"\"lighted\"\" it by following the DE Zoomcamp 2.3.4 video, but before my computer was also capable of running the original docker-compose file\",1643844632.683509,1643845907.058499,U02R09ZR6FQ\\n718433ac-7d4f-43b9-ba31-25dc9cd3a5fb,U02R09ZR6FQ,,,\"Last week I had to use docker-compose due to (i think) path problems also when they start with \"\"c:/\"\", my guess is that the issue is there <@U02TC704A3F>\",1643844632.683509,1643846042.584789,U02R09ZR6FQ\\nbc988a9c-83e2-4af2-8d5e-7d7426b0044b,U02R09ZR6FQ,,,And I have rebuilt docker-compose too many times :stuck_out_tongue_closed_eyes:,1643844632.683509,1643846196.860649,U02R09ZR6FQ\\n7b4e4315-457a-4ee0-b2ac-ccea5b2a976a,U02R09ZR6FQ,,,\"I think I know what could be wrong.\\n\\nIn environment you are setting env variables to the docker container that is running airflow.\\n\\nIn line 61 you are declaring to airflow that your google application credentials is in C:/Users/Gustavo/, that is in your machine, and docker has no connection to it.\\n\\n```GOOGLE_APPLICATION_CREDENTIALS: C:/Users/Gustavo/.google/credentials/google_credentials.json```\\nThis directory doesn\\'t exist in your docker virtual machine.\\n\\nIn volumes, you are mirroring directories, so where you are declaring C:/Users/Gustavo/ for airflow environment variables, you should be declaring where you mirrored C:/Users/Gustavo/.\\n```  volumes:\\n    - ./dags:/opt/airflow/dags\\n    - ./logs:/opt/airflow/logs\\n    - ./plugins:/opt/airflow/plugins\\n    - C:/Users/Gustavo/.google/credentials:/.google/credentials:ro```\\nAnd thats is .google/credentials, in your docker machine.\\n\\nTry changing your code line 61 to this:\\n```GOOGLE_APPLICATION_CREDENTIALS:  /.google/credentials/google_credentials.json```\\nAnd this, in line 62:\\n```AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT: \\'google-cloud-platform://?extra__google_cloud_platform__key_path=/.google/credentials/google_credentials.json\\'```\",1643844632.683509,1643847201.061439,U02TC704A3F\\nbd8df9ca-43e2-45e1-9f32-f81b3ad5a094,U02R09ZR6FQ,,,\"Give me a heads up after you tried, I think it will work.\\n\\n<https://github.com/Leo200467/de-zoomcamp-2022/blob/main/de-zoomcamp-week-2-data-ingestion/airflow/docker-compose.yaml|This is my .yaml file btw>\",1643844632.683509,1643847652.688239,U02TC704A3F\\n19349f37-ae1c-4119-996e-0c95b832d518,U02R09ZR6FQ,,,\"I\\'m running it but I think it will work too, I understood the issue, great explanation btw\",1643844632.683509,1643847768.396619,U02R09ZR6FQ\\n2c3084f1-e831-4ad1-8257-a3f9434ded5f,U02R09ZR6FQ,,,It worked :raised_hands::raised_hands: Thank you!!,1643844632.683509,1643847837.769189,U02R09ZR6FQ\\n2540d733-407a-462b-b2f5-dd8ae4a46cda,,7.0,,\"Hi, in week 2 homework, when I activate the data_ingestion_gcs_dag, the tasks seems to be properly scheduled, but none of them actually get executed (code in reply)\",1643848524.574759,1643848524.574759,U030F0YHDAM\\n02b591d7-2b18-414e-b1a9-d70bc68abc3d,U030F0YHDAM,,,\"`import os`\\n`import logging`\\n\\n`from datetime import datetime`\\n\\n`from airflow import DAG`\\n`from airflow.utils.dates import days_ago`\\n`from airflow.operators.bash import BashOperator`\\n`from airflow.operators.python import PythonOperator`\\n\\n`from google.cloud import storage`\\n`from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator`\\n`import pyarrow.csv as pv`\\n`import pyarrow.parquet as pq`\\n\\n`PROJECT_ID = os.environ.get(\"\"GCP_PROJECT_ID\"\")`\\n`BUCKET = os.environ.get(\"\"GCP_GCS_BUCKET\"\")`\\n\\n`AIRFLOW_HOME = os.environ.get(\"\"AIRFLOW_HOME\"\", \"\"/opt/airflow/\"\")`\\n\\n`URL_PREFIX = \\'<https://s3.amazonaws.com/nyc-tlc/trip+data>\\'`\\n`FILE_NAME = \\'yellow_tripdata_{{ execution_date.strftime(\\\\\\'%Y-%m\\\\\\') }}.csv\\'`\\n`URL_TEMPLATE = URL_PREFIX + \\'/\\' + FILE_NAME`\\n`OUTPUT_FILE_TEMPLATE = AIRFLOW_HOME + \\'/output_\\' + FILE_NAME`\\n`TABLE_NAME_TEMPLATE = \\'yellow_taxi_{{ execution_date.strftime(\\\\\\'%Y_%m\\\\\\') }}\\'`\\n\\n\\n`PARQUET_FILE = FILE_NAME.replace(\\'.csv\\', \\'.parquet\\')`\\n`OUTPUT_FILE_PARQUET = AIRFLOW_HOME + \\'/output_\\' + PARQUET_FILE`\\n`BIGQUERY_DATASET = os.environ.get(\"\"BIGQUERY_DATASET\"\", \\'trips_data_all\\')`\\n\\n`def format_to_parquet(src_file):`\\n    `if not src_file.endswith(\\'.csv\\'):`\\n        `logging.error(\"\"Can only accept source files in CSV format, for the moment\"\")`\\n        `return`\\n    `table = pv.read_csv(src_file)`\\n    `pq.write_table(table, src_file.replace(\\'.csv\\', \\'.parquet\\'))`\\n\\n\\n`# NOTE: takes 20 mins, at an upload speed of 800kbps. Faster if your internet has a better upload speed`\\n`def upload_to_gcs(bucket, object_name, local_file):`\\n    `\"\"\"\"\"\"`\\n    `Ref: <https://cloud.google.com/storage/docs/uploading-objects#storage-upload-object-python>`\\n    `:param bucket: GCS bucket name`\\n    `:param object_name: target path &amp; file-name`\\n    `:param local_file: source path &amp; file-name`\\n    `:return:`\\n    `\"\"\"\"\"\"`\\n    `# WORKAROUND to prevent timeout for files &gt; 6 MB on 800 kbps upload speed.`\\n    `# (Ref: <https://github.com/googleapis/python-storage/issues/74>)`\\n    `storage.blob._MAX_MULTIPART_SIZE = 5 * 1024 * 1024  # 5 MB`\\n    `storage.blob._DEFAULT_CHUNKSIZE = 5 * 1024 * 1024  # 5 MB`\\n    `# End of Workaround`\\n\\n    `client = storage.Client()`\\n    `bucket = client.bucket(bucket)`\\n\\n    `blob = bucket.blob(object_name)`\\n    `blob.upload_from_filename(local_file)`\\n\\n\\n`default_args = {`\\n    `\"\"owner\"\": \"\"airflow\"\",`\\n    `\"\"start_date\"\": days_ago(1),`\\n    `\"\"depends_on_past\"\": False,`\\n    `\"\"retries\"\": 1,`\\n`}`\\n\\n`with DAG(`\\n    `dag_id=\"\"data_ingestion_gcs_dag_multiple\"\",`\\n    `schedule_interval=\"\"@monthly\"\",`\\n    `start_date=datetime(2019, 1, 1),`\\n    `end_date=datetime(2020, 12, 1),`\\n    `default_args=default_args,`\\n    `catchup=True,`\\n    `max_active_runs=1,`\\n    `tags=[\\'dtc-de\\'],`\\n`) as dag:`\\n\\n    `download_dataset_task = BashOperator(`\\n        `task_id=\"\"download_dataset_task\"\",`\\n        `bash_command=f\"\"curl -sS {URL_TEMPLATE} &gt; {OUTPUT_FILE_TEMPLATE}\"\"`        \\n    `)`\\n\\n    `format_to_parquet_task = PythonOperator(`\\n        `task_id=\"\"format_to_parquet_task\"\",`\\n        `python_callable=format_to_parquet,`\\n        `op_kwargs={`\\n            `\"\"src_file\"\": f\"\"{OUTPUT_FILE_TEMPLATE}\"\",`\\n        `},`\\n    `)`\\n\\n    `# TODO: Homework - research and try XCOM to communicate output values between 2 tasks/operators`\\n    `local_to_gcs_task = PythonOperator(`\\n        `task_id=\"\"local_to_gcs_task\"\",`\\n        `python_callable=upload_to_gcs,`\\n        `op_kwargs={`\\n            `\"\"bucket\"\": BUCKET,`\\n            `\"\"object_name\"\": f\"\"raw/{PARQUET_FILE}\"\",`\\n            `\"\"local_file\"\": f\"\"{OUTPUT_FILE_PARQUET}\"\",`\\n        `},`\\n    `)`\\n\\n    `bigquery_external_table_task = BigQueryCreateExternalTableOperator(`\\n        `task_id=\"\"bigquery_external_table_task\"\",`\\n        `table_resource={`\\n            `\"\"tableReference\"\": {`\\n                `\"\"projectId\"\": PROJECT_ID,`\\n                `\"\"datasetId\"\": BIGQUERY_DATASET,`\\n                `\"\"tableId\"\": \"\"external_table\"\",`\\n            `},`\\n            `\"\"externalDataConfiguration\"\": {`\\n                `\"\"sourceFormat\"\": \"\"PARQUET\"\",`\\n                `\"\"sourceUris\"\": [f\"\"gs://{BUCKET}/raw/{PARQUET_FILE}\"\"],`\\n            `},`\\n        `},`\\n    `)`\\n\\n    `download_dataset_task &gt;&gt; format_to_parquet_task &gt;&gt; local_to_gcs_task &gt;&gt; bigquery_external_table_task`\",1643848524.574759,1643848548.569709,U030F0YHDAM\\n54027e31-58b8-4887-acfd-026744508778,U02R09ZR6FQ,,,\"Great!\\nMy pleasure, Gustavo\",1643844632.683509,1643849126.612349,U02TC704A3F\\n425749e9-da4d-411c-8ef0-406fbee00edd,U030F0YHDAM,,,\"Hey Caio\\n\\nI\\'m reading your code and trying to find someting. Meanwhile, can you click in one of these download_dataset_task steps and take a look at the logs?\\n\\nMy airflow isn\\'t running so I can\\'t show a printscreen but you just need to click it and then go for \"\"logs\"\".\",1643848524.574759,1643849530.333199,U02TC704A3F\\n64823253-356e-434e-967a-54f05783414f,U02UX664K5E,,,<@U01AXE0P5M3> Are you saying that with Terraform the transfer is from our AWS account to GCP and not from a public AWS bucket?,1643831310.423529,1643849881.251159,U02UX664K5E\\n93e29b65-e934-41f6-9e06-81edbb7864c6,,1.0,,\"My airflow showing that there is error in my import\\n\\n<http://localhost:8080/home#alerts|DAG Import Errors (1)>\\n\\nBroken DAG: [/opt/airflow/dags/data_ingestion_gcs_dag.py] Traceback (most recent call last):\\n  File \"\"&lt;frozen importlib._bootstrap_external&gt;\"\", line 791, in source_to_code\\n  File \"\"&lt;frozen importlib._bootstrap&gt;\"\", line 219, in _call_with_frames_removed\\n  File \"\"/opt/airflow/dags/data_ingestion_gcs_dag.py\"\", line 110\\n    download_dataset_task &gt;&gt; format_to_parquet_task &gt;&gt; local_to_gcs_task &gt;&gt;\\n                                                                           ^\\nSyntaxError: invalid syntax\",1643850859.871179,1643850859.871179,U02UKLHDWMQ\\nd4f6b56e-e513-40d3-b586-fddd5998136d,U02UKLHDWMQ,,,problem solved,1643850859.871179,1643851648.467439,U02UKLHDWMQ\\n590c9f8d-c68b-46ed-bbb5-893f755f5937,,1.0,,\"I\\'m getting this error when i ran my dags\\n\\n```google.auth.exceptions.DefaultCredentialsError: File /.google/credentials/google_credentials.json was not found.```\\n\",1643852347.057809,1643852347.057809,U02UKLHDWMQ\\n5e5eb072-aff8-492a-92c9-88372a85b41d,U02R09ZR6FQ,,,Hi <@U02R09ZR6FQ> were you able to solve the problem? am having same problem.,1643816806.173399,1643852734.524739,U02UKLHDWMQ\\n6d85754b-5cd4-4bc7-b745-6cb8e1377be9,U02R09ZR6FQ,,,<@U02R09ZR6FQ> problem solved,1643816806.173399,1643853926.869709,U02UKLHDWMQ\\n0868f14d-87c7-4362-8dff-6f3a531499c1,U02UKLHDWMQ,,,Problem solved,1643852347.057809,1643853951.278099,U02UKLHDWMQ\\n32a04027-a19a-4b1e-ab6a-56acad547058,,7.0,,I tried running docker-compose but got this error,1643855193.411479,1643855193.411479,U02TZ1JCVEC\\n27727524-f2bf-46e6-840b-ca2740a24224,U02TZ1JCVEC,,,\"`The Compose file \\'./docker-compose.yaml\\' is invalid because:`\\n`services.airflow-cli.environment.volumes contains [\"\"./dags:/opt/airflow/dags\"\", \"\"./logs:/opt/airflow/logs\"\", \"\"./plugins:/opt/airflow/plugins\"\"], which is an invalid type, it should be a string, number, or a null`\\n`services.airflow-worker.environment.volumes contains [\"\"./dags:/opt/airflow/dags\"\", \"\"./logs:/opt/airflow/logs\"\", \"\"./plugins:/opt/airflow/plugins\"\"], which is an invalid type, it should be a string, number, or a null`\\n`services.airflow-init.environment.volumes contains [\"\"./dags:/opt/airflow/dags\"\", \"\"./logs:/opt/airflow/logs\"\", \"\"./plugins:/opt/airflow/plugins\"\"], which is an invalid type, it should be a string, number, or a null`\\n`services.airflow-scheduler.environment.volumes contains [\"\"./dags:/opt/airflow/dags\"\", \"\"./logs:/opt/airflow/logs\"\", \"\"./plugins:/opt/airflow/plugins\"\"], which is an invalid type, it should be a string, number, or a null`\\n`services.airflow-triggerer.environment.volumes contains [\"\"./dags:/opt/airflow/dags\"\", \"\"./logs:/opt/airflow/logs\"\", \"\"./plugins:/opt/airflow/plugins\"\"], which is an invalid type, it should be a string, number, or a null`\\n`services.airflow-webserver.environment.volumes contains [\"\"./dags:/opt/airflow/dags\"\", \"\"./logs:/opt/airflow/logs\"\", \"\"./plugins:/opt/airflow/plugins\"\"], which is an invalid type, it should be a string, number, or a null`\\n`services.flower.environment.volumes contains [\"\"./dags:/opt/airflow/dags\"\", \"\"./logs:/opt/airflow/logs\"\", \"\"./plugins:/opt/airflow/plugins\"\"], which is an invalid type, it should be a string, number, or a null`\",1643855193.411479,1643855236.196849,U02TZ1JCVEC\\nfd97d95e-4365-4e87-a7fe-1a6e12fdea27,U02TZ1JCVEC,,,any help please,1643855193.411479,1643855245.604079,U02TZ1JCVEC\\n51ca41a2-1bf3-468b-8638-a2773a09ce95,U02U6DR551B,,,\"Does anyone understand how they rank the \"\"learning in public?\"\" I don\\'t, I spent a lot of time writing up my learning experience, and I got the low score. I asked the question in a post but no answer yet.\",1643838239.194739,1643856243.979949,U02UBQJBYHZ\\nf5951ec5-4aca-411d-914c-7d11f681e690,U02U6DR551B,,,I\\'m not solely interested in the grade. I think my learning experience might be helpful for some people because I ran into so many problems but worked them all out.,1643838239.194739,1643856287.129419,U02UBQJBYHZ\\n79BA6F13-BA77-4D9B-80E0-C3C414CDE644,U02TZ1JCVEC,,,\"Have you done these two steps ?\\nmkdir -p ./dags ./logs ./plugins\\n\\necho -e \"\"AIRFLOW_UID=$(id -u)\"\" &gt; .env\",1643855193.411479,1643857863.613329,U02AGF1S0TY\\nFEAD7093-C519-43C5-8B22-2C9A689A2EA9,U02TZ1JCVEC,,,\"Check version as well \\n<https://github.com/docker/compose/issues/4763|https://github.com/docker/compose/issues/4763>\",1643855193.411479,1643859120.904939,U02AGF1S0TY\\n8e646d6f-49ee-49de-94b5-51e31a84581f,U029DM0GQHJ,,,Did anyone confirm if this is using 50000 or the user that is in the .env?,1643307686.127200,1643859559.001679,U030P1LNK5Z\\na29b6eea-a5dc-4efc-b2b8-74a5ff4ea007,U02U6DR551B,,,\"Agree. It also affects leaderboard scores of social media-shy people like me :slightly_smiling_face: But as long as the final results are absolute and not relative, I won\\'t mind it.\",1643838239.194739,1643864396.062529,U02HB9KTERJ\\n1064de34-b593-401f-9a25-bb89787a58da,U02U5DPET47,,,\"If Google allowed you to create a project with this name, then everything is good\",1643836670.890039,1643868434.978289,U01AXE0P5M3\\nbd88f102-5bf3-4589-833d-2ea8422a48af,,4.0,,\"Hello, during the data ingestion to `postgresql`, I have the following error (this error is seen in the official video as well) .  I am wondering what if some chunks are lost or simply errored out? So my question is not about solving the dtype error but about verification of the correctness of the data inserted into `ny_taxi` ?\\n```--2022-02-03 05:03:38--  <https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>\\nResolving <http://s3.amazonaws.com|s3.amazonaws.com> (<http://s3.amazonaws.com|s3.amazonaws.com>)... 52.217.198.104\\nConnecting to <http://s3.amazonaws.com|s3.amazonaws.com> (<http://s3.amazonaws.com|s3.amazonaws.com>)|52.217.198.104|:443... connected.\\nHTTP request sent, awaiting response... 200 OK\\nLength: 125981363 (120M) [text/csv]\\nSaving to: \\'output.csv\\'\\n\\noutput.csv                                                 100%[========================================================================================================================================&gt;] 120.14M   807KB/s    in 2m 31s\\n\\n2022-02-03 05:06:10 (812 KB/s) - \\'output.csv\\' saved [125981363/125981363]\\n\\ninserted another chunk, took 4.873 second\\ninserted another chunk, took 4.904 second\\ninserted another chunk, took 4.949 second\\ninserted another chunk, took 4.750 second\\ninserted another chunk, took 4.827 second\\ninserted another chunk, took 4.878 second\\ninserted another chunk, took 4.860 second\\ninserted another chunk, took 4.807 second\\ninserted another chunk, took 4.838 second\\ninserted another chunk, took 4.991 second\\ninserted another chunk, took 4.931 second\\n/app/ingest_data.py:39: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\\n  df = next(df_iter)\\ninserted another chunk, took 4.761 second\\ninserted another chunk, took 2.997 second\\nTraceback (most recent call last):\\n  File \"\"/app/ingest_data.py\"\", line 65, in &lt;module&gt;\\n    main(args)\\n  File \"\"/app/ingest_data.py\"\", line 39, in main\\n    df = next(df_iter)\\n  File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\"\", line 1187, in __next__\\n    return self.get_chunk()\\n  File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\"\", line 1280, in get_chunk\\n    return self.read(nrows=size)\\n  File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\"\", line 1250, in read\\n    index, columns, col_dict = self._engine.read(nrows)\\n  File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\"\", line 225, in read\\n    chunks = self._reader.read_low_memory(nrows)\\n  File \"\"pandas/_libs/parsers.pyx\"\", line 830, in pandas._libs.parsers.TextReader.read_low_memory\\nStopIteration```\",1643868570.240969,1643868570.240969,U030HKR0WK0\\n5b8e684e-92b9-4770-8090-830e7d176ab4,U02U6DR551B,,,Posting to social media is difficult. We want to motivate you to do it,1643838239.194739,1643868614.937079,U01AXE0P5M3\\n108bad87-e802-4624-bbf2-4d30eb2206c5,U02U6DR551B,,,\"Cris, you get one point for each social media post. It\\'s capped at 7 points\",1643838239.194739,1643868672.305809,U01AXE0P5M3\\nabd75cb6-477f-473a-98cc-aa1a9f4938db,U02U34YJ8C8,,,On whose Mac was it stored???,1643840028.483199,1643868868.347989,U01AXE0P5M3\\n15598853-537c-4cff-a860-116c7d66b65d,U02UBQJBYHZ,,,\"It\\'s 1 point for each post. There\\'s no easy way to take into account time spent on each post, that\\'s why we count the number of URLs you submit in the form\",1643841939.618359,1643869085.062529,U01AXE0P5M3\\n4d5171f9-0419-454d-86d9-79f6852840d2,U02U6DR551B,,,Can you DM me your email?,1643842392.246149,1643869114.940669,U01AXE0P5M3\\n94172615-9c0a-424a-9bcd-8b1de8057160,U02UX664K5E,,,From a public bucket but you still need an AWS account to have the keys,1643831310.423529,1643869270.430609,U01AXE0P5M3\\nae8bc469-829c-4eeb-b338-04398d2a8826,U0316CVHMPT,,,\"Yes, you can rent a cloud VM in GCP\",1643622530.303689,1643869459.138819,U01AXE0P5M3\\ndb8cc5b3-1e84-4744-a0a7-584282f59e93,,4.0,,\"I m currently trying to create local dag script to connect to the pgdatabase. Unfortunately, I m not able to access the env variables(PG_HOST, PG_USER...) that we add in the dag file. I have added entries in the docker-compose.yaml and the .env file as well. Anyone came across such an error or knows how to troubleshoot this?\",1643869666.937519,1643869666.937519,U02QK4ZV4UX\\n6237466e-09ef-4d92-8966-7b4fa0e2ff79,U02BVP1QTQF,,,\"Hi Alvaro, I am stuck at step 3 for setting up SSH access.  When I run gcloud compute config-ssh, this error appears.\\n`Updating project ssh metadata...failed.`\\n`ERROR: (gcloud.compute.config-ssh) Could not add SSH key to project metadata:`\\n\\xa0`- Required \\'compute.projects.setCommonInstanceMetadata\\' permission for \\'projects/positive-leaf-340006\\'`\",1643661114.567919,1643870055.237419,U02SEB4Q8TW\\n8c4eb03d-1da6-4745-a4f6-bc9e47da9ad5,,2.0,,\"Hi, I\\'m getting `permission denied` error anytime I try to edit and save files in the VM, anybody with a solution? Thanks.\",1643871469.180649,1643871469.180649,U02SEH4PPQB\\n00d93ddb-b18d-4309-829b-b229bbd3fbb7,U02SEH4PPQB,,,\"Check if it\\'s related to this -\\n\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1643699464311449>\",1643871469.180649,1643871558.216259,U02TATJKLHG\\n77bd7175-ab89-4c66-899d-9f198af3c099,U030HKR0WK0,,,\"Hi Mani,\\n\\nThis is not an error. The data should load fine\",1643868570.240969,1643871640.303369,U02TATJKLHG\\na81be1bb-e36a-4a1d-b6d1-cf84ff2c8884,U02SEH4PPQB,,,\"Thank you <@U02TATJKLHG>, it worked\",1643871469.180649,1643871974.091589,U02SEH4PPQB\\n20a1fcc6-8e25-4b39-9b5e-f69a67b14ac3,,24.0,,\"Hi,\\ni have given DAG start date as 2019-01-01, but when i execute it in airflow it runs for the current date 2021-01-04. How to fix it?\",1643873524.132099,1643873524.132099,U02U6DR551B\\n447e7553-7072-46be-923d-8511d5723bcb,U02U6DR551B,,,can you share the code? :slightly_smiling_face:,1643873524.132099,1643873584.588989,U02UA0EEHA8\\n0932e1df-eab3-4573-a384-4f808dcfb414,U02U6DR551B,,,\"default_args = {\\n\\xa0 \\xa0 \"\"owner\"\": \"\"airflow\"\",\\n\\xa0 \\xa0 \"\"depends_on_past\"\": False,\\n\\xa0 \\xa0 \"\"retries\"\": 1,\\n}\\n\\n# NOTE: DAG declaration - using a Context Manager (an implicit way)\\nwith DAG(\\n\\xa0 \\xa0 dag_id=\"\"data_ingestion_gcs_dag\"\",\\n\\xa0 \\xa0 schedule_interval = \"\"@monthly\"\",\\n\\xa0 \\xa0 start_date = datetime(2019,1,1),\\n\\xa0 \\xa0 default_args=default_args,\\n\\xa0 \\xa0 catchup=False,\\n\\xa0 \\xa0 max_active_runs=1,\\n\\xa0 \\xa0 tags=[\\'dtc-de\\'],\\n) as dag:\",1643873524.132099,1643873695.734399,U02U6DR551B\\n3193e063-4ccf-4893-a9cf-fbc6b9095b4d,U02U6DR551B,,,Is your first DAG run starting from 2021-01-04?,1643873524.132099,1643873841.953189,U02TATJKLHG\\n262513FD-7306-41EC-BAB3-0C28044B0F29,U02TZ1JCVEC,,,Yes I have done both steps ,1643855193.411479,1643873879.432519,U02TZ1JCVEC\\nca88180d-b603-40a5-b04b-b1c9fc8f9347,U02QK4ZV4UX,,,can you show what does your code look like?,1643869666.937519,1643873889.185519,U029DM0GQHJ\\nd1b19606-110a-4faf-a461-21a8a8da44c0,U02U6DR551B,,,yws,1643873524.132099,1643873928.602139,U02U6DR551B\\n787c1599-f46e-4b3f-90bf-13cbf209be33,U02U6DR551B,,,yes,1643873524.132099,1643873929.876849,U02U6DR551B\\ndf1fbe36-4254-4889-9b8e-65d654df92a7,U02U6DR551B,,,next run is 2021-01-01,1643873524.132099,1643873952.092319,U02U6DR551B\\nad4143e7-d1af-47da-8614-19d5a241d0f3,U02U6DR551B,,,Can you set `catchup=True`,1643873524.132099,1643873953.340389,U02TATJKLHG\\n5d98ffaa-ed6c-48e2-b04f-31b06847ba5c,U02U6DR551B,,,sorry,1643873524.132099,1643873959.660159,U02U6DR551B\\n090ac4d0-3e71-456a-8223-978bc1a17828,U02U6DR551B,,,next run is 2022-01-01,1643873524.132099,1643873966.577159,U02U6DR551B\\n6211f25b-c77f-4e26-bdd7-31f809295ba3,U02U6DR551B,,,I think setting `catchup=True` should resolve it,1643873524.132099,1643873985.216919,U02TATJKLHG\\ned053701-21a7-4836-90c5-3ee297176525,U02U6DR551B,,,yeah it is most likely catchup :slightly_smiling_face:,1643873524.132099,1643873990.733909,U02UA0EEHA8\\n117e01d7-8f74-45b4-91ad-74a2f1c0866d,U02U6DR551B,,,And your posts indeed help with spreading the word about the course and this community! Thanks for doing that,1643838239.194739,1643873992.849279,U01AXE0P5M3\\n064F7B58-0B2F-42C3-A451-2FAFE59739B9,U02TZ1JCVEC,,,It has been working well to run airflow and dag ,1643855193.411479,1643873999.037519,U02TZ1JCVEC\\n9f218c9b-9725-4580-b801-16a1ba0b65a6,U02U6DR551B,,,,1643873524.132099,1643874002.376079,U02U6DR551B\\n59923aa8-7c77-4d71-b1d9-f3e97324c4d7,U02U6DR551B,,,\"```with DAG(\\n    dag_id=\"\"data_ingestion_gcs_dag\"\",\\n    schedule_interval = \"\"@monthly\"\",\\n    start_date = datetime(2019,1,1),\\n    default_args=default_args,\\n    catchup=True,\\n    max_active_runs=1,\\n    tags=[\\'dtc-de\\'],\\n) as dag```\",1643873524.132099,1643874018.265149,U02UA0EEHA8\\nD4BAA172-FCB0-4C7B-8278-357CEBFF8CBC,U02TZ1JCVEC,,,\"I included the data ingest script and tried docker-compose up again \\nThen that error came up\",1643855193.411479,1643874032.743179,U02TZ1JCVEC\\n1a5b2076-3466-4ad5-a936-e7de8959c6e7,U02U6DR551B,,,\"like this. If you don\\'t set `catchup=True` , your DAG will only run the latest iteration\",1643873524.132099,1643874064.724249,U02UA0EEHA8\\ne9893ba8-a27a-4bd2-88d6-c8835c9d7ba2,U02U6DR551B,,,thanks,1643873524.132099,1643874460.527939,U02U6DR551B\\ncd3c0ced-0557-43c0-8154-80ece3cca416,U02U6DR551B,,,but what is the difference between catchup and backfil?,1643873524.132099,1643874475.511149,U02U6DR551B\\n802e02cf-0c1d-4c2a-af8f-d6083c06bd6f,U02RH0V5K33,,,\"You can install Anaconda, and after what you can use conda installer.\",1642923637.029500,1642925056.032300,U02QP6JM83U\\n94ebe709-8b19-4d9d-9af4-79a526c9b558,,6.0,,please i do a problem running jupyter on my browser. i have done everything possible can someone please help . why is my token not working even after copying and pasting it. am i missing something here?,1642925075.032600,1642925075.032600,U02U9G1P76X\\n305c0773-2f0d-43ce-a8a0-c5fce929272b,U02RH0V5K33,,,in <https://youtu.be/ae-CV2KfoN0|this> video a lot of usefull info,1642923637.029500,1642925220.032900,U02QP6JM83U\\nda1091c3-92a7-44b6-9251-401eef10c377,U02U9G1P76X,,,after running jupyter in terminal which link do you use to open in browser?,1642925075.032600,1642925348.033200,U02QP6JM83U\\ne966cd95-0544-4604-bcae-89d353db4978,U02QP6JM83U,,,Sorry I missed your message. It should be the largest tip for each day. I\\'ll change the form - thanks,1642890600.483500,1642925382.034000,U01AXE0P5M3\\n18822af4-88b5-4bdd-80d9-60319dc87064,U02QP6JM83U,,,oh it seems it was already changed - <@U02QP6JM83U> can you please check if it looks ok now? or maybe I\\'m looking at a different thing,1642890600.483500,1642925439.035700,U01AXE0P5M3\\n767d876e-3c75-4bda-82d6-2dd2c0b659e6,,3.0,,\"Hello everyone, i just subscribed to this data engineering course and saw that introductory videos, can you please brief me about how to setup environment, what kind of application\\'s i need to install now?\\nThanks\",1642925455.036300,1642925455.036300,U02US8E5CLF\\nb9777270-deec-4366-9570-b87028259461,U02QP6JM83U,,,By <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/homework.md|this> link we can see word `Average` as name of question 4. :slightly_smiling_face:,1642890600.483500,1642925665.036400,U02QP6JM83U\\n1894ed35-a936-4f73-ac4d-1a7491b774a8,U02QP6JM83U,,,\"Sorry, maybe it`s ok, but someone can be confused :slightly_smiling_face:\",1642890600.483500,1642925835.036800,U02QP6JM83U\\nf2a2ddc7-5f0a-4feb-895c-82ddf3234955,U02RH0V5K33,,,<@U02QKMCV39R> will wirtual environment solve it?,1642923637.029500,1642925877.037000,U02RH0V5K33\\n89de6c79-1471-4aa0-9eb8-891913552ae8,U02RH0V5K33,,,i am not presently using it,1642923637.029500,1642925890.037200,U02RH0V5K33\\ncf35c511-1d4c-434f-9f00-3eef66fedf69,U02US8E5CLF,,,\"<https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/README.md#environment-setup>\\n\\nthis is the list of things you need to install\",1642925455.036300,1642926008.037400,U01AXE0P5M3\\nf359a45f-bc42-4970-a744-6b24025eb4fc,U02QP6JM83U,,,Got it - thanks! Updated it,1642890600.483500,1642926070.037700,U01AXE0P5M3\\n41885147-FF4A-47DA-B93F-1EB85698AFC2,U02RH0V5K33,,,\"Well i used a virtual env, and did all my pip install there and also working with the virtual env in terminal \",1642923637.029500,1642926473.040200,U02QKMCV39R\\n1c620b39-a9e9-44eb-b0ac-8beb473648f2,U02R09ZR6FQ,,,<@U01AXE0P5M3> that results in this invalid mode,1642891400.489400,1642926579.040400,U02UB8XDCHJ\\n22d66fcf-cea2-4265-859d-bfc58935a108,U02R09ZR6FQ,,,<@U01AXE0P5M3>,1642891400.489400,1642926618.040600,U02UB8XDCHJ\\ncfffb370-a19a-43d5-b5bb-af1fc0592a50,U02RH0V5K33,,,did you experience this error?,1642923637.029500,1642926652.041000,U02RH0V5K33\\ne8e6efb4-66d6-4b3a-9492-51a7a21d5ef5,U02R09ZR6FQ,,,\"Please don\\'t make screenshots, it\\'s very difficult to read them. Copy paste the commands instead\",1642891400.489400,1642926653.041200,U01AXE0P5M3\\n7151AA3B-3FEA-432B-9000-95CB066DBD34,U02RH0V5K33,,,No,1642923637.029500,1642926665.041500,U02QKMCV39R\\n85df00ce-2787-4dac-b69b-849c15a9a5ac,U02RH0V5K33,,,because i am thinking i should start all over again,1642923637.029500,1642926676.041700,U02RH0V5K33\\n9f183f22-8dd4-4334-a2b1-03628338b748,U02R09ZR6FQ,,,\"$ winpty docker run -it  -e POSTGRES_USER=\"\"root\"\"  -e POSTGRES_PASSWORD=\"\"root\"\"  -e POSTGRES_DB=\"\"ny_taxi\"\"  -v d:/zoomcamp/docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data  -p 5432:5432  postgres:13\\ndocker: Error response from daemon: invalid mode: \\\\Git\\\\var\\\\lib\\\\postgresql\\\\data.\\nSee \\'docker run --help\\'.\",1642891400.489400,1642926681.041900,U02UB8XDCHJ\\n7cf82300-dcb6-4d6c-9d26-67dd6d680aa9,U02R09ZR6FQ,,,Can you show how you did it before changing?,1642891400.489400,1642926689.042100,U01AXE0P5M3\\n19ba86f6-9298-4760-8d4b-7953cada926a,U02RH0V5K33,,,did you install miniconda or anaconda?,1642923637.029500,1642926707.042300,U02RH0V5K33\\na873d703-20b7-4cb4-80c1-8edb71363b13,U02RH0V5K33,,,or i should use python venv?,1642923637.029500,1642926739.042500,U02RH0V5K33\\nbc7c3bc5-5014-4858-82d0-f6ff22db2bc9,U02R09ZR6FQ,,,\"I used the below code, which created the folder with the files, but not the volume\\ndocker run -it\\\\\\n  -e POSTGRES_USER=\"\"root\"\"\\\\\\n  -e POSTGRES_PASSWORD=\"\"root\"\"\\\\\\n  -e POSTGRES_DB=\"\"ny_taxi\"\"\\\\\\n  -v //d/data-engineering-zoomcamp/docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data\\\\\\n  -p 5432:5432\\\\\\n  postgres:13\",1642891400.489400,1642926745.042700,U02UB8XDCHJ\\n74f80407-01ec-49c1-99b7-7fe473b9e684,U02US8E5CLF,,,\"Thanks <@U01AXE0P5M3>, pardon me but is there any step by step process, i am completely confused with GCP, SDK, terraform rn. I don\\'t know from where to start.\",1642925455.036300,1642926792.042900,U02US8E5CLF\\ndb8088d7-0f2a-4ac3-b407-36856f9b3da7,U02R09ZR6FQ,,,<@U01AXE0P5M3> I viewed docker volumes in docker desktop and the above code didn\\'t create a volume,1642891400.489400,1642926882.043100,U02UB8XDCHJ\\n8a50ee21-92b0-4bbb-9410-39af9d5a404d,U02RH0V5K33,,,\"Anaconda\\nuse a virtual env\",1642923637.029500,1642927033.043500,U02QKMCV39R\\n9d5e077f-ca70-4cbf-b14a-5ab06d61c572,U02RH0V5K33,,,\"ok thanks, i will restart\",1642923637.029500,1642927062.043700,U02RH0V5K33\\n2b0bb51e-d2ef-4e03-ae43-eaf45d6ea6da,U02US8E5CLF,,,\"1. Install anaconda, and docker desktop in your machine, and follow docker videos. Alexis explains everything in the video.\\n2. There are videos and readme file for GCP and terraform too.\\nAnaconda : Create an environment with python and jupyter installed. This should be enough to get started.\",1642925455.036300,1642927161.044000,U0290EYCA7Q\\nefeb1822-a6a5-41f0-9910-27394ed63884,U02RH0V5K33,,,\"thank you <@U0290EYCA7Q> for your time, i will restart after installing miniconda\",1642923637.029500,1642927266.044300,U02RH0V5K33\\nf231ae51-c752-4c5c-bed0-c23a93cdea42,U02UB8XDCHJ,,,\"<@U0290EYCA7Q> I get the following\\nhp@DESKTOP-THM5GID MINGW64 /d/data-engineering-zoomcamp/docker_sql\\n$ python -c \"\"import psycopg2; psycopg2.connect(database=\\'mydb\\', user=\\'username\\', password=\\'secret\\', port=5432)\"\"\\nTraceback (most recent call last):\\n  File \"\"&lt;string&gt;\"\", line 1, in &lt;module&gt;\\n  File \"\"D:\\\\anaconda3\\\\lib\\\\site-packages\\\\psycopg2\\\\__init__.py\"\", line 122, in connect\\n    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)\\npsycopg2.OperationalError: connection to server at \"\"localhost\"\" (::1), port 5432 failed: FATAL:  password authentication failed for user \"\"username\"\"\",1642914211.023000,1642927373.044700,U02UB8XDCHJ\\n5c5ef18e-b10a-41cf-8b7f-4473c8d945b3,U02UB8XDCHJ,,,\"No - U need to replace mydb, username and password. Just wanted to check if u can connect to db\",1642914211.023000,1642927532.044900,U0290EYCA7Q\\n2f780df0-0280-4552-87f1-ebcbb36c0efb,U02UB8XDCHJ,,,\"I get no output in that case <@U0290EYCA7Q>. It just quits to next prompt\\n$ python -c \"\"import psycopg2; psycopg2.connect(database=\\'ny_taxi\\', user=\\'root\\', password=\\'root\\', port=5432)\"\"\",1642914211.023000,1642927667.045200,U02UB8XDCHJ\\n7fe30824-9fb5-4094-9a9a-3ff1482f73bd,U02UB8XDCHJ,,,You are able to connect then. An issue with pgcli. Can You reinstall and try?,1642914211.023000,1642927740.045500,U0290EYCA7Q\\nc3cdd6ce-d155-4f95-908d-5b876f3f833d,U02UB8XDCHJ,,,<@U0290EYCA7Q> are you able to see the volumes in your docker desktop after running the docker run command?,1642914211.023000,1642927768.045700,U02UB8XDCHJ\\na7ea67be-701a-41b8-afc3-62174f5ab75e,U02R09ZR6FQ,,,Can you try it without double slash? With just one,1642891400.489400,1642927861.045900,U01AXE0P5M3\\n52b29477-d52b-4929-8772-ec0853e157ac,U02UB8XDCHJ,,,\"Yes. I can see it for me, but i didn\\'t use my home directory. I faced different problem. I think you have db installed inside the docker, that\\'s why you were able to connect.\",1642914211.023000,1642927867.046100,U0290EYCA7Q\\n6cdccd56-4702-4e6d-b705-64d20dca904c,U02UB8XDCHJ,,,\"```docker run -it \\\\\\n  -e POSTGRES_USER=\"\"root\"\" \\\\\\n  -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n  -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n  -v ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n  -p 5432:5432 \\\\\\n  --name pg-database \\\\\\n  postgres:13```\",1642914211.023000,1642927983.046400,U0290EYCA7Q\\nd8d83244-b09b-4735-9feb-aa5f5db37542,U02UB8XDCHJ,,,This was my command,1642914211.023000,1642927994.046600,U0290EYCA7Q\\na20c3bf9-eb53-4b4a-9aeb-eb11fadf0a2a,U02U9G1P76X,,,<http://127.0.0.1:8888/?token=276e1cf8e60bc8f3748a9b7f07938efa9c594023e62ac>,1642925075.032600,1642928222.046900,U02U9G1P76X\\na4590aa3-bba8-4db7-9dfb-f0fe09a08555,,5.0,,\"i have an issue after connection my pg admin to postgre , i have been trying to load  this command  pgcli -h localhost -U root -d ny_taxi to see if it works well but this screenshot pops up everytime. what can i do\",1642928489.050200,1642928489.050200,U02U9G1P76X\\nbdcc4441-4f2b-41d4-9365-0d71eae7bab6,U02U9G1P76X,,,how tdo i disable the token or how do i do it that it accepts the token,1642925075.032600,1642928528.050500,U02U9G1P76X\\n35acfc3f-4969-4681-8c23-465ae40ee292,U02UD2P3BQC,,,\"Hi <@U01AXE0P5M3>! By sharing the code publicly, it may lead to a \"\"free rider\"\" issue... How about posting the code in the form?\",1642729727.485900,1642928530.050700,U02UMV78PL0\\n0e9ec1ee-f682-47ae-993b-4cd71985fc61,U02UB8XDCHJ,,,\"hp@DESKTOP-THM5GID MINGW64 /d/data-engineering-zoomcamp/docker_sql\\n$ winpty docker run -it\\\\\\n&gt;   -e POSTGRES_USER=\"\"root\"\"\\\\\\n&gt;   -e POSTGRES_PASSWORD=\"\"root\"\"\\\\\\n&gt;   -e POSTGRES_DB=\"\"ny_taxi\"\"\\\\\\n&gt;   -v ny_taxi_postgres_data:/var/lib/postgresql/data\\\\\\n&gt;   -p 5432:5432\\\\\\n&gt;   --name pg-database\\\\\\n&gt;   postgres:13\\ndocker: Error response from daemon: create ny_taxi_postgres_data;D: \"\"ny_taxi_postgres_data;D\"\" includes inva\\nlid characters for a local volume name, only \"\"[a-zA-Z0-9][a-zA-Z0-9_.-]\"\" are allowed. If you intended to pa\\nss a host directory, use absolute path.\\nSee \\'docker run --help\\'.\",1642914211.023000,1642928639.050900,U02UB8XDCHJ\\n6075ddaa-5f48-4ae8-8938-cc2bbbd7a744,U02UB8XDCHJ,,,<@U0290EYCA7Q> this is what I get now. What mistake am I making?,1642914211.023000,1642928667.051100,U02UB8XDCHJ\\n65fe2178-01e7-4a5a-8dc8-74b51bd57fb0,U02UB8XDCHJ,,,Can you try the full path within quotes? D:/.../.../ny_taxi,1642914211.023000,1642928788.051300,U0290EYCA7Q\\nb023c9f7-01a7-40cb-abbd-52ee7039aaf0,U02TMEUQ7MY,,,<@U02TMEUQ7MY> Did you resolve this?,1642909987.015200,1642928799.051500,U02UB8XDCHJ\\nd49e74d7-11fa-4fdc-8375-b069a2a26dc2,U02UB8XDCHJ,,,and how about giving a space before back slash?,1642914211.023000,1642928866.051700,U0290EYCA7Q\\n5737df83-f0f0-41f8-9790-4e1263008153,U02UB8XDCHJ,,,\"hp@DESKTOP-THM5GID MINGW64 /d/data-engineering-zoomcamp/docker_sql\\n$ winpty docker run -it \\\\\\n&gt;   -e POSTGRES_USER=\"\"root\"\" \\\\\\n&gt;   -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n&gt;   -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n&gt;   -v d:/data-engineering-zoomcamp/docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data\\\\\\n&gt;   -p 5432:5432 \\\\\\n&gt;   --name pg-database \\\\\\n&gt;   postgres:13\\ndocker: Error response from daemon: invalid mode: \\\\Git\\\\var\\\\lib\\\\postgresql\\\\data.\\nSee \\'docker run --help\\'.\",1642914211.023000,1642929122.051900,U02UB8XDCHJ\\nb0a4d309-71e6-4e46-b661-38170da45325,U02UB8XDCHJ,,,\"-v \"\"D:/data-engineering-zoomcamp/docker_sql/ny_taxi_postgres_data\"\": /var/lib/postgresql/data \\\\\",1642914211.023000,1642929239.052100,U0290EYCA7Q\\n07b77cac-90ad-48b6-ac4d-63a2ca5580d2,U02UB8XDCHJ,,,what do u get for this?,1642914211.023000,1642929248.052300,U0290EYCA7Q\\n9f629f56-19fd-4dec-9af0-4c6217065fe6,U02UB8XDCHJ,,,\"<@U0290EYCA7Q> even with the quotes, it is the same\",1642914211.023000,1642929259.052500,U02UB8XDCHJ\\ncc192149-55aa-4c80-9f70-0e8fb72478fc,U02UD2P3BQC,,,\"If somebody copies the code, they will be reported to Santa and won\\'t get a gift this year\",1642729727.485900,1642929308.052700,U01AXE0P5M3\\n56e869fe-9650-497b-9977-977d752a408d,U02UB8XDCHJ,,,where does this GIT come from? \\\\Git\\\\var\\\\lib\\\\postgresql\\\\data,1642914211.023000,1642929406.052900,U0290EYCA7Q\\ne8fb1d5c-0ba4-41ac-8b96-014daa54366d,U02UB8XDCHJ,,,\"<@U0290EYCA7Q> <https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup/2_docker_sql>\\nI am getting the same error as mentioned in the readme file (mentioned at the start)\",1642914211.023000,1642929480.053200,U02UB8XDCHJ\\n05e9b638-4a8a-4fe6-867a-f5694c81df76,U02UB8XDCHJ,,,\"v \"\"D:/data-engineering-zoomcamp/docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data\"\" \\\\ - tried?\",1642914211.023000,1642929525.053500,U0290EYCA7Q\\n8fb10851-cac9-4f62-ac28-a2edb6e28933,U02R09ZR6FQ,,,\"<@U01AXE0P5M3> without the //, it creates a folder called ny_taxi_postgres_data;D, like <@U02QM7H8G4U> mentioned and there is nothing inside the folder. But a volume is created with all the files inside viewable from the docker desktop, but not in my host machine\",1642891400.489400,1642929533.053700,U02UB8XDCHJ\\nccabc193-ea58-4a1a-81cc-87d52c49bca2,U02UB8XDCHJ,,,<@U0290EYCA7Q> the same thing happens,1642914211.023000,1642929590.053900,U02UB8XDCHJ\\nb93690fc-fd94-4888-bebc-25bf3054d019,U02UB8XDCHJ,,,\"Go thru this and try. I don\\'t have a windows machine to confirm. மன்னிக்கவும்.\\n<https://stackoverflow.com/questions/35767929/using-docker-via-windows-console-includes-invalid-characters-pwd-for-a-local-v>\",1642914211.023000,1642929683.054100,U0290EYCA7Q\\n54202aab-0290-44ae-97ae-751b5fabdf2b,U02UB8XDCHJ,,,Again docker container was running for you. That\\'s why you were able to connect using that python statement.,1642914211.023000,1642929784.054400,U0290EYCA7Q\\n4c2a602a-6b3c-4e5a-9a3d-1bb66d5a5db8,U02U9G1P76X,,,\"Which port did you use for forwarding, while running the container?\",1642928489.050200,1642929924.054600,U0290EYCA7Q\\n7d73d0b0-2a6a-429d-a36e-403da84a01e8,U02R09ZR6FQ,,,\"<https://rominirani.com/docker-on-windows-mounting-host-directories-d96f3f056a2c|https://rominirani.com/docker-on-windows-mounting-host-directories-d96f3f056a2c>\\n\\nI  just Googled it. Maybe it\\'ll help?\",1642891400.489400,1642929964.054800,U01AXE0P5M3\\na34b74c7-e00d-4cb5-b6b6-479fc83db3ab,,1.0,,\"Hi. In question about terraform do we have to copy the output after \"\"terraform apply\"\" or after word \"\"yes\"\"? :slightly_smiling_face:\",1642930594.057500,1642930594.057500,U02QL1EG0LV\\na2dceaf3-5cd7-4445-8379-f12aa01c547a,U02QL1EG0LV,,,Copy the whole thing,1642930594.057500,1642930611.057600,U01AXE0P5M3\\nbf1deb94-8e2c-4bd1-9a4d-b4c503b06c9d,U02UD2P3BQC,,,or using a gist),1642729727.485900,1642930668.057900,U02UMV78PL0\\n265ce6f2-f6fc-42d2-a090-1912ddd27ef7,U02U9G1P76X,,,do you have postgres running there?,1642928489.050200,1642930734.058300,U01AXE0P5M3\\n1a0607f9-7a2e-4bcc-93ca-618614730b37,U01AXE0P5M3,,,\"Hi,\",1642534337.023000,1642930822.059100,U02UCV3RAG4\\n14dbdf9e-de35-43c4-9f3f-0975b26a86a6,U01AXE0P5M3,,,\"I have an issue with mounting the local data folder to the docker volume. I am not sure if it’s critical, but it’s a bit irritating.\\nI work on a Windows machine. As per the video instructions, I first created the folder _ny_taxi_postgress_data_\\nThen I run the postgres image:\\ndocker run -it \\\\\\n\\xa0-e POSTGRES_USER=\"\"root\"\" \\\\\\n\\xa0-e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n\\xa0-e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n\\xa0-v /c/Users/…/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgress_data:/var/lib/postgresql/data\\\\\\n\\xa0-p 5431:5432 \\\\\\n\\xa0postgres:13\\n\\xa0\\nAs a result, in my working folder I now have _ny_taxi_postgress_data_ *and* _ny_taxi_postgress_data;_C, both empty.\",1642534337.023000,1642930840.059400,U02UCV3RAG4\\nef6a99c4-d429-48cf-bc43-f2e70b4ea0e8,U02R09ZR6FQ,,,hm it seems quite old - I can\\'t find such settings in my docker desktop,1642891400.489400,1642931562.059900,U01AXE0P5M3\\ne620b146-2d42-44eb-85ca-8f30f98bb4c8,U02TMEUQ7MY,,,Thanks <@U0290EYCA7Q> and <@U02U5GQK25C>. Docker-machine doesnt come with my docker desktop and I was getting some errors installing the executable. Powershell works well.,1642909987.015200,1642932319.060100,U02TMEUQ7MY\\ncfc36376-8709-45db-aa3f-4e62f6aa640a,,1.0,,\"Hi all! As I understand, we need at least Windows 10 to work with Docker:unamused: Windows 7 doesn\\'t have a chance to work with it, does it?\",1642932845.061900,1642932845.061900,U02TCDCDS9H\\nce0f1508-1747-4a24-8ed7-b3daeaf86621,U02R09ZR6FQ,,,\"<@U01AXE0P5M3> yes, it seems it is for windows 10 pro users\",1642891400.489400,1642933190.062000,U02UB8XDCHJ\\n428802b9-4b89-44b4-9abf-9efdeabde205,U02TCDCDS9H,,,or we can run all on GCP on VM with ubuntu and resolve all problems with docker on Windows,1642932845.061900,1642935369.062700,U02ULQFCXL0\\n4cd8d04e-030b-48ca-a098-0d1eac9ce750,U02U9G1P76X,,,\"Hello obiazu, <http://127.0.0.1:8888/|http://127.0.0.1:8888/> \\n\\nClick on that link\",1642925075.032600,1642937663.065100,U02SPLJUR42\\nad6dc5f8-a9f4-4311-aa4c-da8d2ac0ee72,U02U9G1P76X,,,Don\\'t add the token,1642925075.032600,1642937673.065300,U02SPLJUR42\\n4df0f332-e8b3-456d-a294-61d50a0c2a58,,2.0,,\"Hello! For work reasons, I couldn\\'t watch the videos till today, is there a problem if I submit the homework past the deadline?\",1642937890.066800,1642937890.066800,U02TTN6GJJ3\\n4a6919af-359f-49a3-8262-55f289b23931,U02TTN6GJJ3,,,\"We will show the solutions on Monday so I\\'m afraid accepting the homework after that won\\'t be a good idea\\n\\nBut remember that homework is actually optional for passing the course - the most important thing is doing a project\",1642937890.066800,1642938014.066900,U01AXE0P5M3\\n36afd10b-fe28-4964-b080-0e6be765898d,U02TTN6GJJ3,,,\"Ohhh ok ok, thanks <@U01AXE0P5M3>! :raised_hands:\",1642937890.066800,1642938065.067100,U02TTN6GJJ3\\n5158376f-b754-4549-9709-17e34e460817,,,,\"Dear All, I have manually installed Terraform in my Windows system. My Terraform version is v0.12.26.While trying to run terraform init command,it is throwing error as \"\"Unsupported Terraform Core version\"\".\",,1642938587.068900,U02U8QJLZL4\\n23e2202b-e44f-4aaf-b29d-f258b510adaa,,3.0,,\"Detailed error is  -- This configuration does not support Terraform version 0.12.26. To proceed,\\neither choose another supported Terraform version or update the root module\\'s\\nversion constraint.\",1642938603.069200,1642938603.069200,U02U8QJLZL4\\n78f222c0-7118-421d-9f95-571c0d1b16aa,U02U8QJLZL4,,,Is updating terraform not an option?,1642938603.069200,1642938711.069300,U01AXE0P5M3\\n1b19ed26-13ee-4572-a581-790e385c54ca,U02U8QJLZL4,,,\"Thanks for quick response Alexey, I have installed Windows Binary version 1.1.4.This seems to be the latest version.\",1642938603.069200,1642939225.069500,U02U8QJLZL4\\n064b5d2a-1db4-45b9-a9e0-99d1505b05ca,,17.0,,\"Hi there,\\nI keep getting the permission error and can’t come up with a workaround. Did anyone have same issue? MacOS\\n```UTC [41] FATAL:  data directory \"\"/var/lib/postgresql/data\"\" has invalid permissions\\n2022-01-23 11:54:34.121 UTC [41] DETAIL:  Permissions should be u=rwx (0700) or u=rwx,g=rx (0750).```\",1642939459.070600,1642939459.070600,U02ULGHNT33\\n44e30d2a-15e1-4829-a85f-560837daedad,U02ULGHNT33,,,try running `sudo\\xa0chmod\\xa0\\xa0a+rwx /path/to/folder`,1642939459.070600,1642940029.071200,U02ULQFCXL0\\n70c932fb-e097-49f8-86e4-c528ef7446f3,U02U8QJLZL4,,,\"Issue Resolved, my bad. Long time back I installed the earlier version of terraform which was added to the Path of the system variable .This was overriding my current installation.\",1642938603.069200,1642940169.071400,U02U8QJLZL4\\n133e8ceb-81cb-431a-8700-cd75c54a91a0,U02ULGHNT33,,,\"still getting the error with\\n```sudo chmod a+rwx /Users/Documents/PYTHON/2022/DE_Zoomcamp/Docker/ny_taxi_postgres_data```\",1642939459.070600,1642940494.071800,U02ULGHNT33\\n523233b0-4123-4a63-aba1-6b92f73d3d57,U02ULGHNT33,,,Can you try deleting the folder before running docker?,1642939459.070600,1642940530.072000,U01AXE0P5M3\\n42bfc6ff-3180-4370-a6b3-9a1e447a3103,U02ULGHNT33,,,\"the chmod command run successfully but when running docker I get error:\\n```UTC [41] FATAL:  data directory \"\"/var/lib/postgresql/data\"\" has invalid permissions\\n2022-01-23 12:20:09.281 UTC [41] DETAIL:  Permissions should be u=rwx (0700) or u=rwx,g=rx (0750).```\\n\",1642939459.070600,1642940549.072200,U02ULGHNT33\\n1fc7375e-f315-4f2f-9554-b1ddb3516fb6,U02ULGHNT33,,,I deleted the folder then run docker again and keep getting this error. I even see that it first loaded the data to `ny_taxi_postgres_data` but then deleted it and threw a permission exception,1642939459.070600,1642940659.072400,U02ULGHNT33\\n8b780d9c-9e6e-4a7a-8a78-8ff097ac0477,U02ULGHNT33,,,\"here is my bash script:\\n\\n```docker run -it -e POSTGRES_USER=\"\"root\"\" -e POSTGRES_PASSWORD=\"\"root\"\" -e POSTGRES_DB=\"\"ny_taxi\"\" -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data -p 5432:5432 postgres:13```\",1642939459.070600,1642940968.072700,U02ULGHNT33\\n297f3639-ad54-4cee-adef-4b4b5daf5ce1,U02TXGC7WKH,,,that\\'s because the POSTGRES_DB and the folder you specified don\\'t have the same name,1642896501.499200,1642941428.072900,U02U2Q5P61Z\\n90b0d56a-d0f1-4c44-9ea4-eefe62159da9,U02TXGC7WKH,,,so the database shuts down,1642896501.499200,1642941434.073100,U02U2Q5P61Z\\n70a5e904-6c64-4a8c-a344-581316d696e7,U02TXGC7WKH,,,because it\\'s not able to find the name of the POSTGRES DB (ny_taxi) within the folder you specified,1642896501.499200,1642941532.073300,U02U2Q5P61Z\\ne97eaab9-ccbf-49cb-8ae0-51a9f7255b5a,U02ULGHNT33,,,\"I did face this issue, and didn\\'t want to spend time on this right now. So instead of using current directory for volume, I let docker pick its default folder for that.\\n\\n```docker run -it \\\\\\n  -e POSTGRES_USER=\"\"root\"\" \\\\\\n  -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n  -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n  -v ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n  -p 5432:5432 \\\\\\n  --network=pg-network \\\\\\n  --name pg-database \\\\\\n  postgres:13```\\n```(base) % docker volume inspect ny_taxi_postgres_data\\n[\\n    {\\n        \"\"CreatedAt\"\": \"\"2022-01-23T09:34:20Z\"\",\\n        \"\"Driver\"\": \"\"local\"\",\\n        \"\"Labels\"\": null,\\n        \"\"Mountpoint\"\": \"\"/var/lib/docker/volumes/ny_taxi_postgres_data/_data\"\",\\n        \"\"Name\"\": \"\"ny_taxi_postgres_data\"\",\\n        \"\"Options\"\": null,\\n        \"\"Scope\"\": \"\"local\"\"\\n    }\\n]```\",1642939459.070600,1642941602.073500,U0290EYCA7Q\\nccbf2b88-b9c9-4858-b19c-c6f312dfc8cc,U02ULGHNT33,,,\"<@U0290EYCA7Q> thank you for your response. Do I need to specify\\n```  --network=pg-network \\\\\\n  --name pg-database \\\\```\\n\",1642939459.070600,1642942020.074100,U02ULGHNT33\\nfc153834-003b-4ea8-a20c-47f538e61f9a,U02ULGHNT33,,,\"This is for the next video, where we create a network. You can leave it.\",1642939459.070600,1642942111.074400,U0290EYCA7Q\\n860b8840-1636-4e17-9a86-35367a70c662,U02ULGHNT33,,,Thanks it worked! Where can I see the data locally?,1642939459.070600,1642942749.074600,U02ULGHNT33\\nb696f8e7-f5c9-4cbc-a0e8-cbecf873dccb,U02ULGHNT33,,,\"```\"\"Mountpoint\"\": \"\"/var/lib/docker/volumes/ny_taxi_postgres_data/_data\"\",```\\n or docker desktop\",1642939459.070600,1642942889.074900,U0290EYCA7Q\\n5b3efa02-261f-42dd-9d0a-4f4ce896cb9d,U02ULGHNT33,,,I can see it in Docker desktop but not in terminal hmm,1642939459.070600,1642943238.075400,U02ULGHNT33\\n06c7ed56-2373-4944-bcdc-111fc52a01a3,U02ULGHNT33,,,Did you run `docker volume inspect` ?,1642939459.070600,1642943457.075700,U0290EYCA7Q\\n3d9f78ec-6177-4187-b3f1-5923dd3e9a7e,U02TPTXFVQ9,,,Thanks a lot guys! it worked. I faced another error after related to mounting but i just removed the mounting part. It was probably because im using an old version of docker which i sadly have to cuz im using an old version of windows 8 :\\'D,1642870717.391300,1642943734.075900,U02TPTXFVQ9\\ne13d826d-9d57-4b60-b0d5-514a748c365c,U02ULGHNT33,,,\"I don\\'t see it either. It\\'s a symbolic link.\\n\\n```bash-3.2$ ls -ld /var\\nlrwxr-xr-x@ 1 root  wheel  11 Dec  8 05:09 /var -&gt; private/var```\\n\",1642939459.070600,1642943796.076100,U0290EYCA7Q\\nf39f6fdc-8c23-4839-8f55-33ebe0a254a9,U02ULGHNT33,,,okay thanks a lot!,1642939459.070600,1642943857.076900,U02ULGHNT33\\n0742bff6-0374-42c0-91d4-a95329bc10b9,,4.0,,\"hi there, does anyone know at which point Alexey created the ny-rides.json file? got a bit lost...\",1642943928.078000,1642943928.078000,U02UNCG01PT\\n4bb2b898-c777-42d8-98fa-cb6a3df276c1,U02UNCG01PT,,,check this <https://www.youtube.com/watch?v=Hajwnmj0xfQ&amp;list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&amp;index=10>,1642943928.078000,1642944279.078100,U02ULQFCXL0\\n4a67f3cb-eefa-4f2d-973b-1c94b4e999fd,U02UNCG01PT,,,and this <https://www.youtube.com/watch?v=dNkEgO-CExg&amp;list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&amp;index=11>,1642943928.078000,1642944297.078400,U02ULQFCXL0\\na8cc3ef6-9758-4bcc-8c7e-cbf130e9fc2a,U02UNCG01PT,,,Thanks!!,1642943928.078000,1642945756.079000,U02UNCG01PT\\n4175ea72-8226-4edb-af68-0dc45a26d429,,12.0,,\"I have a silly doubt but want to get clarify, can someone explain me from where are we getting the ny_taxi data in postgres. Is this the default database provided by postgres?\",1642947477.080500,1642947477.080500,U0254S545D5\\n2EAD89E4-2A82-448F-9D0B-84D0FB3A4FBE,U0254S545D5,,,Nope we are creating that database in postgres then uploading data in there. The data comes from .csv file which you would need to download it.,1642947477.080500,1642947730.083000,U02UBV4EC8J\\n8683c82a-7898-4ee2-bde8-3cc05744688b,U0254S545D5,,,\"```url=f\\'<postgresql://root:root@localhost:5432/ny_taxi>\\'\\nengine = create_engine(url, client_encoding=\\'utf8\\')\\nprint(pd.io.sql.get_schema(df, name=\\'yellow_taxi_data\\', con=engine))```\\n\",1642947477.080500,1642947743.083200,U0290EYCA7Q\\nc1bb224e-9905-4716-ab23-618a3de44e13,U0254S545D5,,,\"Thanks for your response. But I would like to understand,\\nI just executed the below command on my mac terminal, so from where did it get the data.\\n```docker run -it \\\\\\n    -e POSTGRES_USER=\"\"root\"\"\\\\\\n    -e POSTGRES_PASSWORD=\"\"root\"\"\\\\\\n    -e POSTGRES_DB=\"\"ny_taxi\"\"\\\\\\n    -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n    -p 7000:5432 \\\\\\n    postgres:13```\",1642947477.080500,1642948069.084000,U0254S545D5\\nc9178c32-d3e5-4da9-bfcd-413c6c68237d,U02U34YJ8C8,,,add `--rm` into the command :slightly_smiling_face:,1642630680.235000,1642948089.084200,U02U06KSE3G\\n88899a8f-1655-4d3f-9eaa-93e734bb16cf,U0254S545D5,,,the folder var/lib/postgresql/data..where does it exists?,1642947477.080500,1642948112.084400,U0254S545D5\\n43501a0f-5806-4242-a009-2f94b98888a8,U0254S545D5,,,\"you are saying that in this param \"\"-e POSTGRES_DB=\"\"ny_taxi\"\" to create the database if it\\'s not created already\",1642947477.080500,1642948184.084600,U02T2JGQ8UE\\n5424236e-c953-451d-9d8a-9a71e75c4077,U0254S545D5,,,\"\"\"the folder var/lib/postgresql/data..where does it exists?\"\" =&gt; after : you are telling docker the folder inside de container to save the data that is mapped with your local folder in $(pwd)/ny_taxi_postgres_data\",1642947477.080500,1642948241.084800,U02T2JGQ8UE\\n7319d4c8-aa3a-498f-bd7d-738f61fac306,U0254S545D5,,,local_folder_path:container_path_folder something like that,1642947477.080500,1642948271.085000,U02T2JGQ8UE\\nb9b0bb81-121d-4f5d-a4c6-1ae06fed2f8b,U02U06KSE3G,,,\"Yeah so I run the container like this from inside the folder with my csv (i\\'m on windows 10 as to why I have `^` instead of `\\\\`\\nfor multiline commands)\\n\\n`docker run -it ^`\\n\\xa0 `-e POSTGRES_USER=\"\"root\"\" ^`\\n\\xa0 `-e POSTGRES_PASSWORD=\"\"root\"\" ^`\\n\\xa0 `-e POSTGRES_DB=\"\"ny_taxi\"\" ^`\\n\\xa0 `-v %cd%\\\\pg_data:/var/lib/postgresql/data ^`\\n\\xa0 `-v %cd%:/data ^`\\n\\xa0 `-p 5432:5432 ^`\\n\\xa0 `--network=pg-network ^`\\n\\xa0 `--name pg-database ^`\\n\\xa0 `--rm`\\n\\xa0 `postgres:13`\",1642633799.243900,1642948303.085400,U02U06KSE3G\\n0d0614df-18e7-4cce-b961-3c53d7321fdd,U02U06KSE3G,,,and with the csv mounted I can use `COPY FROM` which is pretty much always faster for bulk inserts.,1642633799.243900,1642948358.085600,U02U06KSE3G\\ndb6f6000-0705-4b26-ba37-b3de46ab2cd6,U0254S545D5,,,\"<@U02T2JGQ8UE> thanks for answering,  after executing the above cmd, the folder ny_taxi_postgres_data got created in my local (I was not having that before)  and it got the files inside.\\nSo in simple terms, I am asking docker to create the the db and I am also giving username password to it by this\\n```  -e POSTGRES_USER=\"\"root\"\"\\\\\\n    -e POSTGRES_PASSWORD=\"\"root\"\"\\\\\\n    -e POSTGRES_DB=\"\"ny_taxi\"\"\\\\```\\nand with this,\\n```-v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\nI am saying docker to copy the data from local folder (ny_taxi_postgres_data) to container (/var/lib/postgresql/data \\\\)```\",1642947477.080500,1642948559.085900,U0254S545D5\\nac35af95-55ad-4d68-808c-5ba21b616f0c,U0254S545D5,,,\"so my confusion is where there is no such folder in my local system, then from where is it getting the data\",1642947477.080500,1642948598.086200,U0254S545D5\\n6d7e88a5-1cf5-4800-bf21-1d9b115f1e4e,U0254S545D5,,,\"try running `docker volume ls` and `docker volume inspect ny_taxi_postgres_data`. Unfortunately, I can\\'t cd into the folder, but able to see the data generated.\",1642947477.080500,1642948760.086400,U0290EYCA7Q\\n9e5b0f27-7cf9-48df-ac4c-cb5312208e25,U0254S545D5,,,<@U0254S545D5> if you are looking for the folder_path in you local machine its created wherever you are executing the comand because you are using ${pwd} command that gives your current position. So if I\\'m in folder_1 it\\'ll be created in folder_1/ny_taxi_postgres_data,1642947477.080500,1642948784.087000,U02T2JGQ8UE\\ndc40e392-6c18-4519-a606-6fba4f63a6d9,,7.0,,\"Hi All, for the homework submission question no.2, we just copy the last line output after run the *terraform apply* command?\\n\\nBecause I saw that there are 4 lines of output after I run the command\",1642949313.088900,1642949313.088900,U02T697HNUD\\n4c25c7a5-28c1-402f-a82b-f6ccd874be39,U02T697HNUD,,,all lines of output,1642949313.088900,1642949491.089400,U02ULQFCXL0\\ncec47b44-b5c0-4c3a-9e30-ac1d6ec6d52d,U02T697HNUD,,,Thanks for the reply <@U02ULQFCXL0> :grin:,1642949313.088900,1642949556.089600,U02T697HNUD\\n00d06a2a-527c-470e-a3b1-60af157af6c2,U0254S545D5,,,\"Thank you all for answering. I also googled and it seems this data is available in PostgreSQL database.\\nexample: <https://github.com/toddwschneider/nyc-taxi-data>\\n\\nplease correct me if I am wrong!\",1642947477.080500,1642949570.089800,U0254S545D5\\n664142c3-cb87-4dea-beca-d554bfcb93c1,U02T697HNUD,,,and it seems there is a lot more than 4 lines,1642949313.088900,1642949714.090400,U02ULQFCXL0\\nc495b367-2e8c-4f6c-8f53-98a669e3f015,U02QKMCV39R,,,i DID,1642873331.407000,1642949754.090600,U02QKMCV39R\\n0a5e60de-fba4-4b55-b350-ca511af3f968,U02T697HNUD,,,oh no I have submitted the google form. Is there any possible I can edit the submission?,1642949313.088900,1642949802.090800,U02T697HNUD\\n6e57f833-1cc6-4941-b0c1-4e1766b163b0,U02T697HNUD,,,you can send another submission,1642949313.088900,1642949892.091000,U02ULQFCXL0\\n99afaef4-d7c0-4a87-a6df-369d5345aa60,U02T697HNUD,,,\"in `homework.md`file we can see this:\\n```You can submit your homework multiple times. In this case, only the last submission will be used.```\",1642949313.088900,1642949966.091200,U02ULQFCXL0\\n254e98a7-05a8-473e-a28c-c4c244e371c0,U02T697HNUD,,,Alright that\\'s cool. I will submit again.,1642949313.088900,1642949981.091400,U02T697HNUD\\ndda23c0f-bc0d-487b-b0b9-d74635d1e579,,,,FYI: This page helps a lot when you need to check some examples as well as theory for PostgreSQL <https://www.postgresqltutorial.com/>,,1642952641.095400,U02T2JGQ8UE\\nefa6bee0-4093-4c85-965a-c87d748dec8b,,3.0,,\"hi i have a question regarding docker compose. how does it know which table it will save the data to, in this case  (yellow_taxi_trips) since in yaml file we did not specify we table we will save to\",1642953011.097900,1642953011.097900,U02U5GQK25C\\n435871c4-5b03-4c6f-836e-1b97932624b6,,5.0,,\"Hi, I\\'m trying to understand the idea behind the example <@U01AXE0P5M3> demonstrated in his SQL tutorial video...\",1642953824.107800,1642953824.107800,U02SBTRTFRA\\n488b61d9-1e3f-4311-95a5-7267b98f3fc4,U02SBTRTFRA,,,\"Why did he use the \\'zones\\' table twice:\\n`...\\nFROM \\n      yellow_taxi_trips t,\\n      zones zpu,\\n      zones zdo\\nWHERE ...`\\n\\nThere\\'s only one \\'zones\\' table, therefore there\\'s only one zones.\"\"LocationID\"\" column; why wasn\\'t it just used directly like this? Am I missing something?\\n\\n`...\\nFROM \\n      yellow_taxi_trips t,\\n      zones z,\\nWHERE ...`\",1642953824.107800,1642953838.107900,U02SBTRTFRA\\n8aff86f5-58d3-44b7-99d4-8b8c970a9a95,U02SBTRTFRA,,,\"One for pick up, another for drop off\",1642953824.107800,1642953857.108100,U01AXE0P5M3\\n0b6342b0-6d4b-4ab0-aa7b-85d344876f37,U02SBTRTFRA,,,\"Thank you.\\nI understand that, I guess I\\'m really just know if doing it the second way i showed would yield a different result than yours did.\",1642953824.107800,1642953965.108400,U02SBTRTFRA\\n42c4b2f6-63ad-4e8b-833b-3c5b255f710f,U02SBTRTFRA,,,You can run it and see what happens =),1642953824.107800,1642954061.109100,U01AXE0P5M3\\n7e56656a-c376-4978-bb77-772ae1a3c279,U02U5GQK25C,,,\"Didn\\'t we build a new image taxi_ingest:v001 for that? Docker Compose is just for staring pg-admin and pg-database containers, in the same network.\",1642953011.097900,1642954152.109700,U0290EYCA7Q\\n9ba09e1b-1403-4653-b4df-b36df1397a03,U02SBTRTFRA,,,\"Okay, I will.\\nThank you\",1642953824.107800,1642954177.109900,U02SBTRTFRA\\ne9a2d776-abce-4e71-b69f-8a7b1e100f9b,U02U5GQK25C,,,\"as <@U0290EYCA7Q> said, docker-compose set up the infrastructure, then we create the table through python script, specifically in this line: \"\" _df.head(n=0).to_sql(name=table_name, con=engine, if_exists=\\'replace\\')_ \"\" and then we are just saying the table name that we are going to insert the data with this variable: _table_name_ and this line of code:  _\"\" <http://df.to|df.to>_sql(name=table_name, con=engine, if_exists=\\'append\\') \"\"_\",1642953011.097900,1642954305.110400,U02T2JGQ8UE\\n42718f2a-85be-4c7b-9805-f98dbc4adcc2,U02U5GQK25C,,,\"When I ran docker-compose up, it created a network called hw_default.\\n\\n```(data-engineering-zoomcamp) MacBook-Air hw % docker network ls\\nNETWORK ID     NAME         DRIVER    SCOPE\\n6de0a5578bb4   bridge       bridge    local\\n98d0decc41ae   hw_default   bridge    local```\\nInspecting network, you can see two containers attached to the network hw_default. Those were created as part of docker compose.\\n\\n```(data-engineering-zoomcamp) MacBook-Air hw % docker network inspect hw_default\\n[\\n    {\\n        \"\"Name\"\": \"\"hw_default\"\",\\n        \"\"Containers\"\": {\\n            \"\"2adf851f637e88e4d3164bc2a84d9ab556887e411e4a0d880fca084cd4e99fe8\"\": {\\n                \"\"Name\"\": \"\"pg-database\"\"\\n            },\\n            \"\"a755abb83c3a1ccf661b21af45d315665fb072bfa942ec952786ecd29b36cc0c\"\": {\\n                \"\"Name\"\": \"\"pg-admin\"\",\"\"\\n            }\\n        },\\n        \"\"Options\"\": {},\\n        \"\"Labels\"\": {\\n            \"\"com.docker.compose.network\"\": \"\"default\"\",\\n            \"\"com.docker.compose.project\"\": \"\"hw\"\",\\n            \"\"com.docker.compose.version\"\": \"\"2.2.3\"\"\\n        }\\n    }\\n]```\",1642953011.097900,1642955004.114700,U0290EYCA7Q\\n23e5c2bf-818a-48f5-8cb4-bb3b2b92ad0e,,2.0,,\"If someone has the same problem as me, try to map 5433 port to 5432, like (5433:5432). Hope it will save your time\\n\\nUPD: issue description\\ndocker container was up, pg image into container worked fine, but i couldn\\'t reached db from cmd (by pgcli) and through sqlalchemy (getting blank OperationalError)\",1642955680.118200,1642955680.118200,U02U0L55BNU\\nf27e64f0-57e6-4e33-9be3-40423679a2ae,U02U0L55BNU,,,Please paste the error.,1642955680.118200,1642955750.118400,U0290EYCA7Q\\n3bafab63-4ebb-4421-944f-f435653e6db7,U02UM74ESE5,,,\"Guys, I tried removing the spaces and it worked but now getting error at next step when I run rest of the code:\\n```python ingest_data.py \\\\\\n    --user= root \\\\\\n    --password= root \\\\\\n    --host= localhost \\\\\\n    --port= 5432 \\\\\\n    --db= ny_taxis \\\\\\n    --table_name= yellow_taxi_data \\\\\\n    --url= ${URL}```\\nError I get is:\\n```AMEYs-MacBook-Air:2_docker_sql amey$ python ingest_data.py --user= root\\xa0\\xa0--password= root \\xa0\\xa0--host= localhost \\xa0\\xa0--port= 5432 \\xa0\\xa0--db= ny_taxis \\xa0\\xa0--table_name= yellow_taxi_data \\xa0\\xa0--url= ${URL}\\n\\xa0File \"\"ingest_data.py\"\", line 23\\n\\xa0\\xa0os.system(f\"\"wget {url} -O {csv_name}\"\")\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0^\\nSyntaxError: invalid syntax\\nAMEYs-MacBook-Air:2_docker_sql amey$```\\n\",1642898827.001000,1642955904.118900,U02UM74ESE5\\ne836ad67-400f-4d2c-9ef3-fd4a52d889d2,,2.0,,\"I tried ingesting data into the postgres database by running the python script on my terminal. But I got this error\\n```usage: ingest_taxi_data.py [-h] --user USER --password PASSWORD --host HOST\\n                           --port PORT --db DB --table_name TABLE_NAME --url\\n                           URL\\ningest_taxi_data.py: error: the following arguments are required: --host, --port, --db, --table_name, --url\\nzsh: command not found: --host=localhost\\nzsh: command not found: --db=ny_taxi```\\nAny help please\",1642956416.122200,1642956416.122200,U02TZ1JCVEC\\n04892fbf-398c-4dbc-a8bc-56303e34b942,U02TZ1JCVEC,,,\"```URL=\"\"<https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>\"\"\\n\\npython ingest_data.py \\\\\\n  --user=root \\\\\\n  --password=root \\\\\\n  --host=localhost \\\\\\n  --port=5432 \\\\\\n  --db=ny_taxi \\\\\\n  --table_name=yellow_taxi_trips \\\\\\n  --url=${URL}```\",1642956416.122200,1642956517.122300,U0290EYCA7Q\\nab46160f-6d30-4cb8-8d76-18c3c9c4c9e3,U02TZ1JCVEC,,,It is loading now. Thank you,1642956416.122200,1642956924.122700,U02TZ1JCVEC\\nb56ccc2d-7fe7-4288-bf2d-db8235cc15c9,,3.0,,\"*Data Talk Club: Data Engineering Course - Week 1 Summary:*\\n• Data Engineering Tools Setup and Configuration - Git, Docker, Docker Compose, Postgres Database, Python, VS Code, Jupyter Notebooks, Terraform, GCP Account with Key services (GCS, BigQuery, Service Accounts etc.)\\n• Quick walkthrough of Datasets: NY Taxi Trips and Zone datasets\\n• Developed data ingestion scripts to download the datasets and loaded onto Postgres database using Python\\n• Containerized data ingestion scripts using Docker and Docker compose\\n• Executed Terraform Scripts to setup and configure the key services in GCP Account\\n• Developed various SQL Queries for different scenarios using the datasets loaded onto Postgres Database <@U01AXE0P5M3> &amp; Team thanks for all your efforts.\",1642957113.123500,1642957113.123500,U02S4A9TA5Q\\nb57be3f8-6780-46e1-89e3-5b9d8fc68b20,U02AUCL9ZQF,,,Guide me on this Kindly,1642748175.026300,1642957330.125100,U02AUCL9ZQF\\n319e4970-4306-4d86-ab19-39c434abb175,U02AUCL9ZQF,,,I\\'ve tried but the ones I\\'m getting gives me an error,1642748175.026300,1642957348.125400,U02AUCL9ZQF\\n053747bc-106b-4a61-934f-f21b4be9c8c3,,1.0,,\"<@U01AXE0P5M3> As week 1 is nearing its end and after much reflection, I thought I would share some constructive feedback. First, thank you so much for putting together this course, with your active involvement in Slack and the community spirit, the learning experience has been nothing short of amazing! You have done a great job walking us through the basics. I just loved how you took us from hello-world to a complete solution without having everything pre-created and in working order. Personally, coding along your videos is where all the learning happened, I was able not only to absorb the materials but also learnt a lot from your thought process (i.e., resolving issues, putting together a solution etc.).  I think showing the thought process is extremely valuable as this is how students will become more independent and start creating their own solutions and not only replicate a template, especially in a field where it all boils down to solving problems. This is why, it\\'s important to keep this format for the subsequent modules. The terraform part was a bit let down, it was basically a quick overview of the already created files, not so much learning tbh but it was easy enough to supplement with external resources and practice on my own. For future modules, I just hope we would get our hands as dirty as we did this week, it was a lot of headaches but tons of learning. Again, thank you so much for this opportunity and kudos to all the people who helped their classmates move forward! :raised_hands:\",1642957370.125700,1642957370.125700,U02UX664K5E\\nb0160eb5-a4d2-4c6b-a994-3b11b3ef8b82,U02RTJPV6TZ,,,so what\\'s the right way to write the code? could you please share the complete solution?,1642488820.397100,1642957422.125900,U02QC5NN95M\\nd942bfa0-6481-4baf-962b-16d768b40728,U02UX664K5E,,,Thank you for your feedback and I\\'m happy to hear you\\'re enjoying it,1642957370.125700,1642958307.126800,U01AXE0P5M3\\nfc2a28f7-e1df-4125-a465-222cc9e8cc4e,,3.0,,\"After submitting the Form for Homework 1 is there anywhere we can check to make sure it was submitted successfully? Not how many points we received, but just to confirm it was submitted?\",1642959082.132700,1642959082.132700,U02TNEJLC84\\nfd81d08e-8f3a-4db1-9143-3ec4a062a6b0,,2.0,,\"Can, please, someone explain why we install pandas in docker image by using command \"\"... --entrypoint = bash python:3.9\"\"? Why didn\\'t we install pandas when we simply run `docker run -it python:3.9` ?\",1642959093.133100,1642959093.133100,U02TCDCDS9H\\n0cec5616-e7e4-4805-a116-38c29da7be90,,3.0,,\"Has anyone come across a good resource for understanding how pip, conda, and brew interact?\",1642959093.133200,1642959093.133200,U02CPBEH42W\\ne80ab209-3493-4d1e-8001-8b15f052a592,U02TNEJLC84,,,\"You can submit, as many time you want.\",1642959082.132700,1642959111.133600,U0290EYCA7Q\\nf4a1f606-a84f-4bbf-aee4-bcdce46e0815,,13.0,,\"Hi everyone, my table in Pgadmin suddenly has no data.  Posting more details below.\",1642959215.135400,1642959215.135400,U02UAADSJ84\\n239ff1e3-4d95-4194-b69e-58ff67b3e166,U02UAADSJ84,,,\"And when I try to add data to my postgress database it gives me this error: \\'sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name \"\"pg-database\"\" to address: Temporary failure in name resolution\\'\",1642959215.135400,1642959272.135500,U02UAADSJ84\\nb1650d83-b3bf-4104-901f-d88b8bc17cca,U02UAADSJ84,,,\"I am running this script to add data:                  *docker run -it --network=pg-network taxi_ingest:v001 --user=root --password=root --host=pg-database --port=5431 --db=ny_taxi --table_name=yellow_taxi_data --url=\"\" <https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>\"\"*\",1642959215.135400,1642959327.136500,U02UAADSJ84\\n5f994d76-2592-41eb-8771-29d07eb7545a,U02TNEJLC84,,,\"It was submitted, don\\'t worry =)\",1642959082.132700,1642959359.137400,U01AXE0P5M3\\nAD24AA25-50C9-4CF4-9F91-1E44027A1C69,U02UAADSJ84,,,\"Is this after running “docker-compose up”? If so, can you post the contents of your YAML file?\",1642959215.135400,1642959375.138200,U02U34YJ8C8\\n58f96850-b1c9-4a1d-b5b5-873653131c44,U02TCDCDS9H,,,\"Because you can\\'t do it from Python, you need to do it from shell\",1642959093.133100,1642959393.138600,U01AXE0P5M3\\ncbb1bff1-3d21-4b17-9178-9cc13c306348,U02S4A9TA5Q,,,Can i make use of this in my documentation,1642957113.123500,1642959413.139300,U02T0CYNNP2\\n4d2867d0-e8ae-4407-aea0-1ed432a3c93a,U02TCDCDS9H,,,\"Well... you sort of can, but it\\'s better to do it from shell\",1642959093.133100,1642959415.139500,U01AXE0P5M3\\n98825c25-5cd9-47f2-8ed5-39eed99a9bb4,U02TNEJLC84,,,Thanks <@U01AXE0P5M3> I\\'m always a little OCD. :face_with_rolling_eyes:,1642959082.132700,1642959419.139900,U02TNEJLC84\\n7d1a811f-2bd2-4e6a-ad38-03220e3a829a,U02UAADSJ84,,,\"Yes, this happened after running docker-compose up. here is the YAML file:\",1642959215.135400,1642959462.140900,U02UAADSJ84\\nd8759f2f-2eba-4bdf-9402-142a1a4f23b5,U02UAADSJ84,,,\"I think in the yaml the name is different, it doesn\\'t have the hyphen\",1642959215.135400,1642959472.141400,U01AXE0P5M3\\nc9e628f6-d42f-4788-adec-c8ee9e47c429,U02UAADSJ84,,,\"Yes here you go, no hyphen\",1642959215.135400,1642959481.141800,U01AXE0P5M3\\n963276d8-facb-4158-ab43-2c5121651469,U02CPBEH42W,,,\"Could you elaborate on what you mean by \"\"interact?\"\"\",1642959093.133200,1642959523.142600,U02TNEJLC84\\nD72FA0C4-7B11-4D3C-99EB-3AC4547EBBEA,,5.0,,\"```pip3 install pgcli```\\nWorked for me on mac instead of \\n```pip install pgcli```\\nIs that correct?\",1642959566.145000,1642959566.145000,U02T9VA6VJS\\n5B0A46CB-A796-4F89-A2CD-250775902008,U02UAADSJ84,,,\"Yea change “pg-database” in your docker run command to “pgdatabase”\\n\\nYou might also need to change your network name, but do the above first.\",1642959215.135400,1642959588.145600,U02U34YJ8C8\\n873456d1-9a42-4cd4-9eb5-e0a8e9ee90b8,,26.0,,\"When trying pgcli -h localhost -p 5432 -u root -d ny_taxi,  I am seeing\\n\\nconnection to server at \"\"localhost\"\" (::1), port 5432 failed: FATAL:  role \"\"root\"\" does not exist\\n\\nIs this something I need to fix using pgadmin?\",1642959774.147300,1642959774.147300,U02CPBEH42W\\nba333c03-ce7d-4620-bb18-a8a1f2074c54,U02T9VA6VJS,,,\"As long as it works, I\\'d say you\\'re good. pip is just a link to an installer.\",1642959566.145000,1642959786.147400,U02TNEJLC84\\na1f6822e-81d4-449b-a928-e6b65b7c843a,U02CPBEH42W,,,Use a capital -U,1642959774.147300,1642959799.147600,U02TNEJLC84\\nacc6fe7a-bbe3-4334-a9d6-6be257f35045,U02T9VA6VJS,,,That\\'s okay.,1642959566.145000,1642959802.147800,U0290EYCA7Q\\ne0ccf11c-61d5-46a4-9cd8-b61b0cb23570,U02CPBEH42W,,,\"Unfortunately, I still get the same result using a -U\",1642959774.147300,1642959837.148200,U02CPBEH42W\\n4b0b2d8a-3227-4fd3-9d85-3730823b3297,U02CPBEH42W,,,\"I’m not sure I understand the question, but you can take a look at my gist for a Conda and pipenv reference\\n\\n<https://gist.github.com/ziritrion/8024025672ea92b8bdeb320d6015aa0d>\",1642959093.133200,1642959849.148400,U02BVP1QTQF\\nca428125-8770-414c-acc3-406d9ee3b036,,3.0,,\"Hi!\\nI may be paranoid, but I have a doubt about honework Question 2.\\nI understand that we have to copy the entire output of the command. But this output contains google project id. How safe is it to include project id in the answer? I\\'d prefer not to do that. Can I edit the output? For example, replace the numeric part of the ID with an ellipses?\",1642959925.148900,1642959925.148900,U02U85H3W10\\n60ecf1da-7d47-4267-8bbb-8671f550fc3a,U02U85H3W10,,,You can replace it with something else,1642959925.148900,1642959943.149000,U01AXE0P5M3\\n89316978-3345-4afc-ac2e-3a5798f3c1b7,U02CPBEH42W,,,\"Huh. I know you pasted your command in the original message, but can you post what you are using now with the capital \"\"-U\"\"?\",1642959774.147300,1642959983.149200,U02TNEJLC84\\nf6dc454f-a473-4326-8deb-44c4646d4eac,U02U85H3W10,,,The important part here is that you created the environment and ready for the next weeks,1642959925.148900,1642960017.149400,U01AXE0P5M3\\n146583b8-cf57-41d2-abe5-c81ddde2aa14,U02CPBEH42W,,,\"Regarding the differences between them:\\n\\npip is a package installer for Python. I believe that the latest versions handle package dependency better, but historically you could have issues when installing packages. Also, pip does not provide virtual environment management.\\n\\nconda is both a package and virtual environment manager. It works well but is quite heavy. For virtual environment management there are other tools such as pipenv and poetry which are more lightweight and offer some additional advantages, but they’re not package installers.\\n\\nbrew is a general purpose package manager for MacOS which has recently become available for Linux as well. It allows you to install regular apps from the command line. Some apps such as `pgcli` which can be installed with pip are also available from brew. However, when installed with brew, it will run using whatever Python version is present in your path rather than a specific virtual environment.\",1642959093.133200,1642960020.149600,U02BVP1QTQF\\n71b8a47b-fbf2-4333-8d60-5606c92972df,U02U85H3W10,,,Thanks!,1642959925.148900,1642960054.149800,U02U85H3W10\\na0dee373-b7a4-4352-8db0-726de2a449ce,U02T9VA6VJS,,,\"It may mean that you could be using system Python and if something breaks, you system may be affected. I\\'d suggest to use anaconda\",1642959566.145000,1642960085.150000,U01AXE0P5M3\\n52f899bd-8e7b-42d8-bef2-1bd4a0404c83,U02T9VA6VJS,,,\"But yes, it\\'s okay as long as you manage to install everything you need\",1642959566.145000,1642960128.150200,U01AXE0P5M3\\n7477b923-68ad-4aee-9ad0-31c4a5dcc4b7,U02UM74ESE5,,,\"which python version are you using? try to run the command with python3 instead of python.\\n```python3 ingest_data.py ...... here the rest```\",1642898827.001000,1642960131.150400,U02TEKL21JQ\\n76a8c512-6491-41ae-8ba9-da5957c75c96,U02CPBEH42W,,,\"pgcli -h localhost -p 5432 -U root -d ny_taxi\\nconnection to server at \"\"localhost\"\" (::1), port 5432 failed: FATAL:  role \"\"root\"\" does not exist\",1642959774.147300,1642960270.150700,U02CPBEH42W\\nb3ba0724-431d-45ce-ba38-dd545e93d8fc,U02UAADSJ84,,,\"I did but it still gives me this error: \\'sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name \"\"pgdatabase\"\" to address: Temporary failure in name resolution\"\"\\'\",1642959215.135400,1642960318.150900,U02UAADSJ84\\ne97fac8e-3b53-40c2-b6f7-a9cd63755e54,U02CPBEH42W,,,Did some quick testing. If postgres wasn\\'t running you\\'d get a different message. That leads me to believe there may be a typo in your docker command. Can you also share the docker command you are using to run Postgres?,1642959774.147300,1642960366.151100,U02TNEJLC84\\n28a6e628-f181-48bb-b6f6-1a883b7fc15a,U02CPBEH42W,,,\"docker run -it \\\\\\n  -e POSTGRES_USER=\"\"root\"\"\\\\\\n  -e POSTGRES_PASSWORD=\"\"root\"\"\\\\\\n  -e POSTGRES_DB=\"\"ny_taxi\"\"\\\\\\n  -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n  -p 5432:5432 \\\\\\n  postgres:13\",1642959774.147300,1642960395.151300,U02CPBEH42W\\n722e0bcc-0b5f-4452-978a-0f3b39ba1c56,U02CPBEH42W,,,I am running on a mac,1642959774.147300,1642960419.151500,U02CPBEH42W\\nffde6331-8226-41b5-a873-ab68a0ee4fcb,U02CPBEH42W,,,\"Hard to tell and I\\'m not familiar with mac too much, but can you try adding a space at the end of every line?\",1642959774.147300,1642960490.152000,U02TNEJLC84\\nf8c09bd7-355d-486d-bd2b-42545654ee7d,U02CPBEH42W,,,\"```-e POSTGRES_USER=\"\"root\"\" \\\\```\\ninstead of\\n```-e POSTGRES_USER=\"\"root\"\"\\\\```\",1642959774.147300,1642960525.153100,U02TNEJLC84\\n37fe2c5f-72bc-4bbf-b9a6-fb83233775c9,U02UAADSJ84,,,Do I need to change the network name too?,1642959215.135400,1642960684.155900,U02UAADSJ84\\n772c80ad-8043-46fc-8558-680b05c3023b,U02CPBEH42W,,,\"I stopped the docker container and restarted using the spaces before the back slashes, but I still get the same error\\npgcli -h localhost -p 5432 -U root -d ny_taxi\\nconnection to server at \"\"localhost\"\" (::1), port 5432 failed: FATAL:  role \"\"root\"\" does not exist\",1642959774.147300,1642960732.156100,U02CPBEH42W\\n7902a520-05fb-4ce8-a26f-db341cee30b1,U02UAADSJ84,,,\"I’m not an expert on this but I think it could be because of the network name.\\n\\nIn the YAML file we never specified a network name. We initially used `pg-network` as the network name, but that’s now obsolete.\\n\\nBecause we never specified a network name in the YAML file, docker will have created a network and given it a default name.\\n\\nTo find that name, run the below command:\\n\\n`docker network ls`\\n\\nThis will list all running networks. Assuming you’ve already run `docker compose up` you should see a network name in there, probably with ‘SQL’ in the name. Use this in place of `pg-network`\",1642959215.135400,1642960799.156300,U02U34YJ8C8\\n304c48c0-a949-4b68-926d-6170813ced86,U02UAADSJ84,,,\"I’m not 100% confident what I said is correct, but it’s been my understanding of it.\",1642959215.135400,1642960859.156500,U02U34YJ8C8\\n6658b458-f918-4dfe-92f3-930dc2467b11,U02U9G1P76X,,,Thanks let me try it,1642925075.032600,1642961216.157300,U02U9G1P76X\\n2FD26085-D346-4C52-BE5F-DDCE16B2E4B6,U02CPBEH42W,,,\"Maybe try stopping and then removing the container completely, then running the container once more. Send through the output \",1642959774.147300,1642961243.158400,U02U34YJ8C8\\n12087aa1-f88c-4d78-8a78-643536c976ba,U02CPBEH42W,,,\"Also, did you install pgcli with pip or brew? I originally did mine with apt install and got weird behavior. Once I uninstalled it and then installed it with pip the behavior matched that of what I was seeing in the course material.\",1642959774.147300,1642961403.158800,U02TNEJLC84\\n7c904792-0f40-4e8a-964e-7af85ad809e0,U02UAADSJ84,,,\"That worked like a charm, you are pure genius!!  Thank YOU!!!\",1642959215.135400,1642961468.159100,U02UAADSJ84\\nf85979ac-1e5a-428b-a492-476242a60695,,,,We have to install docker and create a new folder while doing it,,1642961666.160600,U02B8H4DMSA\\n5448e63a-fde6-4710-bb65-85a6dc7397fb,,2.0,,What code is expected in the homework code link?,1642961779.161200,1642961779.161200,U02UE7NTLUU\\ne0d3672e-4b6d-49e9-9ce4-cb960028a4fd,U02UE7NTLUU,,,Link to your sql queries,1642961779.161200,1642961791.161300,U01AXE0P5M3\\nf9dd15a3-a724-4bd3-9d7f-eaa56a8e405f,,8.0,,\"When installing the Google Cloud SDK on my Apple MacBook M1 I get the following error message: `File \"\"/Users/[myusername]/google-cloud-sdk/lib/googlecloudsdk/core/util/files.py\"\", line 127, in MakeDir`\\n\\xa0\\xa0`raise Error(`\\n`googlecloudsdk.core.util.files.Error: Could not create directory [/Users/[myusername]/.config/gcloud]: Permission denied.`\\n\\nDoes anyone know how to fix this?\",1642961867.162400,1642961867.162400,U02SF1JK8CU\\n4bbb98b6-7538-4be8-850c-92af5daa2c0f,U02CPBEH42W,,,i used brew (actually the admin used brew for me),1642959774.147300,1642962027.162800,U02CPBEH42W\\n61B6C3B2-C56D-4DB7-9002-EF8F7B176441,,1.0,,When can we expect the week 2 videos to be available on YouTube? ,1642962092.163700,1642962092.163700,U02TBCXNZ60\\n7dd3f0e1-7a0d-478a-8c2f-d91fba80ac2d,U02UE7NTLUU,,,\"Got it, thanks!\",1642961779.161200,1642962107.163900,U02UE7NTLUU\\n215a08c0-a111-4d26-b75d-b373a066a164,U02SF1JK8CU,,,Can you run it with sudo? Not sure if that command is on mac or not.,1642961867.162400,1642962129.164100,U02TNEJLC84\\nec364e98-ca70-4fd6-baf7-443af061e19d,U02SF1JK8CU,,,\"From the error it is saying \"\"I\\'m going to create a file /Users/[myusername]/.config/cloud with the command mkdir(or something like it)\"\" Your system is saying \"\"Hey, you don\\'t have permission to create a directory there\"\"\",1642961867.162400,1642962302.164300,U02TNEJLC84\\n6585f957-7898-44c5-8307-76a521b8962f,U02CPBEH42W,,,I would give it a shot to uninstall it using brew and then reinstalling using pip. Other than the test proposed by <@U02U34YJ8C8> I think that\\'s the only thing left to do.,1642959774.147300,1642962513.164500,U02TNEJLC84\\n8e00b69d-c0d8-44bb-95cb-a5afb61f8032,U02TBCXNZ60,,,after we upload them :slightly_smiling_face: (I still need to record my part to be honest),1642962092.163700,1642962720.165600,U01AXE0P5M3\\nc2b05e1e-aeb4-45a0-afab-1d021a846995,U02SF1JK8CU,,,I have installed the google cloud SDK cleanly and without issue using `brew install --cask google-cloud-sdk`,1642961867.162400,1642963012.166600,U02DD97G6D6\\n70616a96-6996-4f7f-9326-05e88b0469c2,U02CPBEH42W,,,It\\'s not happy about setproctitle for psycopg2 when trying to pip install pgcli.  Trying a few more things.  Thank you for the suggestions!,1642959774.147300,1642963030.166800,U02CPBEH42W\\nc931080a-dcb4-4cd1-b719-e4e987b0402a,U02SF1JK8CU,,,Oh nice. I guess brew is the way to go then.,1642961867.162400,1642963073.167300,U02TNEJLC84\\n5b59d441-cbd1-46b9-8cdd-677d47e07035,,2.0,,\"Hello everyone,\\n`df.head(n=0).to_sql(name=\\'yellow_taxi_data\\', con=engine, if_exists=\\'replace\\')`\\nis giving me this error:\\n\\n```AttributeError: \\'Engine\\' object has no attribute \\'cursor\\'```\\n\",1642963101.168100,1642963101.168100,U02QMPAAG93\\na75e27c0-2fcd-4a8d-8e60-c9ef0fb713b1,U02CPBEH42W,,,\"Think I may have gotten that too. You may need to install the dependencies\\n```brew install libpq-dev python-dev```\\nReally not sure on that, but if I remember I corrected it using.\\n```sudo apt install libpq-dev```\\non my Linux Machine.\",1642959774.147300,1642963227.168500,U02TNEJLC84\\nA9CEAE1C-C6AF-4E99-9588-BA43FA17120F,U02QMPAAG93,,,Restart your kernel or notebook and try again ,1642963101.168100,1642963291.169200,U02U34YJ8C8\\n19727ced-47c5-44be-bed1-3278c558a459,U02QMPAAG93,,,Great! This worked. Thanks <@U02U34YJ8C8>,1642963101.168100,1642963341.169400,U02QMPAAG93\\ne8fd6bd4-27bb-42bc-84f7-b5e697236c55,U02U0L55BNU,,,\"I had to map the ports like you specified, curiously with the same alternative port, but that was because I already had Postgres installed on my Ubuntu and listening to the 5432 port...\",1642955680.118200,1642963344.169600,U02GVGA5F9Q\\nC742934C-1BD1-46A6-A6BD-A9D8E6FDEB14,U02UAADSJ84,,,No problem! The network name can be specified in the YAML but I\\'ve not gotten around to doing it myself. ,1642959215.135400,1642963434.170700,U02U34YJ8C8\\n69fbbbc4-5bda-426d-95d1-caf35424f4c7,U02T9VA6VJS,,,\"I don\\'t know about macs, but it probably isn\\'t very different from ubuntu. Pip is related to Python 2.x whereas Pip3 stands for Python 3.x. On Ubuntu there\\'s a package python-as-python3 that one may install to use pip as an alias of pip3.\",1642959566.145000,1642963752.170900,U02GVGA5F9Q\\ndc524562-8278-4708-b4c1-814218dd12ff,U02CPBEH42W,,,\"Uninstalled, then reinstalled using Alexey\\'s example:\\nconda install -c conda-forge pgcli\\npip install -U mycli\\n\\nThat part went fine, but now I am back to the same error I started with.\\n\\nconnection to server at \"\"localhost\"\" (::1), port 5432 failed: FATAL:  role \"\"root\"\" does not exist\",1642959774.147300,1642963804.171100,U02CPBEH42W\\ne966f591-88da-4311-9607-8d1bac50a5ae,U02SF1JK8CU,,,\"I simply downloaded tar, and extract the file.\",1642961867.162400,1642963831.171300,U0290EYCA7Q\\n53c91b0f-1406-4900-b7fa-2e8ce89c6a89,,1.0,,Hi <@U01AXE0P5M3>. will the course cover production grade content? Or is it going to cover only high level \\'how to\\' for us to play around with the tools?,1642964127.175400,1642964127.175400,U02TPAW5ESG\\ne4e08a25-86b9-4632-8930-c51960dc2c55,,12.0,,\"I\\'m trying again to get video 2 to work. I don\\'t want to have to quit the course. Please someone tell me why I\\'m getting this error when I try to look at the yellow_taxi_data table.\\n`root@localhost:ny_taxi&gt; \\\\d`\\n`+--------+------------------+-------+-------+`\\n`| Schema | Name \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0| Type\\xa0| Owner |`\\n`|--------+------------------+-------+-------|`\\n`| public | yellow_taxi_data | table | root\\xa0|`\\n`+--------+------------------+-------+-------+`\\n`SELECT 1`\\n`Time: 0.038s`\\n`root@localhost:ny_taxi&gt; \\\\d yellow_taxi_data`\\n`column c.relhasoids does not exist`\\n`LINE 2: ... \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0c.relhasrules, c.relhastriggers, c.relhasoi...`\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0`^`\\n\\n`Time: 0.032s`\\n`root@localhost:ny_taxi&gt;`\",1642965033.177400,1642965033.177400,U02UBQJBYHZ\\n58c88e9b-e4c8-460f-b5c3-e642a430cc74,U02TXGC7WKH,,,\"I tried with another folder, it has the same name of the database but nothing changes.\",1642896501.499200,1642965165.178000,U02TXGC7WKH\\n6be032db-665e-4895-a703-72ffdac014cd,,16.0,,\"Second problem: When I open jupyter notebook, I get \"\"connecting to kernel\"\" for some time, then \"\"connection failed.\"\" It tries to connect again. Why?\",1642965194.178600,1642965194.178600,U02UBQJBYHZ\\n8c56fcfa-dc4b-4078-b73e-ddb2a4e370f8,U02CPBEH42W,,,Created a new user and at least got past the role not existing.  Now on to the db not existing.  :woman-shrugging:,1642959774.147300,1642965237.178700,U02CPBEH42W\\n6f1d5189-4406-4ccc-9ef0-05c3023f3754,U02SF1JK8CU,,,Thanks. I will try brew,1642961867.162400,1642965452.180300,U02SF1JK8CU\\n3d8606d5-52eb-401e-b76c-825c56477711,,5.0,,\"how do i configure this in docker compose?\\n```docker run -it \\\\\\n  --network=pg-network \\\\\\n  taxi_ingest:v001 \\\\\\n    --user=root \\\\\\n    --password=root \\\\\\n    --host=pg-database \\\\\\n    --port=5432 \\\\\\n    --db=ny_taxi \\\\\\n    --table_name=yellow_taxi_trips \\\\\\n    --url=${URL}```\\ni have been using docker compose method for pgadmin and pgdatbase , and since I didnt create a network, the above code doesnt run. what should i do now?\",1642965550.181300,1642965550.181300,U02UB8XDCHJ\\n4e074799-0f49-4012-9161-bab6ea7becd4,U02CPBEH42W,,,\"SO weird. I\\'d start from scratch. e.g.\\n```rm -rf &lt;your local mount&gt;\\ndocker kill $(docker ps -q)\\ndocker container prune -y```\\nThen run\\n```docker run -it \\\\\\n    -e POSTGRES_USER=\"\"rot\"\" \\\\\\n    -e POSTGRES_PASSWORD=\"\"root\"\"    \\\\\\n    -e POSTGRES_DB=\"\"ny_taxi\"\"    \\\\\\n    -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n    -p 5432:5432 \\\\\\n    --network=pg-network \\\\\\n    --name pg-database \\\\\\n        postgres:13```\",1642959774.147300,1642965705.183800,U02TNEJLC84\\n99DDAD02-C2D0-4C5C-B1A1-043DA81B6C83,U02UB8XDCHJ,,,Type “docker network ls” in the command window. There will be a network in there that Docker created automatically (assuming one isn\\'t specified in the YAML file).,1642965550.181300,1642965850.187000,U02U34YJ8C8\\n42ae22b8-65e5-4d7b-ad85-11d89415b276,U02UB8XDCHJ,,,\"I\\'ve just had the same issue. It seems as `docker-compose up`  creates a new network.\\n\\nI did `docker network ls`, then `docker network inspect &lt;network_id&gt;`. And used needed network in `docker run` command.\\n\\nIn my case the network called `docker_default`. Seems like it\\'s the folder name + default.\",1642965550.181300,1642965952.187500,U02UCHRV7FU\\n352638a9-95ef-4024-98a2-2efd3f75885b,U02UB8XDCHJ,,,\"docker compose actually create a network. Think It\\'s &lt;folder where docker compose yml resides&gt;_default\\nTry \\ndocker network ls\\ndocker network inspect &lt;network name&gt;\\nThis should show the containers, created by docker compose\",1642965550.181300,1642965977.187900,U0290EYCA7Q\\ne97d9854-fb95-479b-9c0e-7be705d53d11,U02CPBEH42W,,,Are you seeing any red warnings or anything like that when postgres container spins up?,1642959774.147300,1642966015.188100,U02TNEJLC84\\n1ded1fbf-027a-47c5-b715-2ff5c6b282ed,U02UB8XDCHJ,,,thanks <@U0290EYCA7Q> <@U02U34YJ8C8> and <@U02UCHRV7FU>. It is working!,1642965550.181300,1642966093.188400,U02UB8XDCHJ\\n42a510f8-e6c4-476d-8637-0355f519ab56,U02TPAW5ESG,,,\"You can check the syllabus and see if what we have in mind would qualify as production grade or not\\n\\nOf course it\\'s going to be simplified, so no real production. But we want to make it close to real life\",1642964127.175400,1642966122.188600,U01AXE0P5M3\\n237BE965-E7E9-4768-BAB7-E2E8238B500B,U02UBQJBYHZ,,,\"I\\'m not sure but maybe <@U01AXE0P5M3> knows.\\n\\nIt might be to do with pgcli. What version do you have? And how did you install it?\",1642965033.177400,1642966182.190200,U02U34YJ8C8\\nd1129a46-708a-484a-ae5d-0e6edd8ed585,U02CPBEH42W,,,\"You may also need to prune any networks you created. As long as the directory is gone, all containers have been stopped and removed, and docker networks removed you should be able to make this an executable and it will drop you into a password prompt for root.\\n```!#/bin/bash\\n\\ndocker run -it \\\\\\n    -e POSTGRES_USER=\"\"root\"\" \\\\\\n    -e POSTGRES_PASSWORD=\"\"root\"\"    \\\\\\n    -e POSTGRES_DB=\"\"ny_taxi\"\"    \\\\\\n    -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n    -p 5432:5432 \\\\\\n    --network=pg-network \\\\\\n    --name pg-database \\\\\\n        -d postgres:13\\n\\npgcli -h localhost -U root -d ny_taxi```\\n\",1642959774.147300,1642966383.190600,U02TNEJLC84\\nd4ec2bf6-acd4-4020-be6f-012593952629,U02CPBEH42W,,,Actually the docker networks shouldn\\'t matter in this case.,1642959774.147300,1642966408.190800,U02TNEJLC84\\nd14587ce-0698-4aa6-ad61-f375925bee98,U02UBQJBYHZ,,,\"<@U02UBQJBYHZ> if pgcli and postgres versions are not same, you\\'ll see these errors. \\n\\nIf you\\'ve installed postgres engine with pip command, then reinstall pgcli with pip. I guess pgcli currently used is one from \"\"sudo apt\"\".\",1642965033.177400,1642966624.191100,U02H0GUC7ML\\nc2059cb1-b284-4aaa-b95a-a0057578e4d0,U02SF1JK8CU,,,Unfortunately I get the same error when trying to install via brew,1642961867.162400,1642966835.191800,U02SF1JK8CU\\n014dc712-401f-4035-9997-b900e1cca148,U01QGQ8B9FT,,,For anyone else having same issue with psycopg2 I (mac m1) resolved with: `brew install postgresql`  and then `pip3 install psycopg2-binary` ,1642769991.086200,1642967963.192200,U02TQC8GYBW\\nc8bba6f4-37e3-4d09-97a0-7779c0583722,U02UBQJBYHZ,,,\"Perhaps this will help?\\n\\n<https://youtu.be/ae-CV2KfoN0|https://youtu.be/ae-CV2KfoN0>\",1642965033.177400,1642968366.192500,U01AXE0P5M3\\n09115dfd-0f21-4881-b6c5-23be3bf72040,,2.0,,\"Hi! I’m trying to practice creating resources with Terraform.\\nI decided to try and create a VM instance but I don’t understand if I should change the second string from ***default*** to something else\\n\\n`resource \"\"google_compute_instance\"\" \"\"default\"\" {...}`\",1642969160.193600,1642969160.193600,U02CPH3FR33\\nd5b1c2a5-c5be-4a58-b4bd-d63b7b48d9e5,U02CPH3FR33,,,\"In the examples we saw for creating the storage bucket the official documentation has ***static-site***\\n`resource \"\"google_storage_bucket\"\" \"\"static-site\"\" {...}`\\n\\nSejal changed it to ***data-lake-bucket***\\n\\n`resource \"\"google_storage_bucket\"\" \"\"data-lake-bucket\"\" {...}`\\n\\nDoes the text affect the resource functionality or is it just I way to identify was resource is being created?\",1642969160.193600,1642969174.193700,U02CPH3FR33\\n742018e2-00e4-4864-a827-2fe645d6fcab,U02UBQJBYHZ,,,I installed pgcli with pip install pgcli. Version is 1.9.0.,1642965033.177400,1642969417.194000,U02UBQJBYHZ\\n1ef79fc2-de4e-4ca5-b07c-3634dc7add6b,U02CPH3FR33,,,\"It\\'s a way to identify resources, so you can change it to any name you\\'d like\",1642969160.193600,1642969488.194200,U01AXE0P5M3\\n0815db56-632d-4514-91e7-1a2ad36b1ce0,U02UBQJBYHZ,,,I previously had (and still have) Postgres12 and Postgres13 on my computer. Postgres12 because you can\\'t get free AWS with Postgres13.,1642965033.177400,1642969517.194500,U02UBQJBYHZ\\n33659B0E-AB8A-4500-89A1-53BB3DD4F976,U02SF1JK8CU,,,How do I do this with sudo? I am a noob to CLI :grimacing: ,1642961867.162400,1642969581.195400,U02SF1JK8CU\\n1b96e887-3cd8-49ca-b06b-563dd1008c2e,U02UBQJBYHZ,,,\"I have psql installed with pgAdmin for both versions. But that\\'s different from pgcli. I didn\\'t have that before, and I assume I\\'m using Docker Postgres for this exercise.\",1642965033.177400,1642969595.195600,U02UBQJBYHZ\\n6763b4e3-153f-4e57-a5bf-7371d393f60b,U02UB8XDCHJ,,,\"Also I show how to run it with docker compose here \\n\\n<https://youtu.be/tOr4hTsHOzU|https://youtu.be/tOr4hTsHOzU>\",1642965550.181300,1642969643.195900,U01AXE0P5M3\\n39710A79-0265-40EF-87AB-879AFF695788,U02UBQJBYHZ,,,That version of pgcli sounds low. Can you reinstall it using pip? Think my version is 3,1642965033.177400,1642969793.197300,U02U34YJ8C8\\n34738009-2b04-4200-a727-c3151c5835d5,U02UBQJBYHZ,,,\"Have you tried this?\\n\\n<https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/2_docker_sql/README.md#cli-for-postgres|https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/2_docker_sql/README.md#cli-for-postgres>\\n\\nIf you have problems with setting up the environment, I\\'d again recommend going through that video\",1642965033.177400,1642969903.198000,U01AXE0P5M3\\n2b1c5573-9737-427c-b369-389e09c06e1f,U02UBQJBYHZ,,,\"Thanks! I deleted the postgres data directory but now the other problem, that my python notebook can\\'t connect to the notebook server, is preventing me from trying to recreate the table.\",1642965033.177400,1642970339.198400,U02UBQJBYHZ\\n2210c930-28de-4de6-bb77-c6f75ce0761a,U02UBQJBYHZ,,,I had no problem the first time through the video. It was just after had the error messages with \\\\d that I could no longer connect.,1642965033.177400,1642970383.198600,U02UBQJBYHZ\\n123e4372-91da-4a96-b6e4-386a3ffbefc7,U02UBQJBYHZ,,,Because I quit and tried to start over.,1642965033.177400,1642970392.198800,U02UBQJBYHZ\\nd31688e6-e9c2-4979-99ff-ef8cef3ae1a2,,11.0,,\"Hi! I am connecting to pgadmin, but not found the table as shown, noting that i did all previous steps correctly !! where should i take a look? where the problem?! plz help\",1642970528.200900,1642970528.200900,U02RHT0M3M5\\nd5146f02-48dd-4436-ac83-6350fa2b3ea2,U02RHT0M3M5,,,did you put some data there?,1642970528.200900,1642970600.201200,U01AXE0P5M3\\n36d3138a-d9cc-4154-b7d7-5ba69e68219c,U02RHT0M3M5,,,Just refresh Tables. I had the same issue.,1642970528.200900,1642970618.201400,U02QNCUUHEY\\n446a96c8-b4cf-4f84-b667-d82b216ecb99,U02UBQJBYHZ,,,do you see anything in logs?,1642965194.178600,1642970624.201600,U01AXE0P5M3\\ncb830b80-73bc-4294-a1e8-9e94e15cbf90,U02UBQJBYHZ,,,in the terminal where you run jupyter,1642965194.178600,1642970638.201800,U01AXE0P5M3\\n8b11d32f-8977-444b-9341-1607c32bc9b2,U02RHT0M3M5,,,if you refresh you can see it,1642970528.200900,1642970663.202000,U02TF1JFK3J\\nc096d7e8-1bbe-4bfd-a983-fdceec0c4160,U02RHT0M3M5,,,\"I tried refresh! , It dosent show anything:/ how can i check if it contains data or not ?\",1642970528.200900,1642971017.202500,U02RHT0M3M5\\nfa70ea0a-2b83-4ab3-9ea0-c343ce1f9038,U02UBQJBYHZ,,,\"`[W 15:49:15.420 NotebookApp] Timeout waiting for kernel_info reply from 1347b503-b918-497e-bd20-ccc81573a86c`\\n`[E 15:49:15.425 NotebookApp] Error opening stream: HTTP 404: Not Found (Kernel does not exist: 1347b503-b918-497e-bd20-ccc81573a86c)`\\n`[W 15:49:15.429 NotebookApp] 404 GET /api/kernels/1347b503-b918-497e-bd20-ccc81573a86c/channels?session_id=813edfa18e3a49efa3b20c2208c43b55 (127.0.0.1): Kernel does not exist: 1347b503-b918-497e-bd20-ccc81573a86c`\\n`[W 15:49:15.448 NotebookApp] 404 GET /api/kernels/1347b503-b918-497e-bd20-ccc81573a86c/channels?session_id=813edfa18e3a49efa3b20c2208c43b55 (127.0.0.1) 39009.630000ms referer=None`\\n`[W 15:49:15.448 NotebookApp] 404 GET /api/kernels/1347b503-b918-497e-bd20-ccc81573a86c/channels?session_id=813edfa18e3a49efa3b20c2208c43b55 (127.0.0.1): Kernel does not exist: 1347b503-b918-497e-bd20-ccc81573a86c`\\n`[W 15:49:15.449 NotebookApp] 404 GET /api/kernels/1347b503-b918-497e-bd20-ccc81573a86c/channels?session_id=813edfa18e3a49efa3b20c2208c43b55 (127.0.0.1) 16986.640000ms referer=None`\\n`[W 15:49:19.474 NotebookApp] Replacing stale connection: 1347b503-b918-497e-bd20-ccc81573a86c:813edfa18e3a49efa3b20c2208c43b55`\\n`[W 15:49:47.493 NotebookApp] Replacing stale connection: 1347b503-b918-497e-bd20-ccc81573a86c:813edfa18e3a49efa3b20c2208c43b55`\",1642965194.178600,1642971019.202700,U02UBQJBYHZ\\n59346170-56d2-403c-b005-bdf13f6ccd5e,U02UBQJBYHZ,,,\"I have no idea why anything would have changed between when it worked and when it didn\\'t. I have Anaconda Navigator, installed less than a month ago.\",1642965194.178600,1642971080.203000,U02UBQJBYHZ\\nd96bb6bd-8e64-4cf8-9e5d-09a00068f70b,U02UBQJBYHZ,,,\"I think I have a fix for my other problem, but I deleted the database so I need to run the notebook again.\",1642965194.178600,1642971100.203200,U02UBQJBYHZ\\n9deca770-4c35-49c7-bd72-f70b9aebde35,U02UBQJBYHZ,,,I get the same problem when I go through the Anaconda dashboard.,1642965194.178600,1642971160.203400,U02UBQJBYHZ\\nb335ef30-80e0-4481-984b-0a075336ec56,U02R09ZR6FQ,,,I am using Linux. When I ran the docker run command it changed the permissions of dir `ny_taxi_postgres_data`  to 999. I manually change the permissions to +rw and the files are there.,1642891400.489400,1642971360.203600,U01E06WTHNG\\n156ecd6e-e7b5-43a9-97c8-ab4decf4cb1d,U02RHT0M3M5,,,<@U01AXE0P5M3> can you please guide me how to check if it contains data or not&gt;? should i use command to connect to pgcli: pgcli -h localhost -p 5432 -u root -d ny_taxi ?,1642970528.200900,1642971707.204900,U02RHT0M3M5\\n434d04a7-6f13-4e70-be44-212405360c64,U02RHT0M3M5,,,`pgcli -h localhost -p 5432 -u root -d ny_taxi`,1642970528.200900,1642971742.205100,U02QKMCV39R\\nee601058-896f-4bb8-ade9-f09fd8845db0,U02RHT0M3M5,,,run in your terminal,1642970528.200900,1642971815.205600,U02QKMCV39R\\n53c83a05-b67f-4644-bb00-a32d5b8bcb7f,U02UBQJBYHZ,,,it\\'s very difficult to understand what the problem is - I don\\'t have sufficient information to give any recommendation,1642965194.178600,1642972103.206000,U01AXE0P5M3\\nea1ebce6-c826-4579-bfde-fe4b111307b4,U02S4A9TA5Q,,,Sure <@U02T0CYNNP2>,1642957113.123500,1642972227.206200,U02S4A9TA5Q\\nfcf51e04-dbac-4621-8cd0-af27d3a4583c,U02UBQJBYHZ,,,Maybe it\\'s this:,1642965194.178600,1642973034.209400,U02UBQJBYHZ\\n536741b7-a985-4673-b825-cc402eb364ac,U02UBQJBYHZ,,,\"`jupyter-console 6.4.0 requires prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0, but you have prompt-toolkit 1.0.18 which is incompatible.`\\n`ipython 7.29.0 requires prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0, but you have prompt-toolkit 1.0.18 which is incompatible.`\",1642965194.178600,1642973041.209600,U02UBQJBYHZ\\n832f335c-b949-4d88-85ce-517c248f6802,U02UBQJBYHZ,,,But pgcli requires older version of prompt-toolkit.,1642965194.178600,1642973055.209800,U02UBQJBYHZ\\n1870905c-6a88-4fb6-8772-1cc865503b96,U02UBQJBYHZ,,,So I can\\'t run jupyter notebook and pgcli both.,1642965194.178600,1642973072.210000,U02UBQJBYHZ\\na63a5526-dffa-4ebd-992c-49879ec6e308,U02RHT0M3M5,,,\"I found the probme that data is not exist, I will transfer data then refresh agian! many thanks all\",1642970528.200900,1642973250.210500,U02RHT0M3M5\\n3cbd74cc-267b-431f-82e0-28d3290c4d52,U02UBQJBYHZ,,,I think this is the problem. When i upgrade Jupyter-console I get prompt-toolkit version 3 something and then the notebook works.,1642965194.178600,1642973261.210700,U02UBQJBYHZ\\n47401e61-a6b2-4d5e-8dd4-38913d652440,U02UBQJBYHZ,,,But then i upgrade pgcli and it goes back to the older prompt-toolkit.,1642965194.178600,1642973333.210900,U02UBQJBYHZ\\nf30bd5a5-1051-46c3-bfc7-e941c57e6e19,U02TXGC7WKH,,,\"It is working, the problem was that I already had a Postgres Service running in port 5432.\",1642896501.499200,1642974181.212200,U02TXGC7WKH\\n108c4dce-24be-4269-923d-a42def08c981,U02UBQJBYHZ,,,OK the solution is use psql. I have used this now and things are working. Not pgcli. pgcli and jupyter notebook have a dependency mismatch.,1642965194.178600,1642974699.212400,U02UBQJBYHZ\\nc9cabf6f-5467-4c92-aae8-11d19cb2c55c,,2.0,,\"Public Service Announcement: I was stuck because my version of jupyter notebook (for python) required the library \"\"prompt-toolkit\"\" to be one version and pgcli required the library \"\"prompt-toolkit\"\" to be an incompatible version. So I am not going to use pgcli. I hope I can catch up now.\",1642974883.215100,1642974883.215100,U02UBQJBYHZ\\ne209d006-1655-4509-8246-69ae5ab77c92,,17.0,,\"Please guys who is got this error after running pgcli\\n\\nVersion: 1.9.0\\nChat: <https://gitter.im/dbcli/pgcli>\\nMail: <https://groups.google.com/forum/#!forum/pgcli>\\nHome: <http://pgcli.com>\\nroot@localhost:ny_taxi&gt; Exception in thread completion_refresh:\\nTraceback (most recent call last):\\n  File \"\"/home/simeon/anaconda3/lib/python3.9/threading.py\"\", line 973, in _bootstrap_inner\\n    self.run()mpletion: ON  [F3] Multiline: OFF  [F4] Emacs-mode     Refreshing\\n  File \"\"/home/simeon/anaconda3/lib/python3.9/threading.py\"\", line 910, in run\\n    self._target(*self._args, **self._kwargs)\\n  File \"\"/home/simeon/anaconda3/lib/python3.9/site-packages/pgcli/completion_refresher.py\"\", line 68, in _bg_refresh\\n    refresher(completer, executor)\\n  File \"\"/home/simeon/anaconda3/lib/python3.9/site-packages/pgcli/completion_refresher.py\"\", line 110, in refresh_tables\\n    completer.extend_columns(executor.table_columns(), kind=\\'tables\\')\\n  File \"\"/home/simeon/anaconda3/lib/python3.9/site-packages/pgcli/pgcompleter.py\"\", line 204, in extend_columns\\n    for schema, relname, colname, datatype, has_default, default in column_data:\\n  File \"\"/home/simeon/anaconda3/lib/python3.9/site-packages/pgcli/pgexecute.py\"\", line 483, in table_columns\\n    for row in self._columns(kinds=[\\'r\\']):\\n  File \"\"/home/simeon/anaconda3/lib/python3.9/site-packages/pgcli/pgexecute.py\"\", line 478, in _columns\\n    cur.execute(sql)\\npsycopg2.errors.UndefinedColumn: column def.adsrc does not exist\\nLINE 7:                         def.adsrc as default\",1642975381.215900,1642975381.215900,U02UKLHDWMQ\\n090db783-7a77-4139-a8d3-b32420b2e8d2,,7.0,,\"Hello everyone,\\xa0\\nI need some help. I was just trying to network both containers Postgres and PgAdmin. Before this step, I checked that I had imported the correct number of\\xa0 rows . Cf below\\nroot@localhost:ny_taxi&gt; `SELECT count(1) FROM yellow_taxi_data`\\n+---------+\\n| count \\xa0 |\\n|---------|\\n| 1369765 |\\n+---------+\\n\\nAfter networking both containers, I run the following command\\xa0 pgcli -h localhost -p 5431 -u root -d ny_taxi . Now the number of rows imported is different\\xa0\\nroot@localhost:ny_taxi&gt; `SELECT count(1) FROM yellow_taxi_data`\\n+---------+\\n| count \\xa0 |\\n|---------|\\n| 1100000 |\\n+---------+\\nI don’t know why. Can somebody help me sort this out? Should I start from scratch by deleting the containers?\",1642975558.217600,1642975558.217600,U02TB5NK4FL\\nd945f8d3-d8df-4e6c-a9e3-b83e24e02614,U02UM74ESE5,,,\"Thanks <@U02TEKL21JQ>, really appreciate your help. Seems like that fixed the issue but new errors popped up...:hot_face:\\nNow I am getting -\\n```AMEYs-MacBook-Air:2_docker_sql amey$ python3 ingest_data.py \\\\\\n&gt; --user=root \\\\\\n&gt; \\xa0\\xa0--password=root \\\\\\n&gt; \\xa0\\xa0--host=localhost \\\\\\n&gt; \\xa0\\xa0--port=5432 \\\\\\n&gt; \\xa0\\xa0--db=ny_taxis \\\\\\n&gt; \\xa0\\xa0--table_name=yellow_taxi_data \\\\\\n&gt; \\xa0\\xa0--url=${URL}\\nsh: wget: command not found\\nTraceback (most recent call last):\\n\\xa0File \"\"ingest_data.py\"\", line 67, in &lt;module&gt;\\n\\xa0\\xa0main(args)\\n\\xa0File \"\"ingest_data.py\"\", line 27, in main\\n\\xa0\\xa0df_iter = pd.read_csv(csv_name, iterator=True, chunksize=100000)\\n\\xa0File \"\"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/util/_decorators.py\"\", line 311, in wrapper\\n\\xa0\\xa0return func(*args, **kwargs)\\n\\xa0File \"\"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers/readers.py\"\", line 586, in read_csv\\n\\xa0\\xa0return _read(filepath_or_buffer, kwds)\\n\\xa0File \"\"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers/readers.py\"\", line 482, in _read\\n\\xa0\\xa0parser = TextFileReader(filepath_or_buffer, **kwds)\\n\\xa0File \"\"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers/readers.py\"\", line 811, in __init__\\n\\xa0\\xa0self._engine = self._make_engine(self.engine)\\n\\xa0File \"\"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers/readers.py\"\", line 1040, in _make_engine\\n\\xa0\\xa0return mapping[engine](self.f, **self.options)\\xa0# type: ignore[call-arg]\\n\\xa0File \"\"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\"\", line 51, in __init__\\n\\xa0\\xa0self._open_handles(src, kwds)\\n\\xa0File \"\"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers/base_parser.py\"\", line 229, in _open_handles\\n\\xa0\\xa0errors=kwds.get(\"\"encoding_errors\"\", \"\"strict\"\"),\\n\\xa0File \"\"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/common.py\"\", line 707, in get_handle\\n\\xa0\\xa0newline=\"\"\"\",\\nFileNotFoundError: [Errno 2] No such file or directory: \\'output.csv\\'```\\nLooks like it is bz of wget. Can I substitute wget with curl?\",1642898827.001000,1642975768.218000,U02UM74ESE5\\nE394BC73-B9AD-4201-B96B-77C7EF324270,U02TB5NK4FL,,,Yes removing your containers would be a start. Then try re-running the data ingestion container again after running docker-compose ,1642975558.217600,1642976624.219900,U02U34YJ8C8\\n28467F51-9D41-4A50-9C4F-5BE2D253D1FF,U02UKLHDWMQ,,,How did you install pgcli? ,1642975381.215900,1642976699.220600,U02U34YJ8C8\\n3ED8960F-4C10-43A1-85BA-376BCBE1912B,U02UBQJBYHZ,,,I\\'ve no doubt you\\'ll catch up. Have you tried running a jupyter notebook in VS Code btw? You don\\'t have to use the web version if it\\'s causing issues.,1642974883.215100,1642977255.223500,U02U34YJ8C8\\n711ece25-48e9-4a5f-9aec-2a4c1e8cdca9,U02TB5NK4FL,,,\"Your data in files, not in containers.\",1642975558.217600,1642977269.223700,U02QNCUUHEY\\nae97be53-f74c-48bc-8a4e-3950ad010934,U02CPBEH42W,,,i have the same error :disappointed: …,1642959774.147300,1642977610.224000,U02UNQNMH7B\\nbe151228-0e9c-481a-8321-cc2eb5b7efa8,U02CPBEH42W,,,if anyone solves it i would be very interested,1642959774.147300,1642977620.224200,U02UNQNMH7B\\n6b856a1b-f875-4836-923e-8644d695c55c,U02T1BX1UV6,,,\"Hi, I was facing the same issue. When it asks the password, it was freezing. Then I have run into this <@U01AXE0P5M3> <https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642875716427700?thread_ts=1642874048.415600&amp;cid=C01FABYF2RG|comment> . I tried on Anaconda Prompt then it worked.\\n```# for windows\\n\\xa0winpty docker run -it \\\\\\n\\xa0 -e POSTGRES_USER=\"\"root\"\" \\\\\\n\\xa0 -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n\\xa0 -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n\\xa0 -v\\n//$(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n\\xa0 -p 5431:5432 \\\\\\n\\xa0 postgres:13```\\nYou can delete \"\"winpty\"\". It was for my case in Windows.\\n```pgcli -h localhost -p 5431 -u root -d ny_taxi```\",1642416911.154000,1642977806.224400,U02S82E4N4S\\n6163E003-597B-4333-B71E-B9EFB2981FA4,U02CPBEH42W,,,\"<@U02UNQNMH7B> I would first try removing all your containers and starting again. \\n\\nRun ‘docker ps’ to list all running containers. Then run ‘docker stop &lt;container Id&gt;’ on each of them. Then run \\'docker container prune’ to remove all of them. \",1642959774.147300,1642978339.228600,U02U34YJ8C8\\na71254c4-95d5-4b7e-b360-de5371821b18,U02CPBEH42W,,,tried that multiple times … but now i think my problem might actually happen before that … will start a new thread,1642959774.147300,1642978448.229000,U02UNQNMH7B\\n0693dfb2-2c2e-49c8-9677-c8887f074f4b,U02UKLHDWMQ,,,pip install pgcli,1642975381.215900,1642978460.229200,U02UKLHDWMQ\\nE77D4756-75D2-4613-9C44-3CC7FBDD1223,U02UKLHDWMQ,,,Which version do you have? And which version of Python? ,1642975381.215900,1642978520.229900,U02U34YJ8C8\\nca86e187-c89d-4148-ba3c-f8f593b78da9,U02UM74ESE5,,,never mind I just did $ brew install wget and then ran that code and its working,1642898827.001000,1642978900.230200,U02UM74ESE5\\n9c94c575-dba8-440b-8201-04e4f7489b79,U02UM74ESE5,,,Thanks for your help!,1642898827.001000,1642978916.230400,U02UM74ESE5\\n1b34db77-8294-44bd-b814-9081fcfeb165,U02QKMCV39R,,,how? please share,1642873331.407000,1642979309.230700,U02UNQNMH7B\\nc0577260-dfca-484a-ae74-7fb071bff6a5,U02UKLHDWMQ,,,What command did you run specifically?,1642975381.215900,1642979599.231300,U02UR6MFW1F\\nb9f4d58d-ad49-4cb8-bd12-2265b3281036,,1.0,,\"do docker containers come packed with a linux os + shell by default? i was kind of confused with the --entrypoint= bash command since we are using the docker image \"\"python\"\"\",1642980141.233200,1642980141.233200,U02TWFZURD1\\n40b1043a-206b-4deb-82c2-ebcd67195186,U02TWFZURD1,,,\"Yes! Every container is like a Virtual OS based on Linux, that\\'s why you can use the same CLI tools. The \"\"python\"\" image is an addition of python components and dependencies on top of the Linux base.\",1642980141.233200,1642980399.233400,U02UR6MFW1F\\ncf4d408d-bbb9-4103-9795-d373d9d870b4,U02UBQJBYHZ,,,I might try - I feel like it\\'s a house of cards right now and if I move one thing the whole thing will come crashing down!,1642974883.215100,1642980619.233800,U02UBQJBYHZ\\n902aec8c-c6d8-4141-b67d-a10afd1a21a6,U02S4A9TA5Q,,,Thanks,1642957113.123500,1642980692.234000,U02T0CYNNP2\\nc72fec47-f7f6-491a-b81c-c56c22b16750,,4.0,,\"hello guys, I am Mamun, interested to learn data engineering skills. I just got link today from LinkedIn from DPHI member. Its already one week past. I watched 1st week you tube orientation, tomorrow in next class. what you did last week? what should i do to adopt with tomorrow lecture? shall i need to set up anything my own? please suggest me what to do now to walk along with you.\",1642981427.237600,1642981427.237600,U02V4UTSU8M\\n40b29f0a-48c8-424e-a019-404af7d0b415,U02UM74ESE5,,,:+1:,1642898827.001000,1642981616.237700,U02TEKL21JQ\\nca40bf7c-176b-4932-9688-4669dac7281a,U01AXE0P5M3,,,Thanks Alexey for the reply above. That makes sense,1642621263.215300,1642982426.238000,U02T9GHG20J\\n25a88c68-d3c2-4f6c-aeb6-75c3ce56f771,U02T1BX1UV6,,,I was finally able to run it on the windows terminal,1642416911.154000,1642983752.238700,U02T1BX1UV6\\nff399f37-4219-415a-94bc-ca8de467d3bb,U02T1BX1UV6,,,\"I am miles behind in the class, I hope to catch up\",1642416911.154000,1642983769.238900,U02T1BX1UV6\\nb73c52ac-3cb2-42a6-8505-516360e9c334,,5.0,,\"hey guys, i don\\'t think to be able to run postgres successfully using docker. i used the code provided in the course and i receive the following error\\n`Unable to find image \\'1:latest\\' locally`\\n`docker: Error response from daemon: pull access denied for 1, repository does not exist or may require \\'docker login\\': denied: requested access to the resource is denied.`\\n`See \\'docker run --help\\'.`\\nthe command i ran is this\\n`docker run -it \\\\`\\n  `-e POSTGRES_USER=\"\"root\"\" \\\\`\\n  `-e POSTGRES_PASSWORD=\"\"root\"\" \\\\`\\n  `-e POSTGRES_DB=\"\"ny_taxi\"\" \\\\`\\n  `-v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\`\\n  `-p 5432:5432 \\\\`\\n  `postgres:13`\",1642984070.239900,1642984070.239900,U02B8U0QZEK\\n8d23cdcd-8c10-4bbe-8286-ef1e393e0302,U02V4UTSU8M,,,follow the direction in week1 <https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup>,1642981427.237600,1642985289.240600,U02UY1QTGHW\\nf4944860-01ea-4a29-b62b-340b23aa6209,,6.0,,Problem with ssh logout and then login at around 15:50 into video 1.4.1 (Cloud VM + SSH). I\\'m on Ubuntu 20.04. My specific error states: sandyhopbehrens1975@34.116.114.142: Permission denied (publickey). Everything else is working and giving the same output as Alexey. Anyone having similar problems or have any ideas on where I might start to try and solve this problem?,1642985661.243400,1642985661.243400,U02U5SW982W\\nea2f95df-2ace-4fab-8b21-c32beae575e5,U02U5SW982W,,,Hmm probably something to do with the fact that I didn\\'t do `$sudo ssh de-zoomcamp` but now I get back the error `ssh: Could not resolve hostname de-zoomcamp: Temporary failure in name resolution`,1642985661.243400,1642986199.244300,U02U5SW982W\\n3e27461e-a07a-48a1-896b-647d91f16ffb,,6.0,,Should we all do class 1.4.1 or is just for people that had problems setting up the env? The course continues in GCP Virtual Env?,1642986252.245400,1642986252.245400,U02TC704A3F\\nc8052494-0d9d-4954-b85c-e4f204ce5b34,U02TC704A3F,,,Hi <@U02TC704A3F> this part of the course is new. It shows you how to set up Cloud VM in GCP and connect from your host via SSH to the Cloud VM in GCP. I\\'m stalled about 15 minutes in so only know what is going on up to there... Hope that helps,1642986252.245400,1642986588.245500,U02U5SW982W\\n02c6590c-2af3-4c1a-8e79-05c18c2b971b,U02UBQJBYHZ,,,\"I am also using psql because I had the same issue between pgcli and jupyter. But beyond that, with both pgcli and psql I have the same problem with password authentication. When I run the command `psql -h localhost -p 5432 -U root` and enter the password, the output is `psql: error: connection to server at \"\"localhost\"\" (::1), port 5432 failed: FATAL:  password authentication failed for user \"\"root\"\"`.  Any ideas on how to solve this?\",1642965194.178600,1642987583.246000,U02FQPX4SC9\\n6687ac2f-c8f0-4e96-a86f-28646d3d26a3,U02V4UTSU8M,,,Remember to submit the homework for week 1 as well. The deadline for week 1 homework is coming very soon in a few hours.,1642981427.237600,1642988830.246700,U02BRPZKV6J\\n4551fe21-5dd2-4422-9652-f89084ac8dc4,,3.0,,\"Hi, on the Zoomcamp 1.2.3 - Connecting pgAdmin and Postgres video, at the point where i killed the postgres container, created the network and turned it back on again, I noticed that my yellow taxi data had been removed and I had to rerun the ingestion code again. I ran the following inside my Downloads folder: `docker run -it \\\\\\n&gt; -e POSTGRES_USER=\"\"root\"\" \\\\\\n&gt; -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n&gt; -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n&gt; -v \"\"ny-taxi-volume:/var/lib/postgresql/data\"\" \\\\\\n&gt; -p 5432:5432 \\\\\\n&gt; --network=pg-network \\\\\\n&gt; --name pg-database \\\\\\n&gt; postgres:13`. Should I have run this inside the ny-taxi-volume folder to persist the data so that if I had to restart the container, I wouldn\\'t have to rerun the ingestion script ?\",1642989525.249100,1642989525.249100,U02TMP4GJEM\\na45ccd66-81b2-4fb5-acd7-d25d90f867c6,U02TNEJLC84,,,Hi <@U01AXE0P5M3> were you supposed to update the form? It\\'s just that I went in to the form without having read all these messages on Slack and went through the same confusion as the above. The question in the form still reads \\'How many taxi trips were there on January 15?\\' and not \\'How many pick ups were there on January 15?\\' (or something similar). Not sure what you meant by \\'updated instructions\\' here that\\'s all.,1642725704.482700,1642990307.249500,U02U5SW982W\\ne6b92178-a803-48c3-9c1a-8923a8877372,U02TMP4GJEM,,,yes run it inside the folde,1642989525.249100,1642991294.249900,U02SPLJUR42\\n3cf6c46c-9744-4f94-97cd-4a6472c13a54,U02TMP4GJEM,,,folder,1642989525.249100,1642991298.250100,U02SPLJUR42\\nf3ed2fb6-d748-469b-8a18-a98dac78529f,U02TMP4GJEM,,,thanks :smiley:,1642989525.249100,1642991888.250300,U02TMP4GJEM\\n5f333227-0033-4bd8-9de6-16046acd8b80,U02B8U0QZEK,,,<https://blog.idrisolubisi.com/how-to-fix-error-response-from-daemon-pull-access-denied-for-yourusernameyourrepository>,1642984070.239900,1642992705.250700,U02UCJ7JLSE\\n79361c22-44c8-4f30-ab32-cb80bfa5506f,,2.0,,Does anyone know the correct syntax for converting jupyter notebook to py script ? The example in the video is not working. I have installed nbconvert library and tried the following: jupyter nbconvert --to=script upload-data.ipynb. It doesn\\'t work. I get the error `No template sub-directory with name \\'script\\' can be found. Does anyone know what that means ?,1642993934.254200,1642993934.254200,U02TMP4GJEM\\ned50d4ad-ed4f-4f26-ba39-c06dd475bd25,U02TMP4GJEM,,,\"easiest way is to open the notebook in vscode, and then to export it to python script\",1642993934.254200,1642994510.254500,U02ULBM39B4\\n193036fa-a4c1-4869-b737-b1b362151fb5,U02TMP4GJEM,,,\"Thanks anyway but I fixed it. In case this helps anyone, use this <https://stackoverflow.com/questions/63958574/no-template-sub-directory-with-name-lab-found-in-the-following-paths>. Uninstall nbconvert and reinstall the version 5.6.1\",1642993934.254200,1642994717.254700,U02TMP4GJEM\\n83a61950-a013-4817-8c6d-546242116a56,,3.0,,\"What would be really helpful is if a list of libraries and their versions are provided, as used in the video,. That way you could easily check your version against what is used in the video. When you do a pip install ..., you always get the latest version so a requirements list with the versions used in the video would save a lot of frustration.\",1642995076.257900,1642995076.257900,U02TMP4GJEM\\n4865581c-3459-4a87-abef-f7fa3d1e3f83,,1.0,,can anyone clarify what this means? what code are they talking about? the codes from question 3-6?,1642997023.259100,1642997023.259100,U02RREQ7MHU\\nD839E124-EF25-4A4A-9C76-C272AF60999D,U02RREQ7MHU,,,SQL Queries for questions ,1642997023.259100,1642997105.260000,U02AGF1S0TY\\n85484453-3095-49cf-a203-1367b545fa19,,4.0,,\"Hello everyone,\\nPlease, who has a solution to this. I tried copying some lines of code and pasting it on terminal resulted into this error (invalid reference format)\",1642997127.260300,1642997127.260300,U02QMPAAG93\\n5264d2a9-0258-41d2-ad62-11f844c12af4,,3.0,,\"<@U01AXE0P5M3> Hi Alexey , wanted to say thank you again for this course. Just curious during tomorrow\\'s class if you could share the number of people who submitted the homework and other stats so far about the DE course? curious since you shared it in the introduction video for the MLzoom camp regarding the conversion rate? would be interesting to hear\",1642999704.262500,1642999704.262500,U02T9550LTU\\n59e0e841-428c-4129-a173-464dd14e37a3,U02U5SW982W,,,\"show output of this command\\n`cat ~/.ssh/config`\",1642985661.243400,1643000735.263000,U02ULQFCXL0\\n6269fdea-3dd8-400a-ab87-91585c62973e,U02AUCL9ZQF,,,I don\\'t have access to Google Cloud as well as virtual card. Is there any alternative that i can try instead?,1642748175.026300,1643000774.263300,U02TAU2PR7U\\n571fc44b-d67f-4b66-b702-24f3b712ca99,U02T2DX4LG6,,,<@U02UAFF1WU9> Me too,1642787354.152800,1643001058.264000,U02UQNXD9QR\\n36e7b74b-ad2e-4d37-8d9e-27aed7c37d76,U02QMPAAG93,,,maybe adding 4 spaces at start of last line solve the problem,1642997127.260300,1643001114.264300,U02ULQFCXL0\\nc855d7bc-2b83-4614-a9cf-38f931b6f2ed,U02RHT0M3M5,,,I faced the same issue as <http://pd.to|pd.to>_sql() is not executed from script so there is no table and data. try running script and refresh the database.,1642970528.200900,1643001206.264500,U02TAU2PR7U\\nbeffb390-3b59-4156-9ebe-f0d19627d61f,U02RHT0M3M5,,,\"Since we didn\\'t create table manually pandas did it for us , So there is no table that means the script is not executed. if you executed the script without error and still the issue then you can re check credentials to database.\",1642970528.200900,1643001398.264700,U02TAU2PR7U\\n116114d7-2e7e-4783-a1bd-c2140c96ba28,U02UAADSJ84,,,<@U02UAADSJ84> did you resolve the issue ?,1642655334.281200,1643001913.265100,U02UY1QTGHW\\n13bfd770-16cd-4eb0-8aeb-72b43188b264,U01QGQ8B9FT,,,\"I am on Windows but tried this anyway and its the same error?\\n`Defaulting to user installation because normal site-packages is not writeable`\\n`Requirement already satisfied: psycopg2 in c:\\\\users\\\\mistr\\\\appdata\\\\roaming\\\\python\\\\python38\\\\site-packages`\",1642769991.086200,1643002026.265300,U01QGQ8B9FT\\ndcdbf76f-def6-4af7-bced-64af009cd6d1,U02R09ZR6FQ,,,did this get resolved ? I am still getting the blank folder but looks PG got installed edit: on Win11 Pro,1642891400.489400,1643003115.265600,U02UY1QTGHW\\n66f584b8-ac82-4ade-8adb-71ed77f001ff,U02R09ZR6FQ,,,\"```$ winpty docker run -it \\\\\\n&gt;   -e POSTGRES_USER=\"\"root\"\" \\\\\\n&gt;   -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n&gt;   -e POSTGRES_DB=\"\"ny_taxi\"\"\\\\\\n&gt;   -v /c/Users/x-Black/Documents/Learning/DataEngineering/pgdata:/var/lib/postgresql/data \\\\\\n&gt;   -p 5432:5432 \\\\\\n&gt;   postgres:13     \\nThe files belonging to this database system will be owned by user \"\"postgres\"\".\\nThis user must also own the server process.\\n\\nThe database cluster will be initialized with locale \"\"en_US.utf8\"\".\\nThe default database encoding has accordingly been set to \"\"UTF8\"\".\\nThe default text search configuration will be set to \"\"english\"\".\\n\\nData page checksums are disabled.\\n\\nfixing permissions on existing directory /var/lib/postgresql/data ... ok\\ncreating subdirectories ... ok\\nselecting dynamic shared memory implementation ... posix\\nselecting default max_connections ... 100\\nselecting default shared_buffers ... 128MB\\nselecting default time zone ... Etc/UTC\\ncreating configuration files ... ok\\nrunning bootstrap script ... ok\\nperforming post-bootstrap initialization ... ok\\nsyncing data to disk ... ok\\n\\ninitdb: warning: enabling \"\"trust\"\" authentication for local connections\\nYou can change this by editing pg_hba.conf or using the option -A, or\\n--auth-local and --auth-host, the next time you run initdb.\\n\\nSuccess. You can now start the database server using:\\n\\n    pg_ctl -D /var/lib/postgresql/data -l logfile start\\n\\nwaiting for server to start....2022-01-24 05:32:34.566 UTC [49] LOG:  starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on x86_64-pc-linux-gnu,\\ncompiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\\n2022-01-24 05:32:34.568 UTC [49] LOG:  listening on Unix socket \"\"/var/run/postgresql/.s.PGSQL.5432\"\"\\n2022-01-24 05:32:34.577 UTC [50] LOG:  database system was shut down at 2022-01-24 05:32:34 UTC\\n2022-01-24 05:32:34.582 UTC [49] LOG:  database system is ready to accept connections\\n done\\nserver started\\nCREATE DATABASE\\n\\n\\n/usr/local/bin/docker-entrypoint.sh: ignoring /docker-entrypoint-initdb.d/*\\n\\n2022-01-24 05:32:34.816 UTC [49] LOG:  received fast shutdown request\\nwaiting for server to shut down....2022-01-24 05:32:34.818 UTC [49] LOG:  aborting any active transactions\\n2022-01-24 05:32:34.820 UTC [49] LOG:  background worker \"\"logical replication launcher\"\" (PID 56) exited with exit code 1\\n2022-01-24 05:32:34.820 UTC [51] LOG:  shutting down\\n2022-01-24 05:32:34.837 UTC [49] LOG:  database system is shut down\\n done\\nserver stopped\\n\\nPostgreSQL init process complete; ready for start up.\\n\\n2022-01-24 05:32:34.936 UTC [1] LOG:  starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6\\n) 10.2.1 20210110, 64-bit\\n2022-01-24 05:32:34.936 UTC [1] LOG:  listening on IPv4 address \"\"0.0.0.0\"\", port 5432\\n2022-01-24 05:32:34.936 UTC [1] LOG:  listening on IPv6 address \"\"::\"\", port 5432\\n2022-01-24 05:32:34.942 UTC [1] LOG:  listening on Unix socket \"\"/var/run/postgresql/.s.PGSQL.5432\"\"\\n2022-01-24 05:32:34.948 UTC [63] LOG:  database system was shut down at 2022-01-24 05:32:34 UTC\\n2022-01-24 05:32:34.953 UTC [1] LOG:  database system is ready to accept connections\\n^C2022-01-24 05:45:19.817 UTC [1] LOG:  received fast shutdown request\\n2022-01-24 05:45:19.827 UTC [1] LOG:  aborting any active transactions\\n2022-01-24 05:45:19.828 UTC [1] LOG:  background worker \"\"logical replication launcher\"\" (PID 69) exited with exit code 1\\n2022-01-24 05:45:19.828 UTC [64] LOG:  shutting down\\n2022-01-24 05:45:19.845 UTC [1] LOG:  database system is shut down```\\n\",1642891400.489400,1643003214.265800,U02UY1QTGHW\\naa4c0c6d-34cf-4b89-8183-b6835e04ecd8,U02U9G1P76X,,,\"I too got the same problem, I got stuck up here. I could be able to open pgadmin 4 from port 8080 but localhost:5432 is not connecting, and got same error mentioned in the screenshot. While using the command\\n```pgcli -h localhost -U root -d ny_taxi```\\nfrom local machine, same error is reproduced\",1642928489.050200,1643003789.266300,U0300EGP2EL\\nad065c91-401e-4738-a956-3cf465ab23e5,U02U5SW982W,,,Also it\\'s a bit unusual to use sudo for ssh. Why did you need to do it?,1642985661.243400,1643004828.267100,U01AXE0P5M3\\na86c3a83-b1ef-461b-a66c-400a988e2c4a,U02TC704A3F,,,Yes it\\'s for people who had problems with their env,1642986252.245400,1643004866.267300,U01AXE0P5M3\\na8363487-7136-4e47-a298-84b96092dc48,U02U5SW982W,,,\"Again, I think somehow I have introduced some user error here. I ended up pretty much going back to square one with this video and it has now worked. And on the \\'unusual to use sudo\\' I\\'m feeling that \\'unusual\\' (many other adjectives I could add there) is my normal operating procedure. Thank you for responding <@U02ULQFCXL0> and <@U01AXE0P5M3> and trying to help. Very much appreciated :+1:.\",1642985661.243400,1643005726.267700,U02U5SW982W\\nc075ca95-2b6e-46a9-98d4-0992618b9aad,U02TMP4GJEM,,,Thank you! What did you have problems with?,1642995076.257900,1643005942.268000,U01AXE0P5M3\\n1774bd13-c1c8-457d-8ed8-ae8c3c15d324,U02QMPAAG93,,,\"I\\'m not sure how power shell handles line breaks, maybe it works differently from bash\",1642997127.260300,1643006228.268200,U01AXE0P5M3\\nd9fb59a0-1b2a-41c5-8623-51d3d576d02c,U02UBQJBYHZ,,,I don\\'t have that problem. I would post separately and see if anyone else has solved it.,1642965194.178600,1643006630.268800,U02UBQJBYHZ\\n4ad5ad69-49c1-4d6f-ba3d-4d604d4c85db,U01AXE0P5M3,,,\"I am using windows and after installing pgcli and iI tried running pgcli --help, I get the error pgcli is not recognized as an internal or external command, what should i do\",1642511128.453300,1643007333.269300,U02RH0V5K33\\n522879de-b8dd-47bc-a2f0-1cac4f99ce3f,U02T9550LTU,,,Right now there are 148 submissions in the homework form,1642999704.262500,1643007863.269600,U01AXE0P5M3\\nbd64c677-276b-47a9-a2eb-8b7d081af3a9,U02U5SW982W,,,Happy to hear it worked!,1642985661.243400,1643007929.269900,U01AXE0P5M3\\n04313889-b970-4c07-82d1-cd6fa3b5b8c1,U02U5SW982W,,,:tada:,1642985661.243400,1643008757.270200,U02U5SW982W\\n051f3eaa-bc1f-4b60-8ed9-d20bd72730f0,U02QMPAAG93,,,I run docker command in single line on power shell. It works.,1642997127.260300,1643008965.270400,U02NSF7RYP4\\n1cd4c622-1d96-4d1c-8535-e6c58a72c297,U02TNEJLC84,,,I did this in github and probably forgot to do it in the form,1642725704.482700,1643009027.270700,U01AXE0P5M3\\n2edb051b-ed7f-4388-aef1-cb9a4e469b76,U02TMP4GJEM,,,I would suggest to use requirements.txt,1642995076.257900,1643009162.271100,U02NSF7RYP4\\n2f211bd2-73ed-4aa7-a0fe-c8f9e7e7111c,U02UKLHDWMQ,,,\"Jumping onto this thread. I got the same issue as well. I tried installing and uninstalling pgcli, but I am still facing the same error.\\n\\nI am using `Python version 3.9.5`, and did my installation using `pip3 install pgcli` .\\n\\nNot sure if anyone found a resolution for this.\",1642975381.215900,1643009269.271300,U02TZ67LA80\\n34127b40-23e7-4d03-84a9-8ae3349970b8,U02QMPAAG93,,,Without backward slash.,1642997127.260300,1643009677.271600,U02NSF7RYP4\\ndbcca856-67e6-4c7d-93af-853d32a5685e,U02R09ZR6FQ,,,\"<@U02QM7H8G4U> <@U02R09ZR6FQ> <@U02UB8XDCHJ> <@U01AXE0P5M3> Ok I have managed to fix the issue, I think. I am on Win11 Pro. Here is the solution. create the folder as per the video, in my case its names pg_data.\\nRight click on folder &gt; properties &gt; Security tab &gt; edit &gt; add &gt; Everyone (in the text box) &gt; click \\'check names\\' &gt;ok &gt; check all the boxes under allow&gt; ok\\n\\n`winpty docker run -it \\\\`\\n  `-e POSTGRES_USER=\"\"root\"\" \\\\`\\n  `-e POSTGRES_PASSWORD=\"\"root\"\" \\\\`\\n  `-e POSTGRES_DB=\"\"ny_taxi\"\"\\\\`\\n  `-v //c/Users/x-Black/Documents/Learning/DataEngineering/pg_data:/var/lib/postgresql/data \\\\`\\n  `-p 5432:5432 \\\\`\\n  `postgres:13`\",1642891400.489400,1643009723.271800,U02UY1QTGHW\\n67a9bd9c-64b1-4d45-a264-e5b51078b016,U02B8U0QZEK,,,the error persist even after I login,1642984070.239900,1643011051.272800,U02B8U0QZEK\\nf11965a1-b20f-4a4b-af48-878b6cf38767,U02SC4Y6X0U,,,\"yes, I did try and check the GCP service account creation from the video. That was done! However, I am struggling , when I am doing the following from Gitbash\",1642534914.025300,1642579322.106000,U02SC4Y6X0U\\n7275c097-427f-4ecd-a190-fe8e404bf8d8,U02SC4Y6X0U,,,,1642534914.025300,1642579329.106200,U02SC4Y6X0U\\nc7cb5ad2-f27c-4249-843e-4111c93c3c4c,U02SC4Y6X0U,,,I think am going wrong with the path,1642534914.025300,1642579474.106600,U02SC4Y6X0U\\nd0175527-2a7e-41e9-a1b8-f3d94177a48e,U02SC4Y6X0U,,,I did reinstall Anaconda3 and python 3.9 was bundled in it respectively,1642534914.025300,1642579778.106900,U02SC4Y6X0U\\n7b2d6682-e254-4dfd-9d39-45563df7d101,,7.0,,\"```column c.relhasoids does not exist\\nLINE 2: ...                 c.relhasrules, c.relhastriggers, c.relhasoi...```\\nThis is the message I am getting when I run `root@localhost:ny_taxi&gt; \\\\d yellow_taxi_data;` but rest all is working fine.\",1642580546.109300,1642580546.109300,U02T96HEARK\\n8e313727-fefb-4f3a-b796-a983666234f5,U02T96HEARK,,,I am able to add all the chunk of data and I am able to verify the final count also by running `oot@localhost:ny_taxi&gt; select count(1) from yellow_taxi_data`,1642580546.109300,1642580588.109400,U02T96HEARK\\nb46ab5da-efb2-4492-90f9-44fa5b39dfe6,U02T96HEARK,,,\"```+---------+\\n| count   |\\n|---------|\\n| 1369765 |\\n+---------+\\nSELECT 1\\nTime: 0.060s```\\nThis is the output I am getting. Does anyone know the probable reasons behind getting the error message mentioned in the main message of this thread? ( I am using ubuntu  OS)\",1642580546.109300,1642580705.109600,U02T96HEARK\\n458afab7-748c-44be-92fe-b45e20237e31,U02T96HEARK,,,\"I also had this on Ubuntu \\n\\nThis helped\\n\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642511128453300?thread_ts=1642511128.453300&amp;cid=C01FABYF2RG|https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642511128453300?thread_ts=1642511128.453300&amp;cid=C01FABYF2RG>\",1642580546.109300,1642582002.110000,U01AXE0P5M3\\n34bc7955-445f-48f4-8011-a4b512226385,U02T96HEARK,,,\"In the installation video I show how to configure Ubuntu, maybe it\\'ll be helpful\",1642580546.109300,1642582040.110300,U01AXE0P5M3\\ncb50adcb-7a9e-4654-b9a8-c7b5d410fde7,U02T96HEARK,,,\"Yes, that would help everyone using Ubuntu throughout this course.\",1642580546.109300,1642583370.111300,U02T96HEARK\\ne8ec194c-93db-4d66-92cf-87ed5a73ef34,U02T96HEARK,,,\"I shared a link yesterday, take a look\",1642580546.109300,1642584173.111800,U01AXE0P5M3\\nd56b9934-8e99-4015-9ba6-b13028c60067,U02T1BX1UV6,,,\"Trying to get the database again because I started all over again in order to get my pgcli working.\\ndocker: Error response from daemon: invalid mode: \\\\Program Files\\\\Git\\\\var\\\\lib\\\\postgresql\\\\data.\",1642416911.154000,1642584220.112000,U02T1BX1UV6\\n2a4604a8-2730-4bf1-80c7-b72e12a66249,U02T1BX1UV6,,,\"docker: Error response from daemon: invalid mode: \\\\Program Files\\\\Git\\\\var\\\\lib\\\\postgresql\\\\data.\\n\\nThis is the new error I am getting above. I am using gitbash\",1642416911.154000,1642584242.112200,U02T1BX1UV6\\n455e9af9-38cc-4060-bafc-b374ca1dc98e,U02T1BX1UV6,,,<@U01AXE0P5M3>,1642416911.154000,1642584257.112400,U02T1BX1UV6\\nb77195a6-7df1-4d7b-906a-62d366b7f69a,U02T1BX1UV6,,,\"docker run -it \\\\\\n\\xa0 -e POSTGRES_USER=\"\"root\"\" \\\\\\n\\xa0 -e POSTGRES_PASSWORD=\"\"admin\"\" \\\\\\n\\xa0 -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n\\xa0 -v c:/Users/USER/Desktop/data-engineering-zoomcamp-main/data-engineering-zoomcamp-main/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres:/var/lib/postgresql/data \\\\\\n\\xa0 -p 5432:5432 \\\\\\n\\xa0 postgres:13\",1642416911.154000,1642584281.112600,U02T1BX1UV6\\nf56e2d55-b793-4d7c-b8b5-57fca0a36211,U02T1BX1UV6,,,\"No need to tag me please\\n\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642534337023000?thread_ts=1642534337.023000&amp;cid=C01FABYF2RG|https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642534337023000?thread_ts=1642534337.023000&amp;cid=C01FABYF2RG>\",1642416911.154000,1642584446.112900,U01AXE0P5M3\\n9de8366b-3cb3-4408-9c9b-766868eabfb4,U02T1BX1UV6,,,Alright thank you. Sorry for Tagging you.,1642416911.154000,1642584625.113200,U02T1BX1UV6\\na74ca896-6ed8-417b-a669-4f8adeedc835,,10.0,,good morning guy pls I am having issue with postgres,1642584839.113900,1642584839.113900,U029CJ4KY1Z\\n2df69e70-23c9-47f8-9220-8600f1f1b3e2,U02T1BX1UV6,,,No worries. I\\'m monitoring slack and will answer when I have a chance. And maybe someone else will help you as well,1642416911.154000,1642584842.114000,U01AXE0P5M3\\nac005450-891b-429e-bf78-f68fe8b003f1,U029CJ4KY1Z,,,\"docker run -it \\\\\\n&gt;\\n\"\"docker run\"\" requires at least 1 argument.\\nSee \\'docker run --help\\'.\\n\\nUsage:  docker run [OPTIONS] IMAGE [COMMAND] [ARG...]\\n\\nRun a command in a new container\\naslim@Groot:~/Documents/Data-Engineering-Zoomkamp/week_1_basic_n_setup/2_docker_sql$     -e POSTGRES_USER=\"\"root\"\" \\\\\\n&gt;     -e POSTGRE_PASSWORD=\"\"root\"\" \\\\\\n-e: command not found\\naslim@Groot:~/Documents/Data-Engineering-Zoomkamp/week_1_basic_n_setup/2_docker_sql$     -e POSTGRE_DB=\"\"ny_taxi\"\" \\\\\\n&gt;     -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n&gt;     -p 5432:5432 \\\\\\n&gt;     postgres:13^C\",1642584839.113900,1642584877.114600,U029CJ4KY1Z\\n,,,,Please don\\'t forget about it =),,1642584901.115100,U01AXE0P5M3\\n1b34f17d-b536-4314-8e68-1a7e74322770,U029CJ4KY1Z,,,You probably have an extra linebreak?,1642584839.113900,1642584948.115500,U01AXE0P5M3\\ne9a40199-4233-438b-bf95-fcdd7a974b3a,U029CJ4KY1Z,,,Make sure you don\\'t have extra linebreaks  between docker run and the rest of the arguments,1642584839.113900,1642584978.115700,U01AXE0P5M3\\naecbd041-8a5c-45fb-914d-00a3c87cd504,U029CJ4KY1Z,,,now I am getting Invalid reference format,1642584839.113900,1642585112.115900,U029CJ4KY1Z\\n5efd082b-fc86-41f3-8bd7-2180de472c8f,U029CJ4KY1Z,,,\"docker run -it \\\\\\n&gt;     -e POSTGRES_USER=\"\"root\"\" \\\\\\n&gt;     -e POSTGRE_PASSWORD=\"\"root\"\" \\\\\\ndocker: invalid reference format.\\nSee \\'docker run --help\\'.\\naslim@Groot:~/Documents/Data-Engineering-Zoomkamp/week_1_basic_n_setup/2_docker_sql$     -e POSTGRE_DB=\"\"ny_taxi\"\" \\\\\\n&gt;     -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n&gt;     -p 5432:5432 \\\\\\n&gt;     postgres:13^C\",1642584839.113900,1642585115.116200,U029CJ4KY1Z\\nc218dabf-fe05-42c9-b030-aa75274edd46,U02U8SBFWD8,,,\"I advice to buy basic VM in any cloud (I recommend Hetzner, Digital Ocean), and use it to create personal VPN server. It costs 5 euro/month and takes 2 hours to setup.\",1642437561.227400,1642585584.117200,U02T6RMCF7X\\n3727d362-9193-4cd4-8323-e8f28e12ca52,U02TTRBDA1M,,,\"thank you, looks as a very useful reference\",1642438185.232100,1642585632.117400,U02T6RMCF7X\\n82b3107e-4a9d-48cd-8134-a61cfe73a229,U02T96HEARK,,,Oh! Alright.,1642580546.109300,1642585712.117700,U02T96HEARK\\n83b235f0-75ac-49a7-a124-7755e720dc1c,U02R77VC12N,,,\"Thanks <@U01AXE0P5M3>, you\\'re right. If you don\\'t use the dot it still works but opens a windows code instead. WSL2 woes, I\\'d say...\",1642546559.066600,1642585948.118100,U02GVGA5F9Q\\n75d76ce1-5a6f-4ef8-98b2-b52497cc5dd6,U02T1BX1UV6,,,\"could not translate host name \"\"localhhost\"\" to address: Unknown host\\n\\nGot the error above after running\\n\\npgcli -h localhhost -p 5432 -u root -d ny_taxi\",1642416911.154000,1642586029.118300,U02T1BX1UV6\\nd9d5c8d8-4599-423a-9101-141b64553375,U02TBKWL7DJ,,,\"Yes, I have\\n\\nThis is the error I keep getting.\\n\\n<@U02BVP1QTQF> have you been able to connect to the db? I\\'m using M1 I don\\'t know if this error have specifics with the OS - I\\'ve installed `pgcli` but still not working\\nImportError: dlopen(/Users/opeyemi/opt/anaconda3/lib/python3.8/site-packages/psycopg2/_psycopg.cpython-38-darwin.so, 2): Symbol not found: _PQbackendPID\\n  Referenced from: /Users/opeyemi/opt/anaconda3/lib/python3.8/site-packages/psycopg2/_psycopg.cpython-38-darwin.so\\n  Expected in: flat namespace\\n in /Users/opeyemi/opt/anaconda3/lib/python3.8/site-packages/psycopg2/_psycopg.cpython-38-darwin.so\",1642507885.440700,1642586318.118700,U02TBKWL7DJ\\n342b5133-d02e-4cee-9c5c-29de59796e59,U02QS4BD1NF,,,\"Thanks, it has worked\",1642545252.057900,1642586659.121400,U02QS4BD1NF\\nd4d43ffe-dfbb-4c6e-8cf7-5ca0ad5e0b8b,,27.0,,\"Hi if anyone was able to successfully perform the mounting - I needed some help in the postgres setup :\\nProblem : The postgres DB is running but probably the mapping link is not established? (i cannot see any new file in my  `ny_postgres_taxi_data` folder)\",1642586687.122000,1642586687.122000,U01QGQ8B9FT\\ncc768689-0115-43cd-93d3-c14efdeb6d09,U01QGQ8B9FT,,,Which os?,1642586687.122000,1642586735.122100,U02QYKBJWN6\\nf14bf9ed-109f-451d-9721-92f8fdac6052,U01QGQ8B9FT,,,System : WSL(Ubuntu 20.04),1642586687.122000,1642586739.122300,U01QGQ8B9FT\\n206b76b9-265b-4579-83a1-7fff737f6e8c,U01QGQ8B9FT,,,\"`docker run -it \\\\`\\n\\xa0 `-e POSTGRES_USER=\"\"root\"\" \\\\`\\n\\xa0 `-e POSTGRES_PASSWORD\"\"=root\"\" \\\\`\\n\\xa0 `-e POSTGRES_DB=\"\"ny_taxi\"\" \\\\`\\n\\xa0 `-v \"\"/mnt/c/Users/mistr/DTC-DE/Practice/Docker_SQL/ny_taxi_postgres_data\"\":/var/lib/postgresql/data \\\\`\\n\\xa0 `-p 5432:5432 \\\\`\\n\\xa0 `postgres:13`\",1642586687.122000,1642586782.122500,U01QGQ8B9FT\\n0de03028-0b78-41c6-950d-f0d034c2aab3,U01QGQ8B9FT,,,\"I think you need to modify permissions to that folder, try running \"\"sudo chmod  a+rwx /path/to/folder\"\"\",1642586687.122000,1642586817.122700,U02QYKBJWN6\\n4885c0d7-a170-4f64-a028-083cfbc71973,U01QGQ8B9FT,,,`sudo chmod a+rwx /mnt/c/Users/mistr/DTC-DE/Practice/Docker_SQL/ny_taxi_postgres_data` correct?,1642586687.122000,1642586902.122900,U01QGQ8B9FT\\n9dcef821-634b-4979-9fba-28d0840c925c,U01QGQ8B9FT,,,Yes seems correct to me,1642586687.122000,1642586962.123100,U02QYKBJWN6\\n0fcb40df-202d-4c21-83aa-722a7113c8c0,U02TBKWL7DJ,,,\"I’m on an Intel Mac, so I am not familiar with any of M1\\'s specific quirks, sorry. The worst case scenario is that some libraries aren’t available; in that case I’d go the VM  instance route on GCP so that you can easily get a working machine.\\n\\nHowever, the error messages are showing Python 3.8. Did you try using Python 3.9? I strongly suggest you switch to it because that’s what Alexey is using in his videos, and the less variables we’re juggling with the better.\",1642507885.440700,1642586991.123400,U02BVP1QTQF\\n65cabee1-3e34-4bbf-b43b-bda840c4b287,U02QZN0LSBT,,,\"That is amazing, thank you for sharing, My notes are mostly commands, seeing how nicely formatted yours are is inspiring.\",1642548032.081900,1642587014.123700,U02TC8X43BN\\nb7983a24-e058-4f12-8e29-98785bd67192,U01QGQ8B9FT,,,I am having the same issue,1642586687.122000,1642587025.123900,U029CJ4KY1Z\\na447e80c-61da-4d43-a60e-9ad6ce153d71,U01QGQ8B9FT,,,\"It still gives an error :smiling_face_with_tear:\\n\\n`chmod: changing permissions of \\'/var/lib/postgresql/data\\': Operation not permitted`\\n`The files belonging to this database system will be owned by user \"\"postgres\"\".`\\n`This user must also own the server process.`\\n\\n`The database cluster will be initialized with locale \"\"en_US.utf8\"\".`\\n`The default database encoding has accordingly been set to \"\"UTF8\"\".`\\n`The default text search configuration will be set to \"\"english\"\".`\\n\\n`Data page checksums are disabled.`\\n\\n`fixing permissions on existing directory /var/lib/postgresql/data ... initdb: error: could not change permissions of directory \"\"/var/lib/postgresql/data\"\": Operation not permitted`\",1642586687.122000,1642587036.124100,U01QGQ8B9FT\\n1116d2cf-acc6-4998-955c-980aecc6c7b3,U01QGQ8B9FT,,,i cant see the expected file in the ny_taxi_data folder,1642586687.122000,1642587063.124400,U029CJ4KY1Z\\n923fe708-9200-4d43-aff7-4f1aa9640d32,U01QGQ8B9FT,,,i mistakenly create a file instead of a folder,1642586687.122000,1642587093.124600,U029CJ4KY1Z\\n0019e185-c8d6-47c8-83d1-b74c2655e995,U01QGQ8B9FT,,,PostgreSQL Database directory appears to contain a database; Skipping initialization,1642586687.122000,1642587131.125000,U029CJ4KY1Z\\nc81724c8-acd4-440d-a97d-d4cae1d78ce3,U01QGQ8B9FT,,,Do you think it has something to do with the /mnt ? should I omit it again and try?,1642586687.122000,1642587165.125300,U01QGQ8B9FT\\n6733537e-f21e-4a01-8e91-3254286a5aca,U01QGQ8B9FT,,,thats what I am getting anytime i tried to run it again,1642586687.122000,1642587179.125500,U029CJ4KY1Z\\n54d92a3b-f5ce-4304-81d3-8a5295082f90,U02T9VA6VJS,,,\"You need to be more specific than that. The videos already show most of the things you need to know to keep up and it’s assumed that you have some previous knowledge of Python, SQL and bash/zsh.\\n\\nIf you have more specific questions feel free to ask.\",1642543574.046000,1642587207.125700,U02BVP1QTQF\\nc37ada0d-8aa3-4847-b534-85ef1179a757,U01QGQ8B9FT,,,\"<@U01QGQ8B9FT> can you run \"\"sudo ls\"\" in the ny_taxi folder and tell me whether it shows any output?\",1642586687.122000,1642587279.125900,U02QYKBJWN6\\nf8fd5e32-82e3-4fa6-9f5c-4d29220cc775,U01QGQ8B9FT,,,\"Nope, its empty\",1642586687.122000,1642587385.126500,U01QGQ8B9FT\\n17848d3b-95dc-4d3d-9909-301990ea7154,U01QGQ8B9FT,,,\"Then it\\'s another issue, in my case it was showing files with super user so modifying permissions worked for me\",1642586687.122000,1642587463.126800,U02QYKBJWN6\\n6c8d324e-e424-4765-85f4-cb36cf0ba7a5,U02U34YJ8C8,,,\"I do not own an M1 Mac but Docker might involve extra steps, because the containers you will create will be for running on ARM CPU’s rather than x86, which may be an issue when trying to run your containers on the cloud.\",1642551977.086500,1642587590.127100,U02BVP1QTQF\\nb30aca0c-8529-4620-90b3-f35f30abee43,U01QGQ8B9FT,,,\"Vs Code does show me a prompt saying \"\"This workspace is on the Windows file system (/mnt/). For best performance, we recommend moving the workspace to the Linux file system (~/home)\"\". It does make sense to not have files go between os. Could that be a problem?\",1642586687.122000,1642587640.127300,U01QGQ8B9FT\\n7152819f-26ae-43cd-9ee5-b43dda49a9af,U02U34YJ8C8,,,\"Check Docker’s official docs. Apparently they include multiplatform support for M1, It’s likely that you’ll have to tweak your Docker commands and /or configs.\\n\\n<https://docs.docker.com/desktop/mac/apple-silicon/>\",1642551977.086500,1642587650.127500,U02BVP1QTQF\\n6f112ce3-67fd-42ef-8f30-847978077333,U01QGQ8B9FT,,,\"You can try that, it can be a issue\",1642586687.122000,1642587828.127900,U02QYKBJWN6\\n1361e057-7e3d-4d5c-ac1d-77b14e1931f0,U02RTJPV6TZ,,,okay now am abit stuck after running PGAdmin commands... ..errors appear.....i don\\'t understand where to get pg-network,1641565510.010100,1642588325.128100,U02RTJPV6TZ\\n99893b13-e0ff-423d-a39b-3f722805837b,U02RTJPV6TZ,,,\"$ docker run -it \\\\\\n&gt;   -e PGADMIN_DEFAULT_EMAIL=\"\"<mailto:admin@admin.com|admin@admin.com>\"\" \\\\\\n&gt;   -e PGADMIN_DEFAULT_PASSWORD=\"\"root\"\" \\\\\\n&gt;   -p 8080:80 \\\\\\n&gt;   --network=pg-network \\\\\\n&gt;   --name pgadmin-2 \\\\\\n&gt;   dpage/pgadmin4\\nUnable to find image \\'dpage/pgadmin4:latest\\' locally\\nlatest: Pulling from dpage/pgadmin4\\n59bf1c3509f3: Already exists\\ncd7931791ab5: Pull complete\\nbd73c0570f12: Pull complete\\nbc47ad50e4d3: Pull complete\\nc3110949ae48: Pull complete\\n919e3efd60e5: Pull complete\\n2e4f902ac219: Pull complete\\n7777c7dd9a36: Pull complete\\n84f2e2cc0de4: Pull complete\\n486f70e85d15: Pull complete\\n50f5f9845b45: Pull complete\\n4a8dc64909ff: Pull complete\\n01b4a6e56e97: Pull complete\\nf6f59b2ffd86: Pull complete\\nDigest: sha256:e04b209a47a6f9d47367c9bd4c88fa76570b81ecce1579d6a7e4398c4eae038a\\nStatus: Downloaded newer image for dpage/pgadmin4:latest\\ndocker: Error response from daemon: network pg-network not found.\",1641565510.010100,1642588330.128300,U02RTJPV6TZ\\n585c2c01-8768-427f-9980-f2b9d603119f,U02RTJPV6TZ,,,\"$ docker run -it \\\\\\n&gt;   -e PGADMIN_DEFAULT_EMAIL=\"\"<mailto:admin@admin.com|admin@admin.com>\"\" \\\\\\n&gt;   -e PGADMIN_DEFAULT_PASSWORD=\"\"root\"\" \\\\\\n&gt;   -p 8080:80 \\\\\\n&gt;   --network=pg-network \\\\\\n&gt;   --name pgadmin-2 \\\\\\n&gt;   dpage/pgadmin4\\nUnable to find image \\'dpage/pgadmin4:latest\\' locally\\nlatest: Pulling from dpage/pgadmin4\\n59bf1c3509f3: Already exists\\ncd7931791ab5: Pull complete\\nbd73c0570f12: Pull complete\\nbc47ad50e4d3: Pull complete\\nc3110949ae48: Pull complete\\n919e3efd60e5: Pull complete\\n2e4f902ac219: Pull complete\\n7777c7dd9a36: Pull complete\\n84f2e2cc0de4: Pull complete\\n486f70e85d15: Pull complete\\n50f5f9845b45: Pull complete\\n4a8dc64909ff: Pull complete\\n01b4a6e56e97: Pull complete\\nf6f59b2ffd86: Pull complete\\nDigest: sha256:e04b209a47a6f9d47367c9bd4c88fa76570b81ecce1579d6a7e4398c4eae038a\\nStatus: Downloaded newer image for dpage/pgadmin4:latest\\ndocker: Error response from daemon: network pg-network not found.\",1641798212.098100,1642588596.128700,U02RTJPV6TZ\\nc6cfae52-67ce-4777-ad48-8b3657d7fd3f,U02RTJPV6TZ,,,how can i get to pg-network??..help,1641798212.098100,1642588630.128900,U02RTJPV6TZ\\nf657f2d2-c79c-4acf-878c-59390532cb6f,U029CJ4KY1Z,,,\"Try to add \"\"S\"\" to your strings. You have:\\n\\nPOSTGRE_PASSWORD and\\nPOSTGRE_DB\\n\\nMake them so\\n\\nPOSTGRES_PASSWORD and\\nPOSTGRES_DB\",1642584839.113900,1642589391.130800,U02QL1EG0LV\\n099a4018-ddd1-45d3-b799-6824cae994cf,U02RTJPV6TZ,,,resolved used =&gt;  $docker network create pg-network,1641798212.098100,1642589948.132200,U02RTJPV6TZ\\nf55641da-1273-408f-a33c-779084dcb195,U02RTJPV6TZ,,,resolved used =&gt; docker network create pg-network,1641565510.010100,1642589986.132400,U02RTJPV6TZ\\n6f2da43d-8517-4328-9f19-4c8e0434f1a9,U029CJ4KY1Z,,,Your command somehow gets split into two. It should not happen,1642584839.113900,1642590323.132800,U01AXE0P5M3\\nf31fddc0-8c3e-4a0b-820c-b8057be9682a,U01QGQ8B9FT,,,\"You should put the whole thing after -v in quotes, not just the first part\",1642586687.122000,1642590820.133400,U01AXE0P5M3\\neee0dbf0-7546-48cb-a8c9-963eb1a63312,U02TBKWL7DJ,,,\"Alright.\\n\\nI\\'ve checked online I think it\\'s an error specific to new M1 chip\",1642507885.440700,1642591757.134100,U02TBKWL7DJ\\ne5e2b24a-813b-472d-9085-ac5786b00d98,U01AXE0P5M3,,,\"Thanks <@U01AXE0P5M3> for this.\\nI will opt in for this I\\'m having issues setting things up on my M1 chip\",1642544154.050000,1642591935.134300,U02TBKWL7DJ\\nd6d98f9e-a249-462a-a770-64300b3fd93d,,,thread_broadcast,\"wow finally done with all the videos now onto ...homework, <@U01AXE0P5M3> how do we submit H/W? .......i saw comment \"\"form TBA\"\"\",1641798212.098100,1642592819.134900,U02RTJPV6TZ\\ndacd516c-74ae-47e5-8d9e-8bf65448f93c,U02RTJPV6TZ,,,I\\'ll publish it today. In the meantime you can record your answers in a notebook =),1641798212.098100,1642593071.135200,U01AXE0P5M3\\n9ab3ac93-03eb-4a79-a3ac-a3ba8743358c,U02RTJPV6TZ,,,\"We\\'ll also publish more videos, stay tuned =)\",1641798212.098100,1642593102.135400,U01AXE0P5M3\\nA9971DD0-4243-4CF2-9E70-67943C20CA41,U02QZN0LSBT,,,This is fantastic!! Thank you ,1642548032.081900,1642593277.136200,U02U5G6MXJ8\\n37e7fa73-01cf-49f4-a9b8-69d79a87e933,U02T1BX1UV6,,,Typo in localhost,1642416911.154000,1642593661.136400,U02RYUWG4CQ\\n8d527e9a-9be0-47f2-9a3e-a1b560c31942,,3.0,,\"i get error installing pgcli\\n```ERROR: Failed building wheel for pendulum\\nFailed to build pendulum\\nERROR: Could not build wheels for pendulum, which is required to install pyproject.toml-based projects```\\nHow to solve it?\",1642595209.139000,1642595209.139000,U02TKFAL22G\\nb3cdb49b-5a69-4d1b-b7c6-d71787865681,U01QGQ8B9FT,,,\"Yup even after changing the quotes - no files show up in the ny_data folder.\\nAfter execution of docker run {services ...}  it shows that it is ready to accept connections :\\n`PostgreSQL Database directory appears to contain a database; Skipping initialization` \\n`2022-01-19 12:44:32.606 UTC [1] LOG:  starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit`\\n`2022-01-19 12:44:32.608 UTC [1] LOG:  listening on IPv4 address \"\"0.0.0.0\"\", port 5432`\\n`2022-01-19 12:44:32.608 UTC [1] LOG:  listening on IPv6 address \"\"::\"\", port 5432`\\n`2022-01-19 12:44:32.622 UTC [1] LOG:  listening on Unix socket \"\"/var/run/postgresql/.s.PGSQL.5432\"\"`\\n`2022-01-19 12:44:32.633 UTC [26] LOG:  database system was shut down at 2022-01-19 12:42:58 UTC`\\n`2022-01-19 12:44:32.642 UTC [1] LOG:  database system is ready to accept connections`\",1642586687.122000,1642596404.139200,U01QGQ8B9FT\\n315fc697-613d-42e0-87eb-65b4220d82cb,U02T1BX1UV6,,,\"Thank you, Now I have this error\\n\\nconnection to server at \"\"localhost\"\" (::1), port 5432 failed: Connection refused (0x0000274D/10061)\\n        Is the server running on that host and accepting TCP/IP connections?\",1642416911.154000,1642596950.139800,U02T1BX1UV6\\n6d0e846e-3d2d-4f61-8173-c27293701d20,U029CJ4KY1Z,,,\"If I have postgres install on my machine, while configuring it for docker should I use the same former password?\",1642584839.113900,1642597484.141600,U02TZ7KC7U7\\nd3a7efa7-ba2e-4aa4-b77a-36c8a7e2fedf,,6.0,,\"In the third video, when Alexey restarted postgres and pgadmin, his database was left intact, but mine was not - is my db emptying itself when I stop it or something? how can I stop this from happening?\",1642597721.143600,1642597721.143600,U02TVGE99QU\\n8ceb6df0-4b19-4c84-850c-c2f3b52d628d,U02RREQ7MHU,,,\"Tried all the solutions here and it still not working\\n\\n`winpty docker run -it    -e POSTGRES_USER=\"\"root\"\"     -e POSTGRES_PASSWORD=\"\"root\"\"     -e POSTGRES_DB=\"\"ny_taxi\"\"`     `-v \"\"d:/Documents/Learning/ZoomCamp/Docker_intro/ny_taxi_postgres_data:/var/lib/postgresql/data\"\"`     `-p 5432:5432`     `postgres:13`\\ndocker: Error response from daemon: invalid mode: \\\\Program Files\\\\Git\\\\var\\\\lib\\\\postgresql\\\\data.\\nSee \\'docker run --help\\'\\n\\n<@U01AXE0P5M3>\",1642535362.027400,1642598224.143700,U02QS4BD1NF\\n9d474a81-7d24-4ae1-95bd-ed717135f1f0,U02TBKWL7DJ,,,\"Thanks <@U02BVP1QTQF> for suggesting going through the VM on GCP route. I worked seemless, I guess I will just use that throughout the course\",1642507885.440700,1642598603.144100,U02TBKWL7DJ\\n85B9E7EF-2257-41F8-868E-6429BB4E0590,U02TVGE99QU,,,\"The most likely explanation is that you\\'re not setting the volume correctly. Check that all the paths are set correctly and you\\'ve got the proper permissions on your folder.\\n\\nYou didn\\'t mention your OS so I can\\'t give you more details\",1642597721.143600,1642598873.147100,U02BVP1QTQF\\n3930ef75-6d0c-4725-91ed-fcaadc5c5fa8,,1.0,,\"Postgres converts column names to lowercase unless quoted while creating the table. In the `yellow_taxi_data` table we have columns named PULocationID, DOLocationID, VendorID etc. When I am trying to write a query involving these columns I get an error like:\\n```ERROR:  column \"\"pulocationid\"\" does not exist```\\nDoes anyone have any solution for this?\",1642599152.150500,1642599152.150500,U02T96HEARK\\ncb64f938-425f-47cd-9ac1-780514bd4b3f,U02TVGE99QU,,,\"Also, check that you\\'re running the container in the correct directory (the one that contains _ny_taxi_postgres_data_ folder)\",1642597721.143600,1642599954.151200,U02SUUT290F\\ncce7a913-707b-477a-8a33-eac78f89bc4b,U01AXE0P5M3,,,\"Tried this and this was the error I got...Note that drive D is a secondary drive in my laptop (incase this has anything to do with the issue)\\n<@U01AXE0P5M3>\\n\\nwinpty docker run -it     -e POSTGRES_USER=\"\"root\"\"     -e POSTGRES_PASSWORD=\"\"root\"\"     -e POSTGRES_DB=\"\"ny_taxi\"\"     -v /d/Documents/Learning/ZoomCamp/Docker_intro/ny_taxi_postgres_data:/var/lib/postgresql/data     -p 5432:5432     postgres:13\\ndocker: Error response from daemon: Ports are not available: listen tcp 0.0.0.0:5432: bind: An\\n attempt was made to access a socket in a way forbidden by its access permissions.\",1642534337.023000,1642600737.151500,U02QS4BD1NF\\n6bf39564-7550-4926-9615-b26ca4384781,U02T96HEARK,,,\"Use quotes around names, \"\"DOLocationID\"\"\",1642599152.150500,1642600916.152000,U01AXE0P5M3\\n6b1d89e4-7bf1-42b4-a7c1-84e506776bb8,U01AXE0P5M3,,,Try mapping to a different port,1642534337.023000,1642600963.152200,U01AXE0P5M3\\nd41e7700-757a-4797-a12a-1d7b4ac55506,U029CJ4KY1Z,,,The instance in the container is completely independent from your local postgres. So you can use any password you want,1642584839.113900,1642601027.152500,U01AXE0P5M3\\nc12170da-ff3a-4cb9-ab60-65abe917e696,U02T1BX1UV6,,,Is it? =),1642416911.154000,1642601062.152700,U01AXE0P5M3\\nb59c5185-e57f-4ffd-8b71-8bf3b2f2b5dc,,8.0,,\"hi all,\\nstill facing some issue, has anybody passed this steps here:\\n`If you get a message like\\xa0quota exceeded`\\n&gt; `WARNING: Cannot find a quota project to add to ADC. You might receive a \"\"quota exceeded\"\" or \"\"API not enabled\"\" error. Run\\xa0$ gcloud auth application-default set-quota-project\\xa0to add a quota project.`\\n`Then run this:`\\n```PROJECT_NAME=\"\"ny-rides-alexey\"\"\\ngcloud auth application-default set-quota-project ${PROJECT_NAME}```\\n\",1642603021.154900,1642603021.154900,U02UBV4EC8J\\nfa79d4d1-aadb-4c4d-a06c-147e39f04342,,1.0,,Hello everyone! I am new and interested in Data Engineer. What advice do you give me? Thanks.,1642603091.155100,1642603091.155100,U02UA6P27TQ\\nac83d130-764c-46fc-8957-d0ef3dcec35e,U01AXE0P5M3,,,\"<@U01AXE0P5M3>\\n\\nOut of curiosity though not included in the video walkthrough using VM setup.\\n\\nHow do I download the clone repo in the VM to my local to push to github after completing everything since once I shutdown the VM I can\\'t ssh to it until it\\'s on. So that I can have that has my working repo for the course which I can modify going forward.\\n\\nThis video you did is amazing I was able to complete all the setup on GCP VM to use for the course\",1642544154.050000,1642603750.155200,U02TBKWL7DJ\\n114af49d-3e9c-4fd6-954f-c9736276f2e0,U01AXE0P5M3,,,Also are we to answer the questions using the Jupyter notebook using pandas or inside postgreSql with sql or any?,1642544154.050000,1642604044.155400,U02TBKWL7DJ\\ne6a80f93-f5f6-44ed-8ff5-048a4c89d212,,3.0,,\"Hi, does anybody face following message when trying to gcloud auth application-default for the project?  `because the account in ADC does not have the \"\"serviceusage.services.use\"\" permission on this project.`\",1642604415.157000,1642604415.157000,U01PU2VRBEZ\\nb32d4b2d-7eeb-4949-bf61-a898d48c6d4f,U01PU2VRBEZ,,,\"yes, getting the same one.\",1642604415.157000,1642604443.157100,U02UBV4EC8J\\ndf3c5b72-cec9-4baa-b97d-afd1ecb19ca2,U01AXE0P5M3,,,changed port to `-p 5429:5429` and still got same error,1642534337.023000,1642604550.157400,U02QS4BD1NF\\n6accbe57-2d49-486d-8d98-0d90570b7c0d,U02UBV4EC8J,,,Did you run this?,1642603021.154900,1642605197.158400,U01AXE0P5M3\\n2a439156-cb5b-40b5-85f1-402341b897de,U02UA6P27TQ,,,Take the course =),1642603091.155100,1642605218.158600,U01AXE0P5M3\\nae4dec9f-feda-4e1d-a096-d0f19f89afac,U01AXE0P5M3,,,With pgadmin or anything similar to that,1642544154.050000,1642605493.159200,U01AXE0P5M3\\n37cbe363-0c68-43f8-b853-0fe7f459f528,U01AXE0P5M3,,,\"To answer your first question, you\\'ll need to configure ssh on that machine and upload your key to github. I\\'m sure you\\'ll find a lot of examples on the internet for that\",1642544154.050000,1642605548.159400,U01AXE0P5M3\\n1579ea6f-a53d-4938-ac68-0ee18ed0fe04,U01AXE0P5M3,,,\"The second port shouldn\\'t change, only the first\",1642534337.023000,1642605577.159600,U01AXE0P5M3\\n6bf2cbd0-0f9d-486b-b2fb-b64790362c65,U029CJ4KY1Z,,,Thank you for the clarification,1642584839.113900,1642606028.160600,U02TZ7KC7U7\\n761d9431-5b5e-4cb5-9c79-aa94ad19bc24,U01AXE0P5M3,,,Its still producing the same error,1642534337.023000,1642606640.161400,U02QS4BD1NF\\n17ca447f-2017-481e-9fb6-ca888eb005dd,U01AXE0P5M3,,,yea perhaps a read-only channel/ announcement channel would help,1642488983.400100,1642606990.161700,U02AX5NC5B6\\n32c03a49-cd1d-4a7a-a337-d9805381e8cd,,9.0,,\"```engine = create_engine(\\'<postgresql://root:root@localhost:5432/ny_taxi>\\')\\nNoSuchModuleError: Can\\'t load plugin: sqlalchemy.dialects:postgresql```\\nAnnyone know the error?\",1642607618.162800,1642607618.162800,U02TKFAL22G\\n77a8de1b-baba-4b6c-916b-6d16cd9c4138,U02TKFAL22G,,,Are you running this command on jupyter??,1642607618.162800,1642607846.162900,U02QYKBJWN6\\ne3b8cf45-010c-4445-9aa1-227d12008603,U02UBV4EC8J,,,\"yes I did, I am in this stage now:\\nWARNING: Cannot find a quota project to add to ADC. You might receive a \"\"quota exceeded\"\" or \"\"API not enabled\"\" error. Run\\xa0`$ gcloud auth application-default set-quota-project`\\xa0to add a quota project.\",1642603021.154900,1642607869.163100,U02UBV4EC8J\\n0359b35c-efff-4777-a97c-041c3128bdf5,U02UBV4EC8J,,,\"that command needs a parameter, what should be that paramater?\",1642603021.154900,1642607896.163300,U02UBV4EC8J\\ncac1e77a-3592-4993-be36-26f288796384,U02TKFAL22G,,,yeah,1642607618.162800,1642608649.165000,U02TKFAL22G\\n1d1b3ab7-ecee-4df3-8938-d0d547551cdd,,4.0,,\"Hi, I am trying to follow the “learning in public” advice and made my notes from the docker content into a <https://medium.com/@tjanif/datatalksclub-data-engineering-zoomcamp-week-1-a56d0f5a8760|blog post>. (maybe the notes are helpful for someone) :slightly_smiling_face:\",1642608849.165500,1642608849.165500,U02SE2PSSTC\\n898357e2-2c0a-411f-a74a-961a78a0b4cf,U02TKFAL22G,,,\"What is the output of this command: \"\"python -c \\'import sqlalchemy.dialects.postgresql\\'\"\"  ?\",1642607618.162800,1642608879.165600,U02QYKBJWN6\\n18369787-e686-4807-8d10-85ed0012e707,U01AXE0P5M3,,,\"Thank you, It has finally run successfully but the folder  (ny_taxi_postgres_data) was empty...\\nWhat should I do? <@U01AXE0P5M3>\",1642534337.023000,1642609024.166100,U02QS4BD1NF\\n794185f9-1195-488b-b4f6-7e2ff46a9e8f,U02T1BX1UV6,,,I really don\\'t know how to check that,1642416911.154000,1642609205.166700,U02T1BX1UV6\\n19a74286-c5cd-4711-bdab-c7a85dfcd076,U01PU2VRBEZ,,,you would need to pass the Proejct_ID from your account at the end,1642604415.157000,1642609212.166900,U02UBV4EC8J\\naedb77bd-f1f6-4a2e-a697-4116b78ea90a,U02TKFAL22G,,,i run it on cmd and nothing happen,1642607618.162800,1642609358.167400,U02TKFAL22G\\n8264b53b-e768-42bf-991c-dba7b3192445,U02TKFAL22G,,,\"okay and can you try to run all the commands from importing sqlalchemy to creating engine in the cmd python shell,  and see if it throws the same error?\",1642607618.162800,1642609449.167600,U02QYKBJWN6\\naa69c652-7a42-41e9-b445-1206810b24ba,U02TKFAL22G,,,i run in anaconda prompt and nothing happen same like in the video,1642607618.162800,1642609696.167900,U02TKFAL22G\\n627e8c46-56ff-417a-8eab-3e2d7fd35207,U02TKFAL22G,,,It means it is working locally and not working on jupyter?,1642607618.162800,1642609747.168200,U02QYKBJWN6\\nb1a2eb58-6d12-4d65-a3be-c50ba5c2f2ee,U02TKFAL22G,,,yeah right,1642607618.162800,1642609769.168600,U02TKFAL22G\\n7b833ae5-320b-474e-9248-a9e1a68d8fc7,U02SE2PSSTC,,,great content!,1642608849.165500,1642609849.170200,U02R65WFT16\\nc2dfefc9-3a86-4281-b9fb-aebc6ff021a1,,,,\"FYI for those interested, since <@U01AXE0P5M3> mentioned in one of the videos - yellow taxis are the standard NYC taxi that can pick up and drop off anywhere. Green taxis can only pick up in the outer boroughs, i.e. not Manhattan (although I think they can pick up in the very northern part of Manhattan) and can drop off anywhere.\",,1642609898.171300,U025P29E8D9\\nbe77d420-ebe6-43fb-925b-02f13ed1e4b7,U02TKFAL22G,,,\"I need to restart and for now, is going right\",1642607618.162800,1642610159.171600,U02TKFAL22G\\nF2452C95-A1C8-47AC-91FE-430621778553,U02QZN0LSBT,,,This is too good. Especially the memes :grimacing:,1642548032.081900,1642610317.172400,U02TN145GE5\\n6272fd94-df37-450a-9e65-ef1d542f3a09,,2.0,,\"Hi everyone, I was unable to create server in the browser using pg-database directly but if I use IP of the container then I could create the server. May I know why is it happening so. TIA!\",1642610988.174800,1642610988.174800,U02V4412XFA\\nb9f380d9-8dae-4386-a507-62464e0076bd,U02UX664K5E,,,\"This is a good feature to have on in vscode … if you’re using VS code that is <https://code.visualstudio.com/docs/editor/codebasics#:~:text=Save%20%2F%20Auto%20Save%23&amp;text=The%20easiest%20way%20to%20turn,files>.\",1642541435.038900,1642611354.175300,U02TEERF0DA\\nff660b56-3a46-4c51-9c93-089a6d73b275,U02U88WH7D0,,,\"I read in the channel that a virtual bank card works, look for the key word \\'virtual bank card\\' in the channel to see the answers I mean :blush:\",1642511203.454300,1642611457.175700,U01SK4597RS\\n6745d6e9-3682-4691-bdf6-f5d073da1e17,,8.0,,Anyone got this operational error while running the engine.connect() command?,1642611614.176500,1642611614.176500,U02QGA57GRY\\n40dc2370-2543-41ef-b738-9d2a93619b09,U02UBV4EC8J,,,Your project name,1642603021.154900,1642611783.177100,U01AXE0P5M3\\n05F7865A-FC69-47B7-B516-8A3C49F67F36,,,,Week 1 with Docker feels like an Army Physical Fitness Test :man-running: ,,1642611831.177400,U02UX664K5E\\n3b3e596d-3760-4e7b-855a-79ffaceee1b3,U02QGA57GRY,,,Can you show the full error?,1642611614.176500,1642611907.177900,U01AXE0P5M3\\n62412318-2ecc-4753-a733-f15af4db0c05,U02SE2PSSTC,,,Thank you! :blush:,1642608849.165500,1642611920.178200,U02SE2PSSTC\\nC84992A6-9053-4693-A2BF-5DFB96CE3978,U02SE2PSSTC,,,Excellent effort :+1::skin-tone-2:,1642608849.165500,1642612042.178900,U02AGF1S0TY\\n5c3d6e91-b3f7-48a8-b831-d97dd1da1f72,U02UBV4EC8J,,,yeap I got it working Thanks,1642603021.154900,1642612160.179100,U02UBV4EC8J\\n4d5ec40e-ed0a-41fa-913b-7314cb5ffdbf,U02UX664K5E,,,\"<@U02UX664K5E>\\nwhen we execute\\xa0`docker build -t test:pandas .`\\xa0, what we are doing basically is create a docker image named\\xa0`test:pandas`\\nhere \\'pandas\\' is a tag, being more precise :wink:\",1642546712.068400,1642612162.179300,U029DM0GQHJ\\nc1d9bb4c-11e0-4023-887a-02908c2d40e8,U02QGA57GRY,,,\"```---------------------------------------------------------------------------\\nOperationalError                          Traceback (most recent call last)\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\engine\\\\base.py in _wrap_pool_connect(self, fn, connection)\\n   3140         try:\\n-&gt; 3141             return fn()\\n   3142         except dialect.dbapi.Error as e:\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\pool\\\\base.py in connect(self)\\n    300         \"\"\"\"\"\"\\n--&gt; 301         return _ConnectionFairy._checkout(self)\\n    302 \\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\pool\\\\base.py in _checkout(cls, pool, threadconns, fairy)\\n    754         if not fairy:\\n--&gt; 755             fairy = _ConnectionRecord.checkout(pool)\\n    756 \\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\pool\\\\base.py in checkout(cls, pool)\\n    418     def checkout(cls, pool):\\n--&gt; 419         rec = pool._do_get()\\n    420         try:\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\pool\\\\impl.py in _do_get(self)\\n    144                 with util.safe_reraise():\\n--&gt; 145                     self._dec_overflow()\\n    146         else:\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\util\\\\langhelpers.py in __exit__(self, type_, value, traceback)\\n     69             if not self.warn_only:\\n---&gt; 70                 compat.raise_(\\n     71                     exc_value,\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\util\\\\compat.py in raise_(***failed resolving arguments***)\\n    210         try:\\n--&gt; 211             raise exception\\n    212         finally:\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\pool\\\\impl.py in _do_get(self)\\n    141             try:\\n--&gt; 142                 return self._create_connection()\\n    143             except:\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\pool\\\\base.py in _create_connection(self)\\n    246 \\n--&gt; 247         return _ConnectionRecord(self)\\n    248 \\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\pool\\\\base.py in __init__(self, pool, connect)\\n    361         if connect:\\n--&gt; 362             self.__connect(first_connect_check=True)\\n    363         self.finalize_callback = deque()\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\pool\\\\base.py in __connect(self, first_connect_check)\\n    604             with util.safe_reraise():\\n--&gt; 605                 pool.logger.debug(\"\"Error on connect(): %s\"\", e)\\n    606         else:\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\util\\\\langhelpers.py in __exit__(self, type_, value, traceback)\\n     69             if not self.warn_only:\\n---&gt; 70                 compat.raise_(\\n     71                     exc_value,\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\util\\\\compat.py in raise_(***failed resolving arguments***)\\n    210         try:\\n--&gt; 211             raise exception\\n    212         finally:\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\pool\\\\base.py in __connect(self, first_connect_check)\\n    598             self.starttime = time.time()\\n--&gt; 599             connection = pool._invoke_creator(self)\\n    600             pool.logger.debug(\"\"Created new connection %r\"\", connection)\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\engine\\\\create.py in connect(connection_record)\\n    577                         return connection\\n--&gt; 578             return dialect.connect(*cargs, **cparams)\\n    579 \\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\engine\\\\default.py in connect(self, *cargs, **cparams)\\n    558         # inherits the docstring from interfaces.Dialect.connect\\n--&gt; 559         return self.dbapi.connect(*cargs, **cparams)\\n    560 \\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\psycopg2\\\\__init__.py in connect(dsn, connection_factory, cursor_factory, **kwargs)\\n    121     dsn = _ext.make_dsn(dsn, **kwargs)\\n--&gt; 122     conn = _connect(dsn, connection_factory=connection_factory, **kwasync)\\n    123     if cursor_factory is not None:\\n\\nOperationalError: connection to server at \"\"localhost\"\" (::1), port 5432 failed: FATAL:  password authentication failed for user \"\"root\"\"\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\nOperationalError                          Traceback (most recent call last)\\n&lt;ipython-input-28-4070ba9f91a5&gt; in &lt;module&gt;\\n----&gt; 1 engine.connect()\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\engine\\\\base.py in connect(self, close_with_result)\\n   3093         \"\"\"\"\"\"\\n   3094 \\n-&gt; 3095         return self._connection_cls(self, close_with_result=close_with_result)\\n   3096 \\n   3097     @util.deprecated(\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\engine\\\\base.py in __init__(self, engine, connection, close_with_result, _branch_from, _execution_options, _dispatch, _has_events)\\n     89                 connection\\n     90                 if connection is not None\\n---&gt; 91                 else engine.raw_connection()\\n     92             )\\n     93 \\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\engine\\\\base.py in raw_connection(self, _connection)\\n   3172 \\n   3173         \"\"\"\"\"\"\\n-&gt; 3174         return self._wrap_pool_connect(self.pool.connect, _connection)\\n   3175 \\n   3176 \\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\engine\\\\base.py in _wrap_pool_connect(self, fn, connection)\\n   3142         except dialect.dbapi.Error as e:\\n   3143             if connection is None:\\n-&gt; 3144                 Connection._handle_dbapi_exception_noconnection(\\n   3145                     e, dialect, self\\n   3146                 )\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\engine\\\\base.py in _handle_dbapi_exception_noconnection(cls, e, dialect, engine)\\n   2001             util.raise_(newraise, with_traceback=exc_info[2], from_=e)\\n   2002         elif should_wrap:\\n-&gt; 2003             util.raise_(\\n   2004                 sqlalchemy_exception, with_traceback=exc_info[2], from_=e\\n   2005             )\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\util\\\\compat.py in raise_(***failed resolving arguments***)\\n    209 \\n    210         try:\\n--&gt; 211             raise exception\\n    212         finally:\\n    213             # credit to\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\engine\\\\base.py in _wrap_pool_connect(self, fn, connection)\\n   3139         dialect = self.dialect\\n   3140         try:\\n-&gt; 3141             return fn()\\n   3142         except dialect.dbapi.Error as e:\\n   3143             if connection is None:\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\pool\\\\base.py in connect(self)\\n    299 \\n    300         \"\"\"\"\"\"\\n--&gt; 301         return _ConnectionFairy._checkout(self)\\n    302 \\n    303     def _return_conn(self, record):\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\pool\\\\base.py in _checkout(cls, pool, threadconns, fairy)\\n    753     def _checkout(cls, pool, threadconns=None, fairy=None):\\n    754         if not fairy:\\n--&gt; 755             fairy = _ConnectionRecord.checkout(pool)\\n    756 \\n    757             fairy._pool = pool\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\pool\\\\base.py in checkout(cls, pool)\\n    417     @classmethod\\n    418     def checkout(cls, pool):\\n--&gt; 419         rec = pool._do_get()\\n    420         try:\\n    421             dbapi_connection = rec.get_connection()\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\pool\\\\impl.py in _do_get(self)\\n    143             except:\\n    144                 with util.safe_reraise():\\n--&gt; 145                     self._dec_overflow()\\n    146         else:\\n    147             return self._do_get()\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\util\\\\langhelpers.py in __exit__(self, type_, value, traceback)\\n     68             self._exc_info = None  # remove potential circular references\\n     69             if not self.warn_only:\\n---&gt; 70                 compat.raise_(\\n     71                     exc_value,\\n     72                     with_traceback=exc_tb,\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\util\\\\compat.py in raise_(***failed resolving arguments***)\\n    209 \\n    210         try:\\n--&gt; 211             raise exception\\n    212         finally:\\n    213             # credit to\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\pool\\\\impl.py in _do_get(self)\\n    140         if self._inc_overflow():\\n    141             try:\\n--&gt; 142                 return self._create_connection()\\n    143             except:\\n    144                 with util.safe_reraise():\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\pool\\\\base.py in _create_connection(self)\\n    245         \"\"\"\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\"\"\"\\n    246 \\n--&gt; 247         return _ConnectionRecord(self)\\n    248 \\n    249     def _invalidate(self, connection, exception=None, _checkin=True):\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\pool\\\\base.py in __init__(self, pool, connect)\\n    360         self.__pool = pool\\n    361         if connect:\\n--&gt; 362             self.__connect(first_connect_check=True)\\n    363         self.finalize_callback = deque()\\n    364 \\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\pool\\\\base.py in __connect(self, first_connect_check)\\n    603         except Exception as e:\\n    604             with util.safe_reraise():\\n--&gt; 605                 pool.logger.debug(\"\"Error on connect(): %s\"\", e)\\n    606         else:\\n    607             if first_connect_check:\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\util\\\\langhelpers.py in __exit__(self, type_, value, traceback)\\n     68             self._exc_info = None  # remove potential circular references\\n     69             if not self.warn_only:\\n---&gt; 70                 compat.raise_(\\n     71                     exc_value,\\n     72                     with_traceback=exc_tb,\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\util\\\\compat.py in raise_(***failed resolving arguments***)\\n    209 \\n    210         try:\\n--&gt; 211             raise exception\\n    212         finally:\\n    213             # credit to\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\pool\\\\base.py in __connect(self, first_connect_check)\\n    597         try:\\n    598             self.starttime = time.time()\\n--&gt; 599             connection = pool._invoke_creator(self)\\n    600             pool.logger.debug(\"\"Created new connection %r\"\", connection)\\n    601             self.connection = connection\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\engine\\\\create.py in connect(connection_record)\\n    576                     if connection is not None:\\n    577                         return connection\\n--&gt; 578             return dialect.connect(*cargs, **cparams)\\n    579 \\n    580         creator = pop_kwarg(\"\"creator\"\", connect)\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\sqlalchemy\\\\engine\\\\default.py in connect(self, *cargs, **cparams)\\n    557     def connect(self, *cargs, **cparams):\\n    558         # inherits the docstring from interfaces.Dialect.connect\\n--&gt; 559         return self.dbapi.connect(*cargs, **cparams)\\n    560 \\n    561     def create_connect_args(self, url):\\n\\nD:\\\\anaconda3\\\\lib\\\\site-packages\\\\psycopg2\\\\__init__.py in connect(dsn, connection_factory, cursor_factory, **kwargs)\\n    120 \\n    121     dsn = _ext.make_dsn(dsn, **kwargs)\\n--&gt; 122     conn = _connect(dsn, connection_factory=connection_factory, **kwasync)\\n    123     if cursor_factory is not None:\\n    124         conn.cursor_factory = cursor_factory\\n\\nOperationalError: (psycopg2.OperationalError) connection to server at \"\"localhost\"\" (::1), port 5432 failed: FATAL:  password authentication failed for user \"\"root\"\"\\n\\n(Background on this error at: <http://sqlalche.me/e/14/e3q8>)```\\nThis is the error...\",1642611614.176500,1642612830.180100,U02QGA57GRY\\n4fadb3dc-7232-47fc-82ed-fbc8410a355a,,,,\"Found this free course on terraform fundamentals with aws.\\n\\n<https://courses.morethancertified.com/p/rfp-terraform?s=09|https://courses.morethancertified.com/p/rfp-terraform?s=09>\",,1642612977.181500,U0290EYCA7Q\\n69b846fe-1deb-4250-8e72-75892f109ace,,2.0,,\"has anyone else had a result like this when trying to group the taxi data by month? I have the same count of rows as the tutorial video, but it’s almost entirely in January 2021?\",1642613126.183000,1642613126.183000,U02UGA597HS\\nb2741c17-4f46-428a-9705-3ee66560af72,U02TKFAL22G,,,i have the same error too,1642595209.139000,1642613470.183700,U02U5GQK25C\\nda105cc5-9f1a-43f0-bf1d-b883a67df0ff,U01QGQ8B9FT,,,Yup still clueless about why no extra files are getting added in the `ny_taxi_postgres_data` folder,1642586687.122000,1642614350.184200,U01QGQ8B9FT\\n68406cf9-18b1-4047-a0a4-ccf6fc0642ee,U01QGQ8B9FT,,,Tried both Gi bash and WSL(Ubuntu - 20.04),1642586687.122000,1642614388.184400,U01QGQ8B9FT\\n7c854c2c-8552-4708-851f-8614998053ea,,,,\"For who may need. To run head or less in PowerShell (Windows with WSL)\\n<https://stackoverflow.com/questions/9682024/how-to-do-what-head-tail-more-less-sed-do-in-powershell>\",,1642615101.185500,U02CD7E30T0\\ncc277f48-6bfc-4871-b3e7-43654ef35803,,2.0,,\"Hey <@U01AXE0P5M3> I suppose the Green Taxi are the one that pay Elon Musk salary :stuck_out_tongue:  I suppose they must be electric cars. There is a similar approach for Uber.\\n\"\"Uber Green\\nWhat is Uber Green? Uber Green works in exactly the\\xa0*same way as a regular Uber*. Same app, same great service. The only difference is that you\\'ll be riding in a 100% electric vehicle - no hybrids, diesel or petrol - helping to reduce emissions and keep London\\'s air cleaner.\"\"\",1642615521.188400,1642615521.188400,U02CD7E30T0\\ncfc4c996-812b-4c37-9590-c8afe2d48678,,27.0,,\"I was able to install postgres and at the time of running `pgcli`  I get an error when entering root as my password, any idea?\\n```lyesd@LAPTOP-KB2SKS8F MINGW64 ~/Desktop/DE-ZoomCamp/week1_basics_n_setup/2_docker_sql\\n$ pgcli\\nPassword for lyesd: \\nconnection to server at \"\"localhost\"\" (::1), port 5432 failed: FATAL:  password authentication failed for user \"\"lyesd\"\"```\",1642615746.189300,1642615746.189300,U02UX664K5E\\n73A18C60-5601-4C86-A675-C332DF31F730,U02UX664K5E,,,\"Did u run the command \\n$ pgcli -h localhost -p 5432 -u root -d nyc_taxi \",1642615746.189300,1642616006.190700,U02AGF1S0TY\\n5be6d532-3bce-4bb6-8514-14d493a2c2a5,U02UX664K5E,,,\"same here.. i was facing some trouble running this command:\\n\\n```docker run -it \\\\\\n  -e POSTGRES_USER=\"\"root\"\" \\\\\\n  -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n  -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n  -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n  -p 5432:5432 \\\\\\n  postgres:13```\\nso instead i ran and it worked:\\n\\n```docker run -it \\\\\\n  -e POSTGRES_USER=\"\"root\"\" \\\\\\n  -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n  -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n  -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n  -p 5432 \\\\\\n  postgres:13```\\nnow when i try to connect using the below command, it wont:\\n\\n```Password for root: \\nFATAL:  password authentication failed for user \"\"root\"\"```\",1642615746.189300,1642616234.190900,U029DM0GQHJ\\nD52D842E-BB30-41B6-AFAC-5F6E9129CB14,U02UX664K5E,,,<@U029DM0GQHJ> what was the error first time ? My understanding is you have to map local machine port to container port . These errors are strange sometimes ! ,1642615746.189300,1642616565.193600,U02AGF1S0TY\\nf73d9455-3993-47f3-8add-c54532482b1c,U02UX664K5E,,,\"<@U02AGF1S0TY>\\n```docker: Error response from daemon: Ports are not available: listen tcp 0.0.0.0:5432: bind: address already in use.\\nERRO[0000] error waiting for container: context canceled ```\\nthen i came across this on github:\\n\\n<https://github.com/docker/compose/issues/4950>\",1642615746.189300,1642616721.193800,U029DM0GQHJ\\n3BD8CF14-EE83-4904-B6B1-17865BF8705F,U02UX664K5E,,,Did you try stopping and removing all not in use containers and give the Postgres container a fresh run ?  Not sure about this error .,1642615746.189300,1642616987.196500,U02AGF1S0TY\\n11b3b961-39cc-4984-8742-10b727451481,U02UX664K5E,,,\"<@U02AGF1S0TY> That\\'s what I\\'m doing, did: `docker container prune`\",1642615746.189300,1642617063.196700,U02UX664K5E\\nFDE9F259-FC35-4812-9EAB-DAB66FFDF773,U02UX664K5E,,,$ docker system prune -a ,1642615746.189300,1642617111.197400,U02AGF1S0TY\\n4098E645-F891-4422-BF43-154D2998276B,U02UX664K5E,,,It will also remove inactive containers ,1642615746.189300,1642617131.198100,U02AGF1S0TY\\nb207a60f-9877-46b1-8944-64f0ebae2fe7,U02UX664K5E,,,\"<@U02AGF1S0TY> yes, all got removed, now I also want to remove the postgres:13 image and all its associated content, what would be the best command to achieve this as I think I messed up a few things and I want to start completely from scratch\",1642615746.189300,1642617189.198300,U02UX664K5E\\ne9ee9a23-7cb0-4ec7-9fa0-2f2fe3901664,U02UX664K5E,,,\"<@U029DM0GQHJ> I think with the p flag you need to map it otherwise you cannot connect ~(e.g. -p &lt;local_port&gt;:&lt;docker_port&gt;~ ) (corrected! e.g. -p &lt;docker_port&gt;:&lt;local_port&gt; ). I had issues with trying to use the default local port I think because I have a local install of postgres. See mine and other suggestions in this thread:\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642526987496200?thread_ts=1642523808.486200&amp;cid=C01FABYF2RG>\",1642615746.189300,1642617214.198600,U02UE7NTLUU\\nd4b9f746-f1b9-4631-b3b0-9c454bcf27cc,U02QGA57GRY,,,That\\'s a very long one! But it says that it wasn\\'t able to authenticate,1642611614.176500,1642617346.201600,U01AXE0P5M3\\n5C0F93AC-26E7-4EF6-966E-5CA1C7AF4CC2,U02UX664K5E,,,<@U02UX664K5E> first check $ docker images and $ docker container ps and $ docker container ps -a will tell status of images and containers in use and not in use ..your system probe should have removed all of this and given you a clean slate .just cross check before restarting the containers again ,1642615746.189300,1642617380.202800,U02AGF1S0TY\\naa604e92-a249-44f7-92d7-56cd4a532889,U02UX664K5E,,,\"<@U02UE7NTLUU> yes, Alexey suggested to use `-p 5432:5431`\",1642615746.189300,1642617384.203000,U02UX664K5E\\n31881c64-be75-4c3d-8d9e-c0e3ddf95044,U02T1BX1UV6,,,\"When you run docker run, what happened after that? Is it still running?\",1642416911.154000,1642617405.203200,U01AXE0P5M3\\nd95d7b1d-ea62-479b-b05b-f4fb3014f575,U01AXE0P5M3,,,Try to put some data there,1642534337.023000,1642617427.203400,U01AXE0P5M3\\n0c881e59-df18-489d-af7c-bd714cfe6aa0,U02UX664K5E,,,ah ok missed that. I needed to delete my local volume as well before rerunning the command if that hasn\\'t been tried yet.,1642615746.189300,1642617547.203600,U02UE7NTLUU\\n0456dc85-55df-4254-97b6-124dda5ce3ef,U02CD7E30T0,,,\"```Yellow cabs are the official, and iconic, taxis in NYC. \\n\\nGreen cabs are new to the city, since 2013, and the program was created to serve areas of New York not commonly served by yellow medallion cabs.\\n\\nGreen taxis are allowed to pick up passengers in northern Manhattan (north of West 110th street and East 96th street), and anywhere in the Bronx, Brooklyn, Staten Island, and Queens (excluding the airports). \\n\\nThey can drop you off anywhere without restriction. ```\",1642615521.188400,1642617700.203900,U02UD2P3BQC\\n1d35821a-e22c-499f-97c6-f93051ef1c7e,U02QGA57GRY,,,Yesss :disappointed:,1642611614.176500,1642617706.204200,U02QGA57GRY\\nc5582756-7824-4ce7-bbbc-796a944a9660,U02UX664K5E,,,\"Also, you may have already caught this but in your first code block you\\'re not passing in the connection parameters.\",1642615746.189300,1642617964.204400,U02UE7NTLUU\\n3c714cce-dca2-4a72-bcfd-32c06561804f,U02UX664K5E,,,\"<@U02UE7NTLUU>yes, but now I screw something else, doing this all over again with a fresh image\",1642615746.189300,1642618080.204600,U02UX664K5E\\ned88dae1-60c5-4c51-8abc-be8ce84b50e7,U02UGA597HS,,,\"The csv contains data from January, so yes, it\\'s expected. I\\'m wondering about other months though, why they are there\",1642613126.183000,1642618104.204800,U01AXE0P5M3\\nda64ef65-114f-4b60-b6dd-291b110423a8,U02UX664K5E,,,\"<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642617384203000?thread_ts=1642615746.189300&amp;cid=C01FABYF2RG|https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642617384203000?thread_ts=1642615746.189300&amp;cid=C01FABYF2RG>\\n\\nThe other way around: -p 5431:5432\",1642615746.189300,1642618236.205000,U01AXE0P5M3\\n1302f14d-8865-44ff-8317-dcb69f2a3e41,U02UX664K5E,,,\"The first is the port on your computer, the second  - the port on the container. The second should always be 5432\",1642615746.189300,1642618280.205400,U01AXE0P5M3\\nb6256c85-6211-448b-903c-5e7d057a2f69,U02UX664K5E,,,\"<@U01AXE0P5M3> Thanks redoing this all over gain, would it be feasible to specify that when running the image like:\\n\\n```docker run -it \\\\\\n\\xa0 \\xa0 -e POSTGRES_USER=\"\"root\"\" \\\\\\n\\xa0 \\xa0 -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n\\xa0 \\xa0 -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n\\xa0 \\xa0 -v c:/Users/lyesd/Desktop/DE-ZoomCamp/week1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n\\xa0 \\xa0 -p 5431:5432 \\\\\\n\\xa0 \\xa0 postgres:13```\",1642615746.189300,1642618402.205800,U02UX664K5E\\n57062305-5ca7-421d-8779-bd5134879618,,2.0,,\"Hi guys. I need some help with GCP and ssh\\n\\nI created an ssh folder and created a key there. I added the key to GCP.\\n\\nWhen I run the command `ssh de-zoomcamp` I get an error:\\n\\n`/c/Users/\\\\320\\\\230\\\\320\\\\273\\\\321\\\\214\\\\320\\\\275\\\\321\\\\203\\\\321\\\\200/.ssh/config: line 4: Bad configuration option: identifyfile`\\n`/c/Users/\\\\320\\\\230\\\\320\\\\273\\\\321\\\\214\\\\320\\\\275\\\\321\\\\203\\\\321\\\\200/.ssh/config: terminating, 1 bad configuration option`\\n\\nIt\\'s because my folder path contains Cyrillic characters.\\nHow can I solve this problem? As I understand it, just moving the folder to another location won\\'t help.\",1642618526.206600,1642618526.206600,U02Q7466F2T\\ne52d4a3b-9d56-4e3e-9b00-735482a9604a,U02T1BX1UV6,,,docker run is working,1642416911.154000,1642618623.206700,U02T1BX1UV6\\n70e614f1-50ac-48cf-9121-c2b293185655,U02T1BX1UV6,,,\"Yes, it still runninng\",1642416911.154000,1642618635.206900,U02T1BX1UV6\\n176f5f19-dd14-4bc8-ae81-f82e39853e7b,U02Q7466F2T,,,should I do it all over again but in a different folder?,1642618526.206600,1642618702.207100,U02Q7466F2T\\n7879458f-08a2-47c0-9a66-33ad36c783be,U02UX664K5E,,,thanks <@U01AXE0P5M3> &amp; <@U02UE7NTLUU>,1642615746.189300,1642618816.207300,U029DM0GQHJ\\n0083735a-492d-49e7-8540-bfa4a44152b2,U02UX664K5E,,,\"Thanks everyone, it did work this time!\",1642615746.189300,1642620063.208400,U02UX664K5E\\nb90d2f6f-40e3-4cb8-8a86-a9f3f1f502cf,,2.0,,\"```Hi i have this error, not sure how to solve it connection to server at \"\"localhost\"\" (::1), port 5432 failed: Connection refused (0x0000274D/10061)\\n        Is the server running on that host and accepting TCP/IP connections?\\nconnection to server at \"\"localhost\"\" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)\\n        Is the server running on that host and accepting TCP/IP connections?```\",1642620437.210500,1642620437.210500,U02U5GQK25C\\n8a17760d-75c4-47d9-8778-106ceb7d10fb,U02U5GQK25C,,,Do you have have Postgres installed? You need Postgres service running to connect to the DB. If not yet installed head over here to install the version for your OS: <https://www.enterprisedb.com/downloads/postgres-postgresql-downloads>,1642620437.210500,1642620539.210800,U02UX664K5E\\n21e3ce25-ceab-4331-8824-ac7422084988,U02CD7E30T0,,,In the repo you\\'ll find the link to the dataset and find the documentation that Fernando shared plus also some information on the areas and so on,1642615521.188400,1642620785.211100,U01B6TH1LRL\\nd0a1b960-6386-48bd-844c-e5cca7a0c8b9,U02Q7466F2T,,,yes try that,1642618526.206600,1642620872.211400,U01AXE0P5M3\\n76231cba-2d7d-42e8-add7-c6419121e3fa,U02T1BX1UV6,,,Do you do port mapping? Can you try to map to a different port?,1642416911.154000,1642621158.212300,U01AXE0P5M3\\n7ec6dae6-bd57-47f3-85e1-61eb278e8430,U02T1BX1UV6,,,<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642615746189300?thread_ts=1642615746.189300&amp;cid=C01FABYF2RG|https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642615746189300?thread_ts=1642615746.189300&amp;cid=C01FABYF2RG>,1642416911.154000,1642621177.212500,U01AXE0P5M3\\ne8f9b536-6baa-4861-b7a2-9b9ad4975c63,,22.0,,\"If you had a problem with postgres not creating files in the mounted folder and you solved it, what helped? It seems like a common problem\",1642621263.215300,1642621263.215300,U01AXE0P5M3\\ncd6798c2-738e-47c1-9daa-26b995241f00,,,,\"Friends, don\\'t forget to pull the latest version of the course down from github. `cd` into the course directory(where you cloned it) and then, with `git` installed, run `git fetch &amp;&amp; git pull` :slightly_smiling_face:\",,1642621274.215400,U02U06KSE3G\\n715f0d49-23fd-4ad3-805e-9b9bfb5a39dd,U02UGA597HS,,,\"Ah, I thought it was supposed to be for the entire year. Thank you\",1642613126.183000,1642621470.215600,U02UGA597HS\\n5be191db-e45f-4bce-afa3-068f1f97bae6,U02SE2PSSTC,,,\"Indeed, thanks for putting it together!\",1642608849.165500,1642621568.215800,U01AXE0P5M3\\n1b3f5761-d588-411b-ad7e-dcb8bbbdc6b1,U01AXE0P5M3,,,\"This suggestion worked for me:\\n\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642586817122700?thread_ts=1642586687.122000&amp;cid=C01FABYF2RG>\",1642621263.215300,1642623497.216600,U02RW6AR91S\\na2412df0-c36e-4167-a988-c8839823ed44,U01AXE0P5M3,,,Thank you! Are you on mac? or linux?,1642621263.215300,1642623645.216900,U01AXE0P5M3\\n76335ebb-ddce-42e1-91b8-20a46d20d18b,U01AXE0P5M3,,,linux,1642621263.215300,1642623722.217100,U02RW6AR91S\\nfe0fc1a5-87d9-47ed-ab7f-3ec0cc47f4e3,U01PU2VRBEZ,,,I did include my chosen project name in gcloud auth application-default command. It worked with the project id automatically assigned by Google instead!,1642604415.157000,1642624908.218500,U01PU2VRBEZ\\n2c515ee4-1b44-48e6-a52d-14d2dc6b1506,U01AXE0P5M3,,,\"Thanks <@U01AXE0P5M3>\\n\\nI tried restarting my VM on GCP to resume as instructed in the video but when I do `docker-compose up -d` to start the docker instance in the VM i get this message\\n\\n`no configuration file provided: not found`\",1642544154.050000,1642625227.218700,U02TBKWL7DJ\\n42634e0c-896b-4da4-a43d-f7a966235d87,U01AXE0P5M3,,,Which configuration file do I need again?,1642544154.050000,1642625312.218900,U02TBKWL7DJ\\nb72c63b7-4753-462a-8534-3c65a6cd9cef,U01AXE0P5M3,,,\"I adjusted the readme to mention this problem:\\n\\n<https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup/2_docker_sql#linux-and-macos>\",1642621263.215300,1642625725.219200,U01AXE0P5M3\\n2b507c3a-3231-4a95-89ad-912c871ea7d7,U01QGQ8B9FT,,,\"I\\'ve tried both as well, liking WSl one more, I am running terraform and gcloud from here as well\",1642586687.122000,1642625828.219500,U02UBV4EC8J\\n125d150f-166f-4893-a848-68cbde4b3508,U01AXE0P5M3,,,you need to do it in the directory with docker-compose.yaml. preparing the videos for docker compose take more time than expected :disappointed:,1642544154.050000,1642625891.219700,U01AXE0P5M3\\n21ee6123-0bd0-4e4e-a8c0-5eb3a290fc4a,U01QGQ8B9FT,,,\"for terraform I use tfswitch here curl -L <https://raw.githubusercontent.com/warrensbox/terraform-switcher/release/install.sh> | bash\\n\\nit is asome tool, can change the versions. Also, it does install terraform for you.\",1642586687.122000,1642625899.219900,U02UBV4EC8J\\nf5adbe7f-0c6c-49cc-a1d6-72b65317e5cf,U01AXE0P5M3,,,\"I was getting an issue installing \"\"pgcli\"\" using python, so what I did was I am using WSL Ubuntu20.4 I use apt-get install pgcli  and worked. This might still be missing the \"\"psycopg2\"\" which is essential for PostgresQL, resolved that by following this: <https://pypi.org/project/psycopg2/>\",1642621263.215300,1642626107.220100,U02UBV4EC8J\\n4a83504e-f0b7-4b90-b0d1-4b0d30abb818,U01AXE0P5M3,,,this command here should fix it: `pip install psycopg2-binary`,1642621263.215300,1642626143.220400,U02UBV4EC8J\\nf150a2c7-43f2-4993-a9b1-0cd167942c97,U01AXE0P5M3,,,\"BTW, this is an awesome tutorial <@U01AXE0P5M3>, thank you for putting all this\",1642621263.215300,1642626186.220600,U02UBV4EC8J\\ned393a6f-28e4-42c1-a177-a3670338e567,U01AXE0P5M3,,,\"I am in the last piece, will finish it soon, doing the docker-compose portion\",1642621263.215300,1642626234.220800,U02UBV4EC8J\\n951CD68B-E54D-419C-83BC-1F4929D8E1EB,,3.0,,\"Just after some clarity. Are we cloning the git repo locally, and using what\\'s there, or are we setting up everything on our own? I\\'m watching the docker videos and unsure if I should be coding along with it.\",1642626751.223400,1642626751.223400,U02U34YJ8C8\\nb542edba-eff1-4025-a931-9021accdd2e1,U02U34YJ8C8,,,\"I’ve set up everything on my own, but you can also clone the repo and add your stuff and keep it as a separate fork. Whichever is more comfortable for you.\",1642626751.223400,1642626814.223600,U02BVP1QTQF\\n3d33e4f8-8c72-4270-9a70-8878a24b747d,U02U34YJ8C8,,,I find doing everything on my own to be a much better method of learning. Highly recommend trying to code along.,1642626751.223400,1642627327.224200,U02QPQFUU5Q\\n276f1187-dcfb-46b7-85f6-0763743dc1fb,U02QZN0LSBT,,,It is awesome ! Thanks for sharing :slightly_smiling_face:,1642548032.081900,1642627612.224900,U02T2DX4LG6\\neacc8a2b-4546-4163-b661-e4fb5d5c648a,,1.0,,For all the SQL warriors this eve - <https://wiki.postgresql.org/wiki/Don%27t_Do_This#Don.27t_use_BETWEEN_.28especially_with_timestamps.29>,1642628140.226200,1642628140.226200,U02U06KSE3G\\nf129101f-d057-4e94-b169-228379a6e876,U02QGJE2AUE,,,\"The workshop was always available on the git repo to try out. And we have just uploaded an audio clip too (sorry, the video had technical issues). <https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup#gcp--terraform>\",1642574514.102300,1642628239.226300,U01DHB2HS3X\\nc3b5f188-fd39-4545-ac97-78a2c1acb02e,U02U06KSE3G,,,,1642628140.226200,1642628285.226500,U02U06KSE3G\\nBC84D7C6-0D26-4B56-9642-9796C419DB8E,U02U34YJ8C8,,,Thank you! I\\'ll maybe do it on my own as well. Probably learn better that way ,1642626751.223400,1642628466.227900,U02U34YJ8C8\\n03d73a5d-45a8-4938-9f53-be43f9680e7b,,3.0,,\"We have some technical difficulties with preparing the second terraform video. But we have audio!\\n\\n<https://drive.google.com/file/d/1IqMRDwJV-m0v9_le_i2HA_UbM_sIWgWx/view?usp=sharing>\\n\\nSo you can listen to it as a podcast :smiley: (And there are some textual instructions here as well - <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/1_terraform_overview.md>)\\n\\nWe\\'ll work on preparing the video too - but it\\'ll come with a delay\",1642629584.230700,1642629584.230700,U01AXE0P5M3\\nc306218d-5d3d-4989-9d2e-2cd24a9009a9,U02UX664K5E,,,\"Hello I am still facing the same issue, of password authetication failed\",1642615746.189300,1642629973.231100,U02SPLJUR42\\nada67a86-7987-46d6-9106-ed7fec0f0538,U02UX664K5E,,,\"Actually, it still not working me for neither, I tried every possible solution and still can\\'t connect, did the port remapping, tried to build the image with my local user, restarted postgres service, I\\'m hopeless at this point...\\n```lyesd@LAPTOP-KB2SKS8F MINGW64 ~/Desktop/DE-ZoomCamp/week1_basics_n_setup/2_docker_sql\\n$ pgcli -h localhost -p 5432 -u root -d ny_taxi\\nPassword for root: \\nconnection to server at \"\"localhost\"\" (::1), port 5432 failed: FATAL:  password authentication failed for user \"\"root\"\"```\",1642615746.189300,1642630312.232300,U02UX664K5E\\n813f377d-cd74-409b-91fe-6a1ac918ff3e,,24.0,,Hi. Having a bit of trouble with postgres and docker. Can someone assist? Will link error in comments.,1642630680.235000,1642630680.235000,U02U34YJ8C8\\ndcb9b7c5-bff0-4bd9-ad50-2bb01d30bed8,U02U34YJ8C8,,,\"This is what I’m running:\\n\\n```docker run -it \\\\\\n  -e POSTGRES_USER=\"\"root\"\" \\\\\\n  -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n  -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n  -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n  -p 5432 \\\\\\n  postgres:13```\\nI had to remove 5432:5432 and replace with 5432 as it looks like because I already have postgres installed, it’s running on that port.\\n\\nThere error I get when I run the above is:\\n\\n```creating subdirectories ... ok\\nselecting dynamic shared memory implementation ... posix\\nselecting default max_connections ... 100\\nselecting default shared_buffers ... 128MB\\nselecting default time zone ... Etc/UTC\\ncreating configuration files ... ok\\nrunning bootstrap script ... ok\\nperforming post-bootstrap initialization ... 2022-01-19 22:09:17.721 UTC [42] FATAL:  data directory \"\"/var/lib/postgresql/data\"\" has invalid permissions\\n2022-01-19 22:09:17.721 UTC [42] DETAIL:  Permissions should be u=rwx (0700) or u=rwx,g=rx (0750).\\nchild process exited with exit code 1\\ninitdb: removing contents of data directory \"\"/var/lib/postgresql/data\"\"```\\n\",1642630680.235000,1642630864.235400,U02U34YJ8C8\\ne311dde4-ef49-45ca-866f-3c923be6a9a5,U02U34YJ8C8,,,Strangely. I tried it again and now it looks like it’s working…,1642630680.235000,1642631061.235700,U02U34YJ8C8\\n1f9effc5-4519-4901-8f64-4a2305317aae,U02UX664K5E,,,Thank you for the pic Lyes. It certainly beats humidity at 36C. I am envious :slightly_smiling_face:,1642433181.192700,1642632012.235900,U02U5SW982W\\n1141f454-6cdf-4939-ba8c-c983bd4e7515,U02UX664K5E,,,\"connection to server at \"\"localhost\"\" (::1), port 5431 failed: Connection refused (0x0000274D/10061)\\n        Is the server running on that host and accepting TCP/IP connections?\\nconnection to server at \"\"localhost\"\" (127.0.0.1), port 5431 failed: Connection refused (0x0000274D/10061)\\n        Is the server running on that host and accepting TCP/IP connections?\\n\\nIs anyone having the error above?\",1642615746.189300,1642632099.236200,U02T1BX1UV6\\n4a2d9651-5810-43dd-b63f-0d109f22a0eb,U02T1BX1UV6,,,I tried the things suggested in the thread but the error remains the same,1642416911.154000,1642632135.236400,U02T1BX1UV6\\ndb42a301-c1c8-4a83-901e-eccb6568d1a9,U02T1BX1UV6,,,\"connection to server at \"\"localhost\"\" (::1), port 5431 failed: Connection refused (0x0000274D/10061)\\n        Is the server running on that host and accepting TCP/IP connections?\\nconnection to server at \"\"localhost\"\" (127.0.0.1), port 5431 failed: Connection refused (0x0000274D/10061)\\n        Is the server running on that host and accepting TCP/IP connections?\",1642416911.154000,1642632139.236600,U02T1BX1UV6\\n95d28fa3-b32b-4e19-b202-7f1fc859c065,U02UE7NTLUU,,,I was having the same issue myself on a Linux system. I have local postgres versions that I don\\'t want to kill. Simply picking another port (The -p 5431:5432) also worked for me.,1642465146.363400,1642632340.236800,U02U5SW982W\\n9892f4e8-b2bc-43cc-b200-985b468c7edf,U02U34YJ8C8,,,you can also try something like `-p 5431:5432` next time,1642630680.235000,1642632424.237000,U01AXE0P5M3\\nf10c2eb4-b115-44e6-8204-503442bbe8c6,U02UE7NTLUU,,,\"Unlike what I did at first, just make sure when you do the next step in the sequence that you then type $pgcli -h localhost -p 5431 -u root -d ny_taxi\",1642465146.363400,1642632464.237200,U02U5SW982W\\n2fe4e518-d3f4-4e61-b773-8bf3f1757f07,U01AXE0P5M3,,,\"How do I put the data there and what data should I put?\\n\\nDo you mean by \"\"data ingestion\"\"?\",1642534337.023000,1642632465.237400,U02QS4BD1NF\\ne1bb3fca-b90c-4ec9-a83a-6b340be01f69,U02UX664K5E,,,<@U01AXE0P5M3> still having the failed password for user,1642615746.189300,1642632636.237800,U02SPLJUR42\\nd1269447-5e5d-475f-a783-2d6fe799dcf1,U02TVGE99QU,,,I\\'m on Ubuntu. My ny_taxi_postgres_data is in the main data_engineering-zoomcamp directory that was cloned from the course GH. I\\'m running the docker container with postgres from that directory as well as calling pgcli from there.,1642597721.143600,1642632638.238000,U02TVGE99QU\\n454de920-d270-49f3-9f22-79f22a0f6a9e,U02U34YJ8C8,,,\"I have the same issue,  I had already installed postgres and  it does not work if I put -p 5431:5432. Once I removed it, it worked. Is it ok to not specify the port ?\",1642630680.235000,1642632670.238200,U02T2DX4LG6\\n5a2239ad-2a93-4176-862b-051b4217fe74,U02U34YJ8C8,,,If you have postgres installed already you could run the ingestion script but point it at your own installed database instead of the docker one.,1642630680.235000,1642632864.238500,U02U06KSE3G\\n13b2fd18-c666-4142-8d68-6b9687b91c7a,,7.0,,\"Still getting `connection to server at \"\"localhost\"\" (::1), port 5432 failed: FATAL:  password authentication failed for user \"\"root\"\"`  when I try to connect to Postgres.\",1642633246.239400,1642633246.239400,U02UX664K5E\\nf1f38504-4545-40a4-80aa-f4c17997ec23,U02UX664K5E,,,\"is it normal that my container has no IP?\\n```$ docker ps\\nCONTAINER ID   IMAGE         COMMAND                  CREATED          STATUS          PORTS                    NAMES\\n7750be3fadfb   postgres:13   \"\"docker-entrypoint.s…\"\"   18 minutes ago   Up 17 minutes   0.0.0.0:5431-&gt;5432/tcp   gallant_agnesi```\",1642633246.239400,1642633296.239500,U02UX664K5E\\n8f18e240-8b80-425a-b192-e58bb0652cb4,U02U34YJ8C8,,,\"I tried running the initial config using `-p 5431:5432`\\n\\nHowever, when I then run `pgcli -h localhost -p 5431 -u root -d ny_taxi` and enter root as the password I get :\\n\\n```connection to server at \"\"localhost\"\" (::1), port 5431 failed: FATAL:  database \"\"ny_taxi\"\" does not exist```\\nAlso, when I first run the config stuff from the terminal, I get this:\\n\\n```waiting for server to start....2022-01-19 23:01:34.893 UTC [50] FATAL:  data directory \"\"/var/lib/postgresql/data\"\" has invalid permissions\\n2022-01-19 23:01:34.893 UTC [50] DETAIL:  Permissions should be u=rwx (0700) or u=rwx,g=rx (0750).\\n stopped waiting\\npg_ctl: could not start server```\\nHowever when I run it again, it works.\",1642630680.235000,1642633367.239800,U02U34YJ8C8\\ndfafd2fa-2ba3-4a5d-b13a-eab6d4cb19f0,U02UX664K5E,,,Yeah that looks correct. Did you setup the network? `docker network create pg-network`,1642633246.239400,1642633375.240000,U02U06KSE3G\\nce4df4ae-4119-4ed5-b8af-dd0f635424cf,U02U34YJ8C8,,,\"Also, I’m still not 100% sure what the following is doing (I’m new to docker):\\n\\n```docker run -it \\\\\\n  -e POSTGRES_USER=\"\"root\"\" \\\\\\n  -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n  -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n  -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n  -p 5431:5432 \\\\\\n  postgres:13```\\n\",1642630680.235000,1642633447.240600,U02U34YJ8C8\\n5d680ac2-c751-4856-8dc1-1e97ddda0fc9,,1.0,,\"Started uploading the next video - <https://youtu.be/B1WwATwf-vY>\\n\\nin a few minutes (or 10-15) it\\'ll finish uploading. This one is about dockerizing our \"\"data pipeline\"\"\",1642633486.241300,1642633486.241300,U01AXE0P5M3\\n57cd6910-5606-4019-aea4-d68e752aeae3,U02UX664K5E,,,\"@Ilya I had no idea that we needed to setup the network. So I executed the command and I see now the network has been created. I so still get denied access...\\n```$ docker network ls\\nNETWORK ID     NAME         DRIVER    SCOPE\\nb371e0426e03   bridge       bridge    local\\ncc4456f5fb6a   host         host      local\\n263d85d9a166   none         null      local\\nc07f6ce9863b   pg-network   bridge    local```\",1642633246.239400,1642633536.241600,U02UX664K5E\\n0845731e-c7ec-4c7e-b3df-0274d02be527,U02U34YJ8C8,,,\"maybe make both port same, in your case you have 5431:5432\",1642630680.235000,1642633559.241800,U02UBV4EC8J\\n718b74b3-0f91-4493-9426-2c769f6baaa1,U02UX664K5E,,,How are you trying to connect?,1642633246.239400,1642633630.242100,U02U06KSE3G\\n11183009-8ffd-42b4-ae65-778f5db57f54,U02UX664K5E,,,\"Just like in the video:\\n`pgcli -h localhost -p 5432 -u root -d ny_taxi`\",1642633246.239400,1642633692.242300,U02UX664K5E\\n43196f28-6757-49f7-b44d-822f08ac0bb1,U02TVGE99QU,,,\"I made a mistake at some point... I ran postgres with docker mounting it to $(pwd) and the full path:\\n```dan@ubuntu:~/Documents/data-engineering-zoomcamp$ docker run -it \\\\\\n  -e POSTGRES_USER=\"\"root\"\" \\\\\\n  -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n  -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n  -v $(pwd)/data-engineering-zoomcamp/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n  -p 5432:5432 \\\\\\n  --network=pg-network \\\\\\n  --name pg-database \\\\\\n  postgres:13 ```\\nand now I have a subdirectory in data-engineering-zoomcamp called data-engineering-zoomcamp that\\'s owned by root. now when I\\'m trying to re-run the postgres container with just\\n```$(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\```\\nit\\'ll say it\\'s ready to accept connections, but then when I try to connect to the db through pgcli, I get the following:\",1642597721.143600,1642633702.242600,U02TVGE99QU\\n7821c95c-368f-44c1-bc36-765a8939537b,U02TVGE99QU,,,\"```dan@ubuntu:~/Documents/data-engineering-zoomcamp$ pgcli -h localhost -p 5432 -d ny_taxi -U root\\nserver closed the connection unexpectedly\\n\\tThis probably means the server terminated abnormally\\n\\tbefore or while processing the request.```\",1642597721.143600,1642633714.242800,U02TVGE99QU\\nf9a85e93-9e01-427a-bc8a-6ac826eebbf8,U02U34YJ8C8,,,\"<@U02UBV4EC8J> I tried that. But then I get this error in the next step:\\n\\n```connection to server at \"\"localhost\"\" (::1), port 5431 failed: server closed the connection unexpectedly\\n        This probably means the server terminated abnormally\\n        before or while processing the request.```\\n\",1642630680.235000,1642633766.243000,U02U34YJ8C8\\n7a60a7d7-aa84-4394-af13-a9cb4ea7f8f4,U02TVGE99QU,,,So <@U02BVP1QTQF> was correct that I wasn\\'t mounting it correctly. My question is now how do I fix my error,1642597721.143600,1642633795.243700,U02TVGE99QU\\nf7a159d3-339b-42d0-8254-f20a0c69d776,,8.0,,\"Currently getting this error `File \"\"pandas/_libs/parsers.pyx\"\", line 826, in pandas._libs.parsers.TextReader.read_low_memory`\\n`StopIteration` when running the ingestion script.\",1642633799.243900,1642633799.243900,U02U06KSE3G\\n2d6cde5e-f61c-473f-9cd5-9921f4ffedf8,U02U06KSE3G,,,That\\'s expected - because the iterator doesn\\'t have anything else to return,1642633799.243900,1642633832.244100,U01AXE0P5M3\\nfd69cafe-3a9f-429a-a574-437326e99320,U02U06KSE3G,,,\"(not the cleanest way of handling the end of the loop, I know)\",1642633799.243900,1642633850.244300,U01AXE0P5M3\\n83339034-c52a-4f8e-830d-9f442c130666,U02U06KSE3G,,,Honestly just wasted an hour debugging that,1642633799.243900,1642633972.244600,U02U06KSE3G\\ne9820529-1968-4780-ba42-3d8115fceb1f,U02U06KSE3G,,,:face_palm:,1642633799.243900,1642633980.244800,U02U06KSE3G\\na15379d7-2b88-4f31-a12e-d3b1d473793a,U02U34YJ8C8,,,<@U01AXE0P5M3> Does it need to be `-p 5431:5432` or `-p 5431:5431`? Neither seem to work for me.,1642630680.235000,1642633995.245000,U02U34YJ8C8\\n0a1044cd-6ba9-4af8-b4ab-4ff88a3370b1,,15.0,,Any other persons having challenge selecting the DOLocationID field saying doesn\\'t exist in the table,1642634044.245600,1642634044.245600,U02TBKWL7DJ\\n3a86cf5f-5a36-463c-aae9-3ef077d74f66,U02TBKWL7DJ,,,\"try putting quotes around the column name like \"\"DOLocationID\"\"\",1642634044.245600,1642634167.246100,U02U06KSE3G\\n901c8d0d-4d00-47eb-b9a0-281ef6f2241d,U02U34YJ8C8,,,for me I did 5432:5432 and worked. I think you would need to restart the containers with those port since the postgres db runs in container,1642630680.235000,1642634212.246300,U02UBV4EC8J\\n70306022-d9d0-4c16-9467-c3d22d3df185,U02TBKWL7DJ,,,\"yeap works with quotes:\\n\\n`SELECT`\\n\\t(\"\"DOLocationID\"\")\\nFROM\\n\\tyellow_taxi_data\",1642634044.245600,1642634351.246600,U02UBV4EC8J\\n262785b5-5486-43b0-a232-7708f9f3cff2,U02TBKWL7DJ,,,Ooh,1642634044.245600,1642634384.246800,U02TBKWL7DJ\\ncb022f94-48a9-489c-9e7a-9671dd3c81dc,U02TBKWL7DJ,,,Postgres dosen\\'t like capitalisation in column names and while you can create them if will force you to escape them in statements with quotes,1642634044.245600,1642634414.247000,U02U06KSE3G\\n57968b74-a239-4489-87ae-da8d6e6a28c4,U02TBKWL7DJ,,,\"I was beginning to wonder what\\'s wrong.\\n\\nHow come other column don\\'t require a quote?\\n\\nThanks <@U02U06KSE3G> <@U02UBV4EC8J> for coming through\\n\\nI will try it out now\",1642634044.245600,1642634421.247200,U02TBKWL7DJ\\n1b96d9ab-d787-428a-98a7-9de29fa359f7,U02TBKWL7DJ,,,\"I think it does, I just tried VendorID and it failed\",1642634044.245600,1642634503.247700,U02UBV4EC8J\\n3786e2cb-57fe-46ff-822f-69b694613f6b,U02TBKWL7DJ,,,\"anything you run by itself, not wrap with other commands will require quotes\",1642634044.245600,1642634544.247900,U02UBV4EC8J\\n4ae8fcaf-49b5-4d0a-9a9b-984338863a55,U02TBKWL7DJ,,,I mean the others like max(tpep_pickup_datetime) since this is a function it might be handling that for us,1642634044.245600,1642634588.248100,U02UBV4EC8J\\nd72dd692-1b9c-415f-a0d9-4c71de8b2237,U02TBKWL7DJ,,,I did the first 4 questions without using a quote,1642634044.245600,1642634594.248300,U02TBKWL7DJ\\n55964bb0-0616-4db1-92f3-0b6af49931ea,U02TBKWL7DJ,,,That\\'s why I was surprised when this wasn\\'t working for Q5,1642634044.245600,1642634607.248500,U02TBKWL7DJ\\n07fb41f6-daa8-4bca-8098-a66616ce6f58,U02TBKWL7DJ,,,\"Oh\\n\\nTrue <@U02UBV4EC8J>\\n\\nGotya\",1642634044.245600,1642634627.248700,U02TBKWL7DJ\\n1c2c3a9c-d156-40ad-8a83-7a5dfde80c73,U02TBKWL7DJ,,,yea looks like integer vs string type issue here,1642634044.245600,1642634644.248900,U02UBV4EC8J\\n467bea7f-b3ef-4077-80b6-56166531a148,U02TBKWL7DJ,,,\"not an issue, but handling process\",1642634044.245600,1642634675.249100,U02UBV4EC8J\\nc36bf58f-4c99-4a56-a9b1-4d3c4b66ddbb,U02U5GQK25C,,,\"I have the same error, but I have postgres on my system\",1642620437.210500,1642635015.249700,U02T1BX1UV6\\n407f77b5-7724-43ba-a443-f68c784b581a,U02U34YJ8C8,,,\"<@U02UBV4EC8J> I’ve tried resetting everything, but now I get this:\\n\\n\\nData page checksums are disabled.\\n\\n```initdb: error: directory \"\"/var/lib/postgresql/data\"\" exists but is not empty\\nIt contains a dot-prefixed/invisible file, perhaps due to it being a mount point.\\nUsing a mount point directly as the data directory is not recommended.\\nCreate a subdirectory under the mount point.```\\nAlthough I don’t actually have a `/var/lib/postgresql/data` folder on my mac\",1642630680.235000,1642635477.249900,U02U34YJ8C8\\n9bdfe6d0-0999-4ceb-ae97-64eb31b4573d,U02TBKWL7DJ,,,Thanks <@U02UBV4EC8J>,1642634044.245600,1642635498.250500,U02TBKWL7DJ\\n64fe8493-a135-436f-8148-5e09b5835d28,U01AXE0P5M3,,,Despite following all of the advice above I still have a problem with postgres not creating files in the mounted folder.,1642621263.215300,1642635548.250700,U02U5SW982W\\nc65fad13-9e90-4342-b5c8-80ba5a98ddfd,U02U06KSE3G,,,\"Did the import with SQL just for the heck of it :slightly_smiling_face:\\n```DROP TABLE if EXISTS yellow_taxi_trips;\\nCREATE TABLE yellow_taxi_trips (\\n    vendorid        integer,\\n    tpep_pickup_datetime        timestamp,\\n    tpep_dropoff_datetime       timestamp,\\n    passenger_count         integer,\\n    trip_distance       double precision,\\n    ratecodeid      integer,\\n    store_and_fwd_flag      text,\\n    pulocationid        integer,\\n    dolocationid        integer,\\n    payment_type        integer,\\n    fare_amount         double precision,\\n    extra       double precision,\\n    mta_tax         double precision,\\n    tip_amount      double precision,\\n    tolls_amount        double precision,\\n    improvement_surcharg       double precision,\\n    total_amount        double precision,\\n    congestion_surcharge        double precision\\n)\\n\\nCOPY yellow_taxi_trips\\nFROM \\'/data/yellow_tripdata_2021-01.csv\\'\\nWITH (format csv, header)\\n;\\n\\nCOPY 1369765\\nTime: 17.994s (17 seconds), executed in: 17.994s (17 seconds)```\",1642633799.243900,1642636234.251000,U02U06KSE3G\\n8ad780f9-2ae6-4a86-8ca7-43ce5c67228f,U02U34YJ8C8,,,I am running an M1 mac if that makes a difference,1642630680.235000,1642636470.251200,U02U34YJ8C8\\n402b48be-3109-4321-8fac-c0c72d491e41,U01AXE0P5M3,,,Thankssss,1642633486.241300,1642636779.251600,U02T2JGQ8UE\\ndfb1ae7d-2792-41d1-88bd-0c00a1b0010f,U02U34YJ8C8,,,\"So I finally got it I think. I first of all deleted the ny_taxi_posgres_data folder.\\n\\nI then recreated it and adjusted permissions using `sudo chmod a+rwx ny_taxi_postgres_data`\\n\\nI then re-ran the container, only I used `-p 5431:5432`\\n\\nI then ran `pgcli -h localhost -p 5431 -u root -d ny_taxi`\",1642630680.235000,1642636967.251800,U02U34YJ8C8\\nd99d30b2-9779-423e-9b40-0b70c32b5a53,U01AXE0P5M3,,,Sorry ... ended that a little earlier than I wanted. I\\'m operating on Ubuntu 20.04. I have other local versions of postgres running so do not wish to kill any of them so my only change to the docker run command I have issued is the following:  \\'\\'\\'-p 5431:5432\\'\\'\\'.  Really not sure if that has anything to do with my problem (probably not). I then change the permissions on that folder as recommended \\'\\'\\'sudo chmod\\xa0a+rwx\\'\\'\\'. Still nothing. I\\'m wondering if I\\'ve missed something really basic. The database itself fires up fine. I guess part of my issue as well is that I\\'m not sure how volumes are working here. When we put the -v flag in how does docker know where and how to get the data - those files that should be appearing under our folder ny_taxi_postgres_data when we run the command -v ?,1642621263.215300,1642636985.252100,U02U5SW982W\\n2a2dcb8f-7741-4faf-b035-fa0e93bd58e3,U02U34YJ8C8,,,thanks <@U02U34YJ8C8> I tried your method and it worked,1642630680.235000,1642637463.252500,U02SPLJUR42\\n56c43598-87f1-46fa-8d47-7addbc2ecbe4,U02U34YJ8C8,,,<@U02SPLJUR42> That’s great. I actually noticed the permission adjustment line in the ReadMe for week 1 (on Github). I probably should have read that before doing anything.,1642630680.235000,1642637878.252800,U02U34YJ8C8\\nb4bcdfa8-b072-4f35-bc4b-35f50d1b0abc,,4.0,,Hello! I\\'ve obtained this error when I\\'ve tried to group the taxi data. And  I\\'ve obtained this count in the database. I don\\'t know if is necessary to try again or just let it here.,1642638867.256500,1642638867.256500,U02TJ69RKT5\\n164fbd1b-3bb6-493a-9c37-32228b0b7a27,U02TJ69RKT5,,,\"I make it\\n\\n`SELECT`\\n  `COUNT(*) as count`\\n  `FROM yellow_taxi_trips;`\\n`+---------+`\\n`| count   |`\\n`|---------|`\\n`| 1369765 |`\\n`+---------+`\\n\\nand that\\'s how many rows are in the csv, so not sure it\\'s loaded all the data there.\",1642638867.256500,1642639253.257300,U02U06KSE3G\\n541ce9df-c6ca-43ea-8c09-3f3da6e9237b,U01AXE0P5M3,,,Interestingly for me (running in WSL) the files never showed up in my vscode window like yours did in the video. also an `ls` seemed to show they weren\\'t there either. But when I ran `sudo ls` they were shown to me. I\\'m not sure what that\\'s all about,1642621263.215300,1642640216.258200,U02T9GHG20J\\nceb1a0b5-9558-4a58-83ab-b2f58ec8c6da,,3.0,,\"Hi guys, i have a problem with sign in to postgres. i have following the video exactly but still failed to login and my postgres service also already running. Any solution?\",1642640844.260700,1642640844.260700,U02RA8F3LQY\\n3183991b-3755-45a8-8c71-a7d5bbdd6979,,9.0,,\"Hey folks, I think I\\'ve figured out my Postgres/Docker setup on Ubuntu now but I have a question. I just tried to restard my VM to see if everything is still well and good and when I try to reconnect to the postgres docker container via:\\n```docker run -it \\\\\\n  -e POSTGRES_USER=\"\"root\"\" \\\\\\n  -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n  -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n  -v $(pwd):/var/lib/postgresql/data \\\\\\n  -p 5432:5432 \\\\\\n  --network=pg-network \\\\\\n  --name pg-database \\\\\\n  postgres:13```\\nIt\\'s telling me that:\\n``` docker: Error response from daemon: Conflict. The container name \"\"/pg-database\"\" is already in use by container \"\"...\"\" You have to remove (or rename) that container to be able to reuse that name.```\\nI got this before and thought there was some kind of mixup with another container I had made so I switched the older one to pg-database-2, but now I\\'m getting it again. The problem is that obviously that\\'s the container I want to connect to - what should I be doing here?\",1642641667.264100,1642641667.264100,U02TVGE99QU\\neb394874-8453-4baf-8a69-45893b80d04b,U02TVGE99QU,,,I am also curious about this.,1642641667.264100,1642641951.264200,U02UE7NTLUU\\n6b6ea23e-56f4-423b-b4a5-5f0c2ec277ce,U02TVGE99QU,,,\"Run\\n```$ docker kill pg-database\\n$ docker rm pg-database```\\n\",1642641667.264100,1642642179.264400,U02SE1AFBFE\\n802aef20-c169-4ffd-a849-f6aa01390128,U01AXE0P5M3,,,\"Michael - that\\'s exactly what my problem was. I was following along with the video and these internal representations of postgres weren\\'t showing up in my VS Code like they were for Alexy. This is how my confusion started (no, it doesn\\'t take much). Maybe it is something to do with the settings there in VSCode? Are you using the Docker extension in VS Code? Not sure if this makes a difference. However, at least you were able to see the files. I tried `sudo ls`  but still couldn\\'t see the files.\",1642621263.215300,1642642215.264600,U02U5SW982W\\ndeb87865-6887-40ec-9d10-b61b775482a1,U02TVGE99QU,,,Is it possible to restart the container with the environmental variables that were used to create it?,1642641667.264100,1642642318.264800,U02UE7NTLUU\\ne3332bfa-af51-40b6-b76c-7100aedec268,U02TVGE99QU,,,\"you can give a new name or just run docker rm docker_id, if you run docker ps -a , this will show all the containers\",1642641667.264100,1642642906.265700,U02UBV4EC8J\\nef1466c9-cd54-457e-a4e3-5f59dbca1d99,,7.0,,Hi team! I\\'m having some problems installing Google Cloud SDK using Windows and the installer. There is an issue with the SSL aparently. Below the error message if someone can help me.,1642643423.267000,1642643423.267000,U02QDKU4GTY\\n45a192e7-49d8-496c-a1cb-0fead07da010,U02QDKU4GTY,,,\"`Output folder: C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK`\\nDownloading Google Cloud SDK core.\\nExtracting Google Cloud SDK core.\\nCreate Google Cloud SDK bat file: C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\cloud_env.bat\\nInstalling components.\\nWelcome to the Google Cloud SDK!\\nBeginning update. This process may take several minutes.\\nThis will install all the core command line tools necessary for working with\\nthe Google Cloud Platform.\\nTraceback (most recent call last):\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\third_party\\\\urllib3\\\\contrib\\\\pyopenssl.py\"\", line 488, in wrap_socket\\n    cnx.do_handshake()\\n  File \"\"C:\\\\Users\\\\ADMINA~1\\\\AppData\\\\Local\\\\Temp\\\\tmpil4t_39u\\\\python\\\\lib\\\\OpenSSL\\\\SSL.py\"\", line 1894, in do_handshake\\n    self._raise_ssl_error(self._ssl, result)\\n  File \"\"C:\\\\Users\\\\ADMINA~1\\\\AppData\\\\Local\\\\Temp\\\\tmpil4t_39u\\\\python\\\\lib\\\\OpenSSL\\\\SSL.py\"\", line 1632, in _raise_ssl_error\\n    _raise_current_error()\\n  File \"\"C:\\\\Users\\\\ADMINA~1\\\\AppData\\\\Local\\\\Temp\\\\tmpil4t_39u\\\\python\\\\lib\\\\OpenSSL\\\\_util.py\"\", line 57, in exception_from_error_queue\\n    raise exception_type(errors)\\nOpenSSL.SSL.Error: [(\\'SSL routines\\', \\'tls_process_server_certificate\\', \\'certificate verify failed\\')]\\nDuring handling of the above exception, another exception occurred:\\nTraceback (most recent call last):\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\third_party\\\\urllib3\\\\connectionpool.py\"\", line 670, in urlopen\\n    httplib_response = self._make_request(\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\third_party\\\\urllib3\\\\connectionpool.py\"\", line 381, in _make_request\\n    self._validate_conn(conn)\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\third_party\\\\urllib3\\\\connectionpool.py\"\", line 976, in _validate_conn\\n    conn.connect()\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\third_party\\\\urllib3\\\\connection.py\"\", line 361, in connect\\n    self.sock = ssl_wrap_socket(\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\third_party\\\\urllib3\\\\util\\\\ssl_.py\"\", line 382, in ssl_wrap_socket\\n    return context.wrap_socket(sock, server_hostname=server_hostname)\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\third_party\\\\urllib3\\\\contrib\\\\pyopenssl.py\"\", line 494, in wrap_socket\\n    raise ssl.SSLError(\"\"bad handshake: %r\"\" % e)\\nssl.SSLError: (\"\"bad handshake: Error([(\\'SSL routines\\', \\'tls_process_server_certificate\\', \\'certificate verify failed\\')])\"\",)\\nDuring handling of the above exception, another exception occurred:\\nTraceback (most recent call last):\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\third_party\\\\requests\\\\adapters.py\"\", line 439, in send\\n    resp = conn.urlopen(\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\third_party\\\\urllib3\\\\connectionpool.py\"\", line 724, in urlopen\\n    retries = retries.increment(\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\third_party\\\\urllib3\\\\util\\\\retry.py\"\", line 439, in increment\\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host=\\'<http://dl.google.com|dl.google.com>\\', port=443): Max retries exceeded with url: /dl/cloudsdk/channels/rapid/components-2.json (Caused by SSLError(SSLError(\"\"bad handshake: Error([(\\'SSL routines\\', \\'tls_process_server_certificate\\', \\'certificate verify failed\\')])\"\")))\\nDuring handling of the above exception, another exception occurred:\\nTraceback (most recent call last):\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\\\\\bin\\\\bootstrapping\\\\install.py\"\", line 308, in &lt;module&gt;\\n    main()\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\\\\\bin\\\\bootstrapping\\\\install.py\"\", line 285, in main\\n    Install(pargs.override_components, pargs.additional_components)\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\\\\\bin\\\\bootstrapping\\\\install.py\"\", line 155, in Install\\n    InstallOrUpdateComponents(to_install, update=update)\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\\\\\bin\\\\bootstrapping\\\\install.py\"\", line 197, in InstallOrUpdateComponents\\n    _CLI.Execute(\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\googlecloudsdk\\\\calliope\\\\cli.py\"\", line 1013, in Execute\\n    self._HandleAllErrors(exc, command_path_string, specified_arg_names)\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\googlecloudsdk\\\\calliope\\\\cli.py\"\", line 1050, in _HandleAllErrors\\n    exceptions.HandleError(exc, command_path_string, self.__known_error_handler)\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\googlecloudsdk\\\\calliope\\\\exceptions.py\"\", line 547, in HandleError\\n    core_exceptions.reraise(exc)\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\googlecloudsdk\\\\core\\\\exceptions.py\"\", line 146, in reraise\\n    six.reraise(type(exc_value), exc_value, tb)\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\third_party\\\\six\\\\__init__.py\"\", line 693, in reraise\\n    raise value\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\googlecloudsdk\\\\calliope\\\\cli.py\"\", line 987, in Execute\\n    resources = calliope_command.Run(cli=self, args=args)\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\googlecloudsdk\\\\calliope\\\\backend.py\"\", line 809, in Run\\n    resources = command_instance.Run(args)\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\surface\\\\components\\\\update.py\"\", line 124, in Run\\n    update_manager.Update(\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\googlecloudsdk\\\\core\\\\updater\\\\update_manager.py\"\", line 946, in Update\\n    install_state, diff = self._GetStateAndDiff(\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\googlecloudsdk\\\\core\\\\updater\\\\update_manager.py\"\", line 652, in _GetStateAndDiff\\n    latest_snapshot = self._GetLatestSnapshot(version=version,\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\googlecloudsdk\\\\core\\\\updater\\\\update_manager.py\"\", line 635, in _GetLatestSnapshot\\n    return snapshots.ComponentSnapshot.FromURLs(\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\googlecloudsdk\\\\core\\\\updater\\\\snapshots.py\"\", line 175, in FromURLs\\n    data = [\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\googlecloudsdk\\\\core\\\\updater\\\\snapshots.py\"\", line 176, in &lt;listcomp&gt;\\n    (ComponentSnapshot._DictFromURL(\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\googlecloudsdk\\\\core\\\\updater\\\\snapshots.py\"\", line 200, in _DictFromURL\\n    response = installers.MakeRequest(url, command_path)\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\googlecloudsdk\\\\core\\\\updater\\\\installers.py\"\", line 114, in MakeRequest\\n    return _RawRequest(url, headers=headers, timeout=timeout)\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\googlecloudsdk\\\\core\\\\updater\\\\installers.py\"\", line 162, in _RawRequest\\n    return retryer.RetryOnException(\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\googlecloudsdk\\\\core\\\\util\\\\retry.py\"\", line 201, in RetryOnException\\n    exceptions.reraise(exc_info[1], tb=exc_info[2])\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\googlecloudsdk\\\\core\\\\exceptions.py\"\", line 146, in reraise\\n    six.reraise(type(exc_value), exc_value, tb)\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\third_party\\\\six\\\\__init__.py\"\", line 693, in reraise\\n    raise value\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\googlecloudsdk\\\\core\\\\util\\\\retry.py\"\", line 182, in TryFunc\\n    return func(*args, **kwargs), None\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\googlecloudsdk\\\\core\\\\updater\\\\installers.py\"\", line 192, in _ExecuteRequestAndRaiseExceptions\\n    response = requests_session.get(\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\third_party\\\\requests\\\\sessions.py\"\", line 546, in get\\n    return self.request(\\'GET\\', url, **kwargs)\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\googlecloudsdk\\\\core\\\\transport.py\"\", line 251, in WrappedRequest\\n    response = orig_request(*modified_args, **modified_kwargs)\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\googlecloudsdk\\\\core\\\\requests.py\"\", line 216, in WrappedRequest\\n    return orig_request_method(*args, **kwargs)\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\third_party\\\\requests\\\\sessions.py\"\", line 533, in request\\n    resp = self.send(prep, **send_kwargs)\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\third_party\\\\requests\\\\sessions.py\"\", line 646, in send\\n    r = adapter.send(request, **kwargs)\\n  File \"\"C:\\\\Program Files (x86)\\\\Google\\\\Cloud SDK\\\\google-cloud-sdk\\\\lib\\\\third_party\\\\requests\\\\adapters.py\"\", line 514, in send\\n    raise SSLError(e, request=request)\\nrequests.exceptions.SSLError: HTTPSConnectionPool(host=\\'<http://dl.google.com|dl.google.com>\\', port=443): Max retries exceeded with url: /dl/cloudsdk/channels/rapid/components-2.json (Caused by SSLError(SSLError(\"\"bad handshake: Error([(\\'SSL routines\\', \\'tls_process_server_certificate\\', \\'certificate verify failed\\')])\"\")))\\nFormato de par\\xa0metro incorrecto: activa:\\nFailed to install.\",1642643423.267000,1642643494.267200,U02QDKU4GTY\\neb305064-3625-4a77-ac86-d643c1bd24ab,,3.0,,Hi <@U01AXE0P5M3> i\\'m noticing the video : Setting up the environment on Google Cloud . Was this meant to be a bonus video as an alternative way to run the project on a VM or is it a required part?,1642643540.268300,1642643540.268300,U02T9550LTU\\n54620759-f569-4821-a402-b89a4d36cba2,U02U34YJ8C8,,,\"I had the same issue with folder permissions (Mac). I know this doesn’t fix the problem, but instead of changing the folder permissions, I just created a docker volume:\\n\\n`docker volume create db_volume`\\n\\nand then mapped the postgres db to that volume:\\n\\n`-v db_volume:/var/lib/postgresql/data`\\n\\nI’m not too familiar with docker, but I’m assuming I just created a containerized storage for the postgres data rather than it being on host. But changing permissions is definitely easier lol\",1642630680.235000,1642644385.269000,U02SQ1X29GE\\n00f60d0e-8d5e-4a14-9af4-aeee2673da9e,U02V4412XFA,,,Can someone help with this?,1642610988.174800,1642644404.269200,U02V4412XFA\\nbd6342f6-7e60-4a69-aa2d-b7801b204797,U02U34YJ8C8,,,I went to activity monitor on my mac and killed every process named postgres. I hope I didn\\'t wreck anything. Once I did that port 5432 was free and I could run the command.,1642630680.235000,1642646063.270100,U02UBQJBYHZ\\n1d05b4f7-8fcf-48d8-8c08-0fe5bee1892f,U02T8ANTJGM,,,wah sori mas baru liat. siap siap,1641819619.163600,1642648715.270400,U02T8ANTJGM\\na1b42e08-c485-4019-9461-760b2c968421,,4.0,,\"Hi, everyone :slightly_smiling_face: I am doing the first HW. and inserting into DB is much slower than in the examples. 2 minutes on my laptop VS 15 seconds as it was in the video. Did someone experience the same?\",1642649434.272000,1642649434.272000,U02U5L97S6T\\n4d8f0bf2-8e14-4c18-86e2-a74353d9a4fb,U01AXE0P5M3,,,<@U02U5SW982W> is it possible you mounted them to the wrong location? maybe it\\'s creating files on another location on your pc?,1642621263.215300,1642649547.272100,U02T9GHG20J\\n46ac4c7c-cdf5-4a2c-9181-cd5da3d23553,U02U5L97S6T,,,They skipped ahead in the video while data was uploading into DB,1642649434.272000,1642649551.272300,U025P29E8D9\\n2F0CEDCE-74EC-402D-A49F-0EAE00EC85D3,U02QDKU4GTY,,,\"Looks like firewall to me. Are you running on company laptop, company network by any chance?\",1642643423.267000,1642649909.273500,U02UBV4EC8J\\nc12d08b0-91f6-42c9-af29-42cb1a1ab375,U02U5L97S6T,,,\"Thanks :slightly_smiling_face: But still if you look at the output in the github in jupyter notebook, it will show that their code executed much faster: <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/2_docker_sql/upload-data.ipynb>\",1642649434.272000,1642651476.274400,U02U5L97S6T\\nd4618c6c-870a-46dd-92d4-c0cc131225db,U01AXE0P5M3,,,Hmm... not too sure Michael but I\\'m on Ubuntu so my line of code for the volume is simply `-v $(pwd)/ny_taxi_postgres_data:/var/lib/postgres/data` . If I then simply open up a terminal (I do this while I\\'m in VSCode) I see that I\\'m already in ~/git/data-engineering-zoomcamp/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data . If I then do `sudo ls` at the prompt it simply comes back with another prompt (it doesn\\'t seem to find anything).,1642621263.215300,1642652386.275600,U02U5SW982W\\ncca85b46-2a92-46d1-bce5-3ee21af6a3b9,U01AXE0P5M3,,,Everything else seems to be working fine though. I\\'ve been able to ingest the data and everything so :woman-shrugging:,1642621263.215300,1642652447.275800,U02U5SW982W\\nc7576c26-9655-4430-bd54-cd6358a7fe42,U02U5L97S6T,,,\"Hmm... I am also getting larger times as well. Mine are approximately double what Alexey is getting in his video. For example,  26.752 seconds for the first chunk in the while loop rather than Alexey\\'s 16.115 seconds. I\\'m not sure exactly what limits this. Is it just our local computing power (mine is pretty crap)?\",1642649434.272000,1642652921.276000,U02U5SW982W\\nf0670ba7-f7d6-4e69-ac24-82d05b30d16d,U02UX664K5E,,,I am also facing the same issue: password authentication failed for user \\'root\\',1642615746.189300,1642653186.276200,U01K0GEJXGF\\nd557f689-b959-4e8a-8fb0-342b05d2d1ef,,6.0,,\"Video 2, I had some problems seeing the columns in the database. I killed the processes that were running and started over. I ran the docker command, got the pgcli to connect to the database, and opened my jupyter notebook. The first line import pandas as pd says it doesn\\'t recognize pandas. What did I miss?\",1642654129.278400,1642654129.278400,U02UBQJBYHZ\\n7a4e4255-19a0-4f9d-b1f1-128fc36b98a8,,,,I feel happy to share that the course repository has been <https://github.com/trending|trending> #1 in GitHub. :rocket::rocket:,,1642655066.279900,U02A83NJTFY\\na015b413-4568-4f2c-95b1-98b9014910d4,,5.0,,\"Hi all, I am getting an access denied error while mounting. Error -&gt; docker: Error response from daemon: mkdir C:UsersnandiOneDriveDocumentsdata_zoomcam2_docker_sqlny_taxi_postgres_data: Access is denied. I saw a resolution for Mac but I am using Windows. I would appreciate any help. Thanks in advance.\",1642655334.281200,1642655334.281200,U02UAADSJ84\\n97a010c9-8479-4c7f-85be-0ca65192d5f8,,8.0,,\"Can someone tell my why \"\"import pandas as pd\"\" in the jupyter notebook might have stopped working?\",1642656449.283700,1642656449.283700,U02UBQJBYHZ\\n5F31B302-D106-4FDE-A61B-4FE0D449CEF3,,4.0,,\"Hi all, I have a windows machine, do I need to install any application to perform  Linux commands?\",1642656673.285800,1642656673.285800,U02SSP7C4SD\\n861fb4f4-87b7-4113-afaf-197df8d5b34c,U02TVGE99QU,,,\"```-v $(pwd):/var/lib/postgresql/data \\\\```\\nreplace that with\\n```-v $(pwd)/some-folder:/var/lib/postgresql/data \\\\```\\n\",1642641667.264100,1642656740.285900,U01AXE0P5M3\\n2c74be8d-cbdd-4517-87f9-40b175b55264,U02T9550LTU,,,Yes - optional video (bonus),1642643540.268300,1642656753.286100,U01AXE0P5M3\\nc9c42479-58df-46ae-a23b-36fe9f1226d8,U02U5L97S6T,,,that probably is the reason,1642649434.272000,1642656840.286400,U01AXE0P5M3\\nfa96154b-a48b-40cf-8bf5-d206873f9913,U02UAADSJ84,,,can you show the full command?,1642655334.281200,1642656902.286900,U01AXE0P5M3\\ne8742780-0b1a-4cfa-8d9c-f6aafb19c75a,U02SSP7C4SD,,,<https://gitforwindows.org/> - since you will need git anyways,1642656673.285800,1642656941.287100,U01AXE0P5M3\\na15267ff-6379-4159-9577-f6b852deed16,U02UBQJBYHZ,,,do you see anything in the terminal where you\\'re running jupyter?,1642656449.283700,1642656961.287400,U01AXE0P5M3\\n81f364b9-25e0-4c76-ba00-d7343a57a1a1,U02UX664K5E,,,\"a few students said that they needed to remove quotes around the env variable values (`-e`)\\ncan you try that?\",1642615746.189300,1642657078.288500,U01AXE0P5M3\\nB264D48F-1E82-4BCF-B2D8-AE2E464049BA,U02SSP7C4SD,,,\"Once I install it, should I install docker and postgress on my machine?\",1642656673.285800,1642657095.289100,U02SSP7C4SD\\nea0ee3c3-9aad-4da7-888c-84e5ee5a7a3f,U01AXE0P5M3,,,\"I think you needed to do `sudo ls` because the owner of these files is not you, but the `docker` user. As long as you can insert the data and query it - all good\",1642621263.215300,1642657192.289300,U01AXE0P5M3\\n3bb7e8f8-afeb-409f-8ba9-5595ebcbdcd5,U02UAADSJ84,,,Sure.,1642655334.281200,1642657259.289600,U02UAADSJ84\\n1edbcc09-afd2-4183-ac94-02622257d6f9,U02UAADSJ84,,,\"winpty docker run -it -e POSTGRES_USER=\"\"root\"\" -e POSTGRES_PASSWORD=\"\"root\"\" -e POSTGRES_DB=\"\"ny_taxi\"\" -v C:\\\\Users\\\\nandi\\\\OneDrive\\\\Documents\\\\data_zoomcam\\\\2_docker_sql\\\\ny_taxi_postgres_data:/var/lib/postgresql/data -p 5432:5432 postgres:13\",1642655334.281200,1642657265.289800,U02UAADSJ84\\n60eebf43-10bf-4b9d-b881-e098cf5616a1,U02U06KSE3G,,,that\\'s nice! did you mount the csv file to /data?,1642633799.243900,1642657325.290000,U01AXE0P5M3\\nf0e8fab9-51a3-4279-9f7a-a9249b4c7455,U02UBQJBYHZ,,,Let me see. Not really. Just some messages from starting up.,1642656449.283700,1642657337.290200,U02UBQJBYHZ\\n42afe528-2992-4ee4-bf82-eadce8b240d7,U01AXE0P5M3,,,\"yes. but it seems that it\\'s not the issue\\n\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642636967251800?thread_ts=1642630680.235000&amp;cid=C01FABYF2RG>\",1642534337.023000,1642657377.290500,U01AXE0P5M3\\n48424fb0-6e4e-4e61-a8a6-464963b22e73,U02UBQJBYHZ,,,\"Previously I had run the test container from the first video successfully, and I went straight to the second video and had no trouble almost to the point where I would put the data into the database.\",1642656449.283700,1642657412.290900,U02UBQJBYHZ\\nb471e065-2962-4678-9390-004e19fa43d0,U02UBQJBYHZ,,,But then I quit everything that was running and started over.,1642656449.283700,1642657438.291100,U02UBQJBYHZ\\n1aec44a4-95fb-46c6-b972-5731d37fbeb6,U02UBQJBYHZ,,,Started over from docker run postgres:13.,1642656449.283700,1642657460.291300,U02UBQJBYHZ\\n3334c72a-87a0-497a-af61-e0ff10c95bc8,U02UBQJBYHZ,,,\"The Dockerfile is the one from the first video. But for postgres we\\'re not doing build, right?\",1642656449.283700,1642657687.293200,U02UBQJBYHZ\\n3c894f89-ad60-4170-80f6-f30bece35767,U02UBQJBYHZ,,,\"For pandas import, did you tried installing pandas if you\\'re not using Anaconda.\\n\\nAnd to view columns or describe tables from pgcli, please try using the same version of pgcli and postgresql. This happened to me. I removed pgcli and reinstalled from pip. It worked then.\",1642654129.278400,1642657709.293400,U02H0GUC7ML\\n966e8a07-5af4-4d3d-80e5-c8170e437c4f,U02UBQJBYHZ,,,\"I am using Anaconda. If I run python from the terminal, I can import pandas.\",1642654129.278400,1642657873.293700,U02UBQJBYHZ\\n48c906ae-86e9-4f29-9f4f-08232822fa8f,U02UBQJBYHZ,,,\"\"\"A connection to the notebook server could not be established. The notebook will continue trying to reconnect. Check your network connection or notebook server configuration.\"\"\",1642654129.278400,1642658329.293900,U02UBQJBYHZ\\n7c305f00-2090-4117-aff4-895696b0e619,U02UBQJBYHZ,,,\"\"\"A connection to the notebook server could not be established. The notebook will continue trying to reconnect. Check your network connection or notebook server configuration.\"\"\",1642656449.283700,1642658340.294100,U02UBQJBYHZ\\n9e7707f7-53a5-41dd-8901-db4c69dca1d8,U01AXE0P5M3,,,\"All good then hopefully. I can insert data and query it. Problem 1 solved. Problem  2 seems to be that my data has not persisted - which I assume is the point of -v in the first place? I\\'m just following along with dezoomcamp01 03 on YouTube and at around 7:40 into the video you go in to query the data and see if it is there. I get nothing except the error \\'relation \"\"yellow_taxi_data\"\"\\' does not exist. I check whether there are any data tables at all with \\\\dt and there\\'s nothing there.\",1642621263.215300,1642658366.294300,U02U5SW982W\\n12a39e5a-ac30-41f9-bdf8-48b3c755b521,U01AXE0P5M3,,,I will re-watch the videos etc. to see if I can pick up whether I\\'ve done something obviously wrong (more than likely),1642621263.215300,1642658422.294500,U02U5SW982W\\n6d7f1420-fb7e-4abe-b099-2f809d47e960,,5.0,,\"Hello, Please can someone help with what am doing wrong?\",1642658846.295100,1642658846.295100,U02U5G0EKEH\\n55d97071-4fad-4ae0-b820-4b227fd62284,U02SSP7C4SD,,,\"docker - yes, postgres - no, you can do it from docker\",1642656673.285800,1642658958.295400,U01AXE0P5M3\\n6aa51563-efda-4dd8-a08b-2573c9d91de3,U01AXE0P5M3,,,\"yes, the data should be persisted...\",1642621263.215300,1642658986.295700,U01AXE0P5M3\\n6e3b3e6f-1fc5-415f-bbb9-998daa5ec4b8,U02UBQJBYHZ,,,<https://discourse.jupyter.org/t/connection-to-the-notebook-server-could-not-be-established/10263|https://discourse.jupyter.org/t/connection-to-the-notebook-server-could-not-be-established/10263>,1642654129.278400,1642659301.296000,U02H0GUC7ML\\n4d8c20f0-2da2-4ae8-a52a-b97aa48aa844,U02TPNLQYDV,,,Hi this is really helpful! I got this warning but I chose \\'y\\'...not sure if this is something you encountered as well?,1642443404.310800,1642659409.296300,U02U3F301FA\\n1ce069f9-7b1c-4cc4-9129-c16828641a31,U02QGJE2AUE,,,Thanks a lot Sejal!,1642574514.102300,1642659625.297100,U02QGJE2AUE\\n439d97e4-57a2-422a-8909-2c5848152338,U02TPNLQYDV,,,\"Somehow I can\\'t get it to work if I start the command from a different directory. I am sure it\\'s a PATH issue but I am very confused by that :sweat_smile:. But found this thread that solved the issue,\",1642443404.310800,1642659834.297300,U02U3F301FA\\n3282679c-6ee5-4a72-bab5-df48bab2e872,U02TPNLQYDV,,,<https://stackoverflow.com/questions/46144267/bash-gcloud-command-not-found-on-mac>,1642443404.310800,1642659835.297500,U02U3F301FA\\n37cb6156-3c87-4f25-878c-5a99062fea4a,U0205L73QNS,,,Hello I am also on PST please add me to the group. Thanks!,1642447403.330800,1642660841.298100,U02T52AQB2R\\n070dd370-5caa-4ef6-8ce8-a740c076d7d2,U02U34YJ8C8,,,<@U02U34YJ8C8> I tried following what you did here and executed this after giving the permissions but the folder doesn\\'t populate.,1642630680.235000,1642661139.298300,U02UAADSJ84\\n2b317d84-f70c-460a-a68c-641ab8c85698,U02RA8F3LQY,,,Hi <@U02RA8F3LQY> did you find a solution to this ? Having the same issue here,1642640844.260700,1642662006.298900,U01K0GEJXGF\\na256bb89-03ed-47ae-8de6-d18744310c61,U02RA8F3LQY,,,still nothing here <@U01K0GEJXGF>,1642640844.260700,1642662037.299100,U02RA8F3LQY\\n950db275-23f2-49a7-b92a-2883ecdf4da8,U01AXE0P5M3,,,Hmmm... I feel another head-ache coming up...,1642621263.215300,1642662505.299300,U02U5SW982W\\nb927105c-4e14-49ff-9195-48cdfd3288c3,U01AXE0P5M3,,,\"I tried changing permissions but was having issues with sudo from bash....was already tired last night, I will try again today.\\n\\nBut I noticed that the postgres data that is supposed to be copied into `ny_taxi_postgres_data` was stored in a volume and I was able to view it using docker desktop\",1642534337.023000,1642663623.299600,U02QS4BD1NF\\n122f5fb2-ad10-424e-a312-405a34ee84a7,U02RA8F3LQY,,,<@U02RA8F3LQY> check this thread <https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642523808486200>,1642640844.260700,1642664277.300000,U01K0GEJXGF\\n16f38b8d-78a4-42af-b527-90ed4f7afa36,U02SSP7C4SD,,,you can use wsl on windows to install ubuntu direct on windows and you can acess it using ssh,1642656673.285800,1642665525.300800,U02UCMYFDT4\\nc16620e8-ee1e-4a46-972b-189e0b184c04,U02U5G0EKEH,,,\"What happens if you replace `--host=localhost` with `--host=0.0.0.0` or `127.0.0.1`? Just a guess but worth a try :wink:\\n\\nThere might also be some problem in your python code (based on the SyntaxError) but that\\'s hard to say from only this screenshot\",1642658846.295100,1642667198.301600,U02SLJ38N2F\\n6ad228a3-187e-4646-9570-f143bfae3ebf,U02U34YJ8C8,,,<@U02UBQJBYHZ> Thanks Cris. I should have tried this. I don’t think you’ll have wrecked anything.,1642630680.235000,1642667570.301900,U02U34YJ8C8\\nf1344e42-0802-4383-ab01-890b714a2c8f,U02U34YJ8C8,,,\"<@U02UAADSJ84> Not sure what the issue. Maybe try following the steps again. Delete the folder, recreate it, then re-run the docker run command. I’d also gone into Docker Desktop and deleted a load of Docker images.\",1642630680.235000,1642667659.302100,U02U34YJ8C8\\n19e02d24-0418-4781-b58c-d69de146963c,,3.0,,\"What happens if you don’t specify:\\n\\n`-v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data`\\n\\nI’m a docker newbie, and just trying to understand things better.\",1642668522.303700,1642668522.303700,U02U34YJ8C8\\n76A81576-4A8B-460E-8AAB-FA249CC41B60,U02U34YJ8C8,,,\"Docker containers are stateless, which means that any changes happening inside the container will disappear once the container is killed. This is by design because it allows you to generate as many containers as needed and all of them will be identical.\\n\\nIf you don\\'t specify an external volume (what you\\'re doing with the -v argument), then Postgres will create a database on a folder inside the container, which inevitably will disappear once you kill it.\\n\\nBy mounting a volume, that internal folder in the container is replicated on an external folder, so that the container can access that info when recreated again. It\\'s the way you handle persistent data with stateless containers.\",1642668522.303700,1642669057.309500,U02BVP1QTQF\\nafe60771-cbb3-47f2-a8da-4cd66a759130,,14.0,,\"Hi guys. I\\'m having trouble with Remote SSH in VS code.\\nI\\'m trying to connect to a host. VS code is asking me to select the platform. I choose linux.\\nThen I get an error\",1642669749.310100,1642669749.310100,U02Q7466F2T\\n1089f099-36ce-4bbd-80bb-e1f3911ab17b,U02Q7466F2T,,,\"`[12:06:37.052] Log Level: 2`\\n`[12:06:37.064] remote-ssh@0.70.0`\\n`[12:06:37.064] win32 x64`\\n`[12:06:37.066] SSH Resolver called for \"\"ssh-remote+de-zoomcamp\"\", attempt 1`\\n`[12:06:37.067] \"\"remote.SSH.useLocalServer\"\": false`\\n`[12:06:37.067] \"\"remote.SSH.showLoginTerminal\"\": false`\\n`[12:06:37.067] \"\"remote.SSH.remotePlatform\"\": {}`\\n`[12:06:37.067] \"\"remote.SSH.path\"\": undefined`\\n`[12:06:37.068] \"\"remote.SSH.configFile\"\": undefined`\\n`[12:06:37.068] \"\"remote.SSH.useFlock\"\": true`\\n`[12:06:37.068] \"\"remote.SSH.lockfilesInTmp\"\": false`\\n`[12:06:37.068] \"\"remote.SSH.localServerDownload\"\": auto`\\n`[12:06:37.068] \"\"remote.SSH.remoteServerListenOnSocket\"\": false`\\n`[12:06:37.069] \"\"remote.SSH.showLoginTerminal\"\": false`\\n`[12:06:37.069] \"\"remote.SSH.defaultExtensions\"\": []`\\n`[12:06:37.069] \"\"remote.SSH.loglevel\"\": 2`\\n`[12:06:37.069] \"\"remote.SSH.serverPickPortsFromRange\"\": {}`\\n`[12:06:37.069] \"\"remote.SSH.enableDynamicForwarding\"\": true`\\n`[12:06:37.070] \"\"remote.SSH.serverInstallPath\"\": {}`\\n`[12:06:37.071] SSH Resolver called for host: de-zoomcamp`\\n`[12:06:37.071] Setting up SSH remote \"\"de-zoomcamp\"\"`\\n`[12:06:37.109] Using commit id \"\"899d46d82c4c95423fb7e10e68eba52050e30ba3\"\" and quality \"\"stable\"\" for server`\\n`[12:06:37.120] Install and start server if needed`\\n`[12:08:57.745] Checking ssh with \"\"ssh -V\"\"`\\n`[12:08:57.837] &gt; OpenSSH_for_Windows_8.1p1, LibreSSL 3.0.2`\\n\\n`[12:08:57.848] Running script with connection command: ssh -T -D 58754 \"\"de-zoomcamp\"\" bash`\\n`[12:08:57.856] Terminal shell path: C:\\\\WINDOWS\\\\System32\\\\cmd.exe`\\n`[12:08:58.197] &gt; ]0;C:\\\\WINDOWS\\\\System32\\\\cmd.exe`\\n`[12:08:58.198] Got some output, clearing connection timeout`\\n`[12:08:58.212] &gt; Bad owner or permissions on C:\\\\\\\\Users\\\\\\\\username/.ssh/config`\\n`&gt; The process tried to write to a nonexistent pipe.`\\n`&gt;` \\xa0 \\xa0 \\xa0 \\xa0 \\n`[12:08:59.498] \"\"install\"\" terminal command done`\\n`[12:08:59.499] Install terminal quit with output:` \\xa0 \\xa0 \\xa0 \\xa0 \\n`[12:08:59.500] Received install output:` \\xa0 \\xa0 \\xa0 \\xa0 \\n`[12:08:59.502] Resolver error: Error:` \\n\\xa0 \\xa0 `at Function.Create (c:\\\\Users\\\\username\\\\.vscode\\\\extensions\\\\ms-vscode-remote.remote-ssh-0.70.0\\\\out\\\\extension.js:1:430425)`\\n\\xa0 \\xa0 `at c:\\\\Users\\\\username\\\\.vscode\\\\extensions\\\\ms-vscode-remote.remote-ssh-0.70.0\\\\out\\\\extension.js:1:428441`\\n\\xa0 \\xa0 `at Object.t.handleInstallOutput (c:\\\\Users\\\\username\\\\.vscode\\\\extensions\\\\ms-vscode-remote.remote-ssh-0.70.0\\\\out\\\\extension.js:1:429004)`\\n\\xa0 \\xa0 `at Object.t.tryInstall (c:\\\\Users\\\\username\\\\.vscode\\\\extensions\\\\ms-vscode-remote.remote-ssh-0.70.0\\\\out\\\\extension.js:1:524212)`\\n\\xa0 \\xa0 `at processTicksAndRejections (internal/process/task_queues.js:93:5)`\\n\\xa0 \\xa0 `at async c:\\\\Users\\\\username\\\\.vscode\\\\extensions\\\\ms-vscode-remote.remote-ssh-0.70.0\\\\out\\\\extension.js:1:487216`\\n\\xa0 \\xa0 `at async Object.t.withShowDetailsEvent (c:\\\\Users\\\\username\\\\.vscode\\\\extensions\\\\ms-vscode-remote.remote-ssh-0.70.0\\\\out\\\\extension.js:1:490561)`\\n\\xa0 \\xa0 `at async Object.t.resolve (c:\\\\Users\\\\username\\\\.vscode\\\\extensions\\\\ms-vscode-remote.remote-ssh-0.70.0\\\\out\\\\extension.js:1:488295)`\\n\\xa0 \\xa0 `at async c:\\\\Users\\\\username\\\\.vscode\\\\extensions\\\\ms-vscode-remote.remote-ssh-0.70.0\\\\out\\\\extension.js:1:564197`\\n`[12:08:59.516] ------`\",1642669749.310100,1642669770.310400,U02Q7466F2T\\nbd416b19-0243-4632-87d5-2a7f492410f2,U02U34YJ8C8,,,\"<@U02BVP1QTQF> Perfect thank you. That makes sense now. So each time I want to use the container, I need to use the `docker run` command again with all the parameters?\",1642668522.303700,1642670162.310600,U02U34YJ8C8\\n967F04B6-EF21-4163-9D89-D3044F3B1B93,U02U34YJ8C8,,,\"Correct. You need to give the container the initial conditions to interact with the exterior; the Dockerfile only sets up the \"\"insides\"\" of the container, so you need to set up the \"\"outsides\"\" with the arguments in docker run.\\n\\nSince most of the time we will use multiple containers that need to talk to each other, we will see tools that make this easier to set up with configuration files rather than having to run multiline commands every single time. But for running a single container with docker run, you need to provide those arguments.\",1642668522.303700,1642670416.315700,U02BVP1QTQF\\nca525b8e-a754-425b-85e5-3c4bc9fd0294,U02T1BX1UV6,,,\"Would you give us the command line, what you use to run postgres in docker? And remind you, that you should keep it running, and run pgcli in another terminal.\",1642416911.154000,1642670934.316000,U02RYUWG4CQ\\nc18212ff-39f2-438e-a125-c2217ddb29dd,U02Q7466F2T,,,Does usual ssh work for you?,1642669749.310100,1642672078.316500,U01AXE0P5M3\\n3fcad4e4-de91-4a56-9293-3584fef19ab0,,8.0,,\"Hello everybody. I got really stuck at Docker container creation. I use WSL Ubuntu. Which path should I type in -v parameter? The thing is, if I specify /mnt/d/docker/ny_taxi I have \"\"changing permissions\"\" error. At least command creates the folder. I can see it in Windows explorer. I used advice with chmod command, didn\\'t helped. But when I specify other path (/home/docker/ny_taxi) it works. It creates folder with all the data somewhere inside linux, where I could reach from windows. Any help would be much appreciated. Thank you.\",1642672783.324500,1642672783.324500,U02QW0M1G9J\\n5d4af54a-0bf4-436e-bb21-fb8987db1bff,,2.0,,\"Hi All, Are the sequences is correct to watch the video?\\n\\n_*Installation &amp; Configuration*_\\n1. Introduction to Terraform concepts and GCP Pre-Requisites\\n<https://www.youtube.com/watch?v=Hajwnmj0xfQ&amp;list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&amp;index=6>\\n2.  Setting up the environment on Google Cloud (Cloud VM + SSH access)\\n<https://www.youtube.com/watch?v=ae-CV2KfoN0&amp;list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&amp;index=7>\\n\\nWeek 1 Tutorial:\\n1. dezoomcamp 01 01\\n2. dezoomcamp 01 02\\n3. dezoomcamp 01 03\\n4. dezoomcamp 01 04\\nWith the right sequence, I can plan on my time to start to study the DE &amp; do the homework:grin:\",1642672832.324700,1642672832.324700,U02T697HNUD\\nf4f8aed5-375f-4530-9a87-60fde4892466,U02QW0M1G9J,,,Permissions for mounted directories in WSL are tricky. Can you try doing it in your wsl home dir?,1642672783.324500,1642672873.324800,U01AXE0P5M3\\nb65109ff-b948-4aeb-b0ad-a133f312ac47,U02QW0M1G9J,,,Yes in home dir container runs and pgcli works after.,1642672783.324500,1642672950.325100,U02QW0M1G9J\\n7ee13f61-5dc1-4d05-a4e8-77039729f970,U02UBQJBYHZ,,,\"It could have been a network issue, you could identify that it\\'s that because there\\'s a sand clock in the tab and probably the cell remains in execution ([*]). \\nJust restart the server or re load and it should be fine. \\nAnother explanation could be that you close the terminal hosting the notebook. In that case just launch jupyter notebook again\",1642656449.283700,1642673053.325600,U01B6TH1LRL\\n9232cd53-7f53-40d1-a493-bf75147a56c4,U02QW0M1G9J,,,\"You gave me an idea just to stick with linux and work as it is, without controlling the workflow from windows\",1642672783.324500,1642673054.325900,U02QW0M1G9J\\ne7603fe5-fbdc-468e-8213-0fb6900afaae,,7.0,,\"About HW: ( I advice to use this thread if you have any question about HW too).\\n\\nI don\\'t get where we are picking the information for \"\"for passengers picked up\\nin central park\"\". Question 5. Perhaps the problem is my geography :P\\nI see the information of Location id in table \"\"taxi_zones\"\" so we just need to Join. But should we filter somehow?\",1642673343.329000,1642673343.329000,U02CD7E30T0\\nb74707cd-f8e6-417d-9ddc-0fd4a1adbadf,U02Q7466F2T,,,This is the first time I\\'m trying it,1642669749.310100,1642673469.329100,U02Q7466F2T\\ncc3c0375-b1da-4374-877e-55ee550bda62,U02UBQJBYHZ,,,\"hey <@U02UBQJBYHZ> this looks like the same issue you raised on the other thread, do you have network issues in general?\",1642654129.278400,1642673758.329900,U01B6TH1LRL\\n4cf411fa-2254-4bcc-99d7-86dd20fe7e66,U02T697HNUD,,,\"Hey Low! The videos have the order in the name, as you noticed and are also linked to the topic in the repo, inside of the week folder\",1642672832.324700,1642673902.330100,U01B6TH1LRL\\n8a314077-8697-4159-8bc0-87e85cc3e5f6,U02T1BX1UV6,,,\"Thank you, I was able to fix this\",1642416911.154000,1642673917.330300,U02T1BX1UV6\\n47f66de8-7a07-4a48-b6f7-0c9458b374bb,U02T1BX1UV6,,,I realized my docker container was not running,1642416911.154000,1642673932.330500,U02T1BX1UV6\\n25d5cfad-4102-4243-bcd4-9cc78a026e7e,U02T1BX1UV6,,,\"However, after running my  pgcli -h localhost -p 5432 -u root -d ny_taxi and entering my password, the terminal freezes and nothing happens\",1642416911.154000,1642673999.330700,U02T1BX1UV6\\n415af749-65ea-4503-a43a-4229937c30d2,U02T1BX1UV6,,,\"I have tried changing to port as recommended in another thread, but it\\'s still not working\",1642416911.154000,1642674021.330900,U02T1BX1UV6\\nc6515241-0309-410a-aba7-2fc2f67b5b16,U02CD7E30T0,,,\"Bah! I do the question and give the answer :stuck_out_tongue:\\nI Have to check the PULocationId where the Zone is Central Park :slightly_smiling_face:\",1642673343.329000,1642674100.331100,U02CD7E30T0\\n2779f3e0-0f3d-4671-a8f2-45a7720b4fcc,U02T697HNUD,,,\"Hi Victoria, ah yes I noticed the order now. Thanks for replying me on this:grin:\",1642672832.324700,1642674131.331300,U02T697HNUD\\n8e41b68a-a52c-4df2-b7be-b0d1c0eb5c59,U02Q7466F2T,,,\"if you do `ssh de-zoomcamp` in your terminal, what happens?\",1642669749.310100,1642675759.331700,U01AXE0P5M3\\n68d6f67b-f22a-4828-8432-c3bfca7fbd26,U02QW0M1G9J,,,\"yes it\\'s better to do it this way. also, if you use VS code, WSL remote extension is quite helpful\",1642672783.324500,1642675807.331900,U01AXE0P5M3\\n7367d6af-2725-4429-b33d-83a3c178ce7f,U02Q7466F2T,,,Everything seems to be fine here.,1642669749.310100,1642679546.333200,U02Q7466F2T\\n33465ee0-59f6-4558-b1ab-7215c3a4f4ae,U02QZN0LSBT,,,<@U01AXE0P5M3> Thank you for the great initiative! I will send you a PR when I am hopefully done with the initial draft of week 1 :smile:,1642548032.081900,1642679679.333700,U02QZN0LSBT\\nf2d93b2b-5e5b-4273-8904-8a9fd0ed1181,U02CD7E30T0,,,\"Am I the only one getting strange error \\n&gt;column \"\"PULocationID\"\" does not exist? Only renaming all columns to lowercase helped, so I guess it\\'s all about postgres syntax\",1642673343.329000,1642679802.334700,U02UECC4H6U\\nf448e807-2818-4fc7-bf61-cae98f6df374,,6.0,,\"Hi just wanted to ask, is this a self paced course? I am still in college and most concepts and technology are new to me so its taking me a while to get a hang of things. So are there any rigid deadlines in which case I should do this later when I can follow along or is it okay to take my time?\",1642680003.337000,1642680003.337000,U02UZBJ2Q6L\\n3c9d738e-776f-427d-b8b2-76993b3379ad,,3.0,,\"However, after running my  pgcli -h localhost -p 5432 -u root -d ny_taxi and entering my password, the terminal freezes and nothing happens.After Changing the port to 5431 also it is same\",1642680059.338500,1642680059.338500,U02R72A51BK\\n455bd99d-f516-459b-ae1c-fb0bd2f4e7a2,U02Q7466F2T,,,Are you on windows? Perhaps you need to configure your windows firewall,1642669749.310100,1642680069.338600,U01AXE0P5M3\\n2f0455c3-0ca2-4eb5-a925-f74ba1d32769,U02TVGE99QU,,,\"I encountered this problem too and I solved it by stopping and restarting the container:\\n`docker stop &lt;container_name&gt;`\\n`docker start &lt;container_name&gt;`\\n\\ncontainer_name in your case would be \"\"pg-database\"\" or \"\"pg-database-2\"\"\",1642641667.264100,1642680357.338800,U02QS4BD1NF\\n2575c961-231d-4345-9cc5-58ba811e3f59,U02R72A51BK,,,\"If you are doing this in git bash, thats the normal reaction most people get.\\nRun the command in powershell and it wont freeze\",1642680059.338500,1642680469.339100,U02QS4BD1NF\\n896b1a98-14aa-4165-a157-a443fe0fa726,U02U34YJ8C8,,,I set up docker on my Mac for the last course with no problem.,1642551977.086500,1642680697.339400,U02CPBEH42W\\n81dfd0f4-0dac-4d3c-9551-241d9b9311b5,U02CD7E30T0,,,Perhaps... I am using DBeaver since I already had in my laptop and connecting to localhost,1642673343.329000,1642680997.339600,U02CD7E30T0\\n9a66f994-e6a2-4eb2-8c0d-681a9bd2da19,U02UZBJ2Q6L,,,\"According to the README in the repo, you need to complete the project in order to get a certificate. If you cannot complete the project in time, you can either work at your own pace if you don’t care about the certificate or you can wait until the next time that the course is run. But it would be best if one of the organizers could confirm this.\",1642680003.337000,1642681078.339800,U02BVP1QTQF\\n3fbb4d1f-c8a6-4438-be46-32ef8fb62864,U02UZBJ2Q6L,,,Thanks for the info! I hope an organiser can confirm this as well.,1642680003.337000,1642681162.340000,U02UZBJ2Q6L\\n81f6659d-94de-4231-ad29-d4eeef7864d6,U02TVGE99QU,,,\"<@U01AXE0P5M3> didn\\'t work. I\\'m wondering if it has something to do with my ny_taxi_postgres_data folder permissions. In one of the videos you were able to take a look at it from vscode but I don\\'t have the permissions, the folder is owned by systemd Core Dumper?\\n<@U02QS4BD1NF> that also didn\\'t work, same issue when I restart.\",1642641667.264100,1642681394.340200,U02TVGE99QU\\nd01c16d1-85d7-44c4-b058-48a3558a48c9,,,thread_broadcast,\"Hi <@U02UBV4EC8J>, yeap I\\'m on a company laptop. I disabled the VPN and I have admin permission but I couldn\\'t install it. Do you know if there is any way to circumvent this?\",1642643423.267000,1642681657.340500,U02QDKU4GTY\\n2fc36dd2-62ed-433d-9231-0bf0c363a363,U02QZN0LSBT,,,\"<@U02QZN0LSBT> my PR with my notes just got accepted even though the notes aren’t finished yet. I think it’s ok if you send the PR already and keep working on your notes afterwards if the link won’t change, but obviously feel free to do what you’re most comfortable with.\",1642548032.081900,1642681879.341200,U02BVP1QTQF\\nd11ec002-9022-42b7-87d5-687818a546ac,U02QZN0LSBT,,,I just saw your notes. I am importing the notes from notion to github as we speak :blush: then I will do a PR,1642548032.081900,1642681974.341400,U02QZN0LSBT\\n046214aa-1c52-4bcc-bdb0-d37134060f7b,U02QZN0LSBT,,,You don’t actually have to host your notes on GitHub; the PR only needs to have a link to your notes. I just host on GitHub because it’s what I’m most comfortable with.,1642548032.081900,1642682168.342200,U02BVP1QTQF\\n7ff70c84-f01a-40e7-9049-7c8ae94b86b3,U02Q7466F2T,,,\"Yes I use windows. Okay, I\\'ll try to set up a firewall. Thank you very much!\",1642669749.310100,1642682383.342400,U02Q7466F2T\\nb5b306f4-d2e6-4989-b8f0-31b2f73137ce,U02UZBJ2Q6L,,,\"Hi <@U02UZBJ2Q6L>\\nYou can watch the videos when it\\'s more convenient for you and take as much time as you need. The weeks are indicative to when will be releasing the content and also each week has a office hour call.\\nWhat will happen after  the 9 weeks is that you won\\'t have those office calls anymore but you can certainly keep using the content and the community for questions (which includes the organizers of the course :slightly_smiling_face: )\",1642680003.337000,1642682475.342700,U01B6TH1LRL\\n4a289331-3b5b-4f2e-9dca-9281bd0f3cd8,U02CD7E30T0,,,<@U02UECC4H6U> you can use the column name in quotes when it containes upper and lowe case,1642673343.329000,1642682530.343000,U01B6TH1LRL\\n901338d4-ea9f-45ac-b0d2-d9ae2a6b3d49,,,,\"Video about docker-compose:\\n\\n<https://www.youtube.com/watch?v=hKI6PkPhpa0&amp;list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&amp;index=7>\",,1642683342.343800,U01AXE0P5M3\\n7c181a23-2e12-4fd0-aa02-3e16f4670946,U02QZN0LSBT,,,\"yes indeed, you can host them anywhere\",1642548032.081900,1642683388.344200,U01AXE0P5M3\\n15da92af-5b5f-4a47-b64f-811c49a8fede,,4.0,,What is an alternative for setting port forwarding in described Setting up the environment on Google Cloud (Cloud VM + SSH access) video? It seems that remote ssh extension is not available for all packaged versions of vscode for linux.,1642683438.345000,1642683438.345000,U02S565BCKX\\n1993d9bd-5b07-427f-9915-ad181505a070,U02TVGE99QU,,,Could be. Try not creating the folder (delete it if it already exists) and running docker run,1642641667.264100,1642683456.345100,U01AXE0P5M3\\na96e4c65-5ba3-4824-819f-8b13877efede,U02S565BCKX,,,<https://www.ssh.com/academy/ssh/tunneling/example>,1642683438.345000,1642683484.345300,U01AXE0P5M3\\n8f63eaaa-d18e-405a-80ea-6577021aa571,U02S565BCKX,,,\"you can also add port forwarding to your ssh config, and every time you ssh to this remote, it\\'ll start port forwarding\\n\\n<https://nerderati.com/2011/03/17/simplify-your-life-with-an-ssh-config-file/>\",1642683438.345000,1642683609.345600,U01AXE0P5M3\\n030dfda7-807b-4ebd-98ba-ae8a407c9c96,U02TVGE99QU,,,\"will deleting that folder not lose the database? also it gives me this error:\\n```The folder \"\"ny_taxi_postgres_data\"\" cannot be handled because you do not have permissions to read it.```\",1642641667.264100,1642683895.346000,U02TVGE99QU\\n86bcc24c-45d6-4220-92cc-5fe9f3c13bfa,U02R72A51BK,,,\"Thanks <@U02QS4BD1NF>  it worked\\nI am getting import error when important pandas :sweat_smile:\",1642680059.338500,1642684319.346400,U02R72A51BK\\n2d67f9dd-a274-45d8-92f0-d53278ef7f92,U02Q7466F2T,,,<https://stackoverflow.com/questions/49926386/openssh-windows-bad-owner-or-permissions|This> helped me.,1642669749.310100,1642684363.346600,U02Q7466F2T\\na101ac61-ef73-440a-b39f-865bcef2781c,U02CD7E30T0,,,\"<@U01B6TH1LRL> tried, didn\\'t work. Also, how should it work with table alias? like data.\"\"PULocationID\"\"?\",1642673343.329000,1642684936.347700,U02UECC4H6U\\n7dc248ca-a9f0-4df9-8e0f-a12e1374dc96,,5.0,,\"I think I am following the instructions exactly as in the video, but when I run `terraform apply` I get an error.\",1642685324.348600,1642685324.348600,U02S565BCKX\\nfe503934-adb6-4929-8528-ccc747d29d61,U02S565BCKX,,,\"```google_storage_bucket.data-lake-bucket: Creating...\\ngoogle_bigquery_dataset.dataset: Creating...\\n╷\\n│ Error: Post \"\"<https://storage.googleapis.com/storage/v1/b?alt=json&amp;prettyPrint=false&amp;project=dct-exercise>\"\": metadata: GCE metadata \"\"instance/service-accounts/default/token?scopes=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform%2Chttps%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email\"\" not defined\\n│ \\n│   with google_storage_bucket.data-lake-bucket,\\n│   on <http://main.tf|main.tf> line 19, in resource \"\"google_storage_bucket\"\" \"\"data-lake-bucket\"\":\\n│   19: resource \"\"google_storage_bucket\"\" \"\"data-lake-bucket\"\" {\\n│ ```\",1642685324.348600,1642685333.348700,U02S565BCKX\\nc3e9b0ec-4491-430b-abda-21447415d5a5,U02QZN0LSBT,,,Awesome. Just did a PR.,1642548032.081900,1642685339.348900,U02QZN0LSBT\\n64c74d9c-ae1b-4ad3-841b-daf7b69febd5,,2.0,,\"Hello! I have a question about Homework 1. The last row of “Zones” dataframe (with `LocationID = 265`) contains the zone with a name “NaN”. If I think that this zone is the answer to some question (5 or 6, no spoilers), how should I write it in the answer? “NaN”, “nan”, “Unknown”? Or does it mean that I have a mistake in my solution, and the answer shouldn’t look like this?\\n\\nMaybe this is a silly question, but I am a bit confused with a task formulation which implies that there should be a real zone in the answer. Instead, I have NaN which is probably used as a missing value in the dataframe :sweat_smile:\",1642685638.351800,1642685638.351800,U02QKCUF9QU\\n5b0ca5c4-11b7-4e04-95e8-37696d1a2c84,U01QGQ8B9FT,,,Having the same problem unfortunately,1642586687.122000,1642685745.352000,U02QY444V8E\\n417e0841-803a-4a7e-bd72-bc16f5d83d1c,U02TJ69RKT5,,,\"Thanks, I will do it again\",1642638867.256500,1642685827.352400,U02TJ69RKT5\\n753d9f3a-621f-415f-b262-b80e25698c14,U02QKCUF9QU,,,write NaN (I\\'ll update the instructions),1642685638.351800,1642685853.352800,U01AXE0P5M3\\ndf896c2f-881d-4fb9-a354-24299f6c25e7,U02QKCUF9QU,,,\"actually I think \"\"Unknown\"\" is better - please do that\",1642685638.351800,1642685873.353000,U01AXE0P5M3\\n94af8db6-20c4-45e4-8dad-9c4b891e9988,U02QW0M1G9J,,,Worked for me too but it created a new folder? What should I do next?,1642672783.324500,1642685960.353200,U01QGQ8B9FT\\n48dda4b3-472c-464f-be14-e56adfee21a7,U02QDKU4GTY,,,\"could be  a firewall on your laptop, are you using windows or mac?\",1642643423.267000,1642686048.353400,U02UBV4EC8J\\n5121f584-7e12-46cd-a50c-96a16f1eaabf,U02R72A51BK,,,powershell worked for me too. but i just want to know why git bash doesnt work ?,1642680059.338500,1642686211.353700,U02U5GQK25C\\nd59c9b0a-e101-477a-a348-91b12e2c0501,U02QGA57GRY,,,\"<@U01AXE0P5M3> I am still getting this error.. any advice on how to run \"\"engine.connect()\"\" would be very helpful..\",1642611614.176500,1642686529.354000,U02QGA57GRY\\nc20de83c-c400-4a34-b06a-b69722f13884,U01QGQ8B9FT,,,When I switch to the root user via \\'sudo -s\\' then \\'ls\\' into \\'ny_taxi_postgres_data\\' Ican see all the file but on vscode it\\'s empty,1642586687.122000,1642686584.354300,U02QY444V8E\\n16644bb7-5eed-4c16-b978-3137a01b8510,U02S565BCKX,,,Did you activate all the apis?,1642685324.348600,1642686723.354500,U01AXE0P5M3\\nfb3dc365-86f4-413e-b543-ba89acff9202,U02QW0M1G9J,,,\"yes, it should create a new folder. next - continue watching the videos :)\",1642672783.324500,1642686786.354700,U01AXE0P5M3\\ndf847e21-0f74-4efc-8427-84284dd987cf,U02QGA57GRY,,,\"no, sorry - I don\\'t know. Check other threads where people had problems, perhaps you\\'ll find a solution\",1642611614.176500,1642686857.355000,U01AXE0P5M3\\n3f126954-4cea-4b09-b605-55e96233cb41,U02Q7466F2T,,,\"<@U01AXE0P5M3> Hi Alexey, thank you for this interesting course! May I ask you what could be the reason for this error? Thank you.\",1642669749.310100,1642687779.355600,U02RR9Z0CCV\\nc7d25cd4-2dcd-44d9-b2d4-e2c28813259c,U02U546D36Z,,,<@U02U546D36Z> Hi! Sorry for the late response. I just want to add the <https://datatalks.club/blog/data-roles.html|link to the article> published by Alexey. It briefly explains the main roles in a data team and their responsibilities.,1642439158.249000,1642687810.356000,U02DNSC6Z52\\n74d1a4e7-c89d-41fe-aca9-350d5041a313,U02Q7466F2T,,,I don\\'t know...,1642669749.310100,1642687832.356300,U01AXE0P5M3\\nd87ecc45-ec93-4802-b5f2-6a8cd47b6e2b,U02U546D36Z,,,<@U02DNSC6Z52> thank you very much!,1642439158.249000,1642688076.356600,U02U546D36Z\\n6c1d517f-1478-475c-8227-950278383bbc,U02Q7466F2T,,,I use Ubuntu 20.04 LTS on Windows. In the terminal everything works. Just can\\'t understand what is wrong here. After I choose Conect to Host... I do not have _de-zoomcamp_ as an option to select. So I add New SSH Host and call it by `ssh de-zoomcamp` command. Then I select ~/.ssh/config and press connect. And this error comes up. Just wonder is it required to use Remote SSH or I can continue the course without this stage?,1642669749.310100,1642688625.357800,U02RR9Z0CCV\\nad112ade-6963-40ff-bcb0-9323e6c2699f,U02Q7466F2T,,,your ssh config is in WSL? It has to be on the host,1642669749.310100,1642688787.358300,U01AXE0P5M3\\na1176102-7e4a-488c-a6c7-5d0268a51f55,U02Q7466F2T,,,\"it\\'s not required to use remote SSH, but it\\'s helpful\",1642669749.310100,1642688807.358500,U01AXE0P5M3\\n286f89b9-52dc-4dc5-b2b3-0fd316748627,U02U5G0EKEH,,,\"Thanks, i look through my code and found the bug\",1642658846.295100,1642689566.360000,U02U5G0EKEH\\na0d6c229-926e-4e4e-b30b-4dfef3079e0a,U02U5G0EKEH,,,i also removed the line breaks(multiline) and used a single line command instead,1642658846.295100,1642689708.360200,U02U5G0EKEH\\nf4bc2986-9d5b-4927-856a-6701e78ecd09,U02V4412XFA,,,\"<@U01AXE0P5M3> - Sorry for tagging you, But I guess you missed this question.\",1642610988.174800,1642690111.362700,U02V4412XFA\\n435d2ecb-04fc-431c-a625-1b686b959833,,13.0,,\"Good Morning Everyone. I was following along to video \"\"Setting up the Environment on Google Cloud (Cloud VM + SSH access)\"\" and it was going smoothly until I try terraform apply. I tried last night and this morning with the same result. I get the message below.\\n``` Error: googleapi: Error 403: The project to be billed is associated with an absent billing account., accountDisabled\\n│ \\n│   with google_storage_bucket.data-lake-bucket,\\n│   on <http://main.tf|main.tf> line 19, in resource \"\"google_storage_bucket\"\" \"\"data-lake-bucket\"\":\\n│   19: resource \"\"google_storage_bucket\"\" \"\"data-lake-bucket\"\" {\\n│ \\n╵\\n╷\\n│ Error: Error creating Dataset: googleapi: Error 400: The project yes has not enabled BigQuery., invalid\\n│ \\n│   with google_bigquery_dataset.dataset,\\n│   on <http://main.tf|main.tf> line 45, in resource \"\"google_bigquery_dataset\"\" \"\"dataset\"\":\\n│   45: resource \"\"google_bigquery_dataset\"\" \"\"dataset\"\" {```\\nDo I just need to enable BigQuery through the console? Should I just move on to the next video and see if something is explained there? Do I need to edit the <http://main.tf|main.tf> file? Has anyone ran into this one yet?\",1642690115.362900,1642690115.362900,U02TNEJLC84\\nc8863a75-31b2-4143-8011-b82b84fecca9,U02QDKU4GTY,,,\"I\\'m on windows, yeap it seems firewall are on\",1642643423.267000,1642690965.365800,U02QDKU4GTY\\n16627A0A-2B39-4653-AF73-084B1E102F2B,U02TNEJLC84,,,Seems you have not attached any billing account to your project ,1642690115.362900,1642691745.367000,U02TBCXNZ60\\nd5b02007-db51-4184-83e2-1f8ff4f85971,U02Q7466F2T,,,\"yes, thank you. Now I see that I made this mistake at the very early beginning\",1642669749.310100,1642692222.367300,U02RR9Z0CCV\\na51e8946-4e03-4754-bdbb-a015e5f79a0a,,2.0,,I have a python question which is different from data engineering course where can I ask the question?,1642692592.368400,1642692592.368400,U02HYRDSYQY\\nac519d42-bdde-45c8-bf63-1c67e121eec0,,24.0,,\"While applying `terraform init` I was not apple to install the demand provider.\\nAny one else had this problem?\\n`Initializing provider plugins...`\\n`- Finding latest version of hashicorp/google...`\\n`- Installing hashicorp/google v4.7.0...`\\n`╷`\\n`│ Error: Failed to install provider`\\n`│`\\n`│ Error while installing hashicorp/google v4.7.0: chmod .terraform/providers/registry.terraform.io/hashicorp/google/4.7.0/linux_amd64/terraform-provider-google_v4.7.0_x5: operation not permitted`\",1642693507.370300,1642693507.370300,U02CD7E30T0\\n2c4a6f0e-f416-4e52-8eb9-55b5180c7dcf,U02TNEJLC84,,,I just checked and billing is enabled on this project. :thinking_face:,1642690115.362900,1642693508.370400,U02TNEJLC84\\n7f8c065e-6b88-4ecf-a262-bf6e0f27b04c,U02UX664K5E,,,\"<@U01AXE0P5M3> After almost loosing my mind, I\\'m now able to connect to postgres. I submitted my issue on the <https://forums.docker.com/t/cant-connect-to-postgres-through-docker/120051|Docker forum> and a Senior Sysadmin with apparently Docker blood running in his veins :D replied to my post telling me that I needed to connect to port `5431` because the mapping was `5431:5432`.  I couldn\\'t believe this was the issue. A rookie mistake from my part. Hey at least now, I have memorized tons of docker commands by trying a zillion times. In case anyone runs into the same issue, I\\'m posting the full solution here.\\n\\n```docker run -it \\\\\\n    -e POSTGRES_USER=root \\\\\\n    -e POSTGRES_PASSWORD=root \\\\\\n    -e POSTGRES_DB=ny_taxi \\\\\\n    -v c:/Users/lyesd/Desktop/DE-ZoomCamp/week1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n    -p 5431:5432 \\\\\\n    postgres:13```\\n`pgcli -h localhost -p 5431 -u root -d ny_taxi`\",1642615746.189300,1642693557.370600,U02UX664K5E\\n4ed0b6a9-e489-47ec-94cd-2732fb8afe14,U02S565BCKX,,,\"As far as I can see, yes.\",1642685324.348600,1642693786.370800,U02S565BCKX\\nccb02bbb-8b7e-43b5-b0ee-d56622f920e9,U02CD7E30T0,,,\"Hi Luis, usually this is when your terraform CLI version is not supporting the provider version. What is the terraform version you\\'re using? And can you update that in `.terraform-version` and `<http://main.tf|main.tf>`\\'s `required_version`?\",1642693507.370300,1642693891.371200,U01DHB2HS3X\\nd69a27ea-440a-4319-b887-20146bf38c7f,U02T9550LTU,,,\"Hi <@U01AXE0P5M3>, may I check if this means that if we chose to use the GCP VM, we need to setup all the pre-requisites (i.e. Docker, PGAdmin, Postgres, and Terraform) in the GCP VM?\",1642643540.268300,1642694071.371800,U02BRPZKV6J\\n44615a1d-55c3-48eb-8117-b20654d34766,U02TNEJLC84,,,\"Okay. Now I\\'m really confused. It worked after I used the name of my .json file we generated for our user, but the video also makes it look like we could enter anything we wanted for the project ID. Am I mistaken? I was prompted twice for it. Also, after creation, the output states that my bucket was created in Europe-West6. Should I have updated the <http://variables.tf|variables.tf>? The video made it seem like it was optional, but not required.\",1642690115.362900,1642694072.372000,U02TNEJLC84\\n096eac70-e40f-4565-8197-48e195cbea12,U02T9550LTU,,,\"Yes, and the video shows how to do it\",1642643540.268300,1642694184.372200,U01AXE0P5M3\\n05033a41-1924-492b-8e68-9f06f548818d,U02CD7E30T0,,,\"`Terraform v1.1.4\\non linux_amd64\",1642693507.370300,1642694235.372400,U02CD7E30T0\\ne75b5cb5-8dec-4b74-9f64-0c6dfdbebc69,U02CD7E30T0,,,I think is 1.1.4,1642693507.370300,1642694250.372600,U02CD7E30T0\\n98b88e1a-7ba2-4984-a3d5-65cb590a62d0,U02CD7E30T0,,,Let me check,1642693507.370300,1642694262.372800,U02CD7E30T0\\n49806c54-b848-400f-80c3-1e286b03c2e4,U02HYRDSYQY,,,<#C01B0JGRBMY|engineering> is probably best,1642692592.368400,1642694366.373100,U01AXE0P5M3\\nbc42dc8d-2de0-4f86-805d-3512ba0e7e46,U02TNEJLC84,,,Think I need to go back through the material and maybe look at the terraform documentation as well.,1642690115.362900,1642694517.373600,U02TNEJLC84\\n6e2825e9-401b-4fdb-9a96-e5860c2034c7,U02CD7E30T0,,,\"Also, which OS are you using?\",1642693507.370300,1642694685.374000,U01DHB2HS3X\\n45c06578-c696-45ae-a1cc-07a3c7ad8103,U02CD7E30T0,,,I am using Ubuntu on WSL (Laptop on Windows),1642693507.370300,1642694791.374200,U02CD7E30T0\\nec4aea88-4aab-4d66-a4c9-be18e771ecf0,U02TNEJLC84,,,\"No, it\\'s not anything. You should enter the project name from gcp\",1642690115.362900,1642694819.374500,U01AXE0P5M3\\n76000cf2-d396-4921-853c-3d76609085f3,U02CD7E30T0,,,Already changed and had the same issue :disappointed:,1642693507.370300,1642694830.374700,U02CD7E30T0\\n6aafcb3c-c2b1-459b-9ad2-ab369ea62016,U02CD7E30T0,,,You can also try using tfenv if you need multiple terraform versions,1642693507.370300,1642694891.375200,U01AXE0P5M3\\n6b137ac3-043a-48ed-b735-9bf67d83c2f0,U02S565BCKX,,,\"Hi <@U02S565BCKX>, could you check again if you have all the right IAM roles, have enabled _*both*_ the IAM APIs (as per the guide), and try again? Also, did the `gcloud auth ...` work for you?\\n\\n<https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/2_gcp_overview.md#setup-for-access|https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/2_gcp_overview.md#setup-for-access>\",1642685324.348600,1642695155.376100,U01DHB2HS3X\\n5fa3064f-d0b2-483f-a429-80a17cf2dd03,,8.0,,sqlalchemy install issue if anyone wants to fight with python envs with me :sweat_smile: :thread:,1642695168.376500,1642695168.376500,U02TF4QP7NH\\n5e228f35-49a1-42a2-969d-59ffd9743a77,U02TF4QP7NH,,,this is what i\\'m seeing in my jupyter notebook,1642695168.376500,1642695211.376600,U02TF4QP7NH\\n5ef8cfe5-1045-4be7-8cfe-a78e5e7511f2,U02CD7E30T0,,,\"Perhaps the GOOGLE_APPLICATION_CREDENTIALS aren\\'t correct.\\nGoing to build a new key\",1642693507.370300,1642695214.377000,U02CD7E30T0\\n6b358f13-8a39-4975-9a00-a7625fb5369e,U02TF4QP7NH,,,\"i\\'ve tried:\\n• pip uninstalling / installing the module a bunch of times\\n• trying in a script \\n• trying in an actual jupyter notebook \",1642695168.376500,1642695230.377200,U02TF4QP7NH\\ne7c8750c-2f34-4380-9853-b1953cf65b54,U02TF4QP7NH,,,the weird thing is it words in my python repl,1642695168.376500,1642695284.377700,U02TF4QP7NH\\nc4d2379a-d2ce-43a4-9900-102c90073ce3,U02TF4QP7NH,,,Try to change venv in the notebook config,1642695168.376500,1642695343.377900,U02Q2933Q2K\\nf78482a0-4b50-41eb-b7ce-0123440f425d,U02TF4QP7NH,,,I\\'d suggest trying conda/anaconda - or at least in a virtual env,1642695168.376500,1642695430.378100,U01AXE0P5M3\\n4f92656e-de0f-437c-aab5-43110855b186,U02TF4QP7NH,,,<@U02Q2933Q2K> dumb question but how do I do that? I know how to change the kernel but that\\'s not the same thing,1642695168.376500,1642695464.378300,U02TF4QP7NH\\nd1784983-df9a-4b7d-a5f2-bbc70acf8e66,,2.0,,\"Hello all,\\nI\\'m able to see the videos on YT in the playlist but not able to map dezoomcamp 01 01 to the videos. Am I missing something? Perhaps something where dezoomcamp 1,2,3,4 are mentioned?\",1642695469.378600,1642695469.378600,U02UP7L3UT0\\n5f1b1203-7afe-4a26-b2a0-c62c48bb8f5d,U02TF4QP7NH,,,\"I meant kernel, sorry\",1642695168.376500,1642695501.378900,U02Q2933Q2K\\nc232689a-46fd-4ccf-88a7-fd8050101a0b,U02TF4QP7NH,,,Yeah not having much luck with changing the kernel but let me try a venv,1642695168.376500,1642695560.379200,U02TF4QP7NH\\nd80190ef-ca12-4cbc-a601-d47ecaa0fa51,U02CD7E30T0,,,1.1.4 is quite new? I can see only 1.1.3 on the website,1642693507.370300,1642695597.379500,U01AXE0P5M3\\nda768912-b73c-4a3d-b8c7-4e0adbddc438,U02CD7E30T0,,,maybe try 1.1.3,1642693507.370300,1642695604.379700,U01AXE0P5M3\\n0436581d-4bfd-48d2-a3a1-09c7439d5cb6,U02TNEJLC84,,,\"I agree. Looks like you\\'re entering \"\"yes\"\" when prompted for \"\"project-id\"\"\",1642690115.362900,1642695680.380000,U01DHB2HS3X\\n57b5911d-f02a-4e66-8b6c-d0c62ac1d52b,U02CD7E30T0,,,Going to remove the current terraform,1642693507.370300,1642696011.380400,U02CD7E30T0\\n08acc68e-8068-4252-b24c-9afeef3d12a7,U02S565BCKX,,,Hi <@U01DHB2HS3X> . Thanks. I didn\\'t have Identity and Access Management (IAM) API on. Now it\\'s working. Thank you again.,1642685324.348600,1642696021.380700,U02S565BCKX\\n4528705d-7aff-4e35-ac31-3b7c7e62e9cc,U02CD7E30T0,,,and install the 1.0.2 (the one on the repo),1642693507.370300,1642696033.380900,U02CD7E30T0\\nc92074b9-15ef-421d-ac1d-277cc862f8b7,U02TNEJLC84,,,\"Also, we have posted an additional audio yesterday for the workshop (video on that would be available soon). The link is in the Week 1 directory\\'s index. The same is also available in the text-based guidelines here: <https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup/1_terraform_gcp/terraform|https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup/1_terraform_gcp/terraform>\",1642690115.362900,1642696126.381100,U01DHB2HS3X\\n2bf03318-edf0-48fe-9cd8-b11925bd120e,U02TNEJLC84,,,Oops. That was one run I did. Going to review the previous video. Wondering how <@U01AXE0P5M3> Got his name so concise while mine has a random string at the end.,1642690115.362900,1642696176.381600,U02TNEJLC84\\n1090856f-ac74-485a-a790-51fca72e5f23,U02TNEJLC84,,,<@U01DHB2HS3X> Yes. Reviewed that this morning and made sure I ran the destroy command after I got it to work. Thank you!,1642690115.362900,1642696275.381900,U02TNEJLC84\\n7bd28193-4603-4388-876d-dd085dd694e5,U02UP7L3UT0,,,The videos were recently renamed. You can watch them in the order of the playlist or follow the order as listed in week 1 of the repo: <https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup>,1642695469.378600,1642696301.382100,U02BVP1QTQF\\nd5c53193-03cd-42d5-9d3e-6ea14c528531,U02CD7E30T0,,,Still no sucess... I think there is some problem with the google_application_credentials,1642693507.370300,1642696377.382800,U02CD7E30T0\\ndab037d2-d188-4e80-8e0f-8e6db78362f9,,,,\"In 30 minutes. If anyone want to know about  how to get started with Google Cloud.\\n\\n<https://cloudonair.withgoogle.com/events/getting-started-cloud-onboard?mkt_tok=ODA4LUdKVy0zMTQAAAGB7nRe5JHNOmwz0bYUI6zsTapddqt856rWdYajiRNvl-PS45VR0vAf7e0223nSCvxl25xnp-g1EVnsFlMkzR5o3PIoQN21Qhp6G1dSnMjamrMXBtra4g|https://cloudonair.withgoogle.com/events/getting-started-cloud-onboard?mkt_tok=ODA4LUdKVy0zMTQAAAGB7nRe5JHNOmwz0bYUI6zsTapddqt856rWdYajiRNvl-PS45VR0vAf7e0223nSCvxl25xnp-g1EVnsFlMkzR5o3PIoQN21Qhp6G1dSnMjamrMXBtra4g>\",,1642696472.384400,U0290EYCA7Q\\n2d0b5caf-476b-43d3-a869-11f2e5a3075c,U02S565BCKX,,,To be honest I don\\'t understand the what exactly is going on with the port forwarding to replicate it. Is this part necessary for the rest of the work we are doing?,1642683438.345000,1642696482.384500,U02S565BCKX\\n03f6ebc3-112f-4daa-b58a-3e47edd70879,U02TNEJLC84,,,\"Cool! Also (just for others reading this thread), the project-id cannot be anything. You have to keep it as per your GCP project ID (from your pre-req setup). This is for TF to know which project it is supposed to create the infra in.\",1642690115.362900,1642696494.384700,U01DHB2HS3X\\nef2a46a7-6640-47cb-bc85-f66db7e2bf2e,,,,,,1642696541.384900,U0290EYCA7Q\\n99c0595b-a459-47e7-b728-0d382d3e96ac,U02S565BCKX,,,if you want to connect to services running remotely - it\\'ll be quite helpful,1642683438.345000,1642696603.385300,U01AXE0P5M3\\n75CADFE9-0C28-4697-9164-DF2DFCDBECA2,,3.0,,\"Let\\'s say, I have two containers in dockers. I want to create a csv file from one container to another container. How can I do that in docker?\",1642696868.386900,1642696868.386900,U02U6DR551B\\ncfc9a927-33f6-48b1-bac8-75aa2eb6b8f7,U02U6DR551B,,,\"with volume mapping\\nor with s3 / google storate / etc\",1642696868.386900,1642696955.387300,U01AXE0P5M3\\nf376dba1-9e2b-4eeb-96a0-6b61ec4e051f,,,thread_broadcast,\"Here it says\\n```// credentials = file(var.credentials)  # Use this if you do not want to set env-var GOOGLE_APPLICATION_CREDENTIALS```\\nMeaning I can hard coded the path?\\nI don\\'t understand why it keeps saying \"\"operation not permitted\"\".\",1642693507.370300,1642697455.387900,U02CD7E30T0\\n9593cffd-9776-4388-9d5f-2d13a070279e,U02TNEJLC84,,,\"In the 8th video the project is \"\"dtc-de\"\", but in the 9th video if you look the project is different and named \"\"ny-rides-alexey\"\" That is why \"\"ny-rides-alexey\"\" is used.\",1642690115.362900,1642697503.388200,U02TNEJLC84\\n46ee6a03-6079-4208-89de-0aaccef51b22,U02CD7E30T0,,,\"<@U02CD7E30T0>  Looks like issue is more specific to your system/OS, than with TF: <https://askubuntu.com/questions/1115564/wsl-ubuntu-distro-how-to-solve-operation-not-permitted-on-cloning-repository|https://askubuntu.com/questions/1115564/wsl-ubuntu-distro-how-to-solve-operation-not-permitted-on-cloning-repository>\\n\\nIt could be that the user accessing WSL is not having `chmod` permissions in the location where it\\'s running TF.\",1642693507.370300,1642698008.388700,U01DHB2HS3X\\n92f059dc-de6a-44d0-8728-3991edbd440e,U02CD7E30T0,,,Right right,1642693507.370300,1642698049.389000,U02CD7E30T0\\n6f453df9-b35c-4464-8904-cff42b0a33fd,U02CD7E30T0,,,\"I just noticed that copying the file to the \"\"WSL path\"\" I get a different error:\\n`permission denied`\",1642693507.370300,1642698164.389400,U02CD7E30T0\\nca5c1934-7a8d-4a92-964f-9c88d90d35ea,U02CD7E30T0,,,\"It could be that the user accessing WSL is not having `chmod` permissions in the location where it\\'s running TF.\\n\\nOne hack could be to try `sudo` , to see if it works... but better to mount the right drive, as given in the link\",1642693507.370300,1642698276.389800,U01DHB2HS3X\\nf3a3960e-85e3-4a48-b402-f358a0795d17,U02CD7E30T0,,,\"I am so DUMB :astonished:\\nI have to change everything in the variables :disappointed:\",1642693507.370300,1642698375.392100,U02CD7E30T0\\n5969D0B3-0E74-4EED-B023-41AA1F64CF90,U02UP7L3UT0,,,<@U02BVP1QTQF> you may also like to share link to your notes in repo as the learning for week 1 is nicely sequenced there .,1642695469.378600,1642698411.392400,U02AGF1S0TY\\nbd7b5f5c-218f-47b3-beb3-8074966ea7b5,U02CD7E30T0,,,\"You\\'re not Dumb. It\\'s just that most technologies are compatible with Linux and MacOS/OSX style, and Windows unfortunately is designed specifically differently. Sorry you\\'re going through this. I hope you find a way.\",1642693507.370300,1642698702.392800,U01DHB2HS3X\\n9c4f6e96-d183-4b94-86d5-285963fea523,U02CD7E30T0,,,But in this case is different. I didn\\'t notice the `<http://variables.tf|variables.tf>` file that needs to be change,1642693507.370300,1642698838.394100,U02CD7E30T0\\n549d23f7-5f1c-4d82-a9a8-cf4f46de1ca2,U02TNEJLC84,,,\"Oh sorry, we just have different projects =)\",1642690115.362900,1642698906.395000,U01AXE0P5M3\\nBD5E60A6-B232-4465-84CE-35F075B3CB28,U02U6DR551B,,,Can I do that in Python?,1642696868.386900,1642699127.396600,U02U6DR551B\\nb3eefdf4-0931-4c3a-baa4-ebf6057e1630,,1.0,,\"Hi everybody, any idea how to deal with this?\",1642699518.399300,1642699518.399300,U02UAADSJ84\\n7f84827f-71e5-45c5-845f-2a384f06c3fa,U02UAADSJ84,,,\"There’s a similar issue, specific to WSL. Maybe this link in the thread would help <https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642698008388700?thread_ts=1642693507.370300&amp;cid=C01FABYF2RG>\",1642699518.399300,1642699944.400000,U01DHB2HS3X\\n3704d0a1-1389-4210-a360-17479126e437,U02U6DR551B,,,Yes,1642696868.386900,1642700217.400300,U01AXE0P5M3\\n40fefa3e-53e6-421f-959a-ffbc40279c11,U02HYRDSYQY,,,Okay :+1::skin-tone-2:,1642692592.368400,1642700260.400500,U02HYRDSYQY\\n23aff11f-8514-4156-99a5-e055eac96898,U02TNEJLC84,,,No worries. Kind of better that way actually. Helps the course information stick in your head better when you run into issues like this. Tested out a few project names to see how you got yours not to have a random string appended to the end. Some of the ones I tried worked (must have been globally unique with just the name) some didn\\'t (must be a project out there with that name already) so got the string at the end. You guys ROCK for creating this content btw. :+1::+1::+1:,1642690115.362900,1642700471.400800,U02TNEJLC84\\n2a4b1378-7863-4e1a-b5e5-1ce2629871b4,,10.0,,\"Hello, I can\\'t execute the following code directly, Where as if use ip address in place of localhost it works. Why is this happening, Please help!\\n```winpty docker run -it --network=pg-network \\\\\\n  taxi_ingest:v001\\\\\\n  --user=root \\\\\\n  --password=root \\\\\\n  --host=localhost \\\\\\n  --port=5432 \\\\\\n  --db=ny_taxi \\\\\\n  --table_name=yellow_taxi_data \\\\\\n  --url=${URL}```\\n\",1642700774.403500,1642700774.403500,U02V4412XFA\\n3faf9df0-25c4-49e4-810b-0bc95f84a0e4,,7.0,,Hi everyone any idea on how to solve this issue?,1642700805.403700,1642700805.403700,U02T2TX1GS2\\ncd3ef5c0-4fb8-4c7c-9c49-da733ef1c6af,U02V4412XFA,,,this is how it works in Docker!!!,1642700774.403500,1642700892.404100,U02U2Q5P61Z\\n55dc6ff0-542e-4aae-8b1a-05e345b31b05,U02V4412XFA,,,hosts are always specified by their ip_addresses within the docker network,1642700774.403500,1642700921.404300,U02U2Q5P61Z\\nca87151f-c7c6-45e2-84ee-f3b2f493d0ee,U02V4412XFA,,,unless you create create a custom docker network and call the other container localhost this won\\'t work,1642700774.403500,1642700972.404500,U02U2Q5P61Z\\n3557a647-83cc-46a2-972e-a800fc3027f8,U02T2TX1GS2,,,is pgcli installed yet?,1642700805.403700,1642700985.404700,U02UBV4EC8J\\n93a372b5-7319-4dd6-9223-bd9d9dd67a64,U02T2TX1GS2,,,\"if so, then this is a path issue\",1642700805.403700,1642701000.404900,U02UBV4EC8J\\n932c53b5-fb6e-4394-97d7-cea368bd85f7,U02V4412XFA,,,\"Hey thankyou, I have created custom network as suggested in the video, still I\\'m experiencing this.\",1642700774.403500,1642701077.405100,U02V4412XFA\\n6dbeffde-1111-496c-a3f0-7022eb2f75b4,U02V4412XFA,,,yes I know,1642700774.403500,1642701089.405300,U02U2Q5P61Z\\nc60df7f0-65c1-4d77-88b5-882f02633fd6,U02V4412XFA,,,I see your custom network,1642700774.403500,1642701094.405500,U02U2Q5P61Z\\nc5dbbe30-65d0-4ecb-b89a-9905659a2703,U02V4412XFA,,,but what is the localhost service reffered to?,1642700774.403500,1642701114.405700,U02U2Q5P61Z\\n50239177-b3ad-4903-a43f-70b32b38eb40,U02V4412XFA,,,--name localhost,1642700774.403500,1642701197.405900,U02U2Q5P61Z\\n5b237676-ed05-4327-934f-ff45b81a7bb8,U02V4412XFA,,,localhost should be the name of your container running pgadmin and then expose the ports on the container running pgadmin and it should work,1642700774.403500,1642701320.407400,U02U2Q5P61Z\\n39dc955b-58da-4e13-a29b-4dc5cfe1ad5d,U02T2TX1GS2,,,\"pgcli is installed\\nShould I add this to the path - \"\"C:\\\\Users\\\\SAMPATH\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages\\\\pgcli\"\"  ? Is this right?\",1642700805.403700,1642701474.410900,U02T2TX1GS2\\naf225f55-af29-40bf-a2f8-df19ba771378,,9.0,,\"<@U01AXE0P5M3>, I can\\'t access postgresql  db from my git bash installed locally on laptop. .  And also can\\'t access jupyter notebook  . I followed all the instructions you posted in video.  When I enter pgcli -h localhost -U root -d ny_taxi , no output\",1642701489.411300,1642701489.411300,U01HNBLMT2N\\nab708e6a-9835-4409-8b22-97ac355c9d11,U02T2TX1GS2,,,Any specific file to include in the path,1642700805.403700,1642701657.412200,U02T2TX1GS2\\nc918fb5a-e396-4efa-8ce5-06702ce2c137,U02V4412XFA,,,\"Okay, I\\'ll try\",1642700774.403500,1642701674.412800,U02V4412XFA\\n4935400b-1a31-4f33-9299-021bebc85529,,13.0,,\"My notes on the Docker and SQL section. I am playing catchup. Hopefully I can finish making all the notes, add necessary memes to them, revise them and have time left for the homework.\\n\\n:link:  <https://itnadigital.notion.site/Docker-and-SQL-2459cc3aab36475c8bab5a3a04493f10>\",1642701743.413300,1642701743.413300,U02QZN0LSBT\\n067cbc80-73f1-4e84-8415-7607f9dca258,U01HNBLMT2N,,,\"Do u use conda? If so, are you in the right environment?\",1642701489.411300,1642701751.413500,U0290EYCA7Q\\n5d5883a1-9879-486c-8f0c-03a666c34e52,U02QZN0LSBT,,,That\\'s a lot of work. :clap: :clap:,1642701743.413300,1642701792.413700,U0290EYCA7Q\\nb1315085-acc7-4274-b598-4b18e4b0d39a,U02QZN0LSBT,,,\"haha I know. Takes 3-4 times the video duration. Had take the notes in \"\"chunks\"\" (someone will get this joke) throughout the day.\",1642701743.413300,1642701882.414100,U02QZN0LSBT\\n56f16d5a-19e9-40c3-825d-ac351ef862f1,U01HNBLMT2N,,,\"<@U0290EYCA7Q>, I\\'m using git bash\",1642701489.411300,1642701929.414400,U01HNBLMT2N\\n88be22f1-e702-4964-9509-df03514967e8,U02QZN0LSBT,,,\"Hey, feel free to sent a PR to link them to the repo :tada:\",1642701743.413300,1642702003.414600,U01B6TH1LRL\\nfa4c2168-d802-4322-b45c-a90650200918,U01HNBLMT2N,,,Can you share more details?,1642701489.411300,1642702125.414900,U01AXE0P5M3\\n8337ca5f-a8d3-4951-b968-a890e057ee47,U02T2TX1GS2,,,For windows I\\'d suggest using anaconda,1642700805.403700,1642702173.415100,U01AXE0P5M3\\n21fdb524-832e-440d-9cdd-9868eb92403b,U02QZN0LSBT,,,I actually did and I messed it up the first time round. It is pending merge right now.,1642701743.413300,1642702233.415400,U02QZN0LSBT\\n5ad75fb9-961b-4fde-a68c-a4b6af9dcbd4,,6.0,,MINGW doesn\\'t do anything after I enter password for pgcli- Am I doing something wrong here?,1642702411.417300,1642702411.417300,U02UAADSJ84\\ndda637c5-19f4-4980-8ae7-2a87b88e90c4,U02QZN0LSBT,,,\"thank you for sharing. I\\'m keeping my notes for this project in notion as well, but more so \"\"learning out loud\"\" and adding posts as I complete tasks  <https://www.notion.so/learnoutloud/Data-Zoom-Camp-Learn-to-be-a-Data-Engineer-79edf759615c46539bafacb67021f6d1>\",1642701743.413300,1642702607.417600,U02UDSGLQER\\n31919a45-53c1-4522-a17e-bf1515167dc5,U01HNBLMT2N,,,Are you getting something like this?,1642701489.411300,1642702740.418000,U02UAADSJ84\\n8f5603b8-d61b-4867-bb34-8bc390666c34,U01HNBLMT2N,,,could you also add this parameter: -p 5432,1642701489.411300,1642702856.418400,U029DM0GQHJ\\nc12fc72c-431d-49fa-8192-9d8a3fee7c7e,U02QZN0LSBT,,,\"Thanks a lot, your notes are really helpful as the videos were a little too fast paced for me. i could read the notes a number of times to grasp the concept.\",1642701743.413300,1642702861.418600,U02U5GQK25C\\n362948e4-500a-47a0-b7b3-ad54626bc81d,U01HNBLMT2N,,,<@U01HNBLMT2N>,1642701489.411300,1642702873.418800,U029DM0GQHJ\\n05649c2f-31c7-470a-92d0-76090cf8ee83,U02QZN0LSBT,,,\"Hey Khoo, you can also try to lower the speed of the video from the settings (I have it in spanish, but shows more or lesss where from). There\\'s also the option to add automatically generated subtitles that could help but I haven\\'t used them yet\",1642701743.413300,1642703013.419000,U01B6TH1LRL\\n7b7d3a9e-28ec-494b-87fc-87586dc78dd4,U02UAADSJ84,,,\"Can you try pgcli -v, to check if it\\'s working?\",1642702411.417300,1642703166.419500,U0290EYCA7Q\\n8bd99f08-1ada-45a2-aa6b-86f82be69096,U02UAADSJ84,,,\"It is. This is the output: $ pgcli -v\\nVersion: 3.3.1\",1642702411.417300,1642703302.419800,U02UAADSJ84\\n92a8a043-a0dd-4be7-a122-752ddbc3afbb,U01HNBLMT2N,,,Same thing what Nandita Gurwara reported in this thread.,1642701489.411300,1642703543.420000,U01HNBLMT2N\\n67eddcac-1b50-40fb-8315-7a53e1c5923b,U02UAADSJ84,,,I tried to use anaconda and got a password authentication failed even though I specified the right password,1642702411.417300,1642703574.420200,U02UAADSJ84\\nabf6b99e-611b-49e6-8389-1cc766076989,U02UAADSJ84,,,,1642702411.417300,1642703579.420400,U02UAADSJ84\\ndf7571c5-983e-44c2-91d2-a7e1e692bd9f,U01HNBLMT2N,,,<@U02UAADSJ84> yes its same.,1642701489.411300,1642703590.420800,U01HNBLMT2N\\ncd6a8bde-4efd-49e1-922b-c7d32cf0b0e2,U02UAADSJ84,,,\"Maybe, ask for the version <@U01AXE0P5M3> is using, and try with that.\",1642702411.417300,1642703811.421300,U0290EYCA7Q\\n765720e0-91a6-4710-b589-b2d7612d6e11,U02QZN0LSBT,,,thx for the tip!,1642701743.413300,1642703949.421600,U02U5GQK25C\\na7a29d72-7bdc-4c01-a1d0-58f771abb3da,,6.0,,My Postgres sql schema isn\\'t changing form the one generated by pandas when running `<http://pd.io|pd.io>.sql.get_schema()` even though my connection to postgres seems to be working...,1642705806.423900,1642705806.423900,U02UX664K5E\\n8b5d0925-5847-425f-a4ad-7f87865d49aa,U02UX664K5E,,,\"Tested my connection and it seems to be fine:\\n```engine.connect()\\n&lt;sqlalchemy.engine.base.Connection at 0x27a142a7ee0&gt;```\",1642705806.423900,1642705836.424000,U02UX664K5E\\n3dac2085-3f9f-4ddb-800b-94bebc593158,U02UX664K5E,,,\"`print(<http://pd.io|pd.io>.sql.get_schema(df, name=\\'yellow_taxi_data\\', con=engine))`\",1642705806.423900,1642705850.424300,U02UX664K5E\\n88d4ca53-df09-4780-9084-98fee1a38735,U02UX664K5E,,,\"This is was gets returned which is basically the same if the connection wasn\\'t specified.\\n```CREATE TABLE \"\"yellow_taxi_data\"\" (\\n\"\"VendorID\"\" INTEGER,\\n  \"\"tpep_pickup_datetime\"\" TIMESTAMP,\\n  \"\"tpep_dropoff_datetime\"\" TIMESTAMP,\\n  \"\"passenger_count\"\" INTEGER,\\n  \"\"trip_distance\"\" REAL,\\n  \"\"RatecodeID\"\" INTEGER,\\n  \"\"store_and_fwd_flag\"\" TEXT,\\n  \"\"PULocationID\"\" INTEGER,\\n  \"\"DOLocationID\"\" INTEGER,\\n  \"\"payment_type\"\" INTEGER,\\n  \"\"fare_amount\"\" REAL,\\n  \"\"extra\"\" REAL,\\n  \"\"mta_tax\"\" REAL,\\n  \"\"tip_amount\"\" REAL,\\n  \"\"tolls_amount\"\" REAL,\\n  \"\"improvement_surcharge\"\" REAL,\\n  \"\"total_amount\"\" REAL,\\n  \"\"congestion_surcharge\"\" REAL\\n)```\",1642705806.423900,1642705859.424500,U02UX664K5E\\n561b29ef-0a94-4dbd-b0e0-82cf588dbd9c,U02UX664K5E,,,What could be wrong? :cry:,1642705806.423900,1642705948.424900,U02UX664K5E\\n18D8309B-E0F8-4328-A8A6-ABCE9024D5E6,U02UX664K5E,,,Did you install sqlalchemy while your notebook was open? If so restart your notebook and try again.,1642705806.423900,1642706711.426700,U02U34YJ8C8\\n39f9a0a7-fd56-4662-939d-6b615cffca4d,,4.0,,\"Hi. I want to ask about homework question 6. What does mean  \"\"pair with the largest\\naverage price for a ride\"\"? Do we need to calculate AVG value from \\'total_amount\\' and then we need to take largest value? But AVG total_amount is one value. I\\'m confused :face_with_raised_eyebrow:\",1642707711.431700,1642707711.431700,U02QL1EG0LV\\n72c9bdc6-d8b5-4017-9f2f-2c5b6c263a2c,U02UAADSJ84,,,I think this is a problem that many people had here. Check other threads,1642702411.417300,1642707788.431800,U01AXE0P5M3\\na519b810-e8da-402d-8abd-dbdadfcbf9f9,U02UX664K5E,,,\"<@U02U34YJ8C8> Thanks Aaron, you were right. Restarted and now it worked.\",1642705806.423900,1642708220.432000,U02UX664K5E\\naf611e17-6708-4fdf-8a2b-df6b4466514c,U02U34YJ8C8,,,\"<@U02U34YJ8C8> When I shutdown my system the docker connection is removed, I have to rerun the command. Is this similar to what you are facing??\",1642630680.235000,1642708267.432200,U02SPLJUR42\\n70da43d7-30a0-47e5-b0b3-f56c04a484da,U02QL1EG0LV,,,I did pair by location id. Pickup and Drop off location,1642707711.431700,1642708405.432400,U02CD7E30T0\\n151be947-2661-4869-a856-fb0b738847b5,,2.0,,\"Hello everyone, I have a small issue, when I shutdown my system and power it up again. I realise the tcp port of my docker postgres container that was specified earlier is cleared and I have to delete the container and rerun the commands again\\n\\n`pgcli -h localhost -p 5433 -u root -d ny_taxi`\\n`could not connect to server: Connection refused`\\n\\t`Is the server running on host \"\"localhost\"\" (127.0.0.1) and accepting`\\n\\t`TCP/IP connections on port 5433?`\",1642708606.435000,1642708606.435000,U02SPLJUR42\\n2cb8c158-dc16-4e61-9932-78427ab0532c,U02QL1EG0LV,,,I understood it. And what about price? :slightly_smiling_face:,1642707711.431700,1642708642.435100,U02QL1EG0LV\\n74b384b0-05a9-4071-93b1-abaddaa7da06,U02SPLJUR42,,,\"output of docker ps\\n\\n                   `PORTS     NAMES`\\n`60c6d607ade3   postgres:13   \"\"docker-entrypoint.s…\"\"   20 minutes ago   Exited (0) 17 minutes ago             postgres14`\",1642708606.435000,1642708912.435700,U02SPLJUR42\\nf6fe6475-b61e-4f1b-8d9e-9ebf7333457a,,2.0,,\"Hello everyone, please I got this error while trying to ingest the data into postgres db:\\n\\n`docker: Error response from daemon: Ports are not available: listen tcp 0.0.0.0:5432: bind: address already in use.`\\n`ERRO[0000] error waiting for container: context canceled`\\n\\nany help please. I am running it on a mac OS\",1642709151.437200,1642709151.437200,U02TZ1JCVEC\\nd7a90705-a8ac-49e4-b1e0-4f9ada16cae8,U02QZN0LSBT,,,Accepted the PR. Thanks a lot!,1642701743.413300,1642709188.437300,U01AXE0P5M3\\n885c4070-8de1-4d01-8fac-dc9509e7fad6,U02QZN0LSBT,,,Love the memes =),1642701743.413300,1642709210.437500,U01AXE0P5M3\\n753a9512-36e4-4d15-bb57-426dc220f1d2,U02UX664K5E,,,\"<@U02UX664K5E> I\\'m facing the same issue, did it work for you?\",1642633246.239400,1642709234.437700,U02UAADSJ84\\n1229dfcc-1777-433c-94f1-40b8c8092efd,U02TZ1JCVEC,,,\"Specify a different port when running docker:\\n\\n`-p 5431:5432`\",1642709151.437200,1642709357.438000,U01AXE0P5M3\\ncf4155b4-0050-4be5-b574-6b0bd12f6867,U02UX664K5E,,,\"<@U02UAADSJ84> Depending on what is the exact issue, potential solutions can be found here: <https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642693557370600?thread_ts=1642615746.189300&amp;cid=C01FABYF2RG>\\n\\nFrom my case, the solution was:\",1642633246.239400,1642709417.438200,U02UX664K5E\\n8be917a6-b12a-4aed-8e96-65a8f596ee70,U02TZ1JCVEC,,,Thank you. It works!,1642709151.437200,1642709732.438800,U02TZ1JCVEC\\n3879c382-dcf1-4395-8707-46776963724d,,1.0,,\"Hello Everyone ,Please my ny_taxi_postgres_data folder is empty after running the postgres image\",1642710534.439500,1642710534.439500,U02T0CYNNP2\\n16740c46-8f09-4e34-8d19-1dc17ef9fe8a,U02QL1EG0LV,,,\"You will calculate the avg total_amount per pair of pickup-dropoff, so it should be the largest among those\",1642707711.431700,1642710579.439700,U01B6TH1LRL\\na0378b50-5251-4916-8510-638dd2654fc8,,,,\"I have followed the instructions on the github repo, but the folder is still empty\",,1642710626.441300,U02T0CYNNP2\\n6dfdcea8-c4f0-492c-9299-d5506182cbe8,,,,And also how can i access the Homework,,1642710650.442100,U02T0CYNNP2\\nf9271dbc-f539-45ee-a883-942e4e57a950,U02UZBJ2Q6L,,,\"Hi <@U01B6TH1LRL>\\nThanks a lot for clarifying that and sorry for my late response! I am glad that there are no time restrictions , relieves a lot of pressure of always creeping deadlines. I am really enjoying the videos as well, great stuff! There is just one more thing if you could help clarify, if I finish all the projects after the tentative schedule is over, is there anyway to gain a certificate? As a complete newbie a certificate would be an easy way to showcase my skills. Any clarification regarding that would be welcome.\",1642680003.337000,1642710767.442200,U02UZBJ2Q6L\\n531c3908-ccc5-4fa8-8836-2db6c5b97d3c,U01QGQ8B9FT,,,Still no solution to this?,1642586687.122000,1642710985.442400,U02T0CYNNP2\\ne32b514f-8330-4295-a91b-b5fc5b52cc40,,3.0,,We added the form for submitting the homework - check it <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/homework.md|here>,1642711429.443500,1642711429.443500,U01AXE0P5M3\\n83fa499a-6f4b-4505-8770-9ffad248bc80,,6.0,,\"Also, a few more people reached out to me asking to split this channel into two - announcements and support\\n\\nwhat do you think about it?\\n\\n:+1: yes let\\'s do that\\n:-1: no, it\\'ll make things more confusing\",1642711485.444800,1642711485.444800,U01AXE0P5M3\\n36f67452-ca8c-4b33-812e-d2663848bdc0,U01AXE0P5M3,,,\"the \"\"announcements\"\" one will not be like <#C01BQC114P2|announcements>,it will be a usual channel. because we\\'re on a free slack, we can have  only one read-only channel\",1642711485.444800,1642711565.445300,U01AXE0P5M3\\nd168d057-a5e2-4784-a850-d9e9c8749d57,U02QL1EG0LV,,,\"Thanks. I think I got it. I should select columns, AVG and then group per columns. And then select a Max value\",1642707711.431700,1642711621.445700,U02QL1EG0LV\\n51d2cd69-da5d-4215-9838-124c4993d4ba,U02UZBJ2Q6L,,,\"Unfortunately, if you deliver after the course is finished you won\\'t get the certificate. To get the certificate the main requirement is to deliver by the end date and have it reviewed by 3 peers.\",1642680003.337000,1642711716.445900,U01B6TH1LRL\\nf164c58a-833e-429b-af40-706ba85f4004,,11.0,,\"I am having trouble getting the Google SDK to install, it doesn\\'t recognize my Python install even though I have it set in PATH and by setting the CLOUDSDK_PYTHON environmental variable.\\xa0Any ideas on how I can resolve?\",1642712037.447300,1642712037.447300,U02USF4NYG4\\nea54c14f-f7a4-435d-b044-3542dd865c6c,,,,\"I saw a few stack overflow questions that mentioned directly putting that information in the .bat script, but wasn\\'t sure where you put that in\",,1642712077.448100,U02USF4NYG4\\n10a68b51-384d-487d-8734-4958d82e4c08,U02USF4NYG4,,,Can you show how you set it?,1642712037.447300,1642712116.448200,U01AXE0P5M3\\nb78143a9-f792-4ed8-92a1-b3a22c5cbd79,U02USF4NYG4,,,\"Also, which environment do you use? Powershell or mingw/cygwin?\",1642712037.447300,1642712156.448400,U01AXE0P5M3\\na5aef21f-03d9-427e-b0ba-bb1fe7699089,U02USF4NYG4,,,\"Not sure if you saw it, but maybe it\\'ll help\\n\\n<https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/windows.md|https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/windows.md>\",1642712037.447300,1642712212.448700,U01AXE0P5M3\\nc776f597-efa3-4876-be20-b625ca523bda,U02USF4NYG4,,,I\\'m on Windows,1642712037.447300,1642712261.449000,U02USF4NYG4\\nf50535f5-e5c1-4737-b4e3-406a8d27bcdc,U02USF4NYG4,,,I\\'ll review that link and see if I missed anything! Thanks,1642712037.447300,1642712317.449300,U02USF4NYG4\\n3c36b94a-26e4-4df6-8aee-13656adefeb9,U02USF4NYG4,,,I have both my user and system variables set to this:,1642712037.447300,1642712357.449500,U02USF4NYG4\\na3e7d987-1cec-4750-be39-faa533e2f351,U02USF4NYG4,,,,1642712037.447300,1642712359.449700,U02USF4NYG4\\n7c168ace-f413-490b-a2fe-85d23e2fab3d,U01AXE0P5M3,,,\"I vote yes. Maybe it won’t be necessary in 2-3 weeks but considering the amount of messages we’re getting compared to what we had in <#C0288NJ5XSA|course-ml-zoomcamp>, it will make it much easier to keep track of any course announcements and new videos in a separate channel and have all Q&amp;A in a separate, somewhat noisier channel.\",1642711485.444800,1642713197.450900,U02BVP1QTQF\\n6eb1a0d4-b86d-4377-b1a2-1df42b9d58fd,,18.0,,\"I’m sharing my notes here, because all the cool kids seem to be doing it :sunglasses:\\n\\n<https://github.com/ziritrion/dataeng-zoomcamp/tree/main/notes>\\n\\nMy notes aren’t very “notey” nor aren’t as fancy as Abd’s; they’re more of a transcript with a few omissions for brevity’s sake. The way to use them is: first watch the video, then go to the notes to look up any specific topic rather than having to go back and forth within the video to find what you were looking for. I do omit a few things such as how to install software unless it’s something very specific, and since I’m using a Mac I don’t really write anything Windows-specific, but they might come in handy.\\n\\nI can’t promise that I will keep doing these notes during the course since I (hopefully) may get busier and have less time to spend on them (it takes a lot of time to write and format them), but I will try anyway.\",1642713863.457200,1642713863.457200,U02BVP1QTQF\\n85c1fc92-ae40-4a6e-b8e4-d91bc51ad29b,U02BVP1QTQF,,,\"Also, my PR got accepted and you can find a link to them in the course repo at the bottom of the page for the first week, so you can always find them there.\",1642713863.457200,1642713926.457500,U02BVP1QTQF\\n0746aefd-0187-443d-8f73-26200a8dd5f9,U01AXE0P5M3,,,Do we need to use the same email we used to sign up with or just make sure we use the same email address for every submission?,1642711429.443500,1642714502.458200,U02TNEJLC84\\n467abf1e-9d6e-4ff3-b380-5cc5f4f6da75,U01AXE0P5M3,,,The same email for every submission,1642711429.443500,1642714628.458600,U01AXE0P5M3\\n42f06f6d-6f7a-4393-8feb-ee4c8b833a34,U01AXE0P5M3,,,It\\'s your unique ID which you\\'ll need throughout the course,1642711429.443500,1642714655.458800,U01AXE0P5M3\\n56b6f5a3-779c-44f5-850d-1052b77abcc8,U02BVP1QTQF,,,\"You have great notes, thanks for doing it!\",1642713863.457200,1642714821.459000,U01AXE0P5M3\\n95e0e28a-e742-4a98-a18b-bf715f18846f,U02USF4NYG4,,,\"So I used MINGW64 to run the install.sh, but I am getting some SSL errors.  I am using a work laptop\",1642712037.447300,1642715429.459700,U02USF4NYG4\\n2091de35-f99f-447b-a70d-d8a62dff8845,U01AXE0P5M3,,,I second <@U02BVP1QTQF> words on this. I\\'m seeing too many support requests.,1642711485.444800,1642716020.460400,U02GVGA5F9Q\\n36277ae0-c0e9-414e-9274-f94e11073884,U02QDKU4GTY,,,\"I am having the same issue, Agustin.  VPN didn\\'t help.  Did you find a resolution?\",1642643423.267000,1642716028.460700,U02USF4NYG4\\n99909e01-d104-4224-947b-995733f00ee3,U02QDKU4GTY,,,Hi <@U02USF4NYG4> I haven\\'t found a solution yet :(,1642643423.267000,1642717579.461700,U02QDKU4GTY\\nFC25705A-2890-4D09-A624-A4F0CD59D1D1,U02BVP1QTQF,,,These are amazing notes! Thanks for sharing. So much better than mine lol,1642713863.457200,1642717879.462800,U02U34YJ8C8\\nb15e00f9-824e-4e8c-a26b-b97fd214ed09,,2.0,,\"Hi everyone! In video # 2, where is <@U01AXE0P5M3> pasting this code? It looks like a hidden file right?\",1642717940.463800,1642717940.463800,U02910MJNBY\\n4744bdee-7b5d-40db-adf1-793c80b9056b,U02910MJNBY,,,I think it is an unsaved file,1642717940.463800,1642718222.464500,U02UE7NTLUU\\n5c87b104-0b9a-40c6-bbfa-d5e1cd193dd0,U01AXE0P5M3,,,Any news about this? I think this might be my problem :( I will try with to understand by listening.,1642629584.230700,1642718275.464800,U02CD7E30T0\\n4d681bd7-f34e-44ca-9092-7222660f0621,U02910MJNBY,,,later it turned to this readme - <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/2_docker_sql/README.md>,1642717940.463800,1642718290.465000,U01AXE0P5M3\\n4bc37122-6487-485b-a01e-a6c9e58e287b,U01AXE0P5M3,,,\"once I finish the video about SQL, I\\'ll take care of this one\",1642629584.230700,1642718371.465300,U01AXE0P5M3\\n598b95eb-bfda-4542-a767-c00b78c77057,U01AXE0P5M3,,,\"Thank you :) \\nI just noticed this video to be that configures Terraform.\",1642629584.230700,1642718668.465500,U02CD7E30T0\\n4bcb3a18-48d2-4901-ac23-94f6b56e3684,,1.0,,\"<@U01AXE0P5M3>, I\\'m getting this error. It was working yesterday. The error is on vm instance in Google cloud.\",1642719177.467400,1642719177.467400,U01HNBLMT2N\\na0ba71d1-dfdd-4d8e-9193-5c664b5b601c,U01HNBLMT2N,,,<@U029DM0GQHJ> no luck with Port number as well,1642701489.411300,1642719284.467700,U01HNBLMT2N\\nb983ee74-9446-4cc7-9272-215da1a1908e,U01HNBLMT2N,,,I got the error fixed. I had shutdown the VM yesterday and DB was turned off,1642719177.467400,1642720702.468600,U01HNBLMT2N\\n6afb4f89-5bf8-4a1c-910c-6bc89bb7316c,,3.0,,\"\\'user@Lenovo MINGW64 ~\\n$ docker run -it ubuntu bash\\nthe input device is not a TTY.  If you are using mintty, try prefixing the command with \\'winpty\\'\\'\",1642721355.469800,1642721355.469800,U02UY1QTGHW\\n68b92595-1005-4820-b268-2b4d3a9f9fb8,,,,How do I fix this ?,,1642721367.470100,U02UY1QTGHW\\n3D27B2BF-76CB-4812-B666-7D44562390A4,,1.0,,\"<@U01AXE0P5M3>  you look super proficient with VSCode key shortcuts, maybe it’s edited videos lol. Any good resources for learning?\",1642721601.473400,1642721601.473400,U02HLE69P19\\nbfa81188-5225-40ea-84e7-aca2a74e604f,,10.0,,\"Guys, I tried to run my data_ingestion.py script and got the following error:\\n`\\'wget\\' is not recognized as an internal or external command`\\n\\nHow to get wget for windows10?\",1642721639.474000,1642721639.474000,U02UX664K5E\\ndc0e7e97-d2bf-472e-b9ae-c6e196175cbd,,1.0,,<@U02UY1QTGHW> add winpty before docker run -it,1642721648.474200,1642721648.474200,U02T8HEJ1AS\\neeb3eece-c552-4933-a6c7-26393a2c3e4c,U02BVP1QTQF,,,\"The best notes I have seen so far, very well structured and organized!\",1642713863.457200,1642721677.474300,U02UX664K5E\\nb2503d8d-8504-4738-954a-216e878d7999,U02T8HEJ1AS,,,Amazing...it worked man. Thanks heaps!,1642721648.474200,1642721775.474600,U02UY1QTGHW\\n8c4a1d52-4607-4c21-8874-f788f086a21c,,2.0,,when is the next class?,1642722251.475100,1642722251.475100,U02UGA597HS\\n2e6f4d4a-6220-44b8-8279-b1585c169649,U02UX664K5E,,,\"do you need at system level or just for python, if just for python, you can run `pip install wget`\",1642721639.474000,1642722317.475200,U02UBV4EC8J\\nb3b70335-6764-4f85-8e36-c0377b728e17,U02UGA597HS,,,Monday 17:00 CEU,1642722251.475100,1642722347.475400,U02UBV4EC8J\\n289d6b7b-b51a-43fc-a707-57a8aeb99e48,U02UBQJBYHZ,,,Usually never. I had to work on non-software project all day so I will come back and see if anything changed while I was away from my computer. :slightly_smiling_face:,1642654129.278400,1642722721.475600,U02UBQJBYHZ\\neba95938-397c-46e0-8d19-1451a119d49f,,4.0,,\"$ winpty docker run -it \\\\\\n&gt;   -e POSTGRES_USER=\"\"root\"\" \\\\\\n&gt;   -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n&gt;   -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n&gt;   -p /192.168.1.222/Users/User1/2_docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n&gt;   -p 5432:5432 \\\\\\n&gt;  postgres:13\\ndocker: Invalid ip address: C.\\nSee \\'docker run --help\\'.\",1642723063.476100,1642723063.476100,U02VBG59VQ9\\n17b28c80-6aa7-4465-9e18-ecc6db7fcc69,,,,any help would be appreciated,,1642723093.476600,U02VBG59VQ9\\n5939D5E8-9663-45E0-A356-F977143236BB,U02VBG59VQ9,,,I think it should be -v for volume ,1642723063.476100,1642723513.477100,U02AGF1S0TY\\n1bec7c70-0aaf-4056-9fa2-5b4279a4883f,U02VBG59VQ9,,,now i got this error <@U02AGF1S0TY>,1642723063.476100,1642723765.477300,U02VBG59VQ9\\nc880cc15-d559-41cd-aed6-5d9075c04551,U02VBG59VQ9,,,\"$ winpty docker run -it \\\\\\n&gt;   -e POSTGRES_USER=\"\"root\"\" \\\\\\n&gt;   -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n&gt;   -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n&gt;   -v /192.168.1.222/Users/User1/2_docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n&gt;   -p 5432:5432 \\\\\\n&gt;  postgres:13\\ndocker: Error response from daemon: mkdir C:\\\\Program Files\\\\Git\\\\192.168.1.222: Access is denied.\\nSee \\'docker run --help\\'.\",1642723063.476100,1642723767.477500,U02VBG59VQ9\\n13e7d073-15fb-4e7a-9ae1-d186669f2697,U02UX664K5E,,,\"@kenny I\\'m trying to execute the python script in bash command line,, so I guess I need this for system level as even `pip install wget` is returning the same error\\n```URL = \"\"<https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>\"\"\\n\\npython ingest_data.py \\\\\\n    --user=root \\\\\\n    --password=root \\\\\\n    --host=localhost \\\\\\n    --port=5431 \\\\\\n    --db=ny_taxi\\n    --table_name=yellow_taxi_trips\\n    --url=${URL}```\",1642721639.474000,1642724538.477700,U02UX664K5E\\nA22F8680-E212-48F9-888F-F42E269A2417,U02VBG59VQ9,,,Not sure ..it is related to volume .did u create Empty folder in working dir ..try removing 192.168.1.122 from path ..also Could try putting entire volume mapping in quotes …,1642723063.476100,1642725683.482500,U02AGF1S0TY\\n34832f8f-9a50-4b22-991a-86380b6d101a,,12.0,,\"I really don\\'t want to ask this, but I\\'ve been wrangling with it for an hour or so. Is question 3 on the homework a typo? How many taxi trips were there on January 15? The answer I\\'m getting is WAY off from any of the choices presented.\",1642725704.482700,1642725704.482700,U02TNEJLC84\\nbffbd992-177d-4125-a6a6-64376e2db167,U02UX664K5E,,,\"I was able to install wget, if anyone needs it, follow the steps from this <https://stackoverflow.com/a/64900250/2308024|post>. \",1642721639.474000,1642727172.483200,U02UX664K5E\\nb735d3de-3414-45ba-84f4-6075833e6e54,U02U5G0EKEH,,,\"same issue, how did you solve this?\",1642658846.295100,1642727391.483500,U02U85FB8TU\\n09b1c885-565e-43be-9d2b-104ac5bc6a2a,U02TNEJLC84,,,I’m also getting a very different answer but my query appears to be correct…,1642725704.482700,1642728077.483700,U02UGA597HS\\naa1d0ec9-f49c-48ce-8c1c-f2419c985d21,U02TNEJLC84,,,Same here,1642725704.482700,1642728687.484100,U02UD2P3BQC\\nc7dea2f1-31d8-47e8-a3ba-6512c65cc16f,U01AXE0P5M3,,,\"you could give it a try with hardcoding the path instead of `$(pwd)` . I don\\'t know if that\\'s the issue, but it\\'s something to try\",1642621263.215300,1642728787.484300,U02T9GHG20J\\n1c39cad9-08c2-4ef5-8104-6e7e349cbd2a,,6.0,,\"The submission form asks for\\n`Your code (link to GitHub or other public code-hosting website). Remember: no code - no scores!`\\n\\nIs this the code for the answers? Or is it the code for the overall course? (including, for example, the Docker compose files)\",1642729727.485900,1642729727.485900,U02UD2P3BQC\\n784738d1-b48a-46a0-a881-0b7a6a05e5e9,U02USF4NYG4,,,i had a similar issue. I reinstalled using the SDK installer exe file and then running all commands in google cloud SDK shell,1642712037.447300,1642730285.486300,U02T9550LTU\\n72bb46c0-ea8d-484c-baca-9d841d1490ea,U02USF4NYG4,,,i also removed the cloudSDK_python,1642712037.447300,1642730309.486500,U02T9550LTU\\n588c1859-3a26-48a2-901f-623786cff5db,U02USF4NYG4,,,before reinstalling,1642712037.447300,1642730314.486700,U02T9550LTU\\n78536cdb-d669-4f7f-bfeb-0d3f1775991c,U01AXE0P5M3,,,\":wave: Another option to consider?: set up dedicated channels for different course topics… That’d  mimic the approach functionality of dedicated edutech messaging apps like CampusWire.  (e.g. #zoomcamp-x where x in { Terraforma, GCP, Docker, Posgres, etc.})\",1642711485.444800,1642730587.487400,U02U5DPET47\\n8d624490-35a7-414b-8fcb-c9b804c6701b,U02UX664K5E,,,I used used powershell and it recognized wget,1642721639.474000,1642730882.487800,U01HNBLMT2N\\n2F831753-17AC-438D-B05D-9F7F1F007DE1,U02UX664K5E,,,\"<@U01HNBLMT2N> Good to know, I had trouble in Bash. \",1642721639.474000,1642730986.488900,U02UX664K5E\\n9392e875-0298-46ea-94d1-7c58c0de3cad,U02SPLJUR42,,,Run the docker command again that you used during install setup.  Since the vm was off db also would have shutdown,1642708606.435000,1642731040.489200,U01HNBLMT2N\\nc3914838-4fd3-469e-96d0-7f33fa24f6ad,,,,Help needed,,1642731164.489500,U02VBG59VQ9\\nfca8a124-9a4e-4bf7-815c-1c3362f5be39,,3.0,,\"C:\\\\Users\\\\User1&gt;terraform init\\nTerraform initialized in an empty directory!\\n\\nThe directory has no Terraform configuration files. You may begin working\\nwith Terraform immediately by creating Terraform configuration files.\",1642731166.489700,1642731166.489700,U02VBG59VQ9\\n373d1a7b-62fa-4318-9292-c973030781f4,,2.0,,\"d751673@W00000R90XVVN1 MINGW64 /abic7/git/data-engineering-zoomcamp/week_1_basics_n_setup/2_docker_sql (master)\\n$ docker build -t dockerfile:pandas .\\n#1 [internal] load build definition from Dockerfile\\n#1 sha256:93d0226b6c74d3305d9049bc9360e91be794d25ba408e9f8b15828b1b87a564f\\n#1 transferring dockerfile:\\n#1 transferring dockerfile: 2B 0.2s done\\n#1 DONE 0.2s\\n\\n#2 [internal] load .dockerignore\\n#2 sha256:0efb3ac9b02833c993ae8de7ef33e9d7870dd2a2c3e43a4ef4c39b515b617092\\n#2 transferring context:\\n#2 CANCELED\\nfailed to solve with frontend dockerfile.v0: failed to read dockerfile: open /var/lib/docker/tmp/buildkit-mount764835203/Dockerfile: no such file or directory\\n\\nd751673@W00000R90XVVN1 MINGW64 /abic7/git/data-engineering-zoomcamp/week_1_basics_n_setup/2_docker_sql (master)\\n$ docker\\n\\nUsage:  docker [OPTIONS] COMMAND\\n\\nA self-sufficient runtime for containers\\n\\nOptions:\\n      --config string      Location of client config files (default\\n                           \"\"C:\\\\\\\\Users\\\\\\\\d751673\\\\\\\\.docker\"\")\\n  -c, --context string     Name of the context to use to connect to the\\n                           daemon (overrides DOCKER_HOST env var and\\n                           default context set with \"\"docker context use\"\")\\n  -D, --debug              Enable debug mode\\n  -H, --host list          Daemon socket(s) to connect to\\n  -l, --log-level string   Set the logging level\\n                           (\"\"debug\"\"|\"\"info\"\"|\"\"warn\"\"|\"\"error\"\"|\"\"fatal\"\")\\n                           (default \"\"info\"\")\\n      --tls                Use TLS; implied by --tlsverify\\n      --tlscacert string   Trust certs signed only by this CA (default\\n                           \"\"C:\\\\\\\\Users\\\\\\\\d751673\\\\\\\\.docker\\\\\\\\ca.pem\"\")\\n      --tlscert string     Path to TLS certificate file (default\\n                           \"\"C:\\\\\\\\Users\\\\\\\\d751673\\\\\\\\.docker\\\\\\\\cert.pem\"\")\\n      --tlskey string      Path to TLS key file (default\\n                           \"\"C:\\\\\\\\Users\\\\\\\\d751673\\\\\\\\.docker\\\\\\\\key.pem\"\")\\n      --tlsverify          Use TLS and verify the remote\\n  -v, --version            Print version information and quit\",1642731532.490100,1642731532.490100,U02UY1QTGHW\\n51e8fece-7d15-4395-9859-8c2ad60d7120,,1.0,,Can someone help me with this issue as  I am not getting the same output as the video docker build -t dockerfile:pandas where dockerfile is the config file,1642731604.491100,1642731604.491100,U02UY1QTGHW\\n5d644124-4727-4ebe-907e-74dd681ea61a,,3.0,,\"Hi everyone, Today while running my postgres container, I forgot to set the --name parameter to pg-database. After a bunch of tries, I am able to successfully access the database from my pgadmin my assigning the ip of the container to the server host. Is there a way I can assign name to the host in my postgres container to pg-database without recreating the container again?\",1642733568.495200,1642733568.495200,U02TTSXUV2B\\nA7E5CBF5-D124-4274-90A0-9D0E856D59EC,U02TTSXUV2B,,,Please check docker automatically gives a random name to container during run time .do docker ps and get default name of your post gres container .I am not sure if you can rename a running container,1642733568.495200,1642734512.498200,U02AGF1S0TY\\n53c6f2a0-cc16-4b9f-8bc7-921246bab476,U02U5G0EKEH,,,oh i had a biu in my code so i compare it with the one git repo  and found the bug.  week_1_basics_n_setup/2_docker_sql/ingest_data.py,1642658846.295100,1642735237.498600,U02U5G0EKEH\\neaf0ee00-a523-4f6c-882b-32e268f387d8,,1.0,,\"<@U01DHB2HS3X>  in version 1.1.4 i kept getting this error\\nC:\\\\Users\\\\User1&gt;terraform init\\nTerraform initialized in an empty directory!\\n\\nThe directory has no Terraform configuration files. You may begin working\\nwith Terraform immediately by creating Terraform configuration files.\",1642735384.000200,1642735384.000200,U02T8HEJ1AS\\n541c4662-a2eb-409c-946f-ddd5e81438ba,U02TTSXUV2B,,,\"Thanks Hari, yes docker assigns a random name . however I was unable to use that on my pgadmin. Only the IP worked. There is a docker rename command, I could try to stop, rename and start again\",1642733568.495200,1642735464.000400,U02TTSXUV2B\\nb709ab09-5bf4-4202-8a8e-bbea7e6cf8ea,U02VBG59VQ9,,,You need to initialize terraform in the directory with the Terraform files (inside the week 1 folder there is a terraform folder),1642731166.489700,1642735467.000600,U02UD2P3BQC\\nd1aeb604-c769-41a7-a218-3b23d663cbf6,U02T8HEJ1AS,,,You need to initialize terraform in the directory with the Terraform files (inside the week 1 folder there is a terraform folder),1642735384.000200,1642735520.000800,U02UD2P3BQC\\n22AFA329-5C32-4BBF-95CB-E3D8C418DB6C,U02TTSXUV2B,,,Oh ..Thanks for the input ,1642733568.495200,1642735791.001700,U02AGF1S0TY\\n83953780-fa75-4fc9-95a7-9aa5c480c188,U02TNEJLC84,,,Thanks. Glad I\\'m not the only one. I\\'m thinking the 15 shouldn\\'t be there based on the options.,1642725704.482700,1642736453.002800,U02TNEJLC84\\n505B0D3B-B8F3-4EAE-9724-D1FFB2C16BCF,,2.0,,\"When running the command docker -it run python:3.8 , I get this error “unknown shorthand flag: ‘I\\' in -it\\n\\nAny thoughts what I can do?\",1642736538.005000,1642736538.005000,U02SSP7C4SD\\n17200c6f-35aa-4127-9e67-b112a2a37cef,U02TJ69RKT5,,,\"I also got the low memory error but it appears to have happened after the completion of the while loop.\\n\\nI have `1369765` rows in my table\",1642638867.256500,1642736636.005100,U02UAFF1WU9\\nc60b82e1-4553-4a9a-969b-f7d01cab0489,,1.0,,\"have you tried, `docker run -it python:3.8`\",1642737885.006400,1642737885.006400,U02UBV4EC8J\\nd1d40679-e4e5-4e1d-a5b8-e7a0f3fab040,,3.0,,\"I am having this problem after running: `docker run -it     -e PGADMIN_DEFAULT_EMAIL=\"\"<mailto:admin@admin.com|admin@admin.com>\"\"     -e PGADMIN_DEFAULT_PASSWORD=\"\"root\"\"     -p 8080:80     --network=pg-network     --name pgadmin    dpage/pgadmin4` . Can someone help me ?:pray:\",1642737967.007100,1642737967.007100,U02T2DX4LG6\\n115b6490-0268-4015-8b2d-7a027df73f0c,,5.0,,\"Hitting an issue getting pgadmin to run, details in the thread.\",1642738762.009400,1642738762.009400,U02UAFF1WU9\\n85e038c2-90ec-448b-9251-eb8d64f3cff0,U02UAFF1WU9,,,\"```steve@steves-mbp DE_Class % docker pull dpage/pgadmin4\\nUsing default tag: latest\\nlatest: Pulling from dpage/pgadmin4\\nDigest: sha256:e04b209a47a6f9d47367c9bd4c88fa76570b81ecce1579d6a7e4398c4eae038a\\nStatus: Image is up to date for dpage/pgadmin4:latest\\n<http://docker.io/dpage/pgadmin4:latest|docker.io/dpage/pgadmin4:latest>\\n\\nsteve@steves-mbp DE_Class % docker run -it \\\\\\n        -e PGADMIN_DEFAULT_EMAIL = \"\"<mailto:admin@admin.com|admin@admin.com>\"\" \\\\\\n        -e PGADMIN_DEFAULT_PASSWORD = \"\"root\"\" \\\\\\n        -p 8080:80 \\\\\\n        dpage/pgadmin4\\ndocker: invalid reference format.\\nSee \\'docker run --help\\'.\\nzsh: no such file or directory: dpage/pgadmin4```\\nAny ideas why I am receiving the `docker;invlaid reference format` error or the `zsh: no such file or directory: dpage/pgadmin4` errors?\",1642738762.009400,1642738810.009600,U02UAFF1WU9\\n9f9f4b40-7050-47e3-afab-41017ea50734,U02T2DX4LG6,,,\"it is complaining about no space on the device, I would check the space and free ip up\",1642737967.007100,1642739194.010000,U02UBV4EC8J\\ne9ec44eb-5e44-4f81-9043-f5595872e01d,U02UAFF1WU9,,,are you getting that when you try with docker-compose as well?,1642738762.009400,1642739251.010300,U02UBV4EC8J\\nc44dd50e-4111-4a2b-bb5c-d14bbfda2608,U02UAFF1WU9,,,\"I got the invalid format with docker-compose 1.2 version, the newer version 2.2.23 I think works fine\",1642738762.009400,1642739284.010500,U02UBV4EC8J\\n9d825f17-eada-42f8-a4a3-8256a9ffcaaa,U02T2DX4LG6,,,\"hi kenny, thanks. the device means postgres?\",1642737967.007100,1642739419.010700,U02T2DX4LG6\\na3615cd3-5bfc-4adc-90a8-9c5480863dab,U02UAFF1WU9,,,\"<@U02UBV4EC8J> I didn\\'t try docker compose, I referenced a stack overflow post and got it to work by running it all on one line weirdly enough.\\n\\n`docker run -e PGADMIN_DEFAULT_EMAIL=\"\"<mailto:admin@admin.com|admin@admin.com>\"\" -e  PGADMIN_DEFAULT_PASSWORD=\"\"root\"\" -p 8080:80 dpage/pgadmin4`\\n\\nworked for me all on one line\",1642738762.009400,1642739779.012200,U02UAFF1WU9\\n74f7ba00-81d4-475a-b2d1-21f10174d0a9,,,,\"sudo chmod a+rwx ny_taxi_postgres_data\\nThis is not working for me, I have not been able to view my database file in the ny_taxi_postgres_data folder. Which command would I use on gitbash because Sudo doesn\\'t work on gitbash?\",,1642739861.013700,U02DFKG7VTL\\n266760d4-3385-4f62-bfba-2efd92652e7b,U02UAFF1WU9,,,:+1:,1642738762.009400,1642739920.013800,U02UBV4EC8J\\nc5c3f2bf-813c-4763-bfdb-a6b5195798de,U02T2DX4LG6,,,I fixed it by removing unused volume objects in docker.,1642737967.007100,1642740416.014300,U02T2DX4LG6\\n9F696D5C-EA3A-44EE-9840-1F44DA23D0D5,U02SSP7C4SD,,,It should be docker run -it python:3.8,1642736538.005000,1642742664.016400,U02AGF1S0TY\\n75f63797-2f42-4b1a-b2b6-926b67c1be88,U02TNEJLC84,,,\"I\\'m having the same trouble.\\n\\nIt turns out answering how many rides were on the 15th was trickier than I initially thought it would be: do you include only rides that were picked up on the 15th, only rides that were dropped off on the 15th, both?  It also looks like there are a lot of pick up and drop off times at exactly midnight (00:00:00), perhaps those represent missing data? Should those be included?\\n\\nBUT...\\n\\nWith any of those choices, I\\'m still not getting close to any of the options either.\",1642725704.482700,1642742728.016600,U02T9GHG20J\\n6e78ee5d-71c5-4bd0-aad3-1f07d135666c,U02UBV4EC8J,,,\"Yes. If you want to run Docker with python 3.8, you should type \"\"python:3.8-slim-buster\"\". Or you can use other versions. Or you can find images on Docker-hub\",1642737885.006400,1642742878.016900,U02QL1EG0LV\\n2bbbc261-b7c0-4038-93aa-539fd2d8c97a,U02SSP7C4SD,,,\"Yes. If you want to run Docker with python 3.8, you should type \"\"python:3.8-slim-buster\"\". Or you can use other versions. Or you can find images on Docker-hub\",1642736538.005000,1642742925.017100,U02QL1EG0LV\\n1f2f0bc5-a158-4be4-877d-9e8fe2b2fb05,U02TNEJLC84,,,\"Same here, <@U01AXE0P5M3> Can you confirm the answers provided on the form are correct?\",1642725704.482700,1642743069.017500,U02T9550LTU\\nc8dd6eb2-e956-4b81-aab5-0a900d48e003,U02T2TX1GS2,,,I started using Ubuntu in virtual box. Its working fine now. I am not having much disk space for anaconda in my C drive. For ubuntu I am using another drive with lots of space in it,1642700805.403700,1642743233.017700,U02T2TX1GS2\\nab19db06-b7fa-4e97-a5e0-885d2fc76aa1,U02UD2P3BQC,,,With SQL queries - I.e. with homework only,1642729727.485900,1642745506.018800,U01AXE0P5M3\\nc8ca4fcd-1cae-478e-9e7f-316df044e7c0,U02TBKWL7DJ,,,\"Thanks, helped me out too!\",1642634044.245600,1642745783.019200,U02T9550LTU\\n52ff7914-fc8d-4380-8383-e40c0310d192,U02HLE69P19,,,I like the multiple select thing. Select a word and try crtl+D,1642721601.473400,1642745824.019400,U01AXE0P5M3\\n0d668d79-a69d-463f-9dca-db87d05447bf,U02UX664K5E,,,Yes you need to install it separately,1642721639.474000,1642745855.019600,U01AXE0P5M3\\n112c2edf-20c9-4ab0-860d-0573ff6d4c93,U02UY1QTGHW,,,Please rewatch the video and do it in the same way,1642731604.491100,1642746034.019800,U01AXE0P5M3\\nd8593d4a-8621-44a3-a533-895974336855,,4.0,,\"Hi everybody, I am getting this error while running ingest_data.py. - &gt; wget\\' is not recognized as an internal or external command,operable program or batch file. I downloaded wget.exe and added to System32 folder and it runs on my cmd and on the virtual env. But VS code complaints about it and throws this error. Am I missing something?\",1642746119.020000,1642746119.020000,U02UAADSJ84\\nf3f0e672-08be-4603-9a99-dd65079e418f,U02UAADSJ84,,,Have you tried rebooting? On windows it often helps =),1642746119.020000,1642746398.020200,U01AXE0P5M3\\n1e9771ed-700f-4d23-a4a5-ebf9b99ac293,U02UAADSJ84,,,\"It worked, thank you! :smiley:\",1642746119.020000,1642746635.021500,U02UAADSJ84\\n13497948-7412-477d-855c-bbb3ef7819a2,U02TNEJLC84,,,I\\'ll check in a few hours,1642725704.482700,1642746639.021800,U01AXE0P5M3\\n62e1accc-d07f-4703-b132-f4871db1ecc5,U02UAADSJ84,,,<https://youtu.be/nn2FB1P_Mn8|https://youtu.be/nn2FB1P_Mn8>,1642746119.020000,1642746747.022300,U01AXE0P5M3\\nca0aae4a-6f2d-444d-825a-2bd1dfd4793b,U02UAADSJ84,,,haha on point!! :laughing:,1642746119.020000,1642747435.023400,U02UAADSJ84\\nc7f60af7-b36a-413f-aaca-7e5159dedf82,U02T1BX1UV6,,,\"<@U01AXE0P5M3>\\ni am not able to copy paste the docker command i.e,\\ndocker run \\xa0-it \\\\\\n\\xa0 -e POSTGRES_USER=\"\"root\"\"\\\\\\n\\xa0 -e POSTGRES_PASSWORD=\"\"root\"\"\\\\\\n\\xa0 -e POSTGRES_DB=\"\"ny_taxi\"\"\\\\ \\xa0\\n\\xa0 -v D:\\\\deng_zoom_camp\\\\week_1_basics_n_setup\\\\docker_sql\\\\ny_taxi_postgres_data:postgres-db-volume:/var/lib/postgresql/data \\\\\\n\\xa0 -p 5432:5432 \\\\\\n\\xa0 postgres:13\\n\\nso what i did , i kept the .sh file and run it from particular folder i.e, specified for D:deng_zoom_camp till docker_sql  but getting error .\\nCan you please guide me what i am doing wrong ?\",1642444157.313800,1642747775.024700,U02UEE4MBEG\\n16426597-acea-4ef9-b375-5a246f07e1a0,,4.0,,I\\'m trying to get the Google cloud credit for my account but my credit card can\\'t go through,1642748175.026300,1642748175.026300,U02AUCL9ZQF\\n603854cd-4c29-4216-ac06-8c8a0ea0b578,,4.0,,\"can any body help me running the docker and postgres ?\\ngetting error like *repository name should lowercase*\\ndocker run\\xa0-it \\\\\\n\\xa0-e POSTGRES_USER=\"\"root\"\"\\\\\\n\\xa0-e POSTGRES_PASSWORD=\"\"root\"\"\\\\\\n\\xa0-e POSTGRES_DB=\"\"ny_taxi\"\"\\\\\\xa0\\n\\xa0-v D:\\\\deng_zoom_camp\\\\week_1_basics_n_setup\\\\docker_sql\\\\ny_taxi_postgres_data:postgres-db-volume:/var/lib/postgresql/data \\\\\\n\\xa0-p 5432:5432 \\\\\\n\\xa0postgres:13\\n\\ni even kep the local D : as d: as well . but same issue\",1642748652.027100,1642748652.027100,U02UEE4MBEG\\n7d5712bf-40b8-4c05-898a-392a84aff76b,U02QZN0LSBT,,,Thank you Alexey!!,1642701743.413300,1642750303.028700,U02QZN0LSBT\\n081783ae-c2c9-409d-84d0-68f888438578,U02UBV4EC8J,,,\"ERROR: (gcloud.auth.application-default.set-quota-project) Cannot add the project \"\"dtc-de\"\" to application default credentials (ADC) as a quota project because the account in ADC does not have the \"\"serviceusage.services.use\"\" permission on this project.\",1642603021.154900,1642750657.028900,U02ULQFCXL0\\nf4420440-69dd-4051-86ab-da96c452dcfd,U02UBV4EC8J,,,but why?,1642603021.154900,1642750671.029100,U02ULQFCXL0\\naf33c1ea-9575-4461-9eca-38f9a215285e,U02UBV4EC8J,,,\"it turns out you need to specify the project ID and not its name, he asked - he decided, thanks to everyone =)\",1642603021.154900,1642750980.029300,U02ULQFCXL0\\n0c7c6807-8fab-4b1c-b7cf-479935a34da4,U0316CVHMPT,,,\"Alright, This is noted!\\nThanks\",1643615860.647929,1643616177.000179,U0316CVHMPT\\n0f0d483f-23cf-4a1f-a12f-77cf728a2f5b,U030HKR0WK0,,,\"Thanks. Which folder do you want me to delete \"\"/var/lib/postgresql/data\"\"? This is inside the docker container!\\nYes, I will document the issue inside the google doc when I figure this out.\",1643615516.282529,1643616234.524899,U030HKR0WK0\\n58e8764a-3a4a-47ca-8a88-ac7a3a35f16f,,8.0,,\"Hello, I’m unable to access gcp. It requested to verify my card payment in order to access the free $300 but it hasn’t been verified till now since last week.\\n\\nI’ve been unable to carry one with week 2, please what can I do \",1643617669.007299,1643617669.007299,U02TZ1JCVEC\\n14f5ba47-751c-4f10-aca5-3335bfa41dbb,U02TZ1JCVEC,,,You can skip the parts about gcp,1643617669.007299,1643617730.017789,U01AXE0P5M3\\nc0ca0e25-60e2-4bd9-a1ed-0524fa020eeb,U030HKR0WK0,,,the one on your host computer - ny_taxi_postgres_data,1643615516.282529,1643617967.838089,U01AXE0P5M3\\nf81f87fe-7902-468c-afef-7996d69acd8a,U02RY943Z0Q,,,\"That’s cool. Even I was moving cities in the last 2 weeks, and as an instructor I missed some deadlines too on releasing some material :slightly_smiling_face: For this, we have extended some homework deadlines. But you can definitely catch up! :the_horns:\",1643569059.017099,1643617989.172289,U01DHB2HS3X\\n26B59496-95F9-4447-B52F-60DF08D4504D,U02TZ1JCVEC,,,Wouldn’t I be need it for the assignment?,1643617669.007299,1643618261.047899,U02TZ1JCVEC\\n387afc15-be06-4df0-b2ad-3ad446d40d47,U02TZ1JCVEC,,,\"yes, but you can do some pasts of the assignment without the access to Gcp\",1643617669.007299,1643618295.272499,U01AXE0P5M3\\n6BFCDD7E-4161-4A16-AC96-9D2966ECDD7A,U02TZ1JCVEC,,,Alright thank you!,1643617669.007299,1643618317.122629,U02TZ1JCVEC\\n9d7ff457-dc33-43c3-92f4-dded9311857c,U02TZ1JCVEC,,,\"e.g. in week 1 questions, 3-6 could be done without GCP. In this week everything could be done without GCP (but in the solution we\\'ll show only the GCP part)\\n\\nweek 3 is tricker, but there are a few things that you can do without GCP as well\",1643617669.007299,1643618344.900919,U01AXE0P5M3\\nbcd2e477-1955-49ff-9f4b-b01ece3682d6,U030HKR0WK0,,,Nope. That does not cut it. I created a local volume and mapped it to postgres data folder. I have documented the resolution in the FAQ google doc. Thanks Alexey for your help. Really appreciated,1643615516.282529,1643618437.229829,U030HKR0WK0\\n39197982-a33d-4653-a1e1-51a2db535f17,U02U5G0EKEH,,,<@U02U5G0EKEH> I think the instructors requested not to be tagged...,1643606923.092969,1643618446.668919,U02HB9KTERJ\\n3418d179-9921-4902-8502-cdc21c81cdb9,U030HKR0WK0,,,The chmod thing helps (see github),1643615516.282529,1643618547.023789,U02HB9KTERJ\\nf010e6f4-5239-4b9b-9889-36b06045da75,,4.0,,\"Just to clarify: I need to run the `terraform apply` from week 1 and then create airflow\\'s docker (since the former creates the gcp data bucket to be used by airflow).\\n\\nI am asking coz there was an advice to run `terraform destroy` in order to \\'save money\\', which I did after completing the assignment from week 1.\",1643618720.680959,1643618720.680959,U02HB9KTERJ\\n93bb1634-3bef-45c4-91c6-c066328d8d57,U02TZ1JCVEC,,,\"It does say that you have to verify, but it doesn\\'t do it. I went to `<http://pay.google.com|pay.google.com>` and there it asked me to verify. It will ask for govt id and credit/debit card picture.\",1643617669.007299,1643618842.889119,U02HB9KTERJ\\n0af8def7-386c-44b6-9561-7871ebf77715,U02HB9KTERJ,,,yes you\\'ll need the bucket that we created on week 1 as well as the big query thing,1643618720.680959,1643618855.902749,U01AXE0P5M3\\na71f674f-cfb5-4b45-b5d0-61a172527034,U025S978QRG,,,\"Great, many thanks!\",1643556926.292499,1643619214.016179,U025S978QRG\\n491F3E19-1D5C-4227-8E2B-2693696B2078,U02TZ1JCVEC,,,\"Great, thanks for this <@U01AXE0P5M3> \",1643617669.007299,1643619217.845339,U02TZ1JCVEC\\nB8667F65-262C-467D-B882-BAF71FA478B7,U02TZ1JCVEC,,,<@U02HB9KTERJ> yes I went through that also ,1643617669.007299,1643619254.665679,U02TZ1JCVEC\\nd64b1977-8a2c-468b-ba28-83a9af533c04,,3.0,,Do we need the data from the GCP Transfer service? I kept it just incase we might need in the next lessons. I would want to remove it if we don\\'t need it so that ~316 GB doesn\\'t consume my credits.,1643619927.635319,1643619927.635319,U02TATJKLHG\\n7b767a46-b256-463c-a02c-a45e1d55446f,U02U0U8LGHM,,,\"<@U02T9GHG20J> to be fair i find this quick fix and go to sleep :smile:\\nbut my to go decision would be to add bash operator with this command between download_dataset and format_to_parquet it should not break anything\",1643560330.528159,1643621630.492439,U02T95PEBJP\\n56b7722b-8123-41fc-8103-311a59062ef7,U02QGA57GRY,,,\"on doing \"\" docker-compose up airflow-init\"\" after \"\"docker-compose build\"\" am getting these errors, am i on the right track?\\n\\n(base) DELL@de-zoomcamp:~/Zcamp/week_2/data_ingestion/alexey/airflow$ docker-compose up airflow-init\\nno such service: airflow-init\",1642529404.010000,1643621672.112299,U02RTJPV6TZ\\n5a24c9b2-0d0f-42ca-b3ab-092d83f80e27,U02T1BX1UV6,,,\"on doing \"\" docker-compose up airflow-init\"\" after \"\"docker-compose build\"\" am getting these errors, am i on the right track?\\n\\n(base) DELL@de-zoomcamp:~/Zcamp/week_2/data_ingestion/alexey/airflow$ docker-compose up airflow-init\\nno such service: airflow-init\",1642556934.089100,1643622042.954989,U02RTJPV6TZ\\n479fa48e-bd54-42f4-a9e5-229dc4d8136d,U02T697HNUD,,,Awesome :clap:thanks <@U01AXE0P5M3> and <@U02T65GT78W> - I look forward to watching the video :),1643596998.639849,1643622221.012209,U02U5SW982W\\nd8e59329-1689-4ec6-ae56-17730fe9763b,,12.0,,\"I just started with week 1\\nDocker failed to start after implementation\\nKindly assist\",1643622530.303689,1643622530.303689,U0316CVHMPT\\n3bc64f7c-658c-4af4-be05-9d8591a6ac2e,U02ULMHKBQT,,,Issue resolved.,1643612900.649649,1643622628.984029,U02ULMHKBQT\\ned066ecc-20a9-48d0-ace7-8a30dce1e8fb,U02TATJKLHG,,,\"The examples shown it in the next videos might be easier to understand if you are using data from transfer service, but it is not a hard requirement\",1643619927.635319,1643623771.313249,U01DFQ82AK1\\n25104c90-a249-4765-9260-44012c3dcda3,U0316CVHMPT,,,\"Can you please share more details, steps you followed, error message, OS, docker version, etc\",1643622530.303689,1643623820.226349,U01DFQ82AK1\\nef27f416-26ba-4c76-9009-06ff8c1449e9,U0316CVHMPT,,,\"Thanks  <@U01DFQ82AK1>\\nI click and installed Docker from this link\",1643622530.303689,1643623944.277219,U0316CVHMPT\\n77d46a77-38e1-450e-b726-4fc843ead2f4,U0316CVHMPT,,,\"I also tried to install wsl\\nI\\'m kinda stuck at this point\",1643622530.303689,1643624056.194939,U0316CVHMPT\\n41244dab-c070-4b6b-a128-9c9ec157c79b,U0316CVHMPT,,,,1643622530.303689,1643624270.466319,U0316CVHMPT\\nebe7bf6f-02bc-4a73-82b4-041c71c623c1,U0316CVHMPT,,,\"If you need to install WSL maybe this will help \\n\\n<https://ubuntu.com/wsl|https://ubuntu.com/wsl>\",1643622530.303689,1643624397.168379,U01AXE0P5M3\\n14982353-8f78-46e2-aafd-e35d1ad08f02,U0316CVHMPT,,,\"Alright, Thanks Alexey\\nDo I need to install WSL to use Docker or can I do without it?\",1643622530.303689,1643624682.165379,U0316CVHMPT\\nc23383db-90dd-4e14-96d1-fb8686b06443,,4.0,,\"Dear all, has anyone tried running new docker-compose-nofrills? \\n\\nI keep getting error: no module named airflow. Please help me how to solve this.\",1643624829.835949,1643624829.835949,U02UNB4G739\\n82769b05-eb61-4c11-861c-f1421bebe787,U0316CVHMPT,,,I think the officially recommended way of using docker on windows is through WSL2. And on windows home it\\'s probably the only option,1643622530.303689,1643624842.976489,U01AXE0P5M3\\nc8193db2-46d5-405a-b63d-24a3ea0a2b6f,U0316CVHMPT,,,\"Regarding your problem, have you tried googling it?\\n\\nI did a quick search and found this: <https://forums.docker.com/t/solved-docker-failed-to-start-docker-desktop-for-windows/106976|https://forums.docker.com/t/solved-docker-failed-to-start-docker-desktop-for-windows/106976>\\n\\nThere were a lot of other things in the search results that could be helpful as well\",1643622530.303689,1643624887.417169,U01AXE0P5M3\\n35b4d6a4-3036-42df-999a-2d636289ef67,U0316CVHMPT,,,\"Okay,\\nI will use the link you shared and get back on how far i have gone\\nThanks for your help\",1643622530.303689,1643624890.389869,U0316CVHMPT\\nec35815a-f7ce-4146-a51e-0da993ab4147,U02UNB4G739,,,I have the same error and wasn\\'t able to solve it. Which OS do you use?,1643624829.835949,1643624999.011759,U01AXE0P5M3\\n23b13107-b115-4d45-bfa5-071420c15025,U02TATJKLHG,,,Also it\\'s 2 cents per GB per month. I don\\'t think it\\'ll consume a lot of your credits,1643619927.635319,1643625148.071159,U01AXE0P5M3\\n2c690211-de2e-425d-b736-12cb1f94dd6b,U02TATJKLHG,,,\"Cool, I\\'ll keep it then. Anyways I was monitoring my credits and it doesn\\'t seem to be eating it up too much as you mentioned Alexey. :smile:\",1643619927.635319,1643625148.659859,U02TATJKLHG\\n3160a883-df8b-4395-8a84-1e6289aa1942,U02T697HNUD,,,\"Alexey, have you tried on WSL2? When you say \"\"on windows\"\" you mean WSL2? I didn\\'t have the time to test it yet, but I intend to...\",1643595374.252149,1643625179.187649,U02GVGA5F9Q\\n3eac0878-afd7-466b-a2a1-ddfc691e3230,U02T697HNUD,,,\"I meant plain windows, but I also tried WSL2 and unfortunately it didn\\'t work either\",1643595374.252149,1643625644.800409,U01AXE0P5M3\\nb1d67177-b9b7-450d-a72c-78a3194f171b,U02T697HNUD,,,I see. At least I know that I am not the only one have this error:sweat_smile:,1643595374.252149,1643625901.321849,U02T697HNUD\\n42ad082b-4e22-4eb1-99ba-085e116c898f,U02T697HNUD,,,Then I keep continue to use the full-version airflow. Thanks a lot!,1643595374.252149,1643625922.930559,U02T697HNUD\\nc91a0d15-a2f6-4ca3-aefd-5eca201fc2d5,U02T697HNUD,,,Thanks everyone to hints me about my inquiry :smile:,1643596998.639849,1643626010.479129,U02T697HNUD\\ne2bde033-edb7-41c2-8ba3-e7e641ae48ea,U02U5G0EKEH,,,you may try to change the start_date to \\'2021-01\\'. And at the same time. Just turn on the DAG in UI. Dont trigger it,1643606875.920119,1643626061.564509,U02T697HNUD\\n42b90f21-8497-4b67-9895-ab0973cff1a6,U02T697HNUD,,,\"Yes, the long video is full of hints! Thanks Alexey! Regarding the original questions, my view is one table for taxis, another for FHVs and other for Zones. I\\'m planning to have the 3 required DAGs, but sharing the same ingestion code.\",1643596998.639849,1643626172.821959,U02GVGA5F9Q\\nd1361df2-fd2e-409a-a07c-402fb3ad7ca4,U0316CVHMPT,,,also you can try activate virtualization on your BIOS setup,1643622530.303689,1643626830.458889,U02RA8F3LQY\\nc4496811-2694-46a1-9385-a753639f4c77,U02T697HNUD,,,That\\'s the best way to go!,1643596998.639849,1643627908.944479,U01AXE0P5M3\\n0e298e37-8911-4eb1-a1e0-d166d79102e4,,11.0,,\"Regarding homework 2:\\nRunning Airflow locally to download/convert/upload the 24 months of yellow taxi data seems to be a bit too much for my laptop (I think that\\'s the problem at least). I\\'m thinking about using Google Transfer Service to get the data across to Big Query, is this a good idea in your opinion?\",1643627929.124989,1643627929.124989,U02TGS5B4R1\\n82368ee0-8ad2-4eb2-9ba0-c50c8e615e3d,U02TGS5B4R1,,,It\\'s the easiest option,1643627929.124989,1643627978.682519,U01AXE0P5M3\\n2ab62351-c919-41c4-8add-e4d48ff5af85,U02TGS5B4R1,,,\"At least the second month succeeded, which makes me think the DAG\\'s logic works\",1643627929.124989,1643628006.146569,U02TGS5B4R1\\nfece9571-8c69-4a90-bec3-d80d33e38989,U02TGS5B4R1,,,\"But you\\'ll need to have the dags! Yes if it works for a few months, it\\'s good\",1643627929.124989,1643628032.954849,U01AXE0P5M3\\n53e14f26-a01e-44d7-a4d9-537d164e0f3d,U02TGS5B4R1,,,I tried using `max_active_runs=1` but this only run the DAG for 2019-01 and stopped afterwards. I had hoped it would do one at a time,1643627929.124989,1643628127.155489,U02TGS5B4R1\\nc1645934-9763-4796-8d67-fb7bc9f4988b,U02TGS5B4R1,,,\"&gt; But you\\'ll need to have the dags! Yes if it works for a few months, it\\'s good\\nAlright :thumbsup:  I\\'ll try one month for FHV as well, and upload the zones data using a DAG, to check the code works\",1643627929.124989,1643628197.634419,U02TGS5B4R1\\n6982622f-2036-4cb8-b7c9-d624e0578e8d,,,,Hi..,,1643628549.756109,U01MFQW46BE\\n9598b158-87e1-4782-9a22-a9f506fa0f82,,,,Please why am I getting this error in the code,,1643628571.182759,U01MFQW46BE\\nae8a05c1-c650-400e-8205-aec51967aea1,,9.0,,\"$   winpty docker run -it \\\\\\n&gt;     -e POSTGRES_USER = \"\"root\"\" \\\\\\n&gt;     -e POSTGRES_PASSWORD = \"\"root\"\" \\\\\\n&gt;     -e POSTGRES_DB = \"\"Ny-taxi\"\" \\\\\\n&gt;     -v c:/Users/wavy/desktop/D.E/ny-taxi-postgress:/var/lib/postgresql/data \\\\\\n&gt;     -p 5432:5432 \\\\\\n&gt;     postgres:13\\ndocker: invalid reference format.\",1643628590.850169,1643628590.850169,U01MFQW46BE\\n653b85d3-0619-4e46-800a-4d6ccbe8e8b1,U02TGS5B4R1,,,\"What I did is, I split into 4 dags for each year. For example, first dag running the data from 2019-01 - 2019-03. It works for me on this\",1643627929.124989,1643628591.570839,U02T697HNUD\\n9b61b299-7a46-4114-84c3-1dd1fd611124,U02T697HNUD,,,\"If I manage to get some time, I\\'ll develop a functional local ingest version. But I want to grasp the terraform/GCP thing too. Let\\'s see how this week goes... Great course! I\\'m loving it!\",1643596998.639849,1643628625.746869,U02GVGA5F9Q\\nfeb8cebf-7315-4558-9995-bc35f226edf3,U01MFQW46BE,,,Hi! Have you checked the FAQ?,1643628590.850169,1643628820.360409,U01AXE0P5M3\\n9f909009-f8ac-4b2e-9b04-dee81218634e,U02TGS5B4R1,,,How did you specify the end date for a dag?,1643627929.124989,1643628943.893659,U01AXE0P5M3\\n3d1fa840-1132-4615-a84b-c7e70841a315,U01MFQW46BE,,,\"I\\'ve actually checked, but I couldn\\'t find any.\",1643628590.850169,1643629081.536039,U01MFQW46BE\\n1d674d28-629a-408f-9ea1-8cccb82eeb2d,U02HB9KTERJ,,,\"Also, you can destroy your setup and recreate with the “apply” command as many times as you want, as long as your TF state remains unchanged. That’s the beauty of using Terraform\",1643618720.680959,1643629395.878049,U01DHB2HS3X\\nb0ca1e88-475e-493b-b5d2-4352f7d5973f,U02TGS5B4R1,,,I set the end date inside the default_args. And schedule it on monthly. Based on my scripts. It will only run 3 times.,1643627929.124989,1643629545.329549,U02T697HNUD\\n116ec62b-f739-4d84-89d9-f0a7e3ea3207,U02TGS5B4R1,,,same here,1643627929.124989,1643629579.574379,U02TGS5B4R1\\n13998535-1823-4744-b7cc-0b3136987b8d,U02UX664K5E,,,................................,1643384587.773619,1643629645.802539,U02UZ493J56\\n8057fe15-55d0-4600-80e0-d994c4678293,U02UBQJBYHZ,,,\"Hi Cris, unfortunately I also went through the same problem earlier when my bandwidth (internet’s upload speed) was slow. This is also why I’ve written a custom python operator to parallelize uploads with chunks of data. But we’ll keep looking for better ways. What’s your upload speed?\\n\\nAlternatively, I would also suggest going for the PG way if it doesn’t work :confused:\",1643598603.077489,1643629702.760519,U01DHB2HS3X\\n5bc9f223-7d4d-44f3-bc58-3e3d26103d6d,U02T697HNUD,,,\"I like this question, and the scope of the solution explained through the answers, so I’m pinning this.\",1643596998.639849,1643630062.958609,U01DHB2HS3X\\nab7c1335-f9f9-4bd3-8c42-718813a0e819,U01MFQW46BE,,,hi <@U01MFQW46BE> can you try replacing `-` with underscores and try once.,1643628590.850169,1643630981.795679,U02RW07CVTJ\\na2f71d8d-8e8c-4d87-b5af-fdc67be168ec,U01MFQW46BE,,,\"Thanks...\\n\\nHave been able to sought it out. I think there shouldn\\'t be a space between the assignment operator and value given.\",1643628590.850169,1643631162.185999,U01MFQW46BE\\n3a57458f-37ee-402b-a298-ee14969ab6c7,U01MFQW46BE,,,\"Yes indeed, I didn\\'t notice it. Did it work after you changed it?\",1643628590.850169,1643631269.921369,U01AXE0P5M3\\nc882d04b-c76a-4463-9ef4-64dd1da43d79,U01MFQW46BE,,,\"Yes, it\\'s.\",1643628590.850169,1643631305.520689,U01MFQW46BE\\n0aac0fb0-ca50-4623-97da-cbf37986e6dc,U02UBQJBYHZ,,,Or a VM in GCP,1643598603.077489,1643631322.111869,U01AXE0P5M3\\nf479e4d2-38d3-4e8c-b0fa-188d57ad7e15,U02TGS5B4R1,,,\"Nice, thanks!\",1643627929.124989,1643631386.750919,U01AXE0P5M3\\n5c5c2638-5b43-4732-be59-597c5a79add3,U01MFQW46BE,,,\"Great, glad to hear it\",1643628590.850169,1643631412.060749,U01AXE0P5M3\\nf03f0fd1-9d3d-4678-a6b9-57b743a80e1b,U01MFQW46BE,,,:+1:,1643628590.850169,1643631452.348059,U02RW07CVTJ\\nf2f2ae49-0cbb-430a-82a1-ff9ae3595e92,U01MFQW46BE,,,:thank_you:,1643628590.850169,1643631474.045549,U01MFQW46BE\\n55b76feb-9cfc-4fcd-a7cb-6d406b931e0b,,6.0,,\"Afternoon guys I am just starting the course.\\nOne quick question. Is it possible to use the Google Free tier again after the one year.\\n\\nI have configured earlier last year.\\nBut looking for a way around how to use a gain.\\nAny workaround Solution please.\\n<@U01AXE0P5M3> @all\",1643631808.988899,1643631808.988899,U02RUUJ2TV5\\n5d1071f1-b346-432f-9e1f-84fc447b2c3b,U02RUUJ2TV5,,,It\\'s for 90 days only. You can create another account though.,1643631808.988899,1643632039.030949,U0290EYCA7Q\\n925cee07-eeec-45b6-9542-f34a6032a0ef,U02RUUJ2TV5,,,Another account with a new Gmail right?,1643631808.988899,1643632183.383909,U02RUUJ2TV5\\n11405977-46a1-4013-aacb-32c38f922a01,U02RUUJ2TV5,,,Please don\\'t tag me,1643631808.988899,1643632211.137459,U01AXE0P5M3\\nc8b663cc-6848-45f5-acbd-149a1c6bc216,U02RUUJ2TV5,,,\"Yes, with different google account.\",1643631808.988899,1643632229.988639,U0290EYCA7Q\\n20be52a3-9bb4-43a5-81cb-5ccc9b107622,,1.0,,\"Hello everyone, maybe I missed this question somewhere up) But has anybody gotten an error for pyarrow in 2020-01 fhv file? It seems to me that there is some \"\"dirty\"\" data\",1643632261.172819,1643632261.172819,U02T65GT78W\\n6726dacc-2e72-491a-bae8-7099ff27ce6d,U02T65GT78W,,,\"Yes, there were a few threads about it. I also have this error. Luckily we don\\'t need 2020 data for week 3\",1643632261.172819,1643632312.770229,U01AXE0P5M3\\nb6187712-9971-4d9e-931d-952411c4e5ab,U02TGS5B4R1,,,Welcome Alexey :smile:,1643627929.124989,1643632521.988449,U02T697HNUD\\n422c0a51-b32f-44c2-99a3-e47353d97c34,U02BVP1QTQF,,,<@U02BVP1QTQF> Btw what do you use to take notes? Just a text editor/IDE or a mark down editor like Typora?,1643397626.412989,1643632854.846149,U02TATJKLHG\\nc58bf970-c3b0-44a4-b8e5-8cb8956a1c9d,U02BVP1QTQF,,,\"I do everything on VScode. I just installed a couple of markdown extensions to make life a little easier (previews, create ToCs and such) but I don’t use any dedicated editors.\",1643397626.412989,1643632928.714879,U02BVP1QTQF\\n296E07DF-6782-40BD-B76B-98FA5583FD4A,U02U0U8LGHM,,,Thanks!,1643560330.528159,1643633619.917809,U02T9GHG20J\\n98cf2775-62a5-4cd4-a1a9-fca4d124d0c4,U02T697HNUD,,,I\\'m glad I read this thread. I was also bracing myself for the 1:17 video. Good to know that there are HW tips in there - will now jump in. Thanks.,1643596998.639849,1643633706.778839,U026040637Z\\nbd04484d-6b1a-4eff-bf5b-9a4ee97ca272,U02BVP1QTQF,,,Ahh I see. I use VScode and Typora. You have some great note taking skills my man :smile:,1643397626.412989,1643633877.587649,U02TATJKLHG\\n9573E497-83B4-479B-8912-68968128333B,,1.0,,Should we be expecting more videos for week3 or is that all for the week??  ,1643634755.092419,1643634755.092419,U02TBCXNZ60\\n1dc15157-ca6a-4bcf-ac95-43b6b5ec7635,U02TBCXNZ60,,,We are working on one more video about Airflow + BigQuery. It is coming soon,1643634755.092419,1643635197.951349,U01DFQ82AK1\\nc4e7f518-26a8-4664-855a-cac1ba0c3ec2,,4.0,,\"Hello everyone, I have potentially silly question. My GCP free tier credit went down from 300 to 223 within a week, without me doing anything apart from homework1. I can\\'t find a breakdown of the costs for the free tier. Anyone with similar experience?\",1643635207.646229,1643635207.646229,U02CGKRHC9E\\n83007b70-c1d2-454d-83a7-cd26d22d34c1,U02CGKRHC9E,,,\"You can find more details in `Billing` &gt; `Report` section\\n<https://console.cloud.google.com/billing>\",1643635207.646229,1643635259.039249,U01DFQ82AK1\\n14c38bb6-abb8-428d-a90f-f4dadd75d1aa,U02CGKRHC9E,,,\"Surprising tho, we have been using GCP quite extensively and still have reached only 248. Better to check if you have some running process reading from BQ on regular basis for example\",1643635207.646229,1643635366.513499,U01DFQ82AK1\\n71eef050-aa67-460b-a2b4-fd5c85dc4948,,2.0,,\"Hello everyone, trying docker-compose-nofrills. airflow webserver start is failing with error \\'\\'\\'/usr/bin/env: ‘bash\\\\r’: No such file or directory \\'\\'\\'. Has anyone faced it. I renamed .env_example to .env and set variables in it.\",1643635535.686019,1643635535.686019,U02NSF7RYP4\\n2f33ff26-8141-4346-b32b-7430dc0d28b9,,,,,,1643635705.530789,U02NSF7RYP4\\nF592ED42-18AA-4524-B4EE-D0B4F25DEB1B,,4.0,,Trying to run the docked-compose build and i get this error. I have no idea from where the copying if the scripts occur. I have been stuck on this for about an hour. Can someone shine me some light?,1643635837.178319,1643635837.178319,U02U07906Q0\\n527add5c-af5a-42d4-be05-984cfbbc5101,U02U07906Q0,,,\"from the github , there\\'s a folder call scripts inside the airflow folder. You just copy this folder into the path where you run the docker-compose command\",1643635837.178319,1643636026.475639,U02T697HNUD\\nC2D14E26-3531-4B3F-9F91-52821A273D15,U02U07906Q0,,,\"Thanks for the <@U02T697HNUD> . Would it be easier to clone their whole repo, rather than trying to configure it all on my own?\",1643635837.178319,1643636219.369209,U02U07906Q0\\n7fa0b455-2985-4bad-9ab5-05bc1d48f5a3,U02SMBGHBUN,,,\"Thanks for sharing what worked for you, nice fix to note\",1643565033.053839,1643636259.096959,U02TWFZURD1\\nb1726657-296f-49e0-8ce8-9b0fc753bb0d,U02U07906Q0,,,Welcome. I prefer configure myself. As you see the github repo consists 2 different docker-compose file.,1643635837.178319,1643636309.109369,U02T697HNUD\\n4a1397cb-1720-4a32-a52f-0082f40b9dcc,U02CGKRHC9E,,,\"<@U01DFQ82AK1>, thank you for your quick reply! It turns out, I am an idiot :flushed:. It\\'s 300 USD = 223 GBP. It took me ages, to set up a valid payment method, and I was used to 270 EUR or 300 USD. In the meantime I forgot that I finally managed to set up a payment method with a UK card.... Embarrassing...\",1643635207.646229,1643636314.865009,U02CGKRHC9E\\n6e022ac0-9d45-4294-b9df-92fff9925544,U02CGKRHC9E,,,\":slightly_smiling_face: i have seen this multiple times, no issues\\nGlad you could find the oversight\",1643635207.646229,1643636376.182939,U01DFQ82AK1\\n0b466123-2026-4712-8df7-e359b102d7dc,U02V4412XFA,,,\"There is an env. variable for the bucket name defined inside \"\"docker-compose.yaml\"\" you should set it to your bucket\\'s ID . also make sure to set the GCP project ID to your own     `GCP_GCS_BUCKET: \"\"dtc_data_lake_pivotal-surfer-336713\"\"` this is at <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/eb1449373a143dd998db074996e29e6f45671bd7/week_2_data_ingestion/airflow/docker-compose.yaml#L64|https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/eb1449373a143dd998[…]29e6f45671bd7/week_2_data_ingestion/airflow/docker-compose.yaml>\",1643570058.391529,1643636649.432989,U02TWFZURD1\\n543842fb-2006-4969-8ecc-34bc85c10715,U02UNB4G739,,,Mac OS. Thanks for letting me know teacher Alexey :upside_down_face:,1643624829.835949,1643636726.704349,U02UNB4G739\\n9e028274-4a72-4ff7-9af6-19313429c892,U02UA8WGF44,,,\"I ran into a similar issue and used \"\"sudo docker ...\"\" to get around it\",1643572099.139669,1643636798.544139,U02TWFZURD1\\nd7c540fc-eee2-4f9d-a400-6568838427c2,,1.0,,\"Hello everyone,\\ngot this error when launching `docker compose up airflow-init`:\\n```airflow-airflow-init-1  |     with open(AIRFLOW_CONFIG, \\'w\\') as file:\\nairflow-airflow-init-1  | PermissionError: [Errno 13] Permission denied: \\'/airflow/airflow.cfg\\'\\nairflow-airflow-init-1  | \\nairflow-airflow-init-1  | ERROR!!!: Too old Airflow version !\\nairflow-airflow-init-1  | The minimum Airflow version supported: 2.2.0. Only use this or higher!\\nairflow-airflow-init-1  | \\nairflow-airflow-init-1 exited with code 1```\\nBut actually I have the airflow version 2.2.3. Anyone that can help me?\",1643637145.978139,1643637145.978139,U02UZ493J56\\ncd006c73-8ce8-4918-8234-44445b22ddfc,U02RUUJ2TV5,,,\"<@U0290EYCA7Q>, do you know if Google screens related accounts? I have several gmail accounts, but they\\'re used as secure recipients from each other. Maybe the best option is to use a brand new, unrelated, even though we have the same name on the credit card...\",1643631808.988899,1643637298.188439,U02GVGA5F9Q\\n6007062b-e60a-4f97-9dbf-692deb54201b,U02UNB4G739,,,Same error here,1643624829.835949,1643637449.691549,U02T0CYNNP2\\n90241fff-a700-4597-a248-50140e75a2a1,U02RUUJ2TV5,,,\"Yes. It didn\\'t allow me to created one after a few ids for the same number. Had to use different number. \\nSame credit card can be used, as long as it is functional.\",1643631808.988899,1643637577.453919,U0290EYCA7Q\\n23ff2199-898f-4e6f-afc8-bebb63aa3fbf,,4.0,,\"hi, at what level do I need to know Python to work as a data engineer? Some say algorithms and data strings, some just airflow operators, because knowing Python is a very ephemeral concept, thank you\",1643638668.420309,1643638668.420309,U02R1SRFPGD\\n5f3c9aa4-aebf-41f5-9269-f17aaa727fcf,U02UZ493J56,,,\"Hi Alberto, did you resolve  `PermissionError: [Errno 13] Permission denied: \\'/airflow/airflow.cfg\\'`   it might be the true culprit\",1643637145.978139,1643639155.102849,U02TWFZURD1\\n74c3b112-bdf3-4887-825b-23f268e0c08c,,,,\"Good afternoon, everyone, please i need help with the error below\\n```$ docker run -it \\\\\\n&gt;   --network=pg-network \\\\\\ndocker: invalid reference format.```\",,1643639468.214329,U02RH0V5K33\\n90b885d2-5584-47ff-a844-07bf69ae2c87,U02TA7FL78A,,,<@U02TA7FL78A> did you solve it ?,1643208872.312700,1643639502.740489,U02QW395UNM\\ne935ecf0-536e-4857-a8b0-93022a4c13a8,,26.0,,\"I was trying to run the below code interactively\\n```docker run -it \\\\\\n  --network=pg-network \\\\  \\n  taxi_ingest:v0001  \\\\\\n    --user=root \\\\\\n    --password=root \\\\\\n    --host=localhost \\\\\\n    --port=5432 \\\\\\n    --db=ny_taxi \\\\\\n    --table_name=yelow_taxi_trips \\\\\\n    --url=\"\"<http://172.31.208.1:8000/yellow_tripdata_2021-01.csv>\"\"```\",1643639563.333059,1643639563.333059,U02RH0V5K33\\n0E22D317-25FA-444B-8504-5D4932E1E310,,14.0,,\"When running the ingest dag, the stage “local_to_gcs_task” fails because of this error.\\n\\nBut i have done the admin roles from week 1 and have terraform running.\\n\\nCan someone please help me? It seems as nothing works….\",1643639767.914879,1643639767.914879,U02U07906Q0\\n44dee4b2-d6c9-4bde-aece-516527b4c2c1,U02U07906Q0,,,\"can you please past the code under the thread , the screenshot is hard to read :slightly_smiling_face:\",1643639767.914879,1643639817.917219,U02TWFZURD1\\n2d2708fb-ea52-42e0-8cb8-83b90110c455,U02TA7FL78A,,,\"<@U02QW395UNM> yeah but I don\\'t which one that solved it, \\n1. Allowed TCP in Windows firewall in my case 50000, 443 \\n2. recreate new service account in GCP\\n3. Started again the terraform from terraform init\",1643208872.312700,1643639823.736549,U02TA7FL78A\\n90e5ab04-f91d-4ccf-9261-6f7a941d9cbb,U02TA7FL78A,,,\"i will try this ,thank you\",1643208872.312700,1643639956.333009,U02QW395UNM\\ne8d6540a-b90c-4678-9a75-ed65a1055920,U02RH0V5K33,,,\"I will make a guess and say that the order might be the issue. can you try moving the name of the image \"\"taxi_ingest:v001\"\"  to the very last line\",1643639563.333059,1643640087.003919,U02TWFZURD1\\n799c88ae-1b35-413d-9f03-4000c35b7c2d,U02U07906Q0,,,\"I seem you no access create storage bucket, I encountered the same problem and change the permission in account services\",1643639767.914879,1643640128.300749,U02TA7FL78A\\n7250B0E6-0838-4A70-AC87-DD8532E6A430,U02U07906Q0,,,<@U02TWFZURD1>  i get the error form log files . “dtc-user@????? does not have storage.objects.create access to the Google Cloud storage,1643639767.914879,1643640148.976999,U02U07906Q0\\n1b2f3f53-f147-4022-a326-b32f45bed0a4,U02RH0V5K33,,,\"```docker run -it \\\\\\n  --network=pg-network \\\\  \\n      --user=root \\\\\\n    --password=root \\\\\\n    --host=localhost \\\\\\n    --port=5432 \\\\\\n    --db=ny_taxi \\\\\\n    --table_name=yelow_taxi_trips \\\\\\n    --url=\"\"<http://172.31.208.1:8000/yellow_tripdata_2021-01.csv>\"\" \\\\\\ntaxi_ingest:v0001  ```\\n\",1643639563.333059,1643640164.711909,U02TWFZURD1\\n320A8102-9BBC-49B9-A618-9B855F79F4D8,U02U07906Q0,,,\"<@U02TA7FL78A>  yes you are correct, but to what should i change it? I just have as the default that we did from week-1 access rights\",1643639767.914879,1643640180.496249,U02U07906Q0\\n32edd548-51be-4336-b744-37e4c3de78b9,U02RH0V5K33,,,\"ok i will try that, though that was the was alexey ran it and it work, one sec let me try it\",1643639563.333059,1643640257.924589,U02RH0V5K33\\n9849451a-61d7-4666-b63f-a5959cb903a5,U02U07906Q0,,,I had a similar issue and the problem on my side was that the service account that I  creates and gave the permissions to wasn\\'t actually linked to the gcp project :upside_down_face:,1643639767.914879,1643640299.769029,U02TWFZURD1\\nC6DD910B-5DF8-48D6-822A-E60ED82B334D,U02U07906Q0,,,<@U02TWFZURD1>  resolution to this is? I still dont know hot fix this,1643639767.914879,1643640357.732279,U02U07906Q0\\n825d92da-1806-43a1-a467-c4288e116248,U02RH0V5K33,,,\"at all time it tried running the first half of the code first and then return the error\\n```docker run -it \\\\\\n  --network=pg-network \\\\```\",1643639563.333059,1643640424.757799,U02RH0V5K33\\n6a00291f-3f8e-4f5a-928e-ff49d2c613de,U02U07906Q0,,,\"I first had to go to the IAM&gt; permissions tab for my intended gcp project . I didn\\'t find the service account listed there so I added it using the \"\"+ ADD\"\" at the top\",1643639767.914879,1643640512.435649,U02TWFZURD1\\nb07b760e-b018-40f3-bb89-74a7befdc788,U02U07906Q0,,,not sure if this is the same issue you are facing.. maybe you can check your project settings as well,1643639767.914879,1643640575.458809,U02TWFZURD1\\nFA3ED833-3292-4FA9-B860-D81DA4F2E2F4,U02U07906Q0,,,Mine are there,1643639767.914879,1643640617.371089,U02U07906Q0\\n1735fac0-7ec7-4278-8d89-10ba5938d732,U02U07906Q0,,,I created new account services actually,1643639767.914879,1643641077.469599,U02TA7FL78A\\n2f064331-88de-448d-9ef4-94a9d4d340fa,U02U07906Q0,,,\"PS: those are the permissions I gave my service account. under \"\"role\"\" these are listed \"\"BigQuery Admin\\n Cloud Asset Service Agent\\n Environment and Storage Object Administrator\\n Media Asset Service Agent\"\"\\n\\nI understand those are not the same as the ones from the setup videos\",1643639767.914879,1643641358.220919,U02TWFZURD1\\n51D0AB91-7F19-4776-93C5-ED22C90E3727,U02U07906Q0,,,\"<@U02TA7FL78A> J woudl appreciete if you could explain me the process, I am really finding it difficult and dont understand\",1643639767.914879,1643641522.532469,U02U07906Q0\\nd89d37f9-8b7a-4d4b-b21e-ace8c2b9a6b6,U02U07906Q0,,,Go to [IAM &amp; Admin](<https://console.cloud.google.com/iam-admin>). Make sure you are in the correct project and have a user listed. In my case that is “dtc-de-user@&lt;project_name&gt;.<http://iam.gserviceaccount.com|iam.gserviceaccount.com>”. If you do not have the previous listed roles click the pencil icon to edit permissions. From there add the needed roles.,1643639767.914879,1643641936.997489,U02TSMSHU5V\\na065f2ee-92d4-4ecc-b2fe-b4e866e22c7e,U02RH0V5K33,,,Oh I realize now that you are trying to run the ingestions script with command line parameters I first thought it was a regular docker run . you are right the order was as it should be,1643639563.333059,1643641959.768209,U02TWFZURD1\\nf5e4bc1d-4d10-4937-982b-e1d983b5e427,U02RH0V5K33,,,,1643639563.333059,1643642112.499149,U02TWFZURD1\\ne9bbe090-6e10-4da3-87e9-2a19f5950427,U02NSF7RYP4,,,\"Try this if you\\'re on VS code\\n\\n<https://qvault.io/clean-code/line-breaks-vs-code-lf-vs-crlf/#:~:text=At%20the%20bottom%20right%20of,has%20the%20correct%20line%20breaks|https://qvault.io/clean-code/line-breaks-vs-code-lf-vs-crlf/#:~:text=At%20the%20bottom%20right%20of,has%20the%20correct%20line%20breaks>.\",1643635535.686019,1643642441.973279,U01AXE0P5M3\\n86163f5c-abfb-4607-8f9a-734bf1390694,U02R1SRFPGD,,,\"Depends. But you need to be proficient in at least one programming language: Java, Scala or Python\",1643638668.420309,1643642574.181159,U01AXE0P5M3\\n65205361-2667-4226-8d28-c3ed43f4c741,U02NSF7RYP4,,,sure. Thanks.,1643635535.686019,1643642700.933609,U02NSF7RYP4\\n35e3c076-a844-440a-8e0f-c51178339bc7,U02RH0V5K33,,,\"thank you, though the error  still exist\",1643639563.333059,1643642715.598509,U02RH0V5K33\\n48f09053-a967-45b5-b022-f4bc66c437ad,U02RH0V5K33,,,<@U01AXE0P5M3> any thought?,1643639563.333059,1643642881.648069,U02RH0V5K33\\n825deffd-9bdf-4528-931d-d200538af640,U02RH0V5K33,,,Can you post the error message you get?,1643639563.333059,1643643017.518529,U01AXE0P5M3\\n335be2a5-e105-4e9e-afd2-25cb644b8ff2,,,,\"I\\'m nearly through week 2 :crossed_fingers: and don\\'t understand why something did work as I expected it to crash!  I moved the credentials file to a new location, as instructed, then I had to restart terraform because I\\'d destroyed the setup after week1.  I thought the credentials file was needed by Terraform to create the infrastructure, so I wouldn\\'t have expected Terraform to have set up access to GCP and BQ.  What am I missing please?\",,1643643121.300709,U01UMAXUPSQ\\n0c2313ac-30bd-45e1-886f-2d9b0811b9cc,,4.0,,\"Hi guys, Have you published results for homework 1?\",1643643145.231689,1643643145.231689,U02T64KSX2S\\n2a9b46dd-1bba-4f53-9515-a21211cf4f5d,U02T64KSX2S,,,\"Not yet. Sorry, I\\'m a bit overwhelmed with things. I\\'ll try to do it this week\",1643643145.231689,1643643268.173889,U01AXE0P5M3\\n6680f070-6467-4c7a-b9e8-7152b041dfbd,U02RH0V5K33,,,Okay I see it outside of the thread. I\\'m not sure what\\'s wrong. Try copy pasting it verbatim without changing anything (apart from url),1643639563.333059,1643643475.849089,U01AXE0P5M3\\n7c3ca3e0-c88b-477d-bef5-edbb36fe176f,U02T64KSX2S,,,\"No problem Alex, thanks\",1643643145.231689,1643643659.361289,U02T64KSX2S\\n48cd808e-4aa4-4ca0-abab-79a0c9658dd1,U02RH0V5K33,,,Also don\\'t spend too much on it if it doesn\\'t start. Later we\\'ll use docker compose which should solve all these problems,1643639563.333059,1643643661.042389,U01AXE0P5M3\\n09ea15af-9131-4c3c-9d2a-72adcd6c841c,U02RH0V5K33,,,\"when i paste it agian, i still got the same error\",1643639563.333059,1643643796.411829,U02RH0V5K33\\n564c80b9-e331-4009-86cd-3a544e41d7ae,U02RH0V5K33,,,,1643639563.333059,1643643841.668429,U02RH0V5K33\\n0571a931-5e02-4c96-ab4d-4ab036c9da60,,2.0,,\"Please, what\\'s the link to post live questions for the office hours?\",1643644145.629879,1643644145.629879,U02SBTRTFRA\\n3966a616-ed17-474c-a2c5-bd4fa8389949,U02UA8WGF44,,,\"It was a problem with the permissions of the data folder  for postgres. I was able to solve it, thanks\",1643572099.139669,1643644200.252509,U02UA8WGF44\\n59a01b6a-c0e0-423a-8bda-a9a70e92f96b,U02T64KSX2S,,,By results you mean the answers or the scores? The answers were published in a youtube video if you are looking for them just incase.,1643643145.231689,1643644604.116069,U02TATJKLHG\\nf1480850-cc37-49e5-ba9d-094d2971d86a,U02SBTRTFRA,,,<https://app.sli.do/event/67N1Ld2jEtaHh5f4eRKoS5/live/questions>,1643644145.629879,1643644616.630429,U02QH3TBA11\\n63419f36-5643-4d56-907a-dd8f01e8ce99,,7.0,,\"I was able to run the highly demanding full version in my laptop. But I am pretty sure it was going to collapse after some hours.\\n\\nWhile changing to the light version I had some access problem to the .sh file using the no-frills version. I am using the WSL/Windows.\\nWith a help from Sejal I got this solution:\\n• I removed the\\xa0`redis`\\xa0queue,\\xa0`worker`,\\xa0`triggerer`\\xa0&amp; `flower` services from the docker-compose.yaml file. Be aware that after line 71 you have a dependecy to Redis too. You have to take off!\\n• I changed Core Executor mode from\\xa0`CeleryExecutor`\\xa0 to\\xa0`LocalExecutor`\\n• Stopped all containers and cleaned everything with `docker-compose down --volumes --rmi all`\\n• Then ran everything like it was a full version\\nNow I only have this services running. It is still highly demanding but works better.\",1643644630.988259,1643644630.988259,U02CD7E30T0\\n5df5a9ac-72ba-474c-baa3-581962119ac3,U02RTJPV6TZ,,,\"some help, wonder what am missing, trying to connect the first docker-compose in week one to that of week2 and am getting this error\\n\\n(base) DELL@de-zoomcamp:~/Zcamp/week_1/docker_sql/docker_compose$ docker-compose ps\\nservices.pgdatabase.networks must be a list\",1641798212.098100,1643644884.380089,U02RTJPV6TZ\\n380ba96d-fa63-4d2b-8b38-fa492b053ee3,U02SBTRTFRA,,,Thank you.,1643644145.629879,1643644992.529969,U02SBTRTFRA\\n14E22193-A4F4-47C0-9AFE-2EF3F08D0661,,3.0,,Is there a link for office hours?,1643645020.299419,1643645020.299419,U02C30MKXPS\\n566b331b-8a07-4db8-a597-ab2e738b1b9b,U02C30MKXPS,,,It\\'s live on the YouTube channel.,1643645020.299419,1643645073.572069,U02SBTRTFRA\\nbc840005-89a4-428d-ab74-4e18a9d154e9,U02C30MKXPS,,,<https://youtu.be/yZ-d7nItpB8>,1643645020.299419,1643645076.100079,U0290EYCA7Q\\n014fdea9-858b-48a0-8c28-328eb1e27070,U02C30MKXPS,,,<https://datatalks-club.slack.com/archives/C02V1Q9CL8K/p1643644746281659|https://datatalks-club.slack.com/archives/C02V1Q9CL8K/p1643644746281659>,1643645020.299419,1643645077.698749,U01AXE0P5M3\\n069d448a-d751-4045-b5c7-82e9f061a5f0,,2.0,,Hi - what\\'s the link to January 31\\' Office Hours Slido?,1643645187.538549,1643645187.538549,U02DG8S854K\\n56cad353-ecc0-4b10-bb29-fcf52cebdbf8,U02DG8S854K,,,<https://datatalks-club.slack.com/archives/C02V1Q9CL8K/p1643644746281659|https://datatalks-club.slack.com/archives/C02V1Q9CL8K/p1643644746281659>,1643645187.538549,1643645308.641119,U02SBTRTFRA\\nb3ca99af-cf6e-4c06-9d78-256dce917c63,U02R1SRFPGD,,,What exactly does it mean to know Python? Very vague),1643638668.420309,1643645328.316999,U02R1SRFPGD\\ne27c4b6b-3738-4d5e-b861-ac1dc1afc24b,U02DG8S854K,,,tx!,1643645187.538549,1643645330.222759,U02DG8S854K\\n1FF5DBDB-6826-43C4-9445-07C53BFB859D,U02R1SRFPGD,,,\"Difficult to say. I think there\\'s just a point when learning a language where you just sort of “get it”. \\n\\nIf you come across a problem, you may not know the exact syntax, but you\\'d be capable of googling the problem, finding the syntax, and refactoring it for your own purposes.\\n\\nThere\\'s basic things that you should know and understand. E.g. for loops, while loops, variables, data types, basic data structures etc\\n\\nJust my opinion \",1643638668.420309,1643645557.790849,U02U34YJ8C8\\na13d1c21-3761-48f5-8ae8-f4be214ddea0,,1.0,,\"One airflow related but conceptual query (might be a silly question) - we have created a new Docker image with our customized libraries related to GCP etc. and now we are running this docker image which means the image is static now. Then , how this image is able to detect the script which we have on our local machine and is not part of Dockerfile. My understanding was - once i build a docker image and if i want to add a new file e.g. DAG - i need to rebuild the image by instructing docker to build the new version of container with a DAG specific py file , similar to the way we did in our week1 exercise for our ingest-data.py... am i missing something here conceptually .....\",1643645855.993989,1643645855.993989,U02U9MNQG7Q\\n2e85c546-db38-4999-a51a-2e80a17de59d,,20.0,,\"Hi all,\\nI try to run the new nofrill-docker compose but get the following error and the webserver container is always restarting because of that, I think:\\n`dtc-de-webserver-1  | ModuleNotFoundError: No module named \\'airflow\\'`\\n```\\nNAME                 COMMAND                  SERVICE             STATUS              PORTS\\ndtc-de-postgres-1    \"\"docker-entrypoint.s…\"\"   postgres            running (healthy)   5432/tcp\\ndtc-de-scheduler-1   \"\"/usr/bin/dumb-init …\"\"   scheduler           running             8080/tcp\\ndtc-de-webserver-1   \"\"./scripts/entrypoin…\"\"   webserver           restarting ```\",1643646196.837139,1643646196.837139,U030FNZC26L\\n8dd268f4-4ddc-4428-bf9a-485e4c054851,U02U5G0EKEH,,,noted.,1643606923.092969,1643646668.225489,U02U5G0EKEH\\ne6d0d8b8-2a81-47c7-9e83-58a79c3b1576,,,,Hi guys. Airflow task crashed at last task,,1643646841.767819,U030MHYAUNS\\n7291c0ba-197f-453c-b8d1-ccc424c4a784,,4.0,,\"```AIRFLOW_CTX_TASK_ID=bigquery_external_table_task\\nAIRFLOW_CTX_EXECUTION_DATE=2022-01-30T00:00:00+00:00\\nAIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-01-30T00:00:00+00:00\\n[2022-01-31, 16:06:05 UTC] {taskinstance.py:1700} ERROR - Task failed with exception\\nTraceback (most recent call last):\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1329, in _run_raw_task\\n    self._execute_task_with_callbacks(context)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1455, in _execute_task_with_callbacks\\n    result = self._execute_task(context, self.task)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1511, in _execute_task\\n    result = execute_callable(context=context)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py\"\", line 1192, in execute\\n    impersonation_chain=self.impersonation_chain,\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/hooks/bigquery.py\"\", line 118, in __init__\\n    impersonation_chain=impersonation_chain,\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/common/hooks/base_google.py\"\", line 214, in __init__\\n    self.extras = self.get_connection(self.gcp_conn_id).extra_dejson  # type: Dict\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/hooks/base.py\"\", line 68, in get_connection\\n    conn = Connection.get_connection_from_secrets(conn_id)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/connection.py\"\", line 410, in get_connection_from_secrets\\n    raise AirflowNotFoundException(f\"\"The conn_id `{conn_id}` isn\\'t defined\"\")\\nairflow.exceptions.AirflowNotFoundException: The conn_id `google_cloud_default` isn\\'t defined\\n[2022-01-31, 16:06:05 UTC] {taskinstance.py:1277} INFO - Marking task as UP_FOR_RETRY. dag_id=axixa2, task_id=bigquery_external_table_task, execution_date=20220130T000000, start_date=20220131T160604, end_date=20220131T160605\\n[2022-01-31, 16:06:05 UTC] {standard_task_runner.py:92} ERROR - Failed to execute job 50 for task bigquery_external_table_task```\\n\",1643646858.324149,1643646858.324149,U030MHYAUNS\\nebaa60ec-e5ed-468b-b6b4-4cdcda2a03a2,,,,whats wrong with this?,,1643646873.542399,U030MHYAUNS\\n802224dc-bbad-4ec9-8c71-3bafe1dc7299,,2.0,,\"<https://gist.github.com/ahvahsky2008/cd46c00e095b068e070e15511d3256a0>\\ndocker-compose\",1643646908.736339,1643646908.736339,U030MHYAUNS\\n25f31011-2067-462a-b05d-990c9f9615d9,U02UBQJBYHZ,,,I don\\'t know the upload speed. It looked like it was trying to fill many of the tables at once. At least it looked that way. They all only had one or two chunks in them but there were at least seven tables.,1643598603.077489,1643647097.646459,U02UBQJBYHZ\\n6eec8987-d9d0-4b1d-88de-0e8dbfaf6eed,U02T64KSX2S,,,I mean the scores,1643643145.231689,1643647463.028399,U02T64KSX2S\\ne6865d4f-e3fa-4e89-83e6-533017e59b7f,,26.0,,\"hello, I\\'m trying to dockerize the ingest_data.py file, but I\\'m getting an error: error checking context: \\'no permission to read from \\'/home/fernanda/Documentos/Zoomcamp/Docker_sql/data/ny_taxi_postgres_data/base/1/13307\\'\\'.\",1643647664.742649,1643647664.742649,U030PFH5CRX\\nd66c7b8c-3e6d-44cf-aee5-ffcca8c034c6,U030PFH5CRX,,,did you manually change the permissions for the ny_taxi_postgres_data?,1643647664.742649,1643648885.966689,U01AXE0P5M3\\n462763b7-f2c2-4694-a50a-d9094b4b6940,U030FNZC26L,,,I have the same problem and haven\\'t figured out what\\'s wrong,1643646196.837139,1643648906.370969,U01AXE0P5M3\\n103186e8-db19-48ac-bf34-3f59a869616d,U030FNZC26L,,,\"so far I use the \"\"usual\"\" docker compose file\",1643646196.837139,1643648921.802559,U01AXE0P5M3\\nd670c0e6-d0d7-4740-91a3-836f6ab74bfc,U02CD7E30T0,,,thanks! I\\'ll give it a try! so no need to run anything from scripts?,1643644630.988259,1643648970.433369,U01AXE0P5M3\\ncfbc4d36-cb91-42d3-b45b-6faea11c438f,U02CD7E30T0,,,did you need to do the init thing for the reduced setup?,1643644630.988259,1643648985.225729,U01AXE0P5M3\\n5f2090f0-6123-4837-9d2b-faf2bab45cec,U030PFH5CRX,,,\"yes, i run this command sudo chmod a+rwx ny_taxi_postgres_data\",1643647664.742649,1643649002.612109,U030PFH5CRX\\nbc456d6b-2d5c-4143-a5f4-e386f11ba4e8,U02TMP4GJEM,,,<@U02TMP4GJEM> try safari chrome doesn\\'t work sometimes,1643499686.008109,1643649047.470089,U02T8HEJ1AS\\ne6910e93-f8c4-4cc8-ab33-b7cd2129bc5e,U030PFH5CRX,,,what happens if you don\\'t do that and let docker create this folder?,1643647664.742649,1643649049.010319,U01AXE0P5M3\\n6ceff948-298e-4051-96e1-fa9fb249ce08,U02R1SRFPGD,,,\"I\\'d add that in addition to syntax it\\'s also about knowing the ecosystem around a programming language - like how you structure your projects, how you run tests, how you manage dependencies, how you build things, etc\",1643638668.420309,1643649118.895289,U01AXE0P5M3\\na3630ee4-bfa4-4e57-a1e5-063532c5183e,U030PFH5CRX,,,showed me the empty folder,1643647664.742649,1643649119.215829,U030PFH5CRX\\n4efce531-0c51-45d4-bf74-c02409618f40,U02RH0V5K33,,,okay I see now - your one command somehow gets split into two,1643639563.333059,1643649161.529629,U01AXE0P5M3\\n9bbf2177-2570-4282-9a1a-affed8f28eda,U030PFH5CRX,,,and then I tried giving permissions to each folder that told me an error but it was endless,1643647664.742649,1643649182.334969,U030PFH5CRX\\n6e761196-8b14-4014-8fbe-cd1906320ae1,U02RH0V5K33,,,\"maybe remove the \"\"\\\\\"\"s and put everything on one line\",1643639563.333059,1643649195.361539,U01AXE0P5M3\\nb93ea328-c688-4445-9ac4-f52f99bd3bea,U030PFH5CRX,,,\"I personally think you shouldn\\'t interfere with permissions - else you risk having weird problems like the one about (not sure if it\\'s related, but might as well be)\",1643647664.742649,1643649248.274489,U01AXE0P5M3\\n9c3854d3-e198-47ff-9aea-486beb3c61e7,U030PFH5CRX,,,\"so maybe do the following\\n\\n• delete this data folder\\n• then create the data folder again (should be empty)\\n• do mapping `-v /path/to/your/project/data/ny_taxi_postgres_data:/i/dont/remember/what\\'s/here`\\n• do docker run \\n• and test it again \",1643647664.742649,1643649371.609159,U01AXE0P5M3\\n5daf90eb-4cf3-47f1-b9ad-5a0720b1cdf9,U02UAFF1WU9,,,<@U02TJ69RKT5> Thanks! Took me a while and several failed runs before I found your redemptive hint! It works!!,1643411641.340209,1643649547.879339,U02CGKRHC9E\\n1769ad02-5fe1-4db1-9136-aef31334e2cf,U030PFH5CRX,,,\"when i tun docker i should run this command docker run -it \\\\\\n  -e POSTGRES_USER=\"\"root\"\" \\\\\\n  -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n  -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n  -v c:/Users/alexe/git/data-engineering-zoomcamp/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n  -p 5432:5432 \\\\\\n  --network=pg-network \\\\\\n  --name pg-database \\\\\\n  postgres:13 ? this has the network to pg-admin\",1643647664.742649,1643649555.500649,U030PFH5CRX\\n33f3bcb3-3a2c-438d-8b18-69c9948fca88,U030PFH5CRX,,,\"-v data/ny_taxi_postgres_data:/var/lib/postgresql/data, This would be the route that I should map?\",1643647664.742649,1643649657.476969,U030PFH5CRX\\n5a44d25f-8519-4dd3-bc1e-0d5e5f7027c0,U030PFH5CRX,,,`-v c:/Users/alexe/git/data-engineering-zoomcamp/week_1_basics_n_setup/2_docker_sql/data/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\`,1643647664.742649,1643649944.064399,U01AXE0P5M3\\n3ed4ee5d-6012-464c-837b-649b6e566b78,U030PFH5CRX,,,maybe don\\'t use `alexe` though - it\\'s my username :smiley: yours might be different,1643647664.742649,1643649968.299949,U01AXE0P5M3\\n4e04f2c7-5053-4cd9-ae26-6e028beb8839,U030PFH5CRX,,,\"but I think you\\'re doing it, right? I mean set it to `fernanda`\",1643647664.742649,1643649982.748529,U01AXE0P5M3\\n619e3b81-5029-4fc7-8fdd-717bd1a959ab,U02CD7E30T0,,,\"Yes, the airflow-init setup will be needed to initialize the db &amp; authenticate airflow user\",1643644630.988259,1643650268.319609,U01DHB2HS3X\\n70f2abbb-426a-40d3-aa5e-95d1f452ab4c,U030PFH5CRX,,,im in ubuntu and when i run -v gives me \\'order not found\\',1643647664.742649,1643650394.670089,U030PFH5CRX\\n4d741362-7b06-4ace-b794-c5a90f05a621,U030FNZC26L,,,\"It’s sad that the “nofrills” version is not compatible with Windows/WSL (if you’re using that), alternatively you can also try this out:\\n\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1643644630988259>\",1643646196.837139,1643650417.236999,U01DHB2HS3X\\ndf982831-bca0-49d5-a6a5-ce0f2ccbdf31,U02U5G0EKEH,,,\"I deleted the dag and refreshed the airflow. It worked, thanks for offering to help\",1643606875.920119,1643650475.212469,U02U5G0EKEH\\n3bd13a0b-cf0d-4d10-9a56-c56d1e6045b0,U030PFH5CRX,,,,1643647664.742649,1643651133.955259,U030PFH5CRX\\n9cce5bd6-62b5-4ec5-852b-a3b47e9bbae4,,7.0,,\"Hi all, just a feedback to all our teachers :grin: (not sure if this was already suggested or not).\\nSince running this course is demanding a lot of your personal time in recording the videos, coding, helping with our issues, in case of future courses, have you considered recording everything up front before the start of the course?\\nLess pressure on you, all videos would be available immediately (you could release them each week or all at the beginning of the course) and you would not need to find the time to go thourgh issues/difficulties we encounter throughout the course.\",1643651160.020469,1643651160.020469,U02VDU19WG4\\nbb430e07-e274-4635-b066-231c46074401,U02VDU19WG4,,,\"Great suggestion and it probably would work great for people who are more organized than me =) \\n\\nOn the bright side, if we decide to go with the 2nd iteration, the videos will be ready already =)\",1643651160.020469,1643651267.371859,U01AXE0P5M3\\n3d9885b3-92a2-4bb1-ba5d-2efa3f8dc767,U030MHYAUNS,,,\"Can you try two things?\\n1. add `/` before your path and after `=` in `AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT`  like this `AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT: \\'google-cloud-platform://?extra__google_cloud_platform__key_path=/./creds/google-credentials.json\\'`\\n2. I don\\'t think this matters but can you define the `GOOGLE_APPLICATION_CREDENTIALS` before the `AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT` ? \\nI saw this is done in the original yaml file.\",1643646858.324149,1643651459.115399,U02TATJKLHG\\ndd1a4774-170d-476d-b87f-ec71a505bf0b,U02CD7E30T0,,,Yes. I kept the init on Sejal recommendations. And it makes sense because we need to initially Airflow somehow.,1643644630.988259,1643652160.789339,U02CD7E30T0\\ne075cf31-c569-4305-95d8-68ce090f57a8,U02VDU19WG4,,,\"Hi Daniele, have you considered we would all have preferred that?\\n\\nBut like Alexey said, what you say could be a fair point, but only in terms of improving future revisions. For the current one, there are a lot of factors involved.\\n\\nFirstly, we had to announce the course start-date in advance, to gauge an idea of the audience.\\n\\nSecondly, since this is the first iteration of this course, we did not know in advance what exactly to expect of the target audience (students), their proficiency level, the variety of device configs (OS’s, for example) used, or of their ability to solve bugs on their own &amp; adapt to workarounds independently without needing full background knowledge (vs. those waiting for explicit instructions or custom setups when stuck). This is a learning experience even for us instructors, especially for such a large &amp; diverse audience.\\n\\nThirdly, this free course also implies a community-based learning style, where we encourage the students to learn from each other &amp; help shape our course; perhaps us instructors could also learn from you on preparing better frameworks/setups based on what works for most students. We also prefer to give students some open space to research on some extra content out of their interest (especially for the pre-requisites/technologies used), without having to spoon-feed them with a pre-defined bundle within the course.\",1643651160.020469,1643652455.494539,U01DHB2HS3X\\n9c9f4596-d8ca-4d8f-87ec-1cf616d3831c,U030FNZC26L,,,\"Thank you. I use ubuntu. Some people mentioned about setting group id in `.env` and some suggested installing airflow using pip, but none of them work.\",1643646196.837139,1643652707.989069,U030FNZC26L\\n336098a7-ee9f-42ea-85a6-70a80371f23c,U02VDU19WG4,,,\"Totally agree Sejal that it is good for us to stumble a bit with the code. I don\\'t think we should immediately get your support at the first issues encountered.\\nMy point was more for making things easier for you and the planning for future zoomcamps, it is too late for this one now :grin:\\nAnyway, thank you again for the effort you all are putting into this!\",1643651160.020469,1643653857.755229,U02VDU19WG4\\n33fc2593-9d03-40c9-81b4-731aefa35ed9,U02RH0V5K33,,,ok i will try it again,1643639563.333059,1643656181.483669,U02RH0V5K33\\nc89fe80f-9cd3-4943-83e4-6782242b3ad9,U02VDU19WG4,,,\"if you have other suggestions, please share them as well!\\n\\nwhen it comes to me personally I\\'m a big procrastinator and can\\'t do anything unless there\\'s a deadline and it\\'s soon. I knew well in advance about the course, but only started preparing one week before :sweat_smile:\",1643651160.020469,1643656229.209199,U01AXE0P5M3\\ncc02cf31-9ef1-4461-82ea-ff29af6e4ed1,U02B8U0QZEK,,,Hi! I think the best way to learn SQL is to practice. The best resource for such purposes that I know <https://www.sql-ex.ru/?Lang=1>,1643312794.147200,1643656273.438389,U02DNSC6Z52\\n63bd3415-0c71-4d88-9f2c-bc0ec71d2f65,U030PFH5CRX,,,try removing the first line and execute the command starting from `docker`,1643647664.742649,1643656424.605689,U01AXE0P5M3\\n7bcd24cf-682e-42c7-ae24-df2221682d49,U02CD7E30T0,,,\"thanks, I\\'ll try that\",1643644630.988259,1643656471.295019,U01AXE0P5M3\\nca452e5c-f3a8-443d-8ff4-1fb6145131a6,U02VDU19WG4,,,The best thing about the way you\\'re presenting this course is that the versions of the code that we download are the same as the ones you use. I feel like I\\'m not learning some stale version of something that\\'s already useless by the time i watch the video.,1643651160.020469,1643656670.432599,U02UBQJBYHZ\\nB60117BE-9599-4775-8C6A-72689A18272F,U02VDU19WG4,,,\"<@U01AXE0P5M3> In retrospect to the productivity religion and its gurus, who plan even the afterlife, you are a confirmation that you can achieve big things in life even if you are a mad procrastinator!  :smile:\",1643651160.020469,1643656708.056959,U02UX664K5E\\n65242507-be66-4a6e-8287-c9c96972e550,U02RH0V5K33,,,\"Thank you, it ran but now this error\\n```sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not connect to server: Connection refused\\n        Is the server running on host \"\"localhost\"\" (127.0.0.1) and accepting\\n        TCP/IP connections on port 5432?\\ncould not connect to server: Cannot assign requested address\\n        Is the server running on host \"\"localhost\"\" (::1) and accepting\\n        TCP/IP connections on port 5432?```\",1643639563.333059,1643656742.723019,U02RH0V5K33\\nbbf4580a-e261-44b9-befa-f616ec02d105,U030MHYAUNS,,,\"```    GOOGLE_APPLICATION_CREDENTIALS: /opt/airflow/creds/google-credentials.json\\n    AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT: \\'google-cloud-platform://?extra__google_cloud_platform__key_path=/opt/airflow/creds/google-credentials.json\\'\\n    ```\",1643646858.324149,1643657143.584089,U030MHYAUNS\\n0f3d2b34-1255-4ae7-a7f0-c5ba78754950,U030MHYAUNS,,,i try pass full path - but it dont work too,1643646858.324149,1643657161.688049,U030MHYAUNS\\nf011f859-73a4-430d-b934-386016310ff1,,8.0,,\"Guys i just started week two today. I\\'m also reading for my Professional Graduate Diploma  exam simultaneously that is why I have been slow.\\nPlease I have a question, I have docker in my Linux how do i configure the docker to use 4GB memory or no need for that\",1643657174.377959,1643657174.377959,U02UKLHDWMQ\\n377415b6-dfa3-407e-8a79-3625acf289c4,U02UKLHDWMQ,,,\"What is the output of this command:\\n```docker run --rm \"\"debian:buster-slim\"\" bash -c \\'numfmt --to iec $(echo $(($(getconf _PHYS_PAGES) * $(getconf PAGE_SIZE))))\\'```\\n\",1643657174.377959,1643657247.478299,U030FNZC26L\\nf104412e-790c-48d0-9673-e6476245369d,U02UKLHDWMQ,,,\"Unable to find image \\'debian:buster-slim\\' locally\\ndocker: Error response from daemon: Head \"\"<https://registry-1.docker.io/v2/library/debian/manifests/buster-slim>\"\": Get \"\"<https://auth.docker.io/token?scope=repository%3Alibrary%2Fdebian%3Apull&amp;service=registry.docker.io>\"\": dial tcp: lookup <http://auth.docker.io|auth.docker.io>: Temporary failure in name resolution.\\nSee \\'docker run --help\\'.\",1643657174.377959,1643657582.686849,U02UKLHDWMQ\\n2d2fe40e-ee81-4a97-8b1f-e5da8337cc5a,U02UKLHDWMQ,,,that is what i got,1643657174.377959,1643657955.421409,U02UKLHDWMQ\\na779e0a6-50d8-424c-b45f-b003c9bac77a,,1.0,,\"I\\'m still trying to set up airflow.. When I ran `docker-compose build` to build the Dockerfile.. I had this obscure message:\\n```postgres uses an image, skipping\\nredis uses an image, skipping\\nairflow-init uses an image, skipping\\nairflow-triggerer uses an image, skipping\\nairflow-worker uses an image, skipping\\nairflow-scheduler uses an image, skipping\\nairflow-webserver uses an image, skipping\\nflower uses an image, skipping```\\nI reasoned that its trying to work on the `docker-compose.yaml` file instead of `Dockerfile`.. and i thought that it might be because my docker-compose version was `1.29.2`.. But I have upgraded it to `2.2.3` now.. and its now giving me a different error message:\\n```6 errors occurred:\\n\\t* unable to prepare context: unable to evaluate symlinks in Dockerfile path: lstat /home/toluwani/Internships/DE-Zoomcamp/week2/airflow/Dockerfile: no such file or directory\\n\\t* unable to prepare context: unable to evaluate symlinks in Dockerfile path: lstat /home/toluwani/Internships/DE-Zoomcamp/week2/airflow/Dockerfile: no such file or directory\\n\\t* unable to prepare context: unable to evaluate symlinks in Dockerfile path: lstat /home/toluwani/Internships/DE-Zoomcamp/week2/airflow/Dockerfile: no such file or directory\\n\\t* unable to prepare context: unable to evaluate symlinks in Dockerfile path: lstat /home/toluwani/Internships/DE-Zoomcamp/week2/airflow/Dockerfile: no such file or directory\\n\\t* unable to prepare context: unable to evaluate symlinks in Dockerfile path: lstat /home/toluwani/Internships/DE-Zoomcamp/week2/airflow/Dockerfile: no such file or directory\\n\\t* unable to prepare context: unable to evaluate symlinks in Dockerfile path: lstat /home/toluwani/Internships/DE-Zoomcamp/week2/airflow/Dockerfile: no such file or directory```\",1643658369.577389,1643658369.577389,U02T9JQAX9N\\n21d80b95-48de-4248-acea-90216c050910,U02RH0V5K33,,,replace localhost with the name of pgadmin container,1643639563.333059,1643658819.968239,U01AXE0P5M3\\n415a58c5-9764-4067-a736-560bfe0e1856,U02T9JQAX9N,,,I have gotten the source of the error.. I used `./Dockerfile` instead of `./DockerFile`,1643658369.577389,1643659112.093979,U02T9JQAX9N\\nad9192db-e88e-4abf-ac17-76f2210bf773,U030PFH5CRX,,,\"i run it but the folder data is empty, its okay?\",1643647664.742649,1643659302.108569,U030PFH5CRX\\n930ed4f7-6463-4613-a145-7cd53f3d8dcd,U02UKLHDWMQ,,,\"for me it\\'s:\\n```Unable to find image \\'debian:buster-slim\\' locally\\nbuster-slim: Pulling from library/debian\\n6552179c3509: Pull complete \\nDigest: sha256:f6e5cbc7eaaa232ae1db675d83eabfffdabeb9054515c15c2fb510da6bc618a7\\nStatus: Downloaded newer image for debian:buster-slim\\n16G```\\n\",1643657174.377959,1643659336.270869,U030FNZC26L\\ne82b8829-76af-4a28-9c2e-40a491c08409,U02RH0V5K33,,,ok one sec,1643639563.333059,1643659384.114479,U02RH0V5K33\\n0374324f-42fb-4d80-bc00-a49e9fc215f5,U02UKLHDWMQ,,,why this error,1643657174.377959,1643659566.336109,U02UKLHDWMQ\\n5e204914-161e-433d-8fd4-7878cfcc47fc,U02UKLHDWMQ,,,\"not sure. Here is the source I used:\\n<https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html>\",1643657174.377959,1643659936.719559,U030FNZC26L\\n155ddc76-851f-4aba-9737-75c6e7b2aa19,U02RH0V5K33,,,\"```sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not connect to server: Connection refused\\n        Is the server running on host \"\"pgadmin\"\" (172.20.0.3) and accepting\\n        TCP/IP connections on port 5432?```\",1643639563.333059,1643659987.628169,U02RH0V5K33\\n3632c4b6-8534-4baf-b82a-5675a89aacba,U030PFH5CRX,,,Can you connect to it?,1643647664.742649,1643660047.342779,U01AXE0P5M3\\nb7d54275-6e48-4518-bc39-fce9feb498d7,U02RH0V5K33,,,I mean postgress sorry,1643639563.333059,1643660104.341389,U01AXE0P5M3\\n4e0600fe-d1c7-47ff-9c7e-91c587849c01,U02RH0V5K33,,,oh ok,1643639563.333059,1643660154.834139,U02RH0V5K33\\n05cb40b8-44b0-42fe-9b1b-e4db5387619a,,1.0,,\"The DockerFile that was used in the `Setting up Airflow with Docker-Compose` video is different from the one on the Github site.. There were two additional lines\\n```COPY scripts scripts\\nRUN chmod +x scripts```\\nI had an error because of this.. I noticed there is a `scripts` folder on the Github repo now.. what is its use?\",1643660691.827249,1643660691.827249,U02T9JQAX9N\\n76fb39fa-0158-4447-bc18-d8ed84175b3a,,16.0,,\"Hi everyone! I’m not sure how many of you have managed to finish the homework for Lesson 2, but if you’re about to get to it, you might find that running the DAGs takes a looooong time because the files you need to download and then upload to GCP are kinda big (the parquet files less so, but the CSVs are pretty substantial). Also, Airflow is kind of a resource hog.\\n\\nIf you have a very fast connection and/or a kickass computer then cool, but if not, then I strongly suggest you use a Cloud Compute VM instance. Alexey already suggested this somewhere, maybe in a video or in the repo notes, I don’t remember.\\n\\nYou should watch Alexey’s video on <https://www.youtube.com/watch?v=ae-CV2KfoN0&amp;list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&amp;index=12> . If you have already but need a quick reference, I’ve created a gist with all of the necessary steps to get a VM running with docker, docker-compose and terraform. I’ve also figured out a simplified way to configure SSH access to VM instances using the gcloud sdk. You can read the gist here: <https://gist.github.com/ziritrion/3214aa570e15ae09bf72c4587cb9d686>\\n\\nGood luck and keep up the good work, everyone!\",1643661114.567919,1643661114.567919,U02BVP1QTQF\\n765bd91d-9e42-4fec-884a-08d261229b9b,U02RH0V5K33,,,it worked. thanks,1643639563.333059,1643661127.900229,U02RH0V5K33\\n6F0FF0E4-18AB-4652-B5FC-A56081ED3BD7,U02BVP1QTQF,,,Thanks for the tips! How long did it take you to do the homework? I\\'ve got a lot on this week,1643661114.567919,1643661200.911369,U02U34YJ8C8\\ne84cf1fb-77b9-4581-8810-85e4a11b71d5,U02BVP1QTQF,,,\"yea definetely, I have been doing everything on VM now\",1643661114.567919,1643661414.987289,U02UBV4EC8J\\n5E3448F2-0191-4872-842B-6816B4F35C7D,U02VDU19WG4,,,I agree with Cris! I don\\'t think the material would be as useful if it was super polished. Watching Alexey troubleshoot errors is even more instructive than when everything goes smoothly,1643651160.020469,1643661512.235319,U02T9GHG20J\\n4faf2dfe-bed4-42ee-be3d-7c936662633d,U02BVP1QTQF,,,\"<@U02U34YJ8C8> It took me longer than I expected because I had a very annoying bug in the DAG files which took me ages to get rid of. Once I found it, running the DAGs locally on my laptop made me worry about the integrity of the cooling fans* and my wifi network was unusable, so I ended up creating a VM and running everything there.\\n\\nIn total I spent Sunday from 5pm to about 11pm, but the code itself was maybe 2 hours, probably less if you’re not a dummy like me. Running the 3 DAGs on the VM instance took like a little less than an hour, maybe?\\n\\n(*) I actually wasn’t, my laptop was toasty but that’s it, please don’t take me very seriously.\",1643661114.567919,1643661526.701359,U02BVP1QTQF\\nF45B090F-20A6-491C-BC3E-AEF97E5192E6,U02SUH9N1FH,,,\"Is the issue resolved? \\n\\nTried to connect to gcp database from my localhost… didn\\'t work.\\n\\nWas able to open jupyter and pgadmin \",1643130741.125400,1643661677.894159,U0305BX4ZCH\\nc4ecbbe4-342a-4dc3-ba79-4c2d2810155a,U02UX664K5E,,,<@U02UBQJBYHZ> Did you use the no-frills yaml or the regular docker-compse.yaml. I am experiencing this issue as well,1643512731.472799,1643661750.074079,U02UAFF1WU9\\nb2fef39f-ab30-4c91-b727-a9901ccb6d05,U02T9JQAX9N,,,that\\'s used for an alternative way of running airflow with compose - <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_2_data_ingestion/airflow/3_setup_nofrills.md>,1643660691.827249,1643662047.665019,U01AXE0P5M3\\ne18766bc-614d-49ba-bd70-7f9986a58491,U02UKLHDWMQ,,,\"so i did docker system info to see the memory allocation it shows this\\n\\nOperating System: Ubuntu 20.04.3 LTS\\n OSType: linux\\n Architecture: x86_64\\n CPUs: 4\\n Total Memory: 3.771GiB\\n\\nIs this sufficient\",1643657174.377959,1643662150.658799,U02UKLHDWMQ\\nb2557d39-f8e2-4499-b627-a5370bb93ed3,U02BVP1QTQF,,,\"&gt; made me worry about the integrity of the cooling fans and my wifi network was unusable\\n:scream:\",1643661114.567919,1643662283.583899,U01AXE0P5M3\\n8ad1d973-b521-4df4-ace0-302feb715c66,U02BVP1QTQF,,,my computer just freezes :smile:,1643661114.567919,1643662293.139669,U01AXE0P5M3\\nbbc7ee79-5e96-4aba-8796-f2737dd8d45d,U02RH0V5K33,,,\"one last question, do i need to change anything in the docker compse yml file?\",1643639563.333059,1643662293.624139,U02RH0V5K33\\n5bfc740f-d84e-47dc-ae2b-b1c171fe9ded,U02RH0V5K33,,,nope,1643639563.333059,1643662324.973799,U01AXE0P5M3\\nf699830d-1131-4e75-9587-8af10da8ad17,U02BVP1QTQF,,,\"yeah, who could have guessed that running 3 separate DAGs with 7 concurrent runs in total could have any effect on the performance of my computer, right? :upside_down_face:\",1643661114.567919,1643662393.679879,U02BVP1QTQF\\n02595dca-107e-4f91-bf47-4fc63ad75788,U02RH0V5K33,,,thanks,1643639563.333059,1643662450.250269,U02RH0V5K33\\n64450a3d-1962-4b11-b76f-d9d4016e25a6,U030PFH5CRX,,,\"yes, the connection is succesfull\",1643647664.742649,1643662507.961759,U030PFH5CRX\\n,USLACKBOT,3.0,tombstone,This message was deleted.,1643662540.919159,1643662540.919159,USLACKBOT\\ne47d1118-c8a4-4ac7-a062-8d5ce872774b,U030PFH5CRX,,,\"but when i try to dockerize the ingest_data.py file, give the error error checking context: \\'can\\'t stat \\'/home/fernanda/Documentos/Zoomcamp/Docker_sql/data/ny_taxi_postgres_data\\'\\'.\",1643647664.742649,1643662621.660689,U030PFH5CRX\\n230ea8a9-1791-4cdf-ad50-35f58af963d3,U02BVP1QTQF,,,Woww.. I haven\\'t reached this point yet.. perhaps I\\'ll try this.. Thanks for the heads up,1643661114.567919,1643662650.116729,U02T9JQAX9N\\nE1C8AAEF-9FA3-45DA-8C46-8789FBD6A0AA,U02UX664K5E,,,\"Did you make sure the google credentials json is located as indicated by <@U02UY1QTGHW> ? If it\\'s telling you that it isn\\'t finding the key then it can only be two issue, the file not being there or an issue with the path as mounted in your yaml file\",1643512731.472799,1643662777.892629,U02UX664K5E\\n31ab54ff-6f6f-4e55-b6ce-8e2d05938e5c,,7.0,,\"Please.. must the AIRFLOW_UID be 50000.. When I ran `echo -e \"\"AIRFLOW_UID=$(id -u)\"\" &gt; .env`.. it was `AIRFLOW_UID=1000` that was there\",1643662783.822149,1643662783.822149,U02T9JQAX9N\\n0ef456fa-3404-452e-8614-6d1c0ca1e5c7,U02UX664K5E,,,I used the regular docker - haven\\'t made it through all videos.,1643512731.472799,1643662833.895389,U02UBQJBYHZ\\n5b96ff14-fa37-44c6-a4e6-7c5a354576b9,U02T9JQAX9N,,,If you’re on Linux then you run the `echo` command to append the actual uid to the .env file. The uid is only guaranteed to be 5000 in MacOS and maybe Windows but I”m not 100% sure,1643662783.822149,1643662875.621309,U02BVP1QTQF\\n248046e4-4c1c-4c37-a28b-7045fafc292f,U02T9JQAX9N,,,\"And also.. should this line in the `docker-compose.yaml` be affected..\\n```user: \"\"${AIRFLOW_UID:-50000}:0\"\"```\",1643662783.822149,1643662921.113339,U02T9JQAX9N\\n6b556b84-f1b7-4718-9446-b10dee0e4bb1,U02T9JQAX9N,,,It has 50000 in it,1643662783.822149,1643662931.077949,U02T9JQAX9N\\n1dd8e722-db09-41c6-a26a-79d97f426ddf,U02T9JQAX9N,,,I didn’t modify the yaml file and it worked fine on both MacOS and the VM instance running Ubuntu,1643662783.822149,1643662998.077169,U02BVP1QTQF\\n2fc0d446-f608-4335-b67c-0b102deda2fb,U030PFH5CRX,,,then I would have to try what appears in the image?,1643647664.742649,1643663076.156889,U030PFH5CRX\\n54e5a8f7-7c59-4d8a-9de4-a92d0d1bbbfb,U030PFH5CRX,,,but when i dont understand whaat to do in mapMap\\xa0`-v $(pwd)/data/ny_taxi_postgres_data:/var/lib/postgresql/data`,1643647664.742649,1643663122.344239,U030PFH5CRX\\n422b76f5-cd05-4805-85fa-010adf76a56a,U02UX664K5E,,,\"<@U02UX664K5E> I got it figured out. I have the file located in my home directory, but the period in front of  `.google/credentials`  was causing the issue. <@U02UBQJBYHZ> thaank you\",1643512731.472799,1643663298.682899,U02UAFF1WU9\\n12E98D5A-7BF9-4144-A397-B40D31DF344A,U02UX664K5E,,,<@U02UAFF1WU9> It always feel good when you get past these issues until the next pops up :laughing: ,1643512731.472799,1643663409.555449,U02UX664K5E\\n3ff9f45a-bb7e-4c15-9a10-78cbb5b3f5d7,U02UX664K5E,,,<@U02UX664K5E> so true. Was stuck on this one for longer then i would like to admit:man-facepalming:,1643512731.472799,1643663444.923349,U02UAFF1WU9\\n647aa212-e6e3-4e9e-bf7f-f50f3c7f5750,U030PFH5CRX,,,\"now create .dockerfile with \"\"data\"\" in it and run again\",1643647664.742649,1643664505.863219,U01AXE0P5M3\\n08c4224a-c3e1-4414-92de-db8ca59042a3,U030PFH5CRX,,,<https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/2_docker_sql/README.md#data-ingestion>,1643647664.742649,1643664536.355379,U01AXE0P5M3\\n8AC6FD0B-877A-490A-BE1A-1700BC3012EE,U02SUH9N1FH,,,\"Never mind, was able to connect using docker service name instead of local host in pgadmin. \",1643130741.125400,1643665128.419049,U0305BX4ZCH\\n7007634d-313f-41e9-9232-d575e18827a8,USLACKBOT,,,\"sorry that it\\'s not a helpful answer, but I thought I must share it with you\\n\\n<https://www.businessinsider.com/how-to-screenshot-on-windows>\",1643662540.919159,1643665282.759979,U01AXE0P5M3\\nb9f67b98-87ae-4dfd-acad-632ef7cc5f81,U030PFH5CRX,,,\"Okay, but i dont understand what is the meaning of `Map`\",1643647664.742649,1643665321.165039,U030PFH5CRX\\na5ba3f16-9de5-432a-9e97-0403f865ddc7,U02T9JQAX9N,,,Ok.. Thanks,1643662783.822149,1643665416.857739,U02T9JQAX9N\\n9f958b19-980b-4dac-9606-5db451e70c01,U030PFH5CRX,,,here I mean do the `-v` thing,1643647664.742649,1643665538.327599,U01AXE0P5M3\\ne2a05765-1a90-45ba-817c-42cee2027d6f,U02T9JQAX9N,,,Check out the video in Alexey’s recent post on <#C02V1Q9CL8K|announcements-course-data-engineering>; he uses the `echo` command too so I guess it’s a good idea to run it always,1643662783.822149,1643665741.959569,U02BVP1QTQF\\ncfae9b1a-83e7-4cf9-a73b-ca3ad273d963,,2.0,,\"Hello Everyone, I am pretty new here. How do I get started ? What I have done so far ? Signed up on slack , cloned the Github repo .\",1643665967.765999,1643665967.765999,U030KNBGP5M\\n8697f334-a962-41ab-9364-c7ded03acaab,U030KNBGP5M,,,Go to week 1 and follow the links there. Have fun!,1643665967.765999,1643666102.940549,U01AXE0P5M3\\na4347e0d-8c1c-41af-9617-8aed409fc769,U02U9MNQG7Q,,,\"You are not copying the DAG scripts to the Docker image.\\n\\nThe `docker-compose.yaml` file contains volume definitions. One of the volumes is for a `dags` folder that you should have in your work folder. You copy your DAG scripts there and then Airflow is able to detect new files when you put them in there.\\n\\nDocker images are immutable but you don’t run images; you run _containers_ which are spawned from the image. Containers are “alive” and even though any changes that happen within will be discarded when the container is killed, containers can affect mounted volumes, which is what is happening with Airflow.\\n\\nI hope it’s more clear now.\",1643645855.993989,1643666274.687499,U02BVP1QTQF\\n4ba89e2e-a9a3-4f59-bac8-08728633844e,U030KNBGP5M,,,Thanks <@U01AXE0P5M3>,1643665967.765999,1643666348.861159,U030KNBGP5M\\nCD0765EB-DC61-4D48-945E-B095CC8A21D5,USLACKBOT,,,Hello <@U01AXE0P5M3>  thanks for the pointer this is a better picture of the error message I get ,1643662540.919159,1643667477.756569,U02V2V1F4B1\\nABF5D9C7-5507-4EBC-BA7D-A505BBD1FBF1,USLACKBOT,,,,1643662540.919159,1643667485.523139,U02V2V1F4B1\\n26346185-0EFE-4B1B-B8ED-269DCD8DD3B9,,1.0,,\"Hello Everyone I got to run my Dag to ingest data into BigQuery, the airflow dag tasks were all successful but when I try opening thr external_table I get the error below. Please can anyone help\",1643668015.000379,1643668015.000379,U02V2V1F4B1\\n2ab4d408-93d5-4200-a40b-9bad6577b482,U02UKLHDWMQ,,,\"Hi <@U02UKLHDWMQ> not sure if this is helpful to you but in my <https://wordpress.com/post/learningdataengineering540969211.wordpress.com/268|blog post> (halfway down) I\\'ve outlined the steps I followed to make sure I have more space on my machine. However, I did manage to do mine successfully on around what you have.\",1643657174.377959,1643668948.151119,U02U5SW982W\\n76c69ab8-7dc8-4c52-948b-2baa837dcc2b,U02E30U011U,,,<@U01AXE0P5M3>  it didn’t help.,1643578051.958769,1643669406.154159,U02E30U011U\\nb42f5bbb-3afc-4c73-90c8-7b31a9396260,U02E30U011U,,,\"```AIRFLOW_CTX_DAG_RUN_ID=manual__2022-01-31T22:35:51.223275+00:00\\n[2022-01-31, 22:43:03 UTC] {warnings.py:110} WARNING - /home/***/.local/lib/python3.7/site-packages/google/auth/_default.py:70: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"\"quota exceeded\"\" or \"\"API not enabled\"\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see <https://cloud.google.com/docs/authentication/>\\n  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\\n\\n[2022-01-31, 22:43:03 UTC] {_default.py:484} WARNING - No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\\n[2022-01-31, 22:43:03 UTC] {taskinstance.py:1700} ERROR - Task failed with exception\\nTraceback (most recent call last):\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1329, in _run_raw_task\\n    self._execute_task_with_callbacks(context)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1455, in _execute_task_with_callbacks\\n    result = self._execute_task(context, self.task)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1511, in _execute_task\\n    result = execute_callable(context=context)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py\"\", line 174, in execute\\n    return_value = self.execute_callable()\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py\"\", line 185, in execute_callable\\n    return self.python_callable(*self.op_args, **self.op_kwargs)\\n  File \"\"/opt/airflow/dags/data_ingestion_gcs_dag.py\"\", line 47, in upload_to_gcs\\n    client = storage.Client()\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/client.py\"\", line 128, in __init__\\n    _http=_http,\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/client.py\"\", line 318, in __init__\\n    _ClientProjectMixin.__init__(self, project=project, credentials=credentials)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/client.py\"\", line 270, in __init__\\n    \"\"Project was not passed and could not be \"\"\\nOSError: Project was not passed and could not be determined from the environment.```\\n\",1643578051.958769,1643669431.761929,U02E30U011U\\n2b51fc36-0a80-4cb8-81ef-aa48e5892b6f,,1.0,,\"Hi! :wave::wave:\\n\\n I\\'m joining the course late. How can I catch up?\",1643669979.550789,1643669979.550789,U030E6300LF\\n989cbe56-764b-43a6-8a42-e7b04b4b3b72,,2.0,,\"I don\\'t understand what this mean : \"\"On Linux, the (airflow) quick-start needs to know your host user-id and needs to have group id set to 0.\"\"\\n\\nhow do i set the group id to 0.\",1643670186.472959,1643670186.472959,U02UKLHDWMQ\\n85d19254-33eb-468a-a87b-49ec80db81fb,U02E30U011U,,,\"I just posted a video in <#C02V1Q9CL8K|announcements-course-data-engineering>, check it out. Maybe it\\'ll help you\",1643578051.958769,1643670273.691299,U01AXE0P5M3\\n13c98e97-5016-4521-b3bc-fc94743e80e5,U02E30U011U,,,I hope you won\\'t notice that I don\\'t have a shadow there,1643578051.958769,1643670306.076549,U01AXE0P5M3\\n30212e14-60e7-4262-99d4-ce3835b6d869,,6.0,,\"Is it possible that the dynamic *csv file naming Alexey came up with in the DAG is not working with transferring files to GCS?\\n\\n```Broken DAG: [/opt/airflow/dags/data_ingestion_gcp.py] Traceback (most recent call last): File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py\"\", line 188, in apply_defaults result = func(self, *args, **kwargs) File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py\"\", line 537, in __init__ \"\"arguments were:\\\\n**kwargs: {k}\"\".format(c=self.__class__.__name__, k=kwargs, t=task_id), airflow.exceptions.AirflowException: Invalid arguments were passed to LocalFilesystemToGCSOperator (task_id: local_to_gcs_task). Invalid arguments were: **kwargs: {\\'python_callable\\': &lt;function upload_to_gcs at 0x7f3ced619710&gt;, \\'op_kwargs\\': {\\'bucket\\': \\'nyc_yellow_taxi_de_is\\', \\'object_name\\': \"\"raw/yellow_tripdata_{{ execution_date.strftime(\\'%Y-%m\\') }}.parquet\"\", \\'local_file\\': \"\"/opt/airflowoutput_tripdata_{{ execution_date.strftime(\\'%Y-%m\\') }}.parquet\"\"}}```\",1643671443.618389,1643671443.618389,U02Q7JMT9P1\\nc04ae4cc-747f-45d1-9a45-1092108e4c54,U02E30U011U,,,\"<@U01AXE0P5M3> I tried to follow the video, but I have this error again. In docker-compose.yaml I specify:\\n`GCP_PROJECT_ID: \\'theta-gizmo-339119\\'`\\n`GCP_GCS_BUCKET: \"\"dtc_data_lake_theta-gizmo-339119\"\"`\\n\\n  `volumes:`\\n    `- ./dags:/opt/airflow/dags`\\n    `- ./logs:/opt/airflow/logs`\\n    `- ./plugins:/opt/airflow/plugins`\\n    `- /Users/tatanakuprianova/.google/credentials:/.google/credentials:ro`\",1643578051.958769,1643673242.341599,U02E30U011U\\n9c9e18d6-55f0-48f9-b085-f1dc3e10e4b6,U030PFH5CRX,,,\"thanks, its working now  :smile:\",1643647664.742649,1643677435.041079,U030PFH5CRX\\n1dac2bd9-925e-45a5-b8d9-8c703cfe9feb,U02UNB4G739,,,I’m getting the same error on Mac OS 10.15.7.,1643624829.835949,1643681275.598449,U02U8CB58G3\\n6a2aa24e-6164-4dc9-9bf2-3693a1ba9522,,1.0,,\"```*** Reading local file: /opt/airflow/logs/data_ingestion_gcs_dagv02/format_to_parquet_task/2019-01-02T02:00:00+00:00/1.log\\n[2022-02-01, 01:45:48 UTC] {taskinstance.py:1032} INFO - Dependencies all met for &lt;TaskInstance: data_ingestion_gcs_dagv02.format_to_parquet_task scheduled__2019-01-02T02:00:00+00:00 [queued]&gt;\\n[2022-02-01, 01:45:48 UTC] {taskinstance.py:1032} INFO - Dependencies all met for &lt;TaskInstance: data_ingestion_gcs_dagv02.format_to_parquet_task scheduled__2019-01-02T02:00:00+00:00 [queued]&gt;\\n[2022-02-01, 01:45:48 UTC] {taskinstance.py:1238} INFO - \\n--------------------------------------------------------------------------------\\n[2022-02-01, 01:45:48 UTC] {taskinstance.py:1239} INFO - Starting attempt 1 of 2\\n[2022-02-01, 01:45:48 UTC] {taskinstance.py:1240} INFO - \\n--------------------------------------------------------------------------------\\n[2022-02-01, 01:45:48 UTC] {taskinstance.py:1259} INFO - Executing &lt;Task(PythonOperator): format_to_parquet_task&gt; on 2019-01-02 02:00:00+00:00\\n[2022-02-01, 01:45:48 UTC] {standard_task_runner.py:52} INFO - Started process 2111 to run task\\n[2022-02-01, 01:45:48 UTC] {standard_task_runner.py:76} INFO - Running: [\\'***\\', \\'tasks\\', \\'run\\', \\'data_ingestion_gcs_dagv02\\', \\'format_to_parquet_task\\', \\'scheduled__2019-01-02T02:00:00+00:00\\', \\'--job-id\\', \\'513\\', \\'--raw\\', \\'--subdir\\', \\'DAGS_FOLDER/data_ingestion_taxi_data.py\\', \\'--cfg-path\\', \\'/tmp/tmpd4s1t7ir\\', \\'--error-file\\', \\'/tmp/tmphk9gicjg\\']\\n[2022-02-01, 01:45:48 UTC] {standard_task_runner.py:77} INFO - Job 513: Subtask format_to_parquet_task\\n[2022-02-01, 01:45:48 UTC] {logging_mixin.py:109} INFO - Running &lt;TaskInstance: data_ingestion_gcs_dagv02.format_to_parquet_task scheduled__2019-01-02T02:00:00+00:00 [running]&gt; on host c388c61d897b\\n[2022-02-01, 01:45:48 UTC] {warnings.py:110} WARNING - /home/***/.local/lib/python3.7/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing \\'execution_date\\' from the template is deprecated and will be removed in a future version. Please use \\'data_interval_start\\' or \\'logical_date\\' instead.\\n  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))\\n\\n[2022-02-01, 01:45:48 UTC] {taskinstance.py:1426} INFO - Exporting the following env vars:\\nAIRFLOW_CTX_DAG_OWNER=***\\nAIRFLOW_CTX_DAG_ID=data_ingestion_gcs_dagv02\\nAIRFLOW_CTX_TASK_ID=format_to_parquet_task\\nAIRFLOW_CTX_EXECUTION_DATE=2019-01-02T02:00:00+00:00\\nAIRFLOW_CTX_DAG_RUN_ID=scheduled__2019-01-02T02:00:00+00:00\\n[2022-02-01, 01:45:55 UTC] {local_task_job.py:154} INFO - Task exited with return code Negsignal.SIGKILL\\n[2022-02-01, 01:45:55 UTC] {taskinstance.py:1277} INFO - Marking task as UP_FOR_RETRY. dag_id=data_ingestion_gcs_dagv02, task_id=format_to_parquet_task, execution_date=20190102T020000, start_date=20220201T014548, end_date=20220201T014555\\n[2022-02-01, 01:45:55 UTC] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check```\\nwhy my format_to_parquet task is not running properly? any solution to this?\",1643681802.610819,1643681802.610819,U02RA8F3LQY\\n048e9f3f-96a7-48f1-b0a5-ce896ebbddcd,,31.0,,\"Please can some help me with why am getting this error\\n\\nStep 12/16 : COPY google_credentials.json $AIRFLOW_HOME/google_credentials.json\\n6 errors occurred:\\n\\t* Status: COPY failed: file not found in build context or excluded by .dockerignore: stat google_credentials.json: file does not exist, Code: 1\\n\\t* Status: COPY failed: file not found in build context or excluded by .dockerignore: stat google_credentials.json: file does not exist, Code: 1\\n\\t* Status: COPY failed: file not found in build context or excluded by .dockerignore: stat google_credentials.json: file does not exist, Code: 1\\n\\t* Status: COPY failed: file not found in build context or excluded by .dockerignore: stat google_credentials.json: file does not exist, Code: 1\\n\\t* Status: COPY failed: file not found in build context or excluded by .dockerignore: stat google_credentials.json: file does not exist, Code: 1\\n\\t* Status: COPY failed: file not found in build context or excluded by .dockerignore: stat google_credentials.json: file does not exist, Code: 1\",1643686137.792409,1643686137.792409,U02UKLHDWMQ\\naa1f6ef1-6f67-430f-a325-fa3b320db9e0,U030E6300LF,,,\"1. Hit the Git Repo\\n2. Follow the links from Week 1\\n3. Skip the things you think you know in order to catchup\\n4. If things don\\'t work on local, don\\'t waste time, follow the GCP VM setup video by Alexey and use that throughout the course\\n5. Use FAQ to search for questions, ask if not found there.\\n6. Enjoy learning :smile:\",1643669979.550789,1643691936.918659,U02TATJKLHG\\ncfb00361-86fc-4dde-9e97-1fc0a16b284e,U02UKLHDWMQ,,,Seems like it is not able to find the `google_credentials.json` file. Can you check that you have passed the right path to the file?,1643686137.792409,1643692188.990909,U02TATJKLHG\\nb2906a8a-9f31-4f75-aee0-aafddef1b2d7,,5.0,,\"Hello everyone. I am new for week 2. If we do this course with vm instance, is it secure to store our google application credential json file in instance? is there any other ways to set google credential environment variable? Thank you\",1643692723.273259,1643692723.273259,U02T8ANTJGM\\n9b479c5b-7992-418f-91d6-5ef46cf6b127,U030MHYAUNS,,,\"The above won\\'t work unless your credentials file is present in your AIRFLOW_HOME path. You just have to create  `.google/credentials/google_credentials`  directories, place the `google_credentials.json` file there and pass `/.google/credentials/google_credentials.json`\",1643646858.324149,1643693731.009449,U02TATJKLHG\\ned433678-6afb-445f-9fe9-ad6afea59475,U02T9JQAX9N,,,\"when you run the echo command and it saves the UID there, the docker-compose will use that ID. And this command means use the UID if exist (in `.env`) and if not, use 50000. `:-` means otherwise.\\ncheck this thread\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1643442862769809>\",1643662783.822149,1643694652.040789,U030FNZC26L\\n3e26ad4b-7e11-4006-b23e-38ac10bfce09,U02UKLHDWMQ,,,I think you should set AIRFLOW_UID=0 in   .env file,1643670186.472959,1643695392.533579,U02QL1EG0LV\\n556de8fa-78b5-46ba-8269-65be50b4b3d8,U02E30U011U,,,\"Have you tried authenticating with this service account previously?\\n\\nMaybe this will help?\\n\\n<https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/windows.md#google-cloud-sdk-authentication|https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/windows.md#google-cloud-sdk-authentication>\\n\\nTry doing it outside of airflow\",1643578051.958769,1643697245.822929,U01AXE0P5M3\\n67a4ddf6-aab1-4884-bec8-ab446ff36690,U02CD7E30T0,,,Thanks Alexey. Video is really helpful. I got light docker running. Please add video to the DE playlist.,1643644630.988259,1643697478.540989,U02NSF7RYP4\\nf083c158-dadb-4de3-bd3a-1b1949878828,U02CD7E30T0,,,Not taking much memory now.,1643644630.988259,1643697649.271279,U02NSF7RYP4\\n00e2f47a-359d-443c-a011-448bfada7a76,U02E30U011U,,,Also maybe you can try downloading new keys for the account,1643578051.958769,1643698070.547759,U01AXE0P5M3\\n96415005-67a5-4ffa-a127-d7bfadbb999c,U02BVP1QTQF,,,Wow you actually added an asterisk :laughing:,1643661114.567919,1643698277.818079,U01AXE0P5M3\\n6591b2be-5ac1-49bc-aece-d09ff3534570,,3.0,,\"Hi, i’m running VM, my scripts are open on VS Code. When i modify Docker or Docker-compose files, i can save them without problems. But when i try to modify and save python scripts i get an error :\\n`Failed to save \\'data_ingestion_cloud_dag.py\\': Unable to write file \\'<vscode-remote://ssh-remote>+de-zoomcamp/home/natalia/data_engineering_course/week_2/airflow/dags/data_ingestion_cloud_dag.py\\' (NoPermissions (FileSystemError): Error: EACCES: permission denied, open \\'/home/natalia/data_engineering_course/week_2/airflow/dags/data_ingestion_cloud_dag.py\\')`\\nThanks a lot in advance!\",1643699464.311449,1643699464.311449,U021RS6DVUZ\\nfc93408a-ed63-4589-9094-833b20fff174,U021RS6DVUZ,,,\"Hi Natalia, you can do `sudo chown -R &lt;user&gt; &lt;path to your directory&gt;` in your VM to change the ownership of your directory that contains the python scripts.\",1643699464.311449,1643699617.554579,U02TATJKLHG\\nc8b445d1-394c-4cbd-91a2-a6e933c5aa2e,U021RS6DVUZ,,,\"From your error message, I can guess the command would look something like this `sudo chown -R natalia ~/data_engineering_course`\",1643699464.311449,1643699729.006079,U02TATJKLHG\\n1f491d45-c4e9-433a-9146-7dfd3e7433f8,U021RS6DVUZ,,,thanks!,1643699464.311449,1643699833.860509,U021RS6DVUZ\\n1c54c60f-dc6e-4899-9308-5e5864188450,U02RA8F3LQY,,,Looks like Airflow runs out of resources: <https://forum.astronomer.io/t/sigkill-when-running-dags-locally-astro-dev-start/1064>,1643681802.610819,1643701232.548839,U02Q7JMT9P1\\n244e3ee9-61d5-461f-a95f-22c6b2c51ff3,,4.0,,\"Hello everybody! Tell me, please (I searched for this question in the chat, but I didn\\'t find it)\\nIn HW2, I imported all the parquet files into google cloud storage, but in BigQuery, only one of these parquet files is used in the dataset. Do I need to connect with the others parquet files, or will we do it later?\",1643701530.318549,1643701530.318549,U02R2PU9NLD\\nA59A0A28-8992-4E0B-8B74-DA95378828D8,U02R2PU9NLD,,,Check the status of your Dag. You will get to know what happened ,1643701530.318549,1643701772.140149,U02TBCXNZ60\\na4219243-b867-4f29-89e2-aee92b4b4f9f,U02R2PU9NLD,,,We\\'ll do it in week 3. For now just copying the files to GCS is sufficient,1643701530.318549,1643701800.835849,U01AXE0P5M3\\nD1B48202-2A4F-48BF-9CCA-06D200172701,U02R2PU9NLD,,,Or just check the los folder if you don\\'t want to go through the UI ,1643701530.318549,1643701803.543429,U02TBCXNZ60\\n4c906d76-079f-4687-bd77-4d95683d398c,,,thread_broadcast,\"Hi all, some help , when i try making changes of \"\"docker-compose\"\" file of week1, and i include a \"\"airflow\"\" same changes like made by <@U01AXE0P5M3>, try to link it to the docker-compose file of week2, i get an error bellow,..any ideas of what i can do\\n\\n(base) DELL@de-zoomcamp:~/Zcamp/week_1/docker_sql/docker_compose$ docker network ls\\nNETWORK ID     NAME                     DRIVER    SCOPE\\n249c3424672f   airflow_default          bridge    local\\n3bb1ad279b3b   bridge                   bridge    local\\nc479b3894556   docker_compose_default   bridge    local\\n56e98fc2db56   host                     host      local\\n35e35f66db81   none                     null      local\\n(base) DELL@de-zoomcamp:~/Zcamp/week_1/docker_sql/docker_compose$ docker-compose up\\n(root) Additional property network is not allowed\\n(base) DELL@de-zoomcamp:~/Zcamp/week_1/docker_sql/docker_compose$\",1642534337.023000,1643701811.854909,U02RTJPV6TZ\\nac90438a-27a5-4937-bb3b-72a04c7a77d0,U02R2PU9NLD,,,Thanks!,1643701530.318549,1643701823.121469,U02R2PU9NLD\\nd2fad0b4-5e8c-475b-8aa4-e0ef83854347,U01AXE0P5M3,,,Can you show your compose file?,1642534337.023000,1643702182.135219,U01AXE0P5M3\\n54fb3414-ec97-4372-bd53-9e9405f0cc6e,U01AXE0P5M3,,,<@U02UCV3RAG4> have you resolved your issue?,1642534337.023000,1643702229.760359,U01AXE0P5M3\\n563073CB-9697-4785-BCB4-4987BF160689,U02T8ANTJGM,,,\"I uploaded my credentials to the VM instance. If you have set up SSH access properly then I think it should be perfectly fine to store the credentials in there. Since the instance comes with gcloud sdk, perhaps there is a way to connect to all other services without the need to upload the credentials but I\\'m not knowledgeable enough to figure out how.\",1643692723.273259,1643702239.456419,U02BVP1QTQF\\nca7526a4-5a83-42d1-a8e4-64967609418b,,,,\"default_args = {\\n\\xa0 \\xa0 \"\"owner\"\": \"\"airflow\"\",\\n\\xa0 \\xa0 \"\"start_date\"\": days_ago(1),\\n\\xa0 \\xa0 \"\"depends_on_past\"\": False,\\n\\xa0 \\xa0 \"\"retries\"\": 1,\\n}\",,1643702294.869089,U02U6DR551B\\nbea9ca5c-af02-4c3f-861d-9dc4f9ec44af,,4.0,,what does days_ago(1) means here?,1643702306.563189,1643702306.563189,U02U6DR551B\\n5D1A1564-BA2D-4900-89DF-E284B4C3AEC4,U02V2V1F4B1,,,\"I\\'m sorry that no one has answered you yet, but without any additional info I don\\'t think we can help you. Try deleting the table and running the dag from the beginning\",1643668015.000379,1643702352.749419,U02BVP1QTQF\\n7ce8b5c0-7c0d-45ed-b875-834aa14f81a0,U02T8ANTJGM,,,\"In AWS it\\'s possible to assign a role to an instance, and then the instance doesn\\'t need any credentials. Perhaps something like that is also possible in GCP, but quick Googling didn\\'t yield any results\",1643692723.273259,1643702373.555219,U01AXE0P5M3\\n8B77BCAE-260C-4229-8A22-7329578DCC0B,U02TBTX45LK,,,\"Not sure I know the fix, but by default airflow uses SQLite which only works with “SequentialExecutor”. Looks like you\\'re using “LocalExecutor”\\n\\nI think we configured it in the YAML to use PostgreSQL and we also configured it to use “CeleryExecutor”.\\n\\nMaybe check the original Yaml and see if you can add the relevant stuff to your current Yaml \",1643229595.416000,1643270901.001400,U02U34YJ8C8\\n51f114e9-1a34-4347-90bf-1ed137862529,,21.0,,\"I followed exactly all instructions for running dockerized airflow, but still I get this error here. Am I missing the installation of a package? Looks like there is folder not existing and/or I am denied permissions :smile: error in thread\",1643271742.004500,1643271742.004500,U02UA0EEHA8\\n727a0fb2-66ae-4912-a7ea-6d1e15a2c280,U02UA0EEHA8,,,\"Please move the log to the thread, it takes too much space\",1643271742.004500,1643271983.006500,U01AXE0P5M3\\nb0d45b6c-b81a-459d-adae-686a99ce119a,,26.0,,\". Hello All, Need help! I have installed docker container on Ubuntu and loaded all data  to the database tables.\",1643271990.006800,1643271990.006800,U02U58SJPHU\\n80d15150-8d4a-4df4-b2cf-9aee10890175,,,,and trying to connect to docker container -&gt; postgres db from my host computer; not able to connect.,,1643272028.008700,U02U58SJPHU\\nc61e6fc0-2679-4266-ba16-c2dff1c973af,U02UA0EEHA8,,,\"```airflow-airflow-scheduler-1  | ....................\\nairflow-airflow-scheduler-1  | ERROR! Maximum number of retries (20) reached.\\nairflow-airflow-scheduler-1  |\\nairflow-airflow-scheduler-1  | Last check result:\\nairflow-airflow-scheduler-1  | $ airflow db check\\nairflow-airflow-scheduler-1  | Unable to load the config, contains a configuration error.\\nairflow-airflow-scheduler-1  | Traceback (most recent call last):\\nairflow-airflow-scheduler-1  |   File \"\"/usr/local/lib/python3.7/pathlib.py\"\", line 1273, in mkdir\\nairflow-airflow-scheduler-1  |     self._accessor.mkdir(self, mode)\\nairflow-airflow-scheduler-1  | FileNotFoundError: [Errno 2] No such file or directory: \\'/opt/airflow/logs/scheduler/2022-01-27\\'\\nairflow-airflow-scheduler-1  |\\nairflow-airflow-scheduler-1  | During handling of the above exception, another exception occurred:\\nairflow-airflow-scheduler-1  |\\nairflow-airflow-scheduler-1  | Traceback (most recent call last):\\nairflow-airflow-scheduler-1  |   File \"\"/usr/local/lib/python3.7/logging/config.py\"\", line 563, in configure\\nairflow-airflow-scheduler-1  |     handler = self.configure_handler(handlers[name])\\nairflow-airflow-scheduler-1  |   File \"\"/usr/local/lib/python3.7/logging/config.py\"\", line 736, in configure_handler\\nairflow-airflow-scheduler-1  |     result = factory(**kwargs)\\nairflow-airflow-scheduler-1  |   File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/log/file_processor_handler.py\"\", line 47, in __init__\\nairflow-airflow-scheduler-1  |     Path(self._get_log_directory()).mkdir(parents=True, exist_ok=True)\\nairflow-airflow-scheduler-1  |   File \"\"/usr/local/lib/python3.7/pathlib.py\"\", line 1277, in mkdir\\nairflow-airflow-scheduler-1  |     self.parent.mkdir(parents=True, exist_ok=True)\\nairflow-airflow-scheduler-1  |   File \"\"/usr/local/lib/python3.7/pathlib.py\"\", line 1273, in mkdir\\nairflow-airflow-scheduler-1  |     self._accessor.mkdir(self, mode)\\nairflow-airflow-scheduler-1  | PermissionError: [Errno 13] Permission denied: \\'/opt/airflow/logs/scheduler\\'\\nairflow-airflow-scheduler-1  |\\nairflow-airflow-scheduler-1  | The above exception was the direct cause of the following exception:\\nairflow-airflow-scheduler-1  |\\nairflow-airflow-scheduler-1  | Traceback (most recent call last):\\nairflow-airflow-scheduler-1  |   File \"\"/home/airflow/.local/bin/airflow\"\", line 5, in &lt;module&gt;\\nairflow-airflow-scheduler-1  |     from airflow.__main__ import main\\nairflow-airflow-scheduler-1  |   File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/__init__.py\"\", line 46, in &lt;module&gt;\\nairflow-airflow-scheduler-1  |     settings.initialize()\\nairflow-airflow-scheduler-1  |   File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/settings.py\"\", line 492, in initialize\\nairflow-airflow-scheduler-1  |     LOGGING_CLASS_PATH = configure_logging()\\nairflow-airflow-scheduler-1  |   File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/logging_config.py\"\", line 74, in configure_logging\\nairflow-airflow-scheduler-1  |     raise e\\nairflow-airflow-scheduler-1  |   File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/logging_config.py\"\", line 69, in configure_logging\\nairflow-airflow-scheduler-1  |     dictConfig(logging_config)\\nairflow-airflow-scheduler-1  |   File \"\"/usr/local/lib/python3.7/logging/config.py\"\", line 800, in dictConfig\\nairflow-airflow-scheduler-1  |     dictConfigClass(config).configure()\\nairflow-airflow-scheduler-1  |   File \"\"/usr/local/lib/python3.7/logging/config.py\"\", line 571, in configure\\nairflow-airflow-scheduler-1  |     \\'%r\\' % name) from e\\nairflow-airflow-scheduler-1  | ValueError: Unable to configure handler \\'processor\\'\\nairflow-airflow-scheduler-1  |\\nairflow-airflow-scheduler-1 exited with code 1\\nairflow-airflow-worker-1     | ....................\\nairflow-airflow-worker-1     | ERROR! Maximum number of retries (20) reached.\\nairflow-airflow-worker-1     |\\nairflow-airflow-worker-1     | Last check result:\\nairflow-airflow-worker-1     | $ airflow db check\\nairflow-airflow-worker-1     | Unable to load the config, contains a configuration error.\\nairflow-airflow-worker-1     | Traceback (most recent call last):\\nairflow-airflow-worker-1     |   File \"\"/usr/local/lib/python3.7/pathlib.py\"\", line 1273, in mkdir\\nairflow-airflow-worker-1     |     self._accessor.mkdir(self, mode)\\nairflow-airflow-worker-1     | FileNotFoundError: [Errno 2] No such file or directory: \\'/opt/airflow/logs/scheduler/2022-01-27\\'\\nairflow-airflow-worker-1     |\\nairflow-airflow-worker-1     | During handling of the above exception, another exception occurred:\\nairflow-airflow-worker-1     |\\nairflow-airflow-worker-1     | Traceback (most recent call last):\\nairflow-airflow-worker-1     |   File \"\"/usr/local/lib/python3.7/logging/config.py\"\", line 563, in configure\\nairflow-airflow-worker-1     |     handler = self.configure_handler(handlers[name])\\nairflow-airflow-worker-1     |   File \"\"/usr/local/lib/python3.7/logging/config.py\"\", line 736, in configure_handler\\nairflow-airflow-worker-1     |     result = factory(**kwargs)\\nairflow-airflow-worker-1     |   File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/log/file_processor_handler.py\"\", line 47, in __init__\\nairflow-airflow-worker-1     |     Path(self._get_log_directory()).mkdir(parents=True, exist_ok=True)\\nairflow-airflow-worker-1     |   File \"\"/usr/local/lib/python3.7/pathlib.py\"\", line 1277, in mkdir\\nairflow-airflow-worker-1     |     self.parent.mkdir(parents=True, exist_ok=True)\\nairflow-airflow-worker-1     |   File \"\"/usr/local/lib/python3.7/pathlib.py\"\", line 1273, in mkdir\\nairflow-airflow-worker-1     |     self._accessor.mkdir(self, mode)\\nairflow-airflow-worker-1     | PermissionError: [Errno 13] Permission denied: \\'/opt/airflow/logs/scheduler\\'\\nairflow-airflow-worker-1     |\\nairflow-airflow-worker-1     | The above exception was the direct cause of the following exception:\\nairflow-airflow-worker-1     |\\nairflow-airflow-worker-1     | Traceback (most recent call last):\\nairflow-airflow-worker-1     |   File \"\"/home/airflow/.local/bin/airflow\"\", line 5, in &lt;module&gt;\\nairflow-airflow-worker-1     |     from airflow.__main__ import main\\nairflow-airflow-worker-1     |   File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/__init__.py\"\", line 46, in &lt;module&gt;\\nairflow-airflow-worker-1     |     settings.initialize()\\nairflow-airflow-worker-1     |   File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/settings.py\"\", line 492, in initialize\\nairflow-airflow-worker-1     |     LOGGING_CLASS_PATH = configure_logging()\\nairflow-airflow-worker-1     |   File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/logging_config.py\"\", line 74, in configure_logging\\nairflow-airflow-worker-1     |     raise e\\nairflow-airflow-worker-1     |   File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/logging_config.py\"\", line 69, in configure_logging\\nairflow-airflow-worker-1     |     dictConfig(logging_config)\\nairflow-airflow-worker-1     |   File \"\"/usr/local/lib/python3.7/logging/config.py\"\", line 800, in dictConfig\\nairflow-airflow-worker-1     |     dictConfigClass(config).configure()\\nairflow-airflow-worker-1     |   File \"\"/usr/local/lib/python3.7/logging/config.py\"\", line 571, in configure\\nairflow-airflow-worker-1     |     \\'%r\\' % name) from e\\nairflow-airflow-worker-1     | ValueError: Unable to configure handler \\'processor\\'\\nairflow-airflow-worker-1     |\\nairflow-airflow-worker-1 exited with code 1\\nairflow-airflow-webserver-1  | ....................\\nairflow-airflow-webserver-1  | ERROR! Maximum number of retries (20) reached.\\nairflow-airflow-webserver-1  |\\nairflow-airflow-webserver-1  | Last check result:\\nairflow-airflow-webserver-1  | $ airflow db check\\nairflow-airflow-webserver-1  | Unable to load the config, contains a configuration error.\\nairflow-airflow-webserver-1  | Traceback (most recent call last):\\nairflow-airflow-webserver-1  |   File \"\"/usr/local/lib/python3.7/pathlib.py\"\", line 1273, in mkdir\\nairflow-airflow-webserver-1  |     self._accessor.mkdir(self, mode)\\nairflow-airflow-webserver-1  | FileNotFoundError: [Errno 2] No such file or directory: \\'/opt/airflow/logs/scheduler/2022-01-27\\'\\nairflow-airflow-webserver-1  |\\nairflow-airflow-webserver-1  | During handling of the above exception, another exception occurred:\\nairflow-airflow-webserver-1  |\\nairflow-airflow-webserver-1  | Traceback (most recent call last):\\nairflow-airflow-webserver-1  |   File \"\"/usr/local/lib/python3.7/logging/config.py\"\", line 563, in configure\\nairflow-airflow-webserver-1  |     handler = self.configure_handler(handlers[name])\\nairflow-airflow-webserver-1  |   File \"\"/usr/local/lib/python3.7/logging/config.py\"\", line 736, in configure_handler\\nairflow-airflow-webserver-1  |     result = factory(**kwargs)\\nairflow-airflow-webserver-1  |   File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/log/file_processor_handler.py\"\", line 47, in __init__\\nairflow-airflow-webserver-1  |     Path(self._get_log_directory()).mkdir(parents=True, exist_ok=True)\\nairflow-airflow-webserver-1  |   File \"\"/usr/local/lib/python3.7/pathlib.py\"\", line 1277, in mkdir\\nairflow-airflow-webserver-1  |     self.parent.mkdir(parents=True, exist_ok=True)\\nairflow-airflow-webserver-1  |   File \"\"/usr/local/lib/python3.7/pathlib.py\"\", line 1273, in mkdir\\nairflow-airflow-webserver-1  |     self._accessor.mkdir(self, mode)\\nairflow-airflow-webserver-1  | PermissionError: [Errno 13] Permission denied: \\'/opt/airflow/logs/scheduler\\'\\nairflow-airflow-webserver-1  |\\nairflow-airflow-webserver-1  | The above exception was the direct cause of the following exception:\\nairflow-airflow-webserver-1  |\\nairflow-airflow-webserver-1  | Traceback (most recent call last):\\nairflow-airflow-webserver-1  |   File \"\"/home/airflow/.local/bin/airflow\"\", line 5, in &lt;module&gt;\\nairflow-airflow-webserver-1  |     from airflow.__main__ import main\\nairflow-airflow-webserver-1  |   File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/__init__.py\"\", line 46, in &lt;module&gt;\\nairflow-airflow-webserver-1  |     settings.initialize()\\nairflow-airflow-webserver-1  |   File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/settings.py\"\", line 492, in initialize\\nairflow-airflow-webserver-1  |     LOGGING_CLASS_PATH = configure_logging()\\nairflow-airflow-webserver-1  |   File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/logging_config.py\"\", line 74, in configure_logging\\nairflow-airflow-webserver-1  |     raise e\\nairflow-airflow-webserver-1  |   File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/logging_config.py\"\", line 69, in configure_logging\\nairflow-airflow-webserver-1  |     dictConfig(logging_config)\\nairflow-airflow-webserver-1  |   File \"\"/usr/local/lib/python3.7/logging/config.py\"\", line 800, in dictConfig\\nairflow-airflow-webserver-1  |     dictConfigClass(config).configure()\\nairflow-airflow-webserver-1  |   File \"\"/usr/local/lib/python3.7/logging/config.py\"\", line 571, in configure\\nairflow-airflow-webserver-1  |     \\'%r\\' % name) from e\\nairflow-airflow-webserver-1  | ValueError: Unable to configure handler \\'processor\\'\\nairflow-airflow-webserver-1  |\\nairflow-airflow-webserver-1 exited with code 1```\",1643271742.004500,1643272034.009000,U02UA0EEHA8\\n621c1e2f-ca9c-4c02-9ff2-37748800b3ee,U02UA0EEHA8,,,I think it tries to find logs that don\\'t exist. Does the web interface open for you?,1643271742.004500,1643272063.010600,U01AXE0P5M3\\n9a955e31-28e4-45d5-b1f8-d2fa0ce9bbd4,,,,Can you pls help me hoe we can connect? from my host computer to db which is running in docker container,,1643272063.010800,U02U58SJPHU\\n578fdabf-9f8e-4af5-9a79-f570d84ff847,,,,pls advice,,1643272067.011000,U02U58SJPHU\\nc22ba3f7-406e-479d-a6e6-24e333efb471,,8.0,,\"Hi All apologies for the noob question I have everything set up and running, But restarted my machine and I cant see my servers running at <http://localhost:8080/browser/> What are the steps needed to get the servers up and running to query the ingested data ?\",1643272081.011200,1643272081.011200,U02UY1QTGHW\\nbdd82c0a-d80e-451f-a85a-36d5cf861a80,,3.0,,\"Hi guys, I’m getting this error when i tried to run the dag task (following the video under 2.3.2) but somehow i’m running into errors (see errors in thread) among the logs.\\n\\ncould i ask which part of `data_ingestion_gcs_dag.py` does the gcloud authentication? can’t seem to locate that chunk of codes. thanks!\",1643272098.011400,1643272098.011400,U02ULMHKBQT\\n0484be0e-f3e5-4670-a70b-cc9018960bec,U02ULMHKBQT,,,\"````File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/auth/_default.py\"\", line 488, in default\\n    raise exceptions.DefaultCredentialsError(_HELP_MESSAGE)\\ngoogle.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS or explicitly create credentials and re-run the application. For more information, please see <https://cloud.google.com/docs/authentication/getting-started>\\n[2022-01-27, 16:07:55 +08] {local_task_job.py:154} INFO - Task exited with return code 1\\n[2022-01-27, 16:07:55 +08] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check````\\n\",1643272098.011400,1643272112.011500,U02ULMHKBQT\\n7ae14b15-75a5-4829-8a9f-36306ae37b1b,U02UA0EEHA8,,,\"nope, it seems airflow is not initialized\",1643271742.004500,1643272203.011700,U02UA0EEHA8\\n3e605b6b-b22c-4f0a-884b-6c181aa40061,U02UA0EEHA8,,,\"But you run the init thing, didn\\'t you?\",1643271742.004500,1643272247.011900,U01AXE0P5M3\\nf20ba10e-a1d7-4948-aec3-1354f0fe14e8,U02UA0EEHA8,,,\"yes, and there seems to be no error here (log attached)\\n```docker-compose up airflow-init\\ntime=\"\"2022-01-27T09:28:36+01:00\"\" level=warning msg=\"\"The \\\\\"\"AIRFLOW_UID\\\\\"\" variable is not set. Defaulting to a blank string.\"\"\\ntime=\"\"2022-01-27T09:28:36+01:00\"\" level=warning msg=\"\"The \\\\\"\"AIRFLOW_UID\\\\\"\" variable is not set. Defaulting to a blank string.\"\"\\nNetwork airflow_default  Creating\\nNetwork airflow_default  Created\\nContainer airflow-redis-1  Creating\\nContainer airflow-postgres-1  Creating\\nContainer airflow-postgres-1  Created\\nContainer airflow-redis-1  Created\\nContainer airflow-airflow-init-1  Creating\\nContainer airflow-airflow-init-1  Created\\nAttaching to airflow-airflow-init-1\\nairflow-airflow-init-1  |\\nairflow-airflow-init-1  | WARNING!!!: AIRFLOW_UID not set!\\nairflow-airflow-init-1  | If you are on Linux, you SHOULD follow the instructions below to set\\nairflow-airflow-init-1  | AIRFLOW_UID environment variable, otherwise files will be owned by root.\\nairflow-airflow-init-1  | For other operating systems you can get rid of the warning with manually created .env file:\\nairflow-airflow-init-1  |     See: <https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html#setting-the-right-airflow-user>\\nairflow-airflow-init-1  |\\nairflow-airflow-init-1  | The container is run as root user. For security, consider using a regular user account.\\nairflow-airflow-init-1  |\\nairflow-airflow-init-1  | /home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/dialects/postgresql/base.py:3573 SAWarning: Predicate of partial index idx_dag_run_queued_dags ignored during reflection\\nairflow-airflow-init-1  | /home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/dialects/postgresql/base.py:3573 SAWarning: Predicate of partial index idx_dag_run_running_dags ignored during reflection\\nairflow-airflow-init-1  | DB: <postgresql+psycopg2://airflow>:***@postgres/airflow\\nairflow-airflow-init-1  | [2022-01-27 08:28:47,814] {db.py:921} INFO - Creating tables\\nairflow-airflow-init-1  | INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.\\nairflow-airflow-init-1  | INFO  [alembic.runtime.migration] Will assume transactional DDL.\\nairflow-airflow-init-1  | Upgrades done\\nairflow-airflow-init-1  | [2022-01-27 08:28:54,195] {manager.py:512} WARNING - Refused to delete permission view, assoc with role exists DAG Runs.can_create User\\nairflow-airflow-init-1  | airflow already exist in the db\\nairflow-airflow-init-1  | 2.2.3\\nairflow-airflow-init-1 exited with code 0```\",1643271742.004500,1643272285.012100,U02UA0EEHA8\\n2e17b3c8-c8bc-49e9-9b69-967eb5267f04,U02UY1QTGHW,,,this is the status in docker. Even if I start the containers i still don\\'t see the db\\'s in pgAdmin halp !,1643272081.011200,1643272309.012300,U02UY1QTGHW\\n73b3a55f-6c67-4180-a13e-3ecb93d5511d,U02UA0EEHA8,,,Yes I see in the logs that the webserver exited with code 1. Can you remove the postgres volume that compose created and try one more time?,1643271742.004500,1643272313.013100,U01AXE0P5M3\\na5547e9d-a465-4454-980a-d44e5ccc4ef2,U02U58SJPHU,,,\"Access database from pgadmin to continue with the course.\\n\\nDid You get any message for pgcli command?\",1643271990.006800,1643272331.013300,U0290EYCA7Q\\n0d963dac-6750-49ce-9583-2534f3bd96ce,U02UA0EEHA8,,,There\\'s a warning that airflow user is not set. Have you set it?,1643271742.004500,1643272345.013600,U01AXE0P5M3\\na1de14f4-ef89-4bd6-8294-dad8b618f323,U02UA0EEHA8,,,\"I did earlier, but then read in the official doc that it is not necessary on Windows\",1643271742.004500,1643272436.013900,U02UA0EEHA8\\n768b0c69-1f76-4e0e-94f7-a810e4333c13,U02UY1QTGHW,,,\"After confirming database is running, You should run ingestion script to load data. This could be done either by running the script locally, or firing up another container within the same network to do that.\",1643272081.011200,1643272446.014300,U0290EYCA7Q\\n4846c0d8-2684-40cf-a2da-ccd841f50d0c,U02U58SJPHU,,,I am trying to connect from host computer.,1643271990.006800,1643272461.014600,U02U58SJPHU\\n463aad0e-1bf4-4634-864f-0f5dadb14e77,U02UY1QTGHW,,,\"```URL=\"\"<https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>\"\"\\n\\npython ingest_data.py \\\\\\n  --user=root \\\\\\n  --password=root \\\\\\n  --host=localhost \\\\\\n  --port=5432 \\\\\\n  --db=ny_taxi \\\\\\n  --table_name=yellow_taxi_trips \\\\\\n  --url=${URL}\\n\\ndocker run -it \\\\\\n  --network=pg-network \\\\\\n  taxi_ingest:v001 \\\\\\n    --user=root \\\\\\n    --password=root \\\\\\n    --host=pg-database \\\\\\n    --port=5432 \\\\\\n    --db=ny_taxi \\\\\\n    --table_name=yellow_taxi_trips \\\\\\n    --url=${URL}```\",1643272081.011200,1643272479.014800,U0290EYCA7Q\\n078a539c-203f-4fd3-bf77-2ab815a25bba,U02U58SJPHU,,,and postgress is running in docker container.,1643271990.006800,1643272498.015000,U02U58SJPHU\\n189fcd40-b435-476b-a9be-536a07794b7f,U02UA0EEHA8,,,\"I still had to do it even on windows. How do you run it? Mingw, power shell or some other way?\",1643271742.004500,1643272499.015200,U01AXE0P5M3\\n924aeb3a-3f42-43bf-be57-8bc14ddc684e,U02UY1QTGHW,,,:arrow_up: for reference,1643272081.011200,1643272500.015400,U0290EYCA7Q\\n198ad2ee-9397-4345-b5f7-824de72a0618,U02UY1QTGHW,,,<@U0290EYCA7Q> thanks i\\'ll give it a go,1643272081.011200,1643272514.015600,U02UY1QTGHW\\n85a2f5b0-5cdc-434b-832a-e59945df246c,U02UA0EEHA8,,,\"and I tried removing everything already, basically all volumes mounted are exiting with code 1 :confused: I tried running with cmd and mingw, same result\",1643271742.004500,1643272538.015800,U02UA0EEHA8\\n3fa6dccd-9817-4461-b9ac-4762966a5ea6,U02U58SJPHU,,,Did you get any message using pgcli?,1643271990.006800,1643272542.016000,U0290EYCA7Q\\n4f735358-bace-42c7-8bf8-4ecb6453a6b3,U02UY1QTGHW,,,\"also does stopping the containers flush the data, hence the reingestion ?\",1643272081.011200,1643272552.016200,U02UY1QTGHW\\n137e3bc8-b448-41f3-bb97-4f3434b4e051,U02UA0EEHA8,,,\"I\\'d try to remove the volume it created and redo the whole thing again, and make sure you set airflow user before you build and before you run init\",1643271742.004500,1643272562.016400,U01AXE0P5M3\\n6af2c59c-867a-4c00-b45e-225b926794d3,U02UA0EEHA8,,,\"I will try, thanks a lot for the help Alex. Will post here below if I have again the same issue\",1643271742.004500,1643272596.016700,U02UA0EEHA8\\nf20842f5-2865-442c-a3a0-9bed412da67e,U02UA0EEHA8,,,I hope it gets resolved!,1643271742.004500,1643272610.016900,U01AXE0P5M3\\n304a7775-54cb-458d-af81-758e19debe69,U02UY1QTGHW,,,\"Think it should not, because of volume mount. Maybe someone will answer.\",1643272081.011200,1643272651.017900,U0290EYCA7Q\\neac3e176-a2ed-4be7-810b-e03b3342775e,,9.0,,\"`table = pv.read_csv(src_file)    pq.write_table(table, src_file.replace(\\'.csv\\', \\'.parquet\\'))`\\n\\nWhy do we need to use pq.write_table after reading the file with pyarrow.read_csv <@U01AXE0P5M3> <@U01DHB2HS3X>\",1643272697.018700,1643272697.018700,U02SPLJUR42\\n49407371-542a-4ad0-a376-bad6a1dc7199,U02SPLJUR42,,,To save it to parquet,1643272697.018700,1643272723.018800,U01AXE0P5M3\\nb262f44d-57a8-4864-b0c0-48b9d18c6642,U02U58SJPHU,,,Connection error.,1643271990.006800,1643272741.019000,U02U58SJPHU\\n7f4eeb59-6dfc-4403-97a0-45fee9f93cc7,U02U58SJPHU,,,one qq,1643271990.006800,1643272745.019200,U02U58SJPHU\\necc0b12c-7c32-4c04-8af3-d63ba7d8958d,U02U58SJPHU,,,do we need to setup on GCP?,1643271990.006800,1643272754.019400,U02U58SJPHU\\n180b0c1c-e35c-4c22-9417-fee20c1d2ce5,U02SPLJUR42,,,thanks so what is the pv.read_csv doing,1643272697.018700,1643272758.019600,U02SPLJUR42\\nb49b3348-0c4a-4317-ac67-94a421a61b0e,U02SPLJUR42,,,like any need for that?,1643272697.018700,1643272774.019800,U02SPLJUR42\\na365d9b5-a739-4457-bf3b-7ba33fd38613,U02U58SJPHU,,,i am trying to do setup on ubuntu virtual machine?,1643271990.006800,1643272777.020000,U02U58SJPHU\\necb6e04e-fc34-4c15-9ced-08a20c3e6497,U02U58SJPHU,,,just VM,1643271990.006800,1643272786.020200,U02U58SJPHU\\nc2fffe86-9e9b-4dc3-90b0-56d95df28c74,U02SPLJUR42,,,\"You won\\'t believe it, but it\\'s reading a csv file =)\",1643272697.018700,1643272788.020400,U01AXE0P5M3\\n62560d62-7773-45ce-acc3-fbe0680e4b18,U02U58SJPHU,,,okay,1643271990.006800,1643272807.020600,U0290EYCA7Q\\n17202590-1e52-4e04-a8b5-8f7ec2a6a0a1,U02U58SJPHU,,,did you install extension for remote ssh on VS Code?,1643271990.006800,1643272830.020800,U0290EYCA7Q\\n74c5a583-03a0-4cf4-8a17-ba9c9bdcda65,U02SPLJUR42,,,So you have a csv file and you want to turn it  into a parquet file. That\\'s what this piece of code is doing,1643272697.018700,1643272847.021000,U01AXE0P5M3\\n7771afb2-cb26-43df-817a-9a35053eff09,U02ULMHKBQT,,,\"think i’ve figured it out - it’s under `docker-compose.yaml`\\n`x-airflow-common` &gt; `environment` &gt; `GOOGLE_APPLICATION_CREDENTIALS`\",1643272098.011400,1643272852.021200,U02ULMHKBQT\\n9cfd58c4-4d19-43bb-a350-88b0f4798bb3,U02U58SJPHU,,,no,1643271990.006800,1643272855.021400,U02U58SJPHU\\n51189e52-41d4-447e-b75f-292a3e64914e,U02SPLJUR42,,,still confusing though,1643272697.018700,1643272883.021600,U02SPLJUR42\\n59e77fe9-2247-4172-b1d6-56aadd497311,U02U58SJPHU,,,i did not install it. i think i need to start from beginning,1643271990.006800,1643272886.021900,U02U58SJPHU\\na816a8cd-91dd-4e08-a395-a519aaa34af9,U02U58SJPHU,,,Its just an extension,1643271990.006800,1643272904.022200,U0290EYCA7Q\\nb8d3be94-cc0f-4ca1-83e3-78ee4aec292f,U02U58SJPHU,,,Let me try that.,1643271990.006800,1643272904.022300,U02U58SJPHU\\n63f9c848-f8cc-497c-bee2-790b6bc89e4f,U02SPLJUR42,,,is the table creating a schema for the write_table function??,1643272697.018700,1643272920.022600,U02SPLJUR42\\n92da59d6-f21c-40b8-a72b-09784e4c0a14,U02U58SJPHU,,,\"if you are sure, your db is running in VM.\",1643271990.006800,1643272978.022800,U0290EYCA7Q\\n7d92d5e3-5fdc-43d1-870f-a967a9ce7ca3,U02U58SJPHU,,,You can load VM files on VS code,1643271990.006800,1643272990.023200,U0290EYCA7Q\\ne3947f5d-2651-4df2-947a-d4d79c5d0589,U02U58SJPHU,,,\"Open the terminal, and do port forwarding. It\\'s well explained in the video\",1643271990.006800,1643273020.023400,U0290EYCA7Q\\ndb52060d-de72-448b-8202-a25c765766e1,U02U58SJPHU,,,I did install.,1643271990.006800,1643273054.023600,U02U58SJPHU\\n448abe46-e545-4d8d-aa67-acba4b46de51,U02U58SJPHU,,,Do You use port 5432 in local? I mean you have postgres installed in your local?,1643271990.006800,1643273137.023800,U0290EYCA7Q\\nee764915-c47a-4f8b-9f82-787f57b72548,U02U58SJPHU,,,No I did install postgres on Uubuntu VM (guest),1643271990.006800,1643273190.024000,U02U58SJPHU\\na0dc5e62-8a63-49b3-9604-878802df5555,U02U58SJPHU,,,\"My machine windows. and installed VM, and istalled ubuntu\",1643271990.006800,1643273282.024200,U02U58SJPHU\\n20c6e47f-d727-42e2-8d5d-27296a214609,U02U58SJPHU,,,\"Open the terminal, and do port forwarding. It\\'s well explained in the video - I am not sure. I get this point\",1643271990.006800,1643273307.024400,U02U58SJPHU\\n97d6736c-f955-4bce-941f-024f3be5d88e,U02U58SJPHU,,,any inputs on this help.,1643271990.006800,1643273321.024600,U02U58SJPHU\\nf39b18c5-ece7-4cd6-8f03-4a3007205123,U02U58SJPHU,,,<https://youtu.be/ae-CV2KfoN0?t=1947>,1643271990.006800,1643273372.024800,U0290EYCA7Q\\n5c2c21d7-ac75-4164-a41b-c217bf4041d1,U02CD7E30T0,,,Check what Sejal said on top. The env-vars for the project id and datalake.,1643211560.323300,1643273572.025200,U02CD7E30T0\\n37285b02-daf3-46bd-874d-b5a692107912,U02U58SJPHU,,,Thank you Looking.,1643271990.006800,1643273712.025500,U02U58SJPHU\\nf26ca988-c0e7-4252-920f-c1c28ad5ad31,U02T96HEARK,,,\"Thanks, this helped, It works now. I missed enabling one of the APIs.\",1643265061.489000,1643274134.025700,U02T96HEARK\\nc1eea66d-fa2d-4a1c-89fc-ecc676098cd3,,12.0,,\"How can one download existing service-account-authkeys.json? Do we need to create the key again, and store it as json?\",1643274368.027300,1643274368.027300,U0290EYCA7Q\\nb2a4d302-fb42-418f-916d-80dc0ea52c95,U02SPLJUR42,,,\"Thanks for bringing up this question. Although the function is obvious, I think it makes sense to cover why we\\'re converting to parquet files. I\\'ll be creating a final video of this series for some theory on best practices. Would be up in a day or 2\",1643272697.018700,1643274384.027400,U01DHB2HS3X\\n06fb8502-54e9-4116-a37f-7862eb5b7c68,U02CD7E30T0,,,\"Thanks i have solved the issue, now working fine\",1643211560.323300,1643274417.027800,U02T0CYNNP2\\n61fa0204-24e7-455d-bf38-07ef3813cda0,U02SPLJUR42,,,okay thanks,1643272697.018700,1643274443.028000,U02SPLJUR42\\nee3bc21c-fad6-4f6d-8353-bc318175ce6d,U02T0CYNNP2,,,Where did you view your log error,1643234608.442300,1643274596.028400,U02T0CYNNP2\\n12abdc97-bdb9-41e5-b808-9e59ac344f4f,U0290EYCA7Q,,,\"```gcloud auth application-default login```\\nAfter running the command you can see the json downloaded to your local.\\nYou can create an evn variable `GOOGLE_APPLICATION_CREDENTIALS` from that\",1643274368.027300,1643274659.028600,U01DFQ82AK1\\na6b539ed-ccd4-4e36-81ed-a556acfa5f81,,5.0,,\"<@U01DHB2HS3X> Instead of defining a python function to upload data file to GCS, why can\\'t we use FileToGoogleCloudStorageOperator? Also,\\nPlease brief what is the difference between these two approaches and why did you choose the first one?\",1643275045.033500,1643275045.033500,U02S9JS3D2R\\n40aa3b88-24dc-4aeb-aa09-7532b576bcb4,U02S9JS3D2R,,,I am not sure but I think has something to do with the HW (the TODO),1643275045.033500,1643275708.033700,U02CD7E30T0\\n28a687ef-545b-495d-ae8c-b997f1b4e648,U02ULMHKBQT,,,It is an environment variable,1643272098.011400,1643275753.033900,U02CD7E30T0\\ne73c8feb-26ec-475e-b74f-5ef45c7225f1,U02S9JS3D2R,,,\"There are various ways of doing it, and there are also multiple Operators available. I just picked one. But feel free to play around and try your own  as per whichever seems more performant.\",1643275045.033500,1643275835.034200,U01DHB2HS3X\\n433c624c-b163-4d44-97dc-eb5b58bc4175,U02S9JS3D2R,,,\"In my case, I had to write a custom function to parallelize uploading large data with chunks, on lower bandwidths (internet). Otherwise, it was resulting in timeout errors\",1643275045.033500,1643275967.034800,U01DHB2HS3X\\na88746c5-e3fb-4735-a190-d4b9f92838cb,U02TMP4GJEM,,,\"okay i surely need some help <@U02SUUT290F> <@U01AXE0P5M3>,  iam trying to run \\'docker compose up\"\" using the \"\"docker-compose.yaml\"\" and my pgadmin is able to pick up but the database is just shutting down\",1643061968.489400,1643276309.035700,U02RTJPV6TZ\\ncaef891e-3cb0-4072-bbc6-540893969540,U02TMP4GJEM,,,\"$ docker-compose up\\n\\nCreating network \"\"docker_compose_default\"\" with the default driver\\nCreating docker_compose_pgdatabase_1 ...\\nCreating docker_compose_pgadmin_1    ...\\nCreating docker_compose_pgadmin_1    ... done\\nCreating docker_compose_pgdatabase_1 ... done\\nAttaching to docker_compose_pgadmin_1, docker_compose_pgdatabase_1\\npgdatabase_1  | The files belonging to this database system will be owned by user \"\"postgres\"\".\\npgdatabase_1  | This user must also own the server process.\\npgdatabase_1  |\\npgdatabase_1  | The database cluster will be initialized with locale \"\"en_US.utf8\"\".\\npgdatabase_1  | The default database encoding has accordingly been set to \"\"UTF8\"\".\\npgdatabase_1  | The default text search configuration will be set to \"\"english\"\".\\npgdatabase_1  |\\npgdatabase_1  | Data page checksums are disabled.\\npgdatabase_1  |\\npgdatabase_1  | fixing permissions on existing directory /var/lib/postgresql/data ... ok\\npgdatabase_1  | creating subdirectories ... ok\\npgdatabase_1  | selecting dynamic shared memory implementation ... posix\\npgdatabase_1  | selecting default max_connections ... 100\\npgdatabase_1  | selecting default shared_buffers ... 128MB\\npgdatabase_1  | selecting default time zone ... Etc/UTC\\npgdatabase_1  | creating configuration files ... ok\\npgdatabase_1  | running bootstrap script ... ok\\npgdatabase_1  | performing post-bootstrap initialization ... ok\\npgdatabase_1  | syncing data to disk ... initdb: warning: enabling \"\"trust\"\" authentication for local connections\\npgdatabase_1  | You can change this by editing pg_hba.conf or using the option -A, or\\npgdatabase_1  | --auth-local and --auth-host, the next time you run initdb.\\npgdatabase_1  | ok\\npgdatabase_1  |\\npgdatabase_1  |\\npgdatabase_1  | Success. You can now start the database server using:\\npgdatabase_1  |\\npgdatabase_1  |     pg_ctl -D /var/lib/postgresql/data -l logfile start\\npgdatabase_1  |\\npgdatabase_1  | waiting for server to start....2022-01-27 09:17:48.566 UTC [50] LOG:  starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\\npgdatabase_1  | 2022-01-27 09:17:48.570 UTC [50] LOG:  listening on Unix socket \"\"/var/run/postgresql/.s.PGSQL.5432\"\"\\npgdatabase_1  | 2022-01-27 09:17:48.608 UTC [51] LOG:  database system was shut down at 2022-01-27 09:17:43 UTC\\npgdatabase_1  | 2022-01-27 09:17:48.639 UTC [50] LOG:  database system is ready to accept connections\\npgdatabase_1  |  done\\npgdatabase_1  | server started\\npgdatabase_1  | CREATE DATABASE\\npgdatabase_1  |\\npgdatabase_1  |\\npgdatabase_1  | /usr/local/bin/docker-entrypoint.sh: ignoring /docker-entrypoint-initdb.d/*\\npgdatabase_1  |\\npgdatabase_1  | 2022-01-27 09:17:52.174 UTC [50] LOG:  received fast shutdown request\\npgdatabase_1  | waiting for server to shut down...2022-01-27 09:17:52.179 UTC [50] LOG:  aborting any active transactions\\npgdatabase_1  | 2022-01-27 09:17:52.182 UTC [50] LOG:  background worker \"\"logical replication launcher\"\" (PID 57) exited with exit code 1\\npgdatabase_1  | 2022-01-27 09:17:52.183 UTC [52] LOG:  shutting down\\npgdatabase_1  | .2022-01-27 09:17:52.328 UTC [50] LOG:  database system is shut down\\npgdatabase_1  |  done\\npgdatabase_1  | server stopped\\npgdatabase_1  |\\npgdatabase_1  | PostgreSQL init process complete; ready for start up.\\npgdatabase_1  |\\npgdatabase_1  | 2022-01-27 09:17:52.428 UTC [1] LOG:  starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\\npgdatabase_1  | 2022-01-27 09:17:52.429 UTC [1] LOG:  listening on IPv4 address \"\"0.0.0.0\"\", port 5432\\npgdatabase_1  | 2022-01-27 09:17:52.429 UTC [1] LOG:  listening on IPv6 address \"\"::\"\", port 5432\\npgdatabase_1  | 2022-01-27 09:17:52.444 UTC [1] LOG:  listening on Unix socket \"\"/var/run/postgresql/.s.PGSQL.5432\"\"\\npgdatabase_1  | 2022-01-27 09:17:52.475 UTC [64] LOG:  database system was shut down at 2022-01-27 09:17:52 UTC\\npgdatabase_1  | 2022-01-27 09:17:52.528 UTC [1] LOG:  database system is ready to accept connections\\npgadmin_1     | NOTE: Configuring authentication for SERVER mode.\\npgadmin_1     |\\npgadmin_1     | [2022-01-27 09:18:01 +0000] [1] [INFO] Starting gunicorn 20.1.0\\npgadmin_1     | [2022-01-27 09:18:01 +0000] [1] [INFO] Listening at: http://[::]:80 (1)\\npgadmin_1     | [2022-01-27 09:18:01 +0000] [1] [INFO] Using worker: gthread\\npgadmin_1     | [2022-01-27 09:18:01 +0000] [89] [INFO] Booting worker with pid: 89\",1643061968.489400,1643276402.035900,U02RTJPV6TZ\\n0a6525da-db99-4065-9085-7d11dd7f0de1,U02TMP4GJEM,,,\"wondeing what iam doing wrong\\nhere is my docker-compose.yaml file\\n\\nservices:\\n\\xa0 \\xa0 pgdatabase:\\n\\xa0 \\xa0 \\xa0 \\xa0 image: postgres:13\\n\\xa0 \\xa0 \\xa0 \\xa0 environment:\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 - name=value\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 - POSTGRES_USER= root\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 - POSTGRES_PASSWORD= root\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 - POSTGRES_DB= ny_taxi\\n\\xa0 \\xa0 \\xa0 \\xa0 volumes:\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 - \"\"./ny_taxi_postgres_data2:/var/lib/postgresql/data:rw\"\"\\n\\xa0 \\xa0 \\xa0 \\xa0 ports:\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 - \"\"5432:5432\"\" \\xa0 \\n\\xa0 \\xa0 \\xa0 \\xa0\\n\\n\\xa0 \\xa0 pgadmin:\\n\\xa0 \\xa0 \\xa0 \\xa0 image: dpage/pgadmin4\\n\\xa0 \\xa0 \\xa0 \\xa0 environment:\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 - PGADMIN_DEFAULT_EMAIL=<mailto:admin@admin.com|admin@admin.com>\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 - PGADMIN_DEFAULT_PASSWORD=root\\n\\xa0 \\xa0 \\xa0 \\xa0 \\n\\xa0 \\xa0 \\xa0 \\xa0 ports:\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 - \"\"8080:80\"\"\",1643061968.489400,1643276515.036300,U02RTJPV6TZ\\n52184f9d-a1a1-46bd-9e05-a520d53ef782,U02UA0EEHA8,,,\"still not working, I removed all volumes and images and re-run all the instructions, still same issue..I tried to google it, but no solutions\",1643271742.004500,1643276558.036500,U02UA0EEHA8\\nfa60742b-5d23-40c6-b8fa-27fbf3cabf0a,U0290EYCA7Q,,,Thanks.,1643274368.027300,1643276573.036700,U0290EYCA7Q\\nd594a550-e221-4201-acc4-cb9dc5665736,U02TMP4GJEM,,,\"a folder \"\"ny_taxi_postgres_data2\"\" is able to be created with contents in it\",1643061968.489400,1643276793.036900,U02RTJPV6TZ\\n39ed1975-b9f3-4492-a87e-6246509d394d,U02TMP4GJEM,,,pgadmin opens well but connection fails,1643061968.489400,1643276931.037400,U02RTJPV6TZ\\nb9618d7a-a98a-4dc2-8264-b3aaba7b3a93,U02TMP4GJEM,,,,1643061968.489400,1643276939.037600,U02RTJPV6TZ\\n3bca0384-5522-4b12-9098-09a1703163a9,U02TMP4GJEM,,,\"&gt; pgdatabase_1\\xa0| 2022-01-27 09:17:52.528 UTC [1] LOG:\\xa0database system is ready to accept connections\\nYour database is working\",1643061968.489400,1643277007.038000,U01AXE0P5M3\\nc58a319e-e8c4-4b70-be9d-7fd47d83c8f4,U02TMP4GJEM,,,\"you need to use port 5432\\n\\n<https://www.youtube.com/watch?v=tOr4hTsHOzU&amp;list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&amp;index=13>\",1643061968.489400,1643277031.038200,U01AXE0P5M3\\n7aae5391-e556-4081-911f-84466d26798b,U02TMP4GJEM,,,port 5432 fails too,1643061968.489400,1643277103.038500,U02RTJPV6TZ\\n4c73865a-244e-49c8-95f9-61fc0e52d321,U02TMP4GJEM,,,the error is different now. make sure you put the correct passwrd,1643061968.489400,1643277174.038900,U01AXE0P5M3\\n887c526f-f550-490e-8850-332e37285f9f,U02TMP4GJEM,,,am surely stuck since morning,1643061968.489400,1643277238.039100,U02RTJPV6TZ\\n790fde79-4519-4a34-b642-953bc35115d9,U02TMP4GJEM,,,\"am using password \"\"root\"\"\",1643061968.489400,1643277256.039300,U02RTJPV6TZ\\n29c6c20d-6ef5-4c15-af44-f2d3104f35d4,U0290EYCA7Q,,,The steps are available in the GitHub repo for week1/terraform_gcp . It\\'s also covered in the companion Youtube videos of this course,1643274368.027300,1643277339.039600,U01DHB2HS3X\\nb847b6cf-5a66-47cc-9313-28fb00cec087,U02TMP4GJEM,,,remove extra spaces from your yaml environment section,1643061968.489400,1643277365.040200,U01AXE0P5M3\\neaf41b45-36d5-41cc-8a19-28448c3c7312,,6.0,,will this be a problem if I don\\'t set the AIRFLOW_UID?,1643277422.040900,1643277422.040900,U02RA8F3LQY\\n60c07585-bd2d-4b55-88fc-a0b72746f048,U02RA8F3LQY,,,For me it doesn\\'t work if I don\\'t set it (tested on Windows and Linux),1643277422.040900,1643277476.041200,U01AXE0P5M3\\n18009539-d478-4c5d-a6ec-362127b4a092,U0290EYCA7Q,,,\"I mean what\\'s the difference between ADC, and service-account json? Which one we should add to the ENV variable?\",1643274368.027300,1643277488.041400,U0290EYCA7Q\\nbee4f8b0-873d-4fca-80b8-e36a7a4927f6,U0290EYCA7Q,,,\"Application-default-credentials.json:\\n```{\\n  \"\"client_id\"\": \"\"76....<http://408.apps.googleusercontent.com|408.apps.googleusercontent.com>\"\",\\n  \"\"client_secret\"\": \"\"d-....D0Ty\"\",\\n  \"\"refresh_token\"\": \"\"1/r............................emnY02\"\",\\n  \"\"type\"\": \"\"authorized_user\"\"\\n}```\\nService account\\'s key format:\\n```{\\n  \"\"type\"\": \"\"service_account\"\",\\n  \"\"project_id\"\": \"\"ID\"\",\\n  \"\"private_key_id\"\": \"\"9a4.................................bbaad80\"\",\\n  \"\"private_key\"\": \"\"-----BEGIN PRIVATE KEY-----\\\\nMI................................e\\\\n-----END PRIVATE KEY-----\\\\n\"\",\\n  \"\"client_email\"\": \"\"<mailto:name@project.iam.gserviceaccount.com|name@project.iam.gserviceaccount.com>\"\",\\n  \"\"client_id\"\": \"\"10..................886\"\",\\n  \"\"auth_uri\"\": \"\"<https://accounts.google.com/o/oauth2/auth>\"\",\\n  \"\"token_uri\"\": \"\"<https://oauth2.googleapis.com/token>\"\",\\n  \"\"auth_provider_x509_cert_url\"\": \"\"<https://www.googleapis.com/oauth2/v1/certs>\"\",\\n  \"\"client_x509_cert_url\"\": \"\"<https://www.googleapis.com/robot/v1/metadata/x509/name%40project.iam.gserviceaccount.com>\"\"\\n}```\\n\",1643274368.027300,1643277521.041600,U0290EYCA7Q\\n7d8dbf99-1f7e-4775-a666-7f517cb21ee3,U0290EYCA7Q,,,My old json got deleted.,1643274368.027300,1643277550.041800,U0290EYCA7Q\\nb1630e8b-5443-4373-a6be-c253a6fb0551,U02RA8F3LQY,,,what is the purpose of set the AIRFLOW_UID? is this for airflow gui?,1643277422.040900,1643277631.042100,U02RA8F3LQY\\nf094536c-172a-41e1-abdf-1e3a1c8f1465,U02RA8F3LQY,,,\"not only, also for being able to access the files you put in the dags folder and others. Especially the dags one - this is where you\\'ll be putting your code\",1643277422.040900,1643277693.043100,U01AXE0P5M3\\n4fb25a8c-a3eb-4c85-ad78-4b2d7243a087,U0290EYCA7Q,,,\"I remember calling auth command, after setting service account json. Is there a way to download it again?\\n\\n```Set environment variable to point to your downloaded GCP keys:\\nexport GOOGLE_APPLICATION_CREDENTIALS=\"\"&lt;path/to/your/service-account-authkeys&gt;.json\"\"\\n\\n# Refresh token/session, and verify authentication\\ngcloud auth application-default login```\\n\",1643274368.027300,1643277799.043400,U0290EYCA7Q\\n72666892-4569-4819-a796-2a44a18e9d78,U02TMP4GJEM,,,removing extra spaces surely has saved my day......God when am i becoming a good programmer:smiling_face_with_tear:,1643061968.489400,1643277886.043700,U02RTJPV6TZ\\n96372895-d848-4cc9-b96b-3a53555ba581,U0290EYCA7Q,,,\"This course is making use of a service account and its keys, instead of your own admin account access. Therefore, I advise you to go that way\\n<https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/2_gcp_overview.md#initial-setup|https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/2_gcp_overview.md#initial-setup>\\n\\nOnce you have created a service account, you\\'ll see 3 dots next to it on GUI to generate and download your key. Again,  please watch the associated videos\",1643274368.027300,1643277971.043900,U01DHB2HS3X\\n5dda161d-a8a3-4235-a312-bcc56b16344f,U02RA8F3LQY,,,thanks for the explanation,1643277422.040900,1643278196.044200,U02RA8F3LQY\\ne4729fe3-85ff-4615-85cd-232636e2afe2,U0290EYCA7Q,,,\"Can see the keys, but no idea how to download the json again. I can create another key, and use it for authentication though.\",1643274368.027300,1643278252.044400,U0290EYCA7Q\\n014d082c-73cf-40d9-89a9-601039feffac,U0290EYCA7Q,,,Looks like we need to create it again.,1643274368.027300,1643278362.044800,U0290EYCA7Q\\n9037cd77-00b9-4464-9bc9-9b9e7acec46a,U02SQQYTR7U,,,\"If you use version 2.2.3 you should run it with command \"\"docker compose\"\" without \"\"-\"\" between words\",1643237781.454600,1643278390.045200,U02QL1EG0LV\\n6514f6b7-9a43-4043-a669-fb5ffd969af7,U02RA8F3LQY,,,\"For MacOS, I was able to skip it. For other OS\\'s it asks for it. It is used for your local user to be able to access the services created by Docker.\",1643277422.040900,1643278523.046100,U01DHB2HS3X\\n5793ae90-ff65-41ff-95a1-6ad99d3ff07b,U02SQQYTR7U,,,\"thank for advice <@U02QL1EG0LV>\\ncurrently I already successfully upgrade so docker 2.2.3 and also already install compose switch\\n\\nusing step on official documentation\\n\\n<https://docs.docker.com/compose/cli-command/>\",1643237781.454600,1643278576.047200,U02SQQYTR7U\\n989f9e34-ee5b-4c07-933c-46f6bdb6bc8c,,5.0,,\"About SQL... I think that use LIMIT (1) in query  is not correct, because we could get 2 or more images with similar max values\",1643278789.049800,1643278789.049800,U02QL1EG0LV\\n4e237145-5f15-497e-bf11-20002a7ded6e,U02RA8F3LQY,,,I ran it on Ubuntu. It didn\\'t work form me without setting the variable. I was getting permission errors from  airflow scheduler container. I followed 4th point from \\'week_2_data_ingestion/airflow/1_setup.md\\'.  It also explains why you need to set it.,1643277422.040900,1643278985.049900,U02S9SSURMH\\n3b503ced-c170-46cf-9e62-3d3b64cd4daa,,,,\"About `if __name__ == \"\"__main__\"\"` \\n<https://www.freecodecamp.org/news/if-name-main-python-example/>\",,1643279871.051900,U02CD7E30T0\\n982939cb-e19d-4244-b979-ed365afb44bc,U02UA0EEHA8,,,\"<@U01AXE0P5M3> fyi I tried to run a basic version of dockerized version as described here <https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html#docker-compose-env-variables>, skipping though the `docker-compose build`  part, so I did not create a Dockerfile with some custom images, but used the ready version and skipped directly to `docker-compose up airflow-init` , and I go no issue and could access the web GUI through the localhost..so at least I could understand where the problem is, but not sure how to solve it..\",1643271742.004500,1643280509.052200,U02UA0EEHA8\\n66b5fda9-b521-45ad-a40e-c7df78227a31,U02QL1EG0LV,,,\"That\\'s is a very valid observation!\\nIn this case that was not the case (I originally listed all to add extra options in the multiple choice) that\\'s why I focused in showing only the limit 1.\\nAn even better solution would be to use a dense_rank and filter in 1, that would give you always the records that have the max max, no matter if it\\'s 1 or n.\",1643278789.049800,1643280538.052400,U01B6TH1LRL\\nea236d5d-d485-46f1-8d58-d85aa4ddc4d0,U02UA0EEHA8,,,\"interesting - thanks for sharing. for this week you\\'ll actually need to add a few python libraries to the worker, but you can do it by using the `_PIP_ADDITIONAL_REQUIREMENTS`  env variable\\n\\nLike that\\n\\n```export _PIP_ADDITIONAL_REQUIREMENTS=\"\"pandas pyarrow psycopg2-binary\"\" ... \\n# or put it to .env \\ndocker-compose up```\",1643271742.004500,1643280697.053200,U01AXE0P5M3\\n3659a9b5-c0f0-4403-b758-09cc78268e3d,U02UA0EEHA8,,,or you can try to extend the currently working version and add an `pip install` statement there,1643271742.004500,1643280722.053400,U01AXE0P5M3\\ne96a76a8-81cc-465f-91a4-d7611e923d8d,U02UA0EEHA8,,,ok thank you! will do :slightly_smiling_face:,1643271742.004500,1643280763.053600,U02UA0EEHA8\\ncfb8a91f-68e5-47f5-bee2-34fe9315774f,U02UA0EEHA8,,,please let us know how it works out for you at the end!,1643271742.004500,1643280774.053800,U01AXE0P5M3\\n409688e8-0c00-4a5c-b17e-bcde5364cfbc,U02SFFC9FPC,,,\"Indeed, i have anaconda on my PC for VM;  i would though apparently my GCP has ceased providing the trial access? genuinely unaware as to why or how to rectify since i am a novice with GCP.\",1643130321.120900,1643282054.054000,U02SFFC9FPC\\n1f94129c-daa4-4899-a455-f8e5c232a0f3,U02SFFC9FPC,,,\"I attempted this with other google accounts only to encounter the same issue, though this time the account was inaccessible on the first day of activation. Not sure what is the issue here.\\n\\nEvidence is whenever i attempt create a new project(ie the one instructed to inside your tutorial for terraform) the platform indicates it is unable to succeed in creating it, nor do any of the 3 account i established have permission access to do so.\",1643237452.453700,1643282252.054300,U02SFFC9FPC\\nef075201-c21d-47dc-b432-6d3f262e1fed,U02QL1EG0LV,,,\"Another solution could be to have another extra rank. For example, \"\"if the values are tie then consider the first alphabetic order\"\"\",1643278789.049800,1643282453.054600,U02CD7E30T0\\n75df52a1-2ee9-4031-a853-a59c0aff46f2,U02SFFC9FPC,,,,1643237452.453700,1643282923.055400,U02SFFC9FPC\\n8caecf1f-ec4c-488b-ae8a-1444c9d74244,U02QL1EG0LV,,,I used subquery for this task. But I don\\'t know Is subquery a good practice for it?,1643278789.049800,1643282923.055500,U02QL1EG0LV\\nf2b8b1c6-f4de-4735-9923-696c1fbe0112,,5.0,,\"There is something that is not very clear about the last SQL question. There are actually 2 records in the taxi_zone_lookup table that may be considered \\'Unknown\\':\\n```Unknown\\tNV\\tN/A\\nUnknown\\tNA\\tN/A```\\nIt\\'s not clear if we have to treat them as 2 distinct destinations or merge into a single \\'Unknown\\' destination.\\nI ended up merging into a single destination, and so my answer is different than the presented in the homework results video.\\nWhen you join them, the result is: \"\"Alphabet City / Unknown | 490.54\"\"\\nThe presented result was \"\"\"\"Alphabet City / NA | 2292.40\"\"\",1643283236.059300,1643283236.059300,U030F0YHDAM\\nd9093c95-cd4d-44b9-9f75-f409ec7a8771,U030F0YHDAM,,,You should group by location id,1643283236.059300,1643283479.059800,U02QNCUUHEY\\nf0987a19-63dd-472c-8067-52ef4d54011c,U030F0YHDAM,,,\"The presented query groups by the resulting string, not by id. But as it didn\\'t turn the \\'NA\\' and \\'NV\\' values to \\'Unknown\\', as the question asked, it ended up being treated as 2 distinct destinations\",1643283236.059300,1643283618.060000,U030F0YHDAM\\n84e0ccb7-3ee3-4c5e-917a-148ef9a07c03,U02QL1EG0LV,,,\"Normally in SQL subquery is not a good solution in terms of code readiness. Make the query more confuse.\\nPeople normally says it affects the performance but I am not totally sure on that.\\nTake a look at this article:\\n<https://www.scarydba.com/2016/10/24/sub-query-not-hurt-performance/#:~:text=The%20things%20you%20read%20on,query%20because%20that%20hurts%20performance.%E2%80%9D|https://www.scarydba.com/2016/10/24/sub-query-not-hurt-performance/#:~:text=The%20things%20you%20read%20on[…]%20that%20hurts%20performance.%E2%80%9D>\",1643278789.049800,1643283732.060500,U02CD7E30T0\\nf0775d74-5201-4ba6-9fc5-96ccf7e27c2b,,5.0,,\"Helllo everyone! Not sure what is the cause for the following error when running Airflow \\'local_to_gcs_task\\' :\\n```google.auth.exceptions.DefaultCredentialsError: File /.gc/silicon-vista-338510-d6c314a4fb43.json was not found.```\\nI created a .gc folder during week1 and there is a json file in it. In the docker-compose.yaml I have: `GOOGLE_APPLICATION_CREDENTIALS: /.gc/silicon-vista-338510-d6c314a4fb43.json`\\n`AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT: \\'google-cloud-platform://?extra__google_cloud_platform__key_path=/.gc/silicon-vista-338510-d6c314a4fb43.json\\'`\\n`GCP_PROJECT_ID: \\'silicon-vista-338510\\'`\\n`GCP_GCS_BUCKET: \\'dtc_data_lake_silicon-vista-338510\\'`\",1643284094.063800,1643284094.063800,U02RR9Z0CCV\\n4f18b742-6f73-4886-80cc-9edef06bc8f6,U030F0YHDAM,,,\"In real world, it\\'s always better group by id. Missing names and descriptions are frequent in practic.\",1643283236.059300,1643284146.063900,U02QNCUUHEY\\n1e60f701-bf42-4bf3-b029-eecf418eac65,U030F0YHDAM,,,\"In most cases, yes, but maybe not always. Whe need to check the business rule in order to implement it correctly. In this case, the SQL developer should ask to the client if all the unknow destinations should be treated separately or as a single one, because the results are different\",1643283236.059300,1643284546.064200,U030F0YHDAM\\n1d8ebb7c-1143-4743-9469-32a2f10cdea0,U02RR9Z0CCV,,,\"If this is docker, you can `exec`  into the container and check if the path exists\",1643284094.063800,1643284638.064400,U01DFQ82AK1\\nc19fb34e-464b-4978-b03d-535148e05706,U02QL1EG0LV,,,\"It\\'s encouraged to use CTEs and maybe even window functions over subqueries, specially given that cloud data warehouses have been improving their performance for such functions. It improves readability a lot and helps you modularize your code.\",1643278789.049800,1643285987.064600,U01B6TH1LRL\\nb4a5a429-4f57-46a3-9672-0f4f85e5c064,U02S9JS3D2R,,,\"as i was working on some windowing functions, i got to learn there is a \\'copy\\' command to copy your data from a csv file to the table in postgres and then there was also another option of importing your data from a file to your table\\nit could be because of segregating the tasks so they are handled in a more efficient manner\",1643275045.033500,1643286404.065400,U029DM0GQHJ\\neef2cf06-10aa-4baf-9040-4610c8cafa69,U02U5SW982W,,,\"I just created a new PR with the added section, so once it gets accepted feel free to add your link below :slightly_smiling_face:\",1643256979.476800,1643286439.065700,U02BVP1QTQF\\n0cfc8a56-0080-49b3-8026-5e66f239b034,U02S9JS3D2R,,,<https://stackoverflow.com/questions/2987433/how-to-import-csv-file-data-into-a-postgresql-table>,1643275045.033500,1643286460.066000,U029DM0GQHJ\\n9F39C296-B40F-4C49-883D-64FD7C5C288D,U02T0CYNNP2,,,\"Thanks for the inputs <@U02T0CYNNP2> I will try that solution  \\nFor log files you can go to graphs in DAG and click on the task .on the pop window you will have option to view log files \",1643234608.442300,1643286574.068600,U02AGF1S0TY\\nfe0781e2-6e8a-4f4f-bdbc-f136d6ec0f3e,U02RR9Z0CCV,,,Did you modify docker compose file? Can you share entries to x-airflow-common -&gt; volumes entries from  week_2_data_ingestion/airflow/docker-compose.yaml?,1643284094.063800,1643287365.069100,U02S9SSURMH\\nbb895c3e-bfc1-4ebd-94d9-893d002c953f,,2.0,,I was getting a GPG error on the apt MySQL Client install - for week 2 - changing the airflow version in Dockerfile from 2.2.3 to 2.2.2 seems to work FROM apache/airflow:2.2.2.,1643287994.072400,1643287994.072400,U02HLE69P19\\n50143c19-01fd-4ada-9635-307225549e2f,U02T0CYNNP2,,,Ok..thanks,1643234608.442300,1643288141.073900,U02T0CYNNP2\\n1d5f764f-b627-4aec-a2c3-4f03656101cd,,2.0,,What\\'s the link to the homework 1 solutions ?,1643288377.074900,1643288377.074900,U02TMP4GJEM\\n7308133c-0afd-4c06-9dea-b3939cacab66,U02TMP4GJEM,,,<https://www.youtube.com/watch?v=HxHqH2ARfxM>,1643288377.074900,1643288480.075100,U02S9SSURMH\\n85e5ba02-c79e-445d-9145-e5a1feb950f8,U02TMP4GJEM,,,ta,1643288377.074900,1643288494.075400,U02TMP4GJEM\\n84ad5dc6-3b8e-41b4-a53f-4ba1f585b7a5,U02RR9Z0CCV,,,\"Yes. As <@U02S9SSURMH> said, the volume that I mount in the setup is `.google/credentials` . If you\\'re using `.gc` , you should have that mounted under `volumes:` . The left side of the `:` is your local directory and the right side is the docker container\\'s directory (where it\\'s supposed to mount).\\n\\nCheck line 70 in the docker-compose file:\\n<https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_2_data_ingestion/airflow/docker-compose.yaml#L70|https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_2_data_ingestion/airflow/docker-compose.yaml#L70>\",1643284094.063800,1643288590.075700,U01DHB2HS3X\\n1999201a-9817-4965-acfd-ea1296feaf2c,U02RR9Z0CCV,,,\"I\\'ll also make a note to change this in my setup from `.google/credentials` to `.gc` , so that things flow seamlessly for you all. Thank you for bringing this question.\",1643284094.063800,1643288727.076100,U01DHB2HS3X\\ncce2eb58-c659-46f4-bad0-8238eb8799e0,U02RR9Z0CCV,,,<@U01DFQ82AK1> <@U02S9SSURMH> <@U01DHB2HS3X> Thank you so much for your help! Now everything works :slightly_smiling_face:,1643284094.063800,1643289879.079500,U02RR9Z0CCV\\n3c2d6c6f-ee15-4573-b325-802ac71f7d76,,,,\"Some alternatives to Terraform. If you know any other tools, please reply\\n• <https://www.pulumi.com/|Pulumi>\\n• <https://cloud.google.com/deployment-manager/docs|Google Cloud Deployment Manager>. Google cloud specific.\\n• <https://docs.microsoft.com/en-us/azure/azure-resource-manager/management/overview|Azure Resource Manager>. Azure specific. \\n• <https://aws.amazon.com/cloudformation/|AWS CloudFormation>. AWS specific.\\n• <https://crossplane.io/|Crossplane>\\n\",,1643290283.080800,U02S9SSURMH\\n20464c91-959c-435a-9195-63f87f394cb8,U02VA225CG4,,,Thank you very much!,1643266612.494000,1643290602.081100,U02VA225CG4\\nf57316b1-4d18-4b85-8348-74f49f5e97db,,1.0,,\"<@U01AXE0P5M3> If time permits, possible to make a video on uid and gid in docker container?\",1643291242.083400,1643291242.083400,U0290EYCA7Q\\nd810505f-311e-4a66-861e-57a92f88e693,U02VA225CG4,,,\"This is my go to channel\\n\\n<https://www.youtube.com/c/LearnLinuxtv/playlists>\",1643266612.494000,1643291363.083500,U0290EYCA7Q\\n401a5f4a-0588-46e5-85f1-dda92a82db2e,U0290EYCA7Q,,,\"Great suggestion! But probably not in the scope of this course, we don\\'t want to diverge too far from actual data engineering =)\",1643291242.083400,1643291803.083800,U01AXE0P5M3\\n2f748765-05d5-4cb1-98e3-70083d818f80,U02QZN0LSBT,,,Thanks Sandy. Appreciate it. I will keep working on it :blush:,1643240547.458200,1643292671.085900,U02QZN0LSBT\\n0afdad91-a4b5-431d-8804-614439c81774,,,,\"Hi! I did not make it on time for the first week homework, just had tons of work (and exams). I\\'ll finish it by tomorrow Friday. Just wanted to say i\\'m not leaving the zoomcamp even though i\\'m a bit late :slightly_smiling_face: <@U01AXE0P5M3>\",,1643292907.087700,U02Q51Y4MM5\\n8cf6df88-9c2a-47dd-a126-d7895c7a1e52,,1.0,,\"Hi. In \"\"Setup Airflow Environment with Docker-Compose\"\" it is a bit unclear to me on what we are supposed to do with AIRFLOW_UID and google_credentials.json when working on windows.\\n\\n1.How do we set the AIRFLOW_UID in windows? The command (echo -e \"\"AIRFLOW_UID=$(id -u)\"\" &gt; .env) just created a file called \"\".env\"\" with this text in its content: \"\"-e \"\"AIRFLOW_UID=$(id -u)\"\" \"\" . When running \"\"docker-compose build\"\" this file causes an error .\\nHow should the file .env look like?\\n\\n2. I moved the google_credentials.json to a different path and changed the airflow_home variable in Dockerfile. The file is there, yet I am getting this error:\",1643293627.091900,1643293627.091900,U02V1JC8KR6\\n852fcf82-f05e-47c2-8b8f-dee9149f4edf,U02V1JC8KR6,,,\"`(base) ~ % echo -e \"\"AIRFLOW_UID=$(id -u)\"\"`\\n`AIRFLOW_UID=501`\\n\\n2. You need to pull the latest from github repository.\\n\\nYou can `cat .env`\\n\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1643194989259100?thread_ts=1643189014.245800&amp;cid=C01FABYF2RG>\",1643293627.091900,1643293689.092500,U0290EYCA7Q\\n261d59b0-db12-4829-8941-5e3f83d20eb2,,2.0,,my docker memory won\\'t go up more than 4 G and i need &gt;=5 G for airflow. I am on Mac FYI. Thank you,1643296233.095100,1643296233.095100,U02T8HEJ1AS\\nb4e8f1b6-03a7-4ebd-bb59-bac2f04c73a0,,7.0,,Hi\\xa0<@U01AXE0P5M3>\\xa0i just accidentally delete my week 1 repo which contain the homework of week 1 when doing my week 2 progress. I\\'m afraid that my homework link not working for review. Any solution to this?,1643296263.095600,1643296263.095600,U02RA8F3LQY\\n14592e2d-3d48-4034-9ea7-0d957b40f96a,U02RA8F3LQY,,,\"I am guessing, if you\\'re using Github and you still have your local copy, you can push it again to the repo. The final link should be the same as the original, so the link you submitted should then still work.\",1643296263.095600,1643296953.097100,U02QPBZ3P8D\\n75914d47-4003-4a3b-9719-41000ee77c50,,2.0,,\"Hi All, for the week 2 airflow setup, if we want to use Google Compute Engine to setup. What\\'s the idea spec for the VM? so that we have sufficient credit to go through the whole zoom camp :smile:\",1643296959.097400,1643296959.097400,U02T697HNUD\\n19189059-d895-4678-8da2-2dcdbc01da49,U02RA8F3LQY,,,you deleted whole repo or files in repo?,1643296263.095600,1643299092.099900,U01CXB13GRM\\nbd910d1b-3963-4843-8e8a-425b58b12f13,,8.0,,\"Hello everyone, I’m running into an error when I run the data ingestion pipeline on Airflow. Details to follow.\",1643299377.100400,1643299377.100400,U02Q9P0A0NA\\na2768798-26bb-4796-aefc-dc78fe46acb7,U02Q9P0A0NA,,,\"When I trigger the DAG, it fails at “local_to_gcs_task”. Upon checking logs it shows the below error.\\n```message\"\": \"\"&lt;My service ID&gt;.<http://iam.gserviceaccount.com|iam.gserviceaccount.com> does not have storage.objects.create access to the Google Cloud Storage object.```\\nI have added the roles “Storage Object Admin”, “Storage Admin”, “BigQuery Admin” on IAM. I’m not sure why its failing at this step. Hoping someone could help.\",1643299377.100400,1643299406.100500,U02Q9P0A0NA\\n42758C31-D73A-4305-81F7-D73DD8F44613,U02Q9P0A0NA,,,\"Hope you have created the storage and BQ using terraform prior to this . In case you have deleted the same , u need to recreate . You can go to console and confirm \",1643299377.100400,1643299865.103500,U02AGF1S0TY\\naf443298-76d5-4c8c-80e6-42b66836f553,U02RA8F3LQY,,,You could also check the repo history and may be able to checkout to the stage where you had homework 1 and recover the file,1643296263.095600,1643300506.104100,U01B6TH1LRL\\n8e6b4979-be48-49ce-bd1c-4b870a7572ee,U02RA8F3LQY,,,can you sahred your repo here?,1643296263.095600,1643300523.104300,U01B6TH1LRL\\nFB6DAED6-5723-44FD-B4F0-705E0B26620A,,1.0,,\"Is there a reason I had to go back into GCP and assign roles like storage admin to my service account?\\n\\nI\\'m sure this was done in a previous step, but I had to redo it after getting a DAG error\\n\\nIt\\'s like it reset itself or something. The only role it has was viewer (unless I\\'m getting mixed up)\",1643300962.106700,1643300962.106700,U02U34YJ8C8\\na34f7566-a371-44de-830f-b1b20257c3fc,U02Q9P0A0NA,,,\"Have you used your own project, in the compose.yml? You can\\'t access project of <@U01DHB2HS3X>\\n\\n``` GCP_PROJECT_ID: \\'pivotal-surfer-336713\\'\\n GCP_GCS_BUCKET: \"\"dtc_data_lake_pivotal-surfer-336713\"\"```\",1643299377.100400,1643301363.107000,U0290EYCA7Q\\n042ddc12-b0c1-48a9-be39-15488bf0b3c1,,3.0,,\"Should all nodes be healthy? If so ,how to restart them standalone? <@U01DHB2HS3X> <@U01AXE0P5M3>\\n\\n```bash-3.2$ docker ps\\nCONTAINER ID   IMAGE                       COMMAND                  CREATED          STATUS                      PORTS                              NAMES\\n28d82d2a2f7a   airflow_airflow-worker      \"\"/usr/bin/dumb-init …\"\"   59 minutes ago   Up 58 minutes (unhealthy)   8080/tcp                           airflow-worker\\nda3bcf6cae34   airflow_airflow-webserver   \"\"/usr/bin/dumb-init …\"\"   59 minutes ago   Up 58 minutes (healthy)     0.0.0.0:8080-&gt;8080/tcp             airflow-webserver\\n1b187d7f4d6f   airflow_airflow-scheduler   \"\"/usr/bin/dumb-init …\"\"   59 minutes ago   Up 58 minutes (unhealthy)   8080/tcp                           airflow-scheduler\\n6cb137e568fd   airflow_airflow-triggerer   \"\"/usr/bin/dumb-init …\"\"   59 minutes ago   Up 58 minutes (unhealthy)   8080/tcp                           airflow-triggerer\\n2c4dc3ffa5a7   airflow_flower              \"\"/usr/bin/dumb-init …\"\"   59 minutes ago   Up 58 minutes (healthy)     0.0.0.0:5555-&gt;5555/tcp, 8080/tcp   airflow-flower\\n02960036b56b   redis:latest                \"\"docker-entrypoint.s…\"\"   59 minutes ago   Up 59 minutes (healthy)     6379/tcp                           airflow-redis\\n66a71c2a111f   postgres:13                 \"\"docker-entrypoint.s…\"\"   59 minutes ago   Up 59 minutes (healthy)     0.0.0.0:5432-&gt;5432/tcp             airflow-pgdatabase```\",1643301439.108100,1643301439.108100,U0290EYCA7Q\\n9547afd1-1775-473d-805e-a11db290fc06,U0290EYCA7Q,,,\"They should. I think you can do docker-compose down and name, like you do with up\",1643301439.108100,1643301566.108500,U01AXE0P5M3\\n123ca2f9-083f-4e9d-907a-f9874dbe702a,,,,\"Havent read it yet, but seems interesting on how all the networking happens under the hood with Docker Desktop. Might help understand some of the docker trickery that we are doing here a little better.\\n\\n<https://www.docker.com/blog/how-docker-desktop-networking-works-under-the-hood/>\",,1643301633.109700,U02TATJKLHG\\nb6f84038-6ec2-4734-8737-472c6250586d,,4.0,,\"Issue fixed\\n\\n~Following the video \"\"2.3.1\"\" on Airflow setup, the \"\"docker-compose build\"\" fails with the following log message and google search, does not seem to help much~ :disappointed: ~any suggestions please~\\n\\n``` &gt; [airflow_airflow-triggerer 2/6] RUN apt-get update -qq &amp;&amp; apt-get install vim -qqq:\\n#15 4.239 E: The repository \\'<https://packages.microsoft.com/debian/10/prod> buster Release\\' does not have a Release file.\\n#15 4.357 E: The repository \\'<https://packages.microsoft.com/debian/10/prod> buster Release\\' does not have a Release file.\\n#15 4.218 E: The repository \\'<https://packages.microsoft.com/debian/10/prod> buster Release\\' does not have a Release file.\\n#15 4.200 E: The repository \\'<https://packages.microsoft.com/debian/10/prod> buster Release\\' does not have a Release file.\\n#15 4.163 E: The repository \\'<https://packages.microsoft.com/debian/10/prod> buster Release\\' does not have a Release file.\\n#15 4.181 E: The repository \\'<https://packages.microsoft.com/debian/10/prod> buster Release\\' does not have a Release file.\\n------\\nfailed to solve: rpc error: code = Unknown desc = executor failed running [/bin/bash -o pipefail -o errexit -o nounset -o nolog -c apt-get update -qq &amp;&amp; apt-get install vim -qqq]: exit code: 100\\n(base)```\",1643301733.110800,1643301733.110800,U02SMBGHBUN\\n5d7be31e-d3bd-4d06-ac3f-afb1e99aa795,U02SMBGHBUN,,,I am using the docker-compose.yaml and Dockerfile provided under week2,1643301733.110800,1643301769.110900,U02SMBGHBUN\\n5F92C846-3911-4790-A93C-B2D6F2164E36,U02T8HEJ1AS,,,Have you tried it with just 4GB?,1643296233.095100,1643302127.111500,U02U34YJ8C8\\nCA916CC8-8915-4665-9701-182B19615796,U02SMBGHBUN,,,Try just docker build .,1643301733.110800,1643302193.112100,U02AGF1S0TY\\n523190c5-91a8-467d-ab38-358ce782dae5,U030F0YHDAM,,,\"These are valid points, in my opinion the grouping should be by locationid as well because even though you have no name for two or more zones, those are still different dropoff locations hence not correct to count them as the same.\\n\\nGiven that said, even though I show the grouping by the name concatenation and not the location ids, if you check the data, there\\'s only one NA, the other one is NV and the N/A at the end corresponds to the service zone, not the zone itself and also all location id had a join in the table. Which at the end, means that each dropoff zone counted like one.\\n\\nLet me know if this clarifies the answer\",1643283236.059300,1643302476.112400,U01B6TH1LRL\\n85403175-3c2a-4cca-aa0e-47da208b44dc,U02SMBGHBUN,,,How much RAM and CPU have you allocated to Docker?,1643301733.110800,1643302628.112800,U02U34YJ8C8\\nfcd519da-b38c-4d96-8b8e-f2a9b1bc0e07,U02SMBGHBUN,,,Figured it out. It was a slightly uncommon issue. My work laptop has Netskope and uses a custom root certificate. So i had to cp the root cert into the linux container to make apt-get work,1643301733.110800,1643302691.113000,U02SMBGHBUN\\nFE83EBCB-FEC4-4D1A-BFF7-1DD4AC399F1A,,24.0,,\"Hi, i have a problem with ingesting data to postgres, when i did the conection to the database and i code the command engine.conection(), this give a error like the first picture. But other thing is when i tried to login to the database, its make nothing like the second picture. \\ni hope they can help me, thanks \",1643303881.118400,1643303881.118400,U030PFH5CRX\\nca0220eb-efa9-4f3c-96ea-452772932f7e,U030PFH5CRX,,,Git Bash/\\'pgcli with git bash\\' has some problem which makes it hang during the password input stage. You can try connecting through CMD or a integrated Git Bash terminal in something like VS Code,1643303881.118400,1643304324.119000,U02TATJKLHG\\n43e04233-6763-49d3-999d-074ac777ecb9,U02UY1QTGHW,,,\"I shouldn\\'t, yes. If it does flush the data, something is wrong\",1643272081.011200,1643304514.119300,U01AXE0P5M3\\nb9be728a-f2fd-4e61-a1a4-d5b02d9cde72,U0290EYCA7Q,,,\"Yes I think you can download it only once. If you lost it, you have to create another one\",1643274368.027300,1643304611.119600,U01AXE0P5M3\\nc0dcf221-b70c-47f2-986d-f430e0467ad4,U02HLE69P19,,,I\\'m curious where mysql comes from. Did you change anything?,1643287994.072400,1643304803.119900,U01AXE0P5M3\\nfb0500d2-5243-48d1-92f5-efd6fd8cecab,U030PFH5CRX,,,nothing happend too :disappointed:,1643303881.118400,1643304835.120100,U030PFH5CRX\\nb0225f99-0e41-4d7c-a692-1ac89bf255ee,U030PFH5CRX,,,Can you do `docker ps` and check if the postgres service is running,1643303881.118400,1643305314.120500,U02TATJKLHG\\n059ce95d-1c18-47ae-9920-4a004042b07a,U02RA8F3LQY,,,\"you\\'ll need to create a new repo, set remote to this repo\\n\\n```git remote set-url origin git@github.com:&lt;username&gt;/&lt;repo&gt;.git```\\nand then push\",1643296263.095600,1643305329.120700,U01AXE0P5M3\\nceb613d8-f009-4519-a992-1aaf6ca5e1bf,U02RA8F3LQY,,,\"if you need more info, here\\'s the search query you can use: \"\"git set remote url\"\"\",1643296263.095600,1643305367.120900,U01AXE0P5M3\\n23d0d423-1546-47f2-8282-871179eea7b8,U02RA8F3LQY,,,\"I hope you can fix it soon! it won\\'t affect grading, don\\'t worry\",1643296263.095600,1643305395.121100,U01AXE0P5M3\\nb6e14437-b8c1-480f-b46b-8b3afdf24c54,,1.0,,<@U01DHB2HS3X> did you updated the github for week 2? Is it finalized now <@U01AXE0P5M3>,1643307615.125800,1643307615.125800,U02T8HEJ1AS\\n431c2eba-3aef-4154-834e-76db030f6a5d,,10.0,,\"hello <@U01DHB2HS3X>\\ni was going thru your tutorial video for airflow environment setup when i came across this:\\n`On Linux, the quick-start needs to know your host user-id and needs to have group id set to 0. Otherwise the files created in\\xa0dags,\\xa0logs\\xa0and\\xa0plugins\\xa0will be created with root user. You have to make sure to configure them for the docker-compose:`\\n\\n```echo -e \"\"AIRFLOW_UID=$(id -u)\"\" &gt; .env```\\nbut you nowhere mentioned about setting group id to 0. So will it be set automatically or we would be required to set it as mentioned in the documentation:\\n```echo -e \"\"AIRFLOW_UID=$(id -u)\\\\nAIRFLOW_GID=0\"\" &gt; .env```\",1643307686.127200,1643307686.127200,U029DM0GQHJ\\n20499a54-1d9d-4be8-a739-6cfba7c7351c,U029DM0GQHJ,,,\"I see it in the compose file. `user: \"\"${AIRFLOW_UID:-50000}:0\"\"`\\nbut we are setting 50000 for UID.\\n\\n```(base) % echo ${AIRFLOW_UID:-50000}\\n50000```\\nStill I see 10000 for airflow-init\\n```airflow-init        | AIRFLOW_UID is set to 10000\\nairflow-init        | Current User is root\\nairflow-init        | The container is run as root user. For security, consider using a regular user account.```\",1643307686.127200,1643308015.127300,U0290EYCA7Q\\nbaccb543-71c7-4c93-a9e5-fb989984ea05,U030FNZC26L,,,\"Hi Alexey, How should I create a PR for the blog post? Should I add the link to the readme and create the PR? Can\\'t you just put the link to the readme?\",1642862313.339700,1643308379.127900,U030FNZC26L\\n83dd4910-90fe-43aa-9731-bde5482d2b9a,U029DM0GQHJ,,,\"<@U01DHB2HS3X>\\nso as per dockerfile:\\n\\n# USER airflow\\nUSER $AIRFLOW_UID\\n\\nwill it be 50000 or 501 (in my case when i tried echoing )\",1643307686.127200,1643308675.128800,U029DM0GQHJ\\nf18689bf-162a-4cbd-96ee-58c48d9fac64,U029DM0GQHJ,,,\"Try the one that the command sets for you. If not, set it to 50000\",1643307686.127200,1643308745.129000,U01DHB2HS3X\\n4423066d-aae0-4820-8eb2-8f97f10e7dec,U02SFFC9FPC,,,Attempted a day later with the same result. <@U02U5SW982W>,1643237452.453700,1643308753.129200,U02SFFC9FPC\\nd5edbb9d-8e9e-49b7-851f-7dca0ae75f15,U02SFFC9FPC,,,\"I installed the MSI and the ubuntu executables for WSL2, same result when activating docker upon startup.\\nCould not load file or assembly \\'NLog, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null\\' or one of its dependencies. The located assembly\\'s manifest definition does not match the assembly reference. (Exception from HRESULT: 0x80131040)\\n<@U02GVGA5F9Q>\",1643133164.144400,1643308851.129500,U02SFFC9FPC\\n234c3e93-a2e9-4842-9d4f-240e2d5b9cd5,U029DM0GQHJ,,,\"If it is 50000, it shows airflow inside the docker container for user. Otherwise default user. I think we can use anything &gt; 499\\n\\n```The system User IDs from 0 to 99 should be statically allocated by the system, and shall not be created by applications.\\n\\nThe system User IDs from 100 to 499 should be reserved for dynamic allocation by system administrators and post install scripts using useradd.```\\nairflow use is already there - 50000\\nWe have created a default user.\\n\\n```[2022-01-27, 18:34:25 UTC] {subprocess.py:89} INFO - ***:x:50000:0:First Last,RoomNumber,WorkPhone,HomePhone:/home/***:/bin/bash\\n[2022-01-27, 18:34:25 UTC] {subprocess.py:89} INFO - default:x:10000:0:default user:/home/***:/sbin/nologin\\n[2022-01-27, 18:34:25 UTC] {subprocess.py:89} INFO - default\\n[2022-01-27, 18:34:25 UTC] {subprocess.py:89} INFO - uid=10000(default) gid=0(root) groups=0(root)```\",1643307686.127200,1643308866.129700,U0290EYCA7Q\\n834a774d-a770-4ed4-88e5-35861fa9c3d4,U029DM0GQHJ,,,\"since i already set so if i tried commenting out\\nuser: \"\"${AIRFLOW_UID:-501}:0\"\" in the compose file, will it break the setup?\",1643307686.127200,1643308929.130000,U029DM0GQHJ\\nb45b3b49-96fe-47d8-bee4-d1a454d3edc2,U0290EYCA7Q,,,\"They happen on their own, sometimes it takes awhile depending upon your memory and processor speed. Like I mentioned on another thread, I\\'ll try to trim down the unwanted services, you can also check if the no-frills version in the extras/ directory works\",1643301439.108100,1643308970.130400,U01DHB2HS3X\\nb7f77d8b-093f-4732-94e5-46830e66497a,U029DM0GQHJ,,,\"I don\\'t know. They have 50000 for a reason. In your case, you should see both 50000 and 501 users\",1643307686.127200,1643309107.132100,U0290EYCA7Q\\n7b1120bd-9222-427f-8e6e-efecc38cd4ec,U029DM0GQHJ,,,The only way to find out is to try it out if it works for you. I\\'ll also have to read the documentation to understand.,1643307686.127200,1643309127.132600,U01DHB2HS3X\\nf32c6066-f2cc-4e67-9c02-ba62c81f517c,,3.0,,\"Hi <@U01DHB2HS3X>\\n• In week 1, for the terraform part, after running `terraform init` , `terraform plan` and `terraform apply` , should `terraform destroy` be run too?\\n       or should it be run once the course is completed and the infra is no longer needed?\\n• also, will the credits be charged as long as the infra is provisioned or only when it is being actively used?\",1643309241.134000,1643309241.134000,U0303JLGS23\\n4cdf1951-c76f-40b7-b839-3051b4681d0f,U030PFH5CRX,,,,1643303881.118400,1643309993.134200,U030PFH5CRX\\n7fe71e0b-8c6c-4739-9f92-515c5f128adb,U030PFH5CRX,,,postgres is running,1643303881.118400,1643310023.134600,U030PFH5CRX\\n958af3f4-bae8-45d3-8d1a-b4d93d297544,U02SFFC9FPC,,,\"I\\'d do a complete uninstall. Windows and WSL2 have some quirks, and you may be pushing it too hard. With a clean Windows, then go to the WSL2 Ubuntu\\'s terminal and install Docker per the official instructions. It must work, unless you have a troubled system or troubled machine... With me it worked like a charm!\",1643133164.144400,1643310089.134800,U02GVGA5F9Q\\n03D2F14D-D1E4-4E25-A3E3-BEAB0562C1B5,,5.0,,Hi <@U01AXE0P5M3> . Will there be homework for this week?? ,1643310716.139100,1643310716.139100,U02TBCXNZ60\\nbefded79-8d05-4c60-8883-264417b11574,U0303JLGS23,,,As long as your state file remains preserved/unchanged you can destroy and create as many times,1643309241.134000,1643310909.139700,U01DHB2HS3X\\nb207308d-8905-4eb9-be09-c3ad0eeb4a80,U02UZBJ2Q6L,,,\"Yeah sorry for the late response. Um if you change the path to the linux like path he showed in the video it worked.\\nMounting path structure:\\n```-p /e/zoomcamp/...:/var/lib/postgresql/data```\\nThis was on the github page of week 1,\\n<https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup/2_docker_sql>\",1642769587.082900,1643311442.140000,U02UZBJ2Q6L\\n61ecd5bf-707d-4444-a8cb-bf36ae015152,U02T0CYNNP2,,,\"Hi , running into this error after following the steps mentioned in github readme. Did you find a solution?\",1642710534.439500,1643311627.140300,U02UZBJ2Q6L\\na40f2802-329d-4e59-b5d8-4b96bf341c13,U02TBCXNZ60,,,Well at least one question I saw already :) Inside the DAG file there is a TO DO,1643310716.139100,1643311679.140500,U02CD7E30T0\\nE7106EED-8EDD-45A5-8E55-05FFBC348E5B,U02TBCXNZ60,,,Can you point me to that <@U02CD7E30T0> ,1643310716.139100,1643311733.141300,U02TBCXNZ60\\n3a352041-05a0-4192-8835-b7974af7daea,U02TBCXNZ60,,,I am on my mobile but it is somewhere before the 3th or 4th task,1643310716.139100,1643311819.141500,U02CD7E30T0\\na294122b-09bf-4bff-84d1-53ed1fc6bb71,,5.0,,\"<@U01DHB2HS3X>\\ni am getting this even after allocating more memory:\\n\\n`ERROR - Triggerer\\'s async thread was blocked for ... seconds, likely by a badly-written trigger. Set PYTHONASYNCIODEBUG=1 to get more information on overrunning coroutines.`\",1643311959.142600,1643311959.142600,U029DM0GQHJ\\na6cb1df9-8431-4eb1-8882-4668a07d64de,U030FNZC26L,,,\"Check how Alvaro is doing it\\n\\n<https://github.com/DataTalksClub/data-engineering-zoomcamp/pull/43/files|https://github.com/DataTalksClub/data-engineering-zoomcamp/pull/43/files>\",1642862313.339700,1643312065.142700,U01AXE0P5M3\\nf5770411-3d77-4285-9506-1667aa65b5ce,U029DM0GQHJ,,,I also have this. I see this in the logs when my computer wakes up - after I open my laptop and it goes back to life. I didn\\'t notice any bad consequences of that,1643311959.142600,1643312284.143100,U01AXE0P5M3\\nd20b3699-e195-4ad0-9a2d-6e2c779433d2,,1.0,,Can you please do us a favor and not tag us - unless it\\'s really necessary? We\\'ll see your questions anyways,1643312329.144200,1643312329.144200,U01AXE0P5M3\\necac4ce2-0c3d-43c1-b97c-bf5572f675d6,U029DM0GQHJ,,,\"I see this to too, and It\\'s working fine.\",1643311959.142600,1643312381.144300,U0290EYCA7Q\\neb013254-571d-459a-bda9-e5ab6633d9a0,U029DM0GQHJ,,,\"I also get this, but you can ignore it. If you\\'ve seen the video, I\\'ve mentioned there to ignore these errors as we aren\\'t using these services which are provided by Airflow official template.\",1643311959.142600,1643312417.145000,U01DHB2HS3X\\n50c17eb9-92a2-4392-8df8-a5253fb13d3a,U02T8HEJ1AS,,,Yes we did. No it\\'s not,1643307615.125800,1643312470.145200,U01AXE0P5M3\\n80a304bb-7428-40f1-8c53-80689cd37d1e,U029DM0GQHJ,,,\"Also, try things out first to see if they work for you before posting here. You also don\\'t need to tag us everytime, because others in the community can help you too\",1643311959.142600,1643312476.145400,U01DHB2HS3X\\neab58f2c-13e4-49e4-98e4-dd5c2fe20ce6,U02TBCXNZ60,,,there will be homework this week,1643310716.139100,1643312689.146000,U01AXE0P5M3\\n27967b9b-22b7-41b7-ac7a-111e236adcab,U02TBCXNZ60,,,\"And we\\'ll annouce in the announcements channel when we release it (mostly tomorrow). And don\\'t worry, you\\'ll have sufficient time to finish\",1643310716.139100,1643312744.146300,U01DHB2HS3X\\na5796705-1ff7-4ee9-a1f2-8d9f8c6e729b,,10.0,,\"hello, i started the course without knowing any SQL. do you guys have any recommended quick start resource for me to learn the necessary SQL for this zoomcamp?\",1643312794.147200,1643312794.147200,U02B8U0QZEK\\n11d36278-658a-4a69-ac11-5286ea2948ae,U02U34YJ8C8,,,\"Can\\'t really say, but not a common thing to happen. Maybe you created a new service account?\",1643300962.106700,1643312919.147400,U01DHB2HS3X\\n6ce5e697-039f-4b41-a51a-7dc66fdbec22,U02B8U0QZEK,,,I recommend using the search function on this Slack channel — some great resources have already been posted in earlier threads.,1643312794.147200,1643312934.147700,U02SQ1X29GE\\n32e82680-47ca-4f5f-bcac-7708878614ec,U02T697HNUD,,,\"Using the VM is optional, in case your docker setup doesn’t perform well locally. It just has auto-scaling benefits.\",1643296959.097400,1643313036.148000,U01DHB2HS3X\\n6962f599-3bbd-48a3-8d8c-fe6bf40c5710,U02T8HEJ1AS,,,\"According to the official guidelines, minimum 4GB should be fine. I only recommended 5GB in my setup for better performance.\",1643296233.095100,1643313193.148300,U01DHB2HS3X\\na7872e7c-d985-4f88-a5ee-a4c85bbd1ef2,U02B8U0QZEK,,,<https://www.youtube.com/playlist?list=PLroEs25KGvwzmvIxYHRhoGTz9w8LeXek0>,1643312794.147200,1643313361.148800,U0290EYCA7Q\\n868e06f7-ac77-4cd2-a3be-3fc756ac3dc9,U029DM0GQHJ,,,Oh yes.. slipped off my mind,1643311959.142600,1643313477.149500,U029DM0GQHJ\\nd554d286-5add-47c0-963c-922cc86484be,U02B8U0QZEK,,,\"Two Courses on EDX is on\\n\\n<https://learning.edx.org/course/course-v1:NYUx+NYUx.DBMS.1+2T2020/home>\\n<https://learning.edx.org/course/course-v1:NYUx+NYUx.DBMS.2+2T2020/home>\",1643312794.147200,1643313489.149900,U0290EYCA7Q\\n67686cfe-4f6d-4daa-b4b8-2ab01b0da3c4,U01AXE0P5M3,,,Noted.,1643312329.144200,1643313517.150200,U029DM0GQHJ\\nf71e5069-864d-43d4-b020-3a1c0ce74acd,U02B8U0QZEK,,,\"thanks, i will check them out\",1643312794.147200,1643313521.150400,U02B8U0QZEK\\n54a5d8cc-adf1-45c1-9e31-f6f4b80aee7f,U02B8U0QZEK,,,mode sql tutorial,1643312794.147200,1643313580.150700,U029DM0GQHJ\\nd48c0c0b-17f6-4ccf-a188-1f77bc3ddf7e,U02B8U0QZEK,,,\":arrow_up: <https://mode.com/sql-tutorial/>\\n\\nI used this a lot for syntax, last week.\",1643312794.147200,1643313742.150900,U0290EYCA7Q\\n0d8ed5d8-39fa-4016-a56b-52ffaf24787c,U029DM0GQHJ,,,What happened? Did it work?,1643307686.127200,1643313800.151200,U0290EYCA7Q\\nb3df0b7b-ef1f-40c8-ba79-dd552167c37f,U029DM0GQHJ,,,I did the default way as I would have to wait for another 20 minutes had it gone wrong somewhere,1643307686.127200,1643314164.151700,U029DM0GQHJ\\n3871a464-905f-40e8-813c-6bbc2111ebfd,U02UA0EEHA8,,,\"well, I don\\'t know what the problem was, but I started with a clean slate and now everything is smooth :slightly_smiling_face:\",1643271742.004500,1643314269.152100,U02UA0EEHA8\\n61abf71f-8bd9-4291-98f1-45f0ba0b53e4,U030PFH5CRX,,,\"How do you run it? Can you show what you typed? \\n\\nAlso its not clear what\\'s the error you get on jupyter. Can you show it? (No screenshots please)\",1643303881.118400,1643314693.152300,U01AXE0P5M3\\nb3fdcbe3-952e-496a-b3ad-102e2acbed8d,,3.0,,\"where do i setup this\\nWARN[0000] The \"\"AIRFLOW_UID\"\" variable is not set. Defaulting to a blank string.\\xa0\\nWARN[0000] The \"\"AIRFLOW_UID\"\" variable is not set. Defaulting to a blank string.\",1643314933.153100,1643314933.153100,U02T8HEJ1AS\\n63b0815e-2b3c-44ff-9f8f-2a73281ecdd8,U02T8HEJ1AS,,,\"<https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_2_data_ingestion/airflow/1_setup.md#airflow-setup>\\n\\npoint 4\\n\\n(also check other threads, there was some discussion about that already)\",1643314933.153100,1643315038.153200,U01AXE0P5M3\\n5b19e4fb-aa8e-45a3-9290-6ca7f2ec0682,U02T8HEJ1AS,,,yes Sir,1643314933.153100,1643315647.153600,U02T8HEJ1AS\\na0d15354-8154-4cf2-b973-1b9dda5f4d1b,U02T8HEJ1AS,,,\"I don\\'t remember if you\\'re on windows or not, but it seems for windows it\\'s also important to run it and I remember seeing in one thread how you can do it on plain windows without mingw\",1643314933.153100,1643316535.153800,U01AXE0P5M3\\n0f7aaa46-5b60-4be7-b673-7609c95eab0e,,6.0,,i am facing these errors when i run terraform. Any help?,1643317231.156000,1643317231.156000,U02CUH15ACE\\n51d94a1d-fbcb-4b91-b2b9-db2178c5cb93,,,,\"when building the airflow image, I get a lot of warnings for various scripts installed in this directory that is not on PATH - should I add it?\\n``` WARNING: The script normalizer is installed in \\'/root/.local/bin\\' which is not on PATH.\\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.```\",,1643317236.156300,U02TVGE99QU\\n6cf8d3e2-a2bd-48b8-bd61-9cb4c9e23482,U02CUH15ACE,,,you forgot to switch out the project ID with the placeholder. see where it says Invalid Bucket Name:,1643317231.156000,1643317289.156400,U02TVGE99QU\\nc6b2a5d4-b146-4f14-a6cb-d81af3608e06,U02CUH15ACE,,,&lt;your_gcp_project_id&gt; should be replaced by your actual project id,1643317231.156000,1643317314.156600,U02TVGE99QU\\n04c73379-a27f-4555-9239-4d2473f2f655,U02CUH15ACE,,,the same errors showed up,1643317231.156000,1643317858.156800,U02CUH15ACE\\ne7ca8711-1fb6-42c0-abd0-315207328ca9,U02Q9P0A0NA,,,\"Yes, have created the storage and also updated the right project ID and GCS bucket ID of my service account.\",1643299377.100400,1643318044.157200,U02Q9P0A0NA\\n1d026791-1c19-415c-a784-9b330add5707,U02B8U0QZEK,,,\"I can also recommend mode resources, also check out w3schools, freecodecamp and code academy\",1643312794.147200,1643318441.157500,U01B6TH1LRL\\nbe0b318e-5afc-49cd-b9e8-947641e9b019,U02B8U0QZEK,,,<https://www.datawithdanny.com/courses/serious-sql>,1643312794.147200,1643319632.157900,U02TT277FHD\\n83e5c1bf-54b4-4d76-bd78-048acb3bf6c6,U02B8U0QZEK,,,This is one of the best SQL courses out there. I highly recommend it even though it is a paid course.,1643312794.147200,1643319695.158200,U02TT277FHD\\n0b249531-6ab4-4243-b20c-b1ec832df01d,U02Q9P0A0NA,,,\"Same happened to me, if i run it again. Don\\'t know why it tries to acces pivotal-surfer. Was getting 403.\",1643299377.100400,1643319876.159400,U0290EYCA7Q\\nf7c491c1-d745-4ad8-96e5-61f151195bd1,U02HLE69P19,,,No changes made - it is the same as this documented error - <https://github.com/apache/airflow/issues/20911> - I didn\\'t spend too much investigating,1643287994.072400,1643320613.160900,U02HLE69P19\\n89b799ee-f6c3-445a-8279-6e2aee75b7ff,,4.0,,\"<@U01AXE0P5M3> <@U01DHB2HS3X> if I understood correctly, following is a flow:\\n• In real world, the unstructured data (could be any data) live in Data Lake (Google Cloud storage in this case) and is used as a source. When people upload, do they upload as a .cvs or .parquet format. what is recommended?\\n• as a data engineer I would write a code, that takes above file ad formats and uploads it to BigQuery\\n• as a data engineer, you will write SQL to develop business solution? \",1643322602.141769,1643322602.141769,U02UBV4EC8J\\n3273662c-393b-4ee2-9509-73238c20a814,U01AXE0P5M3,,,<@U01AXE0P5M3> thanks for this. that one video you uploaded on using a vm instead made me learn a lot more than I initially expected. Really appreciate the enthusiasm in your videos.,1642544154.050000,1643324600.362379,U030G7PJ6LQ\\ndb2776c5-1db8-4421-9863-2670d051d477,U02UE7NTLUU,,,\"Starting late but here I am.\\nI had the same issue. I was able to resolve the by updating the code as follows for mounting volume:\\n-v &lt;C://Users//bhask//de-bootcamp//ny_taxi_postgres_data:/var/lib/postgresql/data&gt; \\\\\",1642465146.363400,1643326816.600099,U02SAM9H8TW\\nf2cb0405-1331-482d-923c-bc8deb0b82c8,U02UB8XDCHJ,,,Thanks for the question and answers! I used my eyeballs way too much for the problems.,1643257694.483700,1643327300.029869,U02UBQJBYHZ\\n547ab69b-61aa-4ac9-8333-d4c451406ece,U02T697HNUD,,,I see. let me try again on local setup. Thanks.,1643296959.097400,1643329807.976699,U02T697HNUD\\n2384e95a-597e-45cc-b966-5844b01f0197,U0290EYCA7Q,,,you can type `docker restart airflow-worker` command if you want restart it standalone,1643301439.108100,1643329859.888999,U02T697HNUD\\n409c7812-e7e1-44c3-809d-91e8958ca5ea,U02CUH15ACE,,,\"<@U02CUH15ACE> if you could share the full output of terraform plan, it would help to diagnose\\n```terraform plan```\",1643317231.156000,1643336500.522409,U02QPTB3PU5\\nc6c52fdc-0f80-490f-adef-a211a7300657,,2.0,,\"Hi All, Week 2 has been interesting! Slowly digesting things.. Few questions\\xa0\\n1. How does one do error handling/configurations in airflow? For eg. When I was trying to upload my parquet file to big query it errored due to infrastructure name difference and I went and corrected it and it all worked fine I was wondering how many retries that airflow default do how do I configure it? If I want it to error out on a task and not proceed etc.\\xa0\\n2. Can I orchestrate series and parallel steps in Airflow? I recently started working with StepFunction in AWS and trying to connect my learnings from there\\n\",1643336702.935329,1643336702.935329,U02TTSXUV2B\\nfd52939d-aca4-449b-b844-7a74424e419f,U02TBKWL7DJ,,,Hey <@U02BVP1QTQF>! I”m trying to run the same thing on mac and ran into mounting issues (which usually  comes much later) . So finally deleted the container. Do we have to install any Zsh/Fish/Bash (my guess is bash is by default for mac) - did you get around this issue?,1642507885.440700,1643336857.412769,U01HNUYV81L\\n16a528da-1a29-4217-8fe7-28728aebf608,U02TTSXUV2B,,,\"1. I don\\'t have experience in Airflow, but in the DAG python scripts there is a dictionary `default_args` which contains `retries`. I assume that is where you dictate the default.\",1643336702.935329,1643337919.972759,U02T941CTFY\\n75a45cc4-9e57-4ef5-b06e-d6b7793fe79b,,3.0,,\"Anyone else having this problem when trying to connect Airflow to Postgres?\\n\\n```connection to server at \"\"pgdatabase\"\" (172.25.0.9), port 7575 failed: Connection refused\\n\\tIs the server running on that host and accepting TCP/IP connections?```\\nI\\'ve tried other ports, rebuilding PostgreSQL container, rerun every single docker that is running.\\n\\nMy docker-compose is configured to port 7575:7575. I can connect using pgcli using Bash. Inside Airflow it doesn\\'t work.\",1643340522.269329,1643340522.269329,U02TC704A3F\\n78a0c42a-3821-42bb-8cb1-4b79ea17b430,,,,l,,1643340739.103589,U02RWFJL3GF\\n749e60f5-1951-4a12-ad86-a5a40ce87682,,1.0,,\"Hi everyone,\\nI\\'m stuck at the \\'ingest_data.py\\' part.  when I run the script, I get this error .\",1643340778.942999,1643340778.942999,U02U5G6MXJ8\\n3a193a8e-ca48-400d-a7d4-85d66a22957d,,5.0,,\"Hi All,\\nDo I need to name the google credential folder as \"\".goole/credentials\"\". New upgrade on MAC OS Catalina doesn\\'t allow to name folders starting with [dot]. Anyone found a workaround pls let me know. TA\",1643341570.337969,1643341570.337969,U02UM74ESE5\\n4493bf6f-523f-4a91-8b51-1a835ab6287b,U02U5G6MXJ8,,,solved. I was missing the wget.exe,1643340778.942999,1643343203.178649,U02U5G6MXJ8\\nd9548585-f9b8-4641-9186-d8c83843dd3b,U02UB8XDCHJ,,,\"I recommend reading a book on SQL! I\\'m reading a book called murach\\'s mysql and the book is broken into sections . Section 2, doesn\\'t have to be read in order and you can choose which advanced topics to read about. Try considering books as a great resource for building a stron gfoundation. There are tons of examples throughout and exercises at the end of each chapter to reinforce the knowledge\",1643257694.483700,1643345168.877079,U02T9550LTU\\nec722b6b-1ccd-43ae-8e3e-e3e5b7523eff,,6.0,,<@U01AXE0P5M3> <@U01DHB2HS3X> is there a windows version of the airflow set up (DE Zoomcamp 2.3.1) ?,1643345509.434629,1643345509.434629,U02UY1QTGHW\\na1ee2cda-adee-43b9-82c1-ad9408338a98,U02V4UTSU8M,,,\"<@U02V4UTSU8M> please have a look at the course pre-requisite below.\\n\\n_\"\"To get most out of this course, you should feel comfortable with coding and command line, and know the basics of SQL. Prior experience with Python will be helpful, but you can pick Python relatively fast if you have experience with other programming languages.\"\"_\\n\\nif you cant satisfy the above and cant troubleshoot problems using google then its prob going to be really hard to follow.\",1642981427.237600,1643346163.753619,U02UY1QTGHW\\nfb92e96e-35a7-4709-8d19-2602cab34fcd,U02UX01PGCC,,,scroll to the end <https://github.com/DataTalksClub/data-engineering-zoomcamp>,1643036907.372300,1643346421.236939,U02UY1QTGHW\\nc60f24eb-6ba2-41b5-901c-759e620e9a5f,U02H6BH5VSP,,,me,1643173164.231400,1643347013.569409,U02TN6PQVU2\\n4ca3417c-7513-4949-9210-45fa8fcd05c2,U02UM74ESE5,,,I think you\\'ll have to put the path to any other folder that you create in the `volumes` section of  `docker-compose.yaml` . Change `~/.google/credentials/:/.google/credentials:ro`  to  `/path/to/credentials:/.google/credentials:ro`,1643341570.337969,1643347400.360449,U02TATJKLHG\\n3f2411ab-cf05-44dc-ba17-26e57dc3792c,U02UY1QTGHW,,,You are setting up Airflow over docker so the underlying OS should not be a major issue imo.,1643345509.434629,1643347453.483849,U02TATJKLHG\\nbc808127-42fd-4981-a694-4d15a8d96a9e,U02UY1QTGHW,,,My airflow is setup over docker in Windows,1643345509.434629,1643347481.596829,U02TATJKLHG\\n8795f835-2c56-4767-8720-170b4fa65596,U02TTSXUV2B,,,\"If I understood your second question correctly, you can do `task_A &gt;&gt; [task_B, task_C]`  and that would run the tasks B and C in parallel, while the task A would run before Task B and C.\",1643336702.935329,1643347741.283469,U02TATJKLHG\\ncd82088a-acd5-4bfd-a452-dd98e1f54fbb,U02UZBJ2Q6L,,,\"Yeah I did find that out eventually, thanks :smile:\",1642769587.082900,1643347993.512669,U02TATJKLHG\\n2b73cb83-815c-4f41-ac3b-ac5a45462108,U02UY1QTGHW,,,<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1643312329144200>,1643345509.434629,1643348074.380699,U0290EYCA7Q\\n564378B6-7128-4A31-8EA8-F9612B0FEAB5,U02UM74ESE5,,,Yes!! Realized that I can do that while I was browsing through yaml file. Thanks for your input anyways!,1643341570.337969,1643348268.148479,U02UM74ESE5\\nb54d0a40-7b4e-492a-bc98-7073c5c3d197,U0303JLGS23,,,Thank you,1643309241.134000,1643348699.017359,U0303JLGS23\\nf9805bdf-4b31-4d6b-a357-78cacc5d349e,U030PFH5CRX,,,\"i have the same problem too, after config for pgadmin4, then for config postgres:13 is missing and also port 5432/5431 can\\'t be used.  Can anyone give me a solution for this problem?\",1643303881.118400,1643348867.806179,U02SHV7RJTW\\n55f24410-f2cc-4cce-a3bb-f6377015cafb,,2.0,,\"Github \"\"fetch and merge\"\" doesn\\'t pull all branches from DataTalksClub. Is it expected behaviour?\",1643349739.124859,1643349739.124859,U0290EYCA7Q\\nc0d3e305-5e58-4dfc-ada5-3ce130a74e2b,U02UY1QTGHW,,,What Sejal showed in the videos worked for me on Windows,1643345509.434629,1643351531.711929,U01AXE0P5M3\\n0ef87a8c-a89a-41f3-87ea-c2a70b447772,U02UY1QTGHW,,,\"If you don\\'t use mingw you might have problems saving airflow Id, but check other threads for the solution\",1643345509.434629,1643351585.668509,U01AXE0P5M3\\n843d515f-3c31-4c82-9a60-557f7fcac9ec,U02UY1QTGHW,,,Also PRs are welcome,1643345509.434629,1643351596.252559,U01AXE0P5M3\\nb7cfa7ba-e393-490b-90f1-8e83daee4c29,,3.0,,\"i am unable to launch airflow webserver as the health status won\\'t turn to healthy for all the major airflow images.. i tried increasing memory to 8gb but to no avail.. but i keep getting this error for webserver:\\n```OSError: handle is closed```\",1643354276.441279,1643354276.441279,U029DM0GQHJ\\nc2e71bcf-663e-4efa-ba52-d9136aa48eb5,U0290EYCA7Q,,,\"I\\'m not sure, but why do you need all branches?\",1643349739.124859,1643354366.357869,U01AXE0P5M3\\n11cca4b6-3909-442c-b35e-605ef5e96da4,U02TC704A3F,,,Have you checked the video about networking in docker?,1643340522.269329,1643354424.329219,U01AXE0P5M3\\n3c9a1704-c4ff-466f-b7a4-de2b7f081de2,,,,\"Running this gave 7.8G:\\n```docker run --rm \"\"debian:buster-slim\"\" bash -c \\'numfmt --to iec $(echo $(($(getconf _PHYS_PAGES) * $(getconf PAGE_SIZE))))\\'```\",,1643354434.847249,U029DM0GQHJ\\nbef12a5a-860d-4da3-b275-9e33e9e16c87,,,,,,1643354463.954649,U029DM0GQHJ\\nfcf65420-f64a-4189-bb44-8eba41d40a47,U02UBV4EC8J,,,\"Yes, you got it.\\n\\nParquet file is better. At work we don\\'t use csv, we use json and Parquet. Json is removed after a few weeks because it\\'s too verbose and takes too much space\",1643322602.141769,1643354588.622039,U01AXE0P5M3\\nec57163f-986d-4f43-8db5-51fe456decce,U02UBV4EC8J,,,\"I wouldn\\'t say \"\"any data\"\" though. You still need some structure and some expectations in terms of schema and format. Else your lake will turn into a swamp =)\",1643322602.141769,1643354683.357679,U01AXE0P5M3\\n45589a87-041c-4ed9-8193-28f2ecd4d88b,U02TC704A3F,,,Tldr: within the network use port 5432,1643340522.269329,1643354859.470399,U01AXE0P5M3\\nde677f31-3eb6-49a8-a527-b33a3dbd7efe,U0290EYCA7Q,,,Ankush used a different branch for transfer service. I copied the file anyway.,1643349739.124859,1643356432.966779,U0290EYCA7Q\\n,USLACKBOT,1.0,tombstone,This message was deleted.,1643356664.702239,1643356664.702239,USLACKBOT\\n3fb1d940-525e-49a0-afda-b767211dcb6d,U02H0GUC7ML,,,\"Hi Faisal, I\\'ve covered that part too in the video, i.e. on setting the minimum memory/RAM in Docker (if you\\'re using Docker Desktop). Did you try that?\",1643173811.236300,1643184345.243800,U01DHB2HS3X\\nac1dd52d-f533-47f0-a2ea-202fbe9e3974,,,thread_broadcast,Me too! I\\'d like to give a shoutout to <@U030FNZC26L> whose notes from Week 1 (<https://kargarisaac.github.io/blog/data%20engineering/jupyter/2022/01/18/data-engineering-w1.html>) helped me tremendously. The flow of the notes is :100:,1643179603.242900,1643185793.244400,U02T941CTFY\\n5e0d3892-9bdd-4014-8366-79e1b6d6a90d,U02U5SW982W,,,I\\'m happy it helped :slightly_smiling_face:,1643179603.242900,1643187087.245000,U030FNZC26L\\nf9de5f3c-0a3f-4ea1-92e6-c9d6712b2d09,U02TTSXUV2B,,,\"cool, Thank you <@U01DFQ82AK1>!\",1643075363.020200,1643187462.245200,U02TTSXUV2B\\nfb8e3996-11c8-4596-a8b5-d37011605c8a,,9.0,,\"`Step 12/16 : COPY google_credentials.json $AIRFLOW_HOME/google_credentials.json`\\n`6 errors occurred:`\\n        `* Status: COPY failed: file not found in build context or excluded by .dockerignore: stat google_credentials.json: file does not exist, Code: 1`\\n        `* Status: COPY failed: file not found in build context or excluded by .dockerignore: stat google_credentials.json: file does not exist,` \",1643189014.245800,1643189014.245800,U02V4412XFA\\n3145e8bb-d67d-486d-bfd1-c36ac5f46ca3,U02V4412XFA,,,\"```mv ~/.gc/cosmic-stacker-338802-63e376aa56bc.json ~/.google/credentials/google_credentials.json```\\nHi, I am facing this issue even after doing the above as suggested in video. Can someone help me?\",1643189014.245800,1643189134.245900,U02V4412XFA\\n6a55166b-715d-4980-adb6-83b90c440da5,U02V4412XFA,,,\"Do you see the file inside ~/.google/credentials/? `ls -ltr ~/.google/credentials/`.\\nIs this your json file, downloaded from GCP? `cosmic-stacker-338802-63e376aa56bc.json`\",1643189014.245800,1643189375.246100,U0290EYCA7Q\\nfff902c7-4d62-4c3e-8b70-3bc6f8f50621,,,,\"Hi guys.. everytime I run the ingest_data.py script I get the error OperationalError: connection to server at \"\"localhost\"\" (127.0.0.1), port 5432 failed: Connection refused \\tIs the server running on that host and accepting TCP/IP connections? OR the ERROR FATAL: database \"\"taxi_ny\"\" does not exist.\\nPlease help on how to solve this bug\",,1643190325.248600,U02ECUGHWG4\\n883519e3-28bd-49a2-9fef-4ff7b64b14f9,U02V4412XFA,,,*Yes*,1643189014.245800,1643190420.248700,U02V4412XFA\\n5ba0b513-9191-46e6-acac-08d63e47abda,U02V4412XFA,,,I can see it,1643189014.245800,1643190473.248900,U02V4412XFA\\n1406adc9-c828-4ec4-90ea-b68ea3d42e89,,3.0,,\"Running docker compose up with Airflow\\n\\n<https://streamable.com/ho19a4>\",1643190996.249500,1643190996.249500,U01AXE0P5M3\\n26b9de69-99b4-41aa-89c5-9c09fee50402,U01UKRH6VGT,,,I checked the query code I got in DM and there\\'s a mistake (I didn\\'t say what it is),1643149862.204600,1643191347.249700,U01AXE0P5M3\\n3c5ad6d7-5e23-484d-add8-769ad49d5e75,U01AXE0P5M3,,,Good video to start the day AHAHAHAHHA :rolling_on_the_floor_laughing:,1643190996.249500,1643191966.250300,U02CD7E30T0\\n466d39e0-d7bc-402d-9f93-bf6c40294903,,15.0,,\"Good morning/afternoon/night everyone\\nI installed Docker Desktop on my Windows but I was \"\"asked\"\" to run with WLS2.\\nHowever I want to run Ubuntu with WLS2.\\nI can run Docker in PowerShell but not on the Ubuntu.\\nAnd when I see in the cmd it looks like I have 3 Distros (see picture)\\nShould I uninstall the distro docker desktop and install it inside the Ubuntu?\",1643192586.254600,1643192586.254600,U02CD7E30T0\\nda9f9645-8747-40ec-800c-0c2f96d2d2ff,U02CD7E30T0,,,\"No, you should first install docker desktop for windows and select WSL2 option there - then you\\'ll automatically have docker in WSL\\n\\nIf you decide to install docker separately in your WLS, you\\'ll need to manually make sure your docker service starts every time you need, which I find quite annoying\",1643192586.254600,1643192676.254900,U01AXE0P5M3\\n1b4f7881-136c-4c11-a340-cc67fa884e26,U02CD7E30T0,,,\"it seems I have the same list of distributions:\\n```$ wsl -l\\nWindows Subsystem for Linux Distributions:\\nUbuntu-18.04 (Default)\\ndocker-desktop-data\\ndocker-desktop```\\nThis is fine\",1643192586.254600,1643192745.255100,U01AXE0P5M3\\nb602b580-1832-40f8-b6fa-671e0c0a9b97,U02CD7E30T0,,,But I can\\'t run Docker in Ubuntu,1643192586.254600,1643192768.255300,U02CD7E30T0\\n0a2751cd-fee4-4245-be61-dd88cad416a1,U02CD7E30T0,,,did you install it after installing docker desktop?,1643192586.254600,1643192818.255500,U01AXE0P5M3\\ndcfdb43b-08cb-445f-a1ea-8fc98b5977f3,U02CD7E30T0,,,I think the problem is that I first downloaded Docker Desktop and then installed the Ubuntu Distro,1643192586.254600,1643192831.255700,U02CD7E30T0\\n5ace5b9e-f463-467c-abb7-d16ba2c5c881,U02CD7E30T0,,,\"also make sure you use WSL2, not WSL1\",1643192586.254600,1643192868.256000,U01AXE0P5M3\\n99fa9653-8afb-451f-99fe-c1a9a4703120,U02CD7E30T0,,,right right,1643192586.254600,1643192888.256200,U02CD7E30T0\\nfc0de561-189e-482e-917c-83a6a6a8bda2,U02CD7E30T0,,,They are all 2,1643192586.254600,1643192912.256400,U02CD7E30T0\\nfd529d07-67b7-47e4-b90d-9200299c75c3,U02CD7E30T0,,,\"<https://docs.docker.com/desktop/windows/wsl/>\\n\\nhave you seen this? point #7 might be relevant\",1643192586.254600,1643192915.256600,U01AXE0P5M3\\n2b594fab-e5e3-4b89-a56c-0f0d0885719b,U02CD7E30T0,,,Muito Obrigado (Thank you in Portuguese :slightly_smiling_face: ),1643192586.254600,1643193060.256900,U02CD7E30T0\\n9809870f-6d40-43b2-b2ca-8aff129fad13,U02CD7E30T0,,,It was off,1643192586.254600,1643193066.257300,U02CD7E30T0\\n2a5b6fa0-500e-4fbd-b0a7-8f2e0046be06,U02CD7E30T0,,,\"Now it works and I can do everything on Ubuntu!!! :slightly_smiling_face:\\nI am starting to feel a real Data Engineer :stuck_out_tongue:\",1643192586.254600,1643193175.257500,U02CD7E30T0\\n77aea52f-58e1-480a-8c55-c90f42fa1dcf,U02U8AJV6E6,,,by the way u can write the command,1643109720.071600,1643193856.257800,U02QW395UNM\\nb9dce353-b745-4172-b191-262949a497fa,U02U8AJV6E6,,,\"$ alias docker=\"\"winpty docker\"\" and you can ran commands normally\",1643109720.071600,1643193886.258000,U02QW395UNM\\nf28f17ab-00aa-43a6-91b3-7782985daa03,,3.0,,\"docker: Error response from daemon: invalid mode: \\\\Program Files\\\\Git\\\\var\\\\lib\\\\postgresql\\\\data.\\nSee \\'docker run --help\\'.\",1643193936.258300,1643193936.258300,U02QW395UNM\\n8a985137-ec1a-4afc-881f-ec699ae0c485,U02QW395UNM,,,\"I try to create a volume \"\"\\n```docker volume create postgresql-volume```\\nbut not worked\",1643193936.258300,1643194004.258400,U02QW395UNM\\n4778b6f2-a678-4f5e-bd4b-23f7d4061049,U02V4412XFA,,,\"Hi, <@U01DHB2HS3X> Can you please help me with this!\",1643189014.245800,1643194463.258700,U02V4412XFA\\n43a4552b-5b82-4011-a274-f804f820b652,U02V4412XFA,,,Should we set $AIRFLOW_HOME to something?,1643189014.245800,1643194607.258900,U02V4412XFA\\nb226c92e-0d92-4336-bee6-5c6f05fe4ff2,U02V4412XFA,,,\"Hi Anudeep, it seems like you had already started with the workshop before it was updated yesterday. We were hoping students could wait and do it along with the entire set of videos, because we’re still in the process of updating the workshop material. But either way, please do a new `git pull` of the “main” branch. What you’re seeing here, is an older Dockerfile, and we  are now using `volumes` in “docker-compose” to upload our secret-key. For explanation, you can also see the first part of the video already uploaded yesterday (2.3.1)\",1643189014.245800,1643194989.259100,U01DHB2HS3X\\n9b6e0143-4af4-4ff4-88da-ed117a30ca26,U02V4412XFA,,,\"Okay sure I\\'ll do that, Thank you!\",1643189014.245800,1643195153.259800,U02V4412XFA\\n5d5df0c3-a455-4a0a-a63b-be8707731cc7,U02TBTX45LK,,,\"Is your Airflow_UID set in your `.env` file, or as an Export variable? More explanation in Readme\\'s and video 2.3.1\",1643159253.225200,1643195402.260600,U01DHB2HS3X\\n73df4391-6680-4141-a205-a99d908e2758,U02QW395UNM,,,\"I think i saw this error before when i do docker run postgres:13. I resolve this problem with do double forward slash on absolute path with  like\\n```-v &lt;c://your//path:/var/lib/postgresql/data&gt; \\\\```\",1643193936.258300,1643195420.260900,U02U8SRCHU6\\n17c60941-085d-4585-b2fb-04abfb8099c8,U02QW395UNM,,,\"yes it worked , i just changed\\n```-v &lt;c://your//path:/var/lib/postgresql/data&gt; \\\\```\\nwith\\n```-v $(pwd)//your//path:/var/lib/postgresql/data \\\\```\\nbecause i think c: caused the problem\",1643193936.258300,1643196252.263100,U02QW395UNM\\n1376f949-2f9e-46ed-b761-1a1b65f67d67,,7.0,,\"Hi all, I\\'m still on the week 1 course and having issues with git bash on windows whenever I try and use pgcli it prompts me to type in my password, when I do it shows my password on the console (which seems weird), and then never finishes running once I\\'ve typed it in and pressed enter, I just have to forcibly shut the program. Any ideas?\",1643196570.265400,1643196570.265400,U02TP858APK\\n91e8dbab-ebb6-4e0e-ae38-beae3ccb34d0,U02TP858APK,,,i had the same issue,1643196570.265400,1643197338.265800,U02QW395UNM\\n35a67e25-c453-424b-aa14-8f78d790e03b,U02H0GUC7ML,,,\"In Ubuntu, we don\\'t have docker desktop. I\\'ve tried following some blogs to set min memory/cpu, but nothing worked.\",1643173811.236300,1643197828.266000,U02H0GUC7ML\\n030f1717-733d-47d9-8935-3c2275b795d5,U02TP858APK,,,\"I had the same issue as well. But then when I tried bash in VS Code, it worked. So I just used it. You can use the integrated terminal inside VS code, or just use cmd or anything of that sorts. You can then later on look for as to why this is happening, but for now this would help you to progress.\",1643196570.265400,1643197862.266200,U02TATJKLHG\\nce3403a0-4be3-4331-ab80-c1feddb4448f,U02H0GUC7ML,,,in ubuntu docker works differently and you don\\'t need to set it up - it can use all the available CPU/RAM (which is not always a good thing...),1643173811.236300,1643197916.266400,U01AXE0P5M3\\n32b41709-10cf-4aaa-825b-8965439667f9,,1.0,,<@U01AXE0P5M3>\\xa0Can You please share the config file You used for SSHing VM?,1643198493.267200,1643198493.267200,U0290EYCA7Q\\nc0753bb6-6de0-4b58-887e-76d2c0b61f89,U0290EYCA7Q,,,Found it.,1643198493.267200,1643198666.267300,U0290EYCA7Q\\n3bd57cd6-27cd-47e2-9aa5-209a50d0ac26,U02T697HNUD,,,I see. Thanks for sharing us about Drawboard Alexey :smile:,1643118792.092800,1643199544.270100,U02T697HNUD\\n9912f5f5-cc17-4d0a-94cc-baf3d55e46a7,,4.0,,\"Hi All,\\n\\nJust wanted to share my mid week feedback. The follow-along approach followed by Alexey in the Week 1 worked so great for me, it was like I was building the code and the project from scratch. I learnt a lot, a lot issues also popped up, just like they would in the real world. And solving them made things stick as well.\\n\\nIn this week however, everything is ready and things are being explained as to what is written in the script. But there is no building from bottom-up. Building the Dockerfile (not docker-compose.yaml), starting with a sample Airflow DAG, and then building from there by adding a task at a time, like downloading the file, converting to parquet, connecting to GCP via code, creating BQ tables, all of them could have been separate topics, if not separate videos. That would have helped us code along and build a solution from scratch. With this approach, it feels like I\\'ll have to remember by reading rather than doing. I also understand how difficult it would be to then contain the video length, but if there\\'s a balance that can be found, then that should help.\\n\\nBut this is what I felt, would be glad to hear from others as well if they feel the same or otherwise.\\n\\nAlso, I really really appreciate what you guys are doing and I am your biggest fan. I just wanted to share something constructive :smile:.\",1643200258.278900,1643200258.278900,U02TATJKLHG\\n469e4131-921d-4c59-9232-a0c22ea60664,U02QPBZ3P8D,,,\"The repo has the advantage of being completely managed and updated from vscode, making it very easy to update in the future. I set it up with ssh, so it works seamlessly!\",1643149891.205300,1643200481.279300,U02GVGA5F9Q\\n44718C74-B38E-41C1-B558-8C22EFB19419,U02TATJKLHG,,,\"I think it\\'s a great point, and I fully agree that retention would be pretty bad with a copy-paste approach, I guess we will have to tear down the files, use some documentation and build everything from scratch ourselves after watching the first round of videos. \",1643200258.278900,1643201272.284200,U02UX664K5E\\n943b1792-84e9-43b3-9503-b556807449d4,,2.0,,\"hey guys, still stuck troubleshooting week 1.... I\\'m trying to run docker-compose and I get a \\'permission denied\\' - I think it\\'s because my ny_taxi_postgres_data file is locked and not accessible. full errors in thread\",1643201306.284800,1643201306.284800,U02TVGE99QU\\n48fc024b-9828-4c34-9400-cf11278fabd0,U02TVGE99QU,,,\"permission denied traceback:\\n```Traceback (most recent call last):\\n  File \"\"urllib3/connectionpool.py\"\", line 677, in urlopen\\n  File \"\"urllib3/connectionpool.py\"\", line 392, in _make_request\\n  File \"\"http/client.py\"\", line 1277, in request\\n  File \"\"http/client.py\"\", line 1323, in _send_request\\n  File \"\"http/client.py\"\", line 1272, in endheaders\\n  File \"\"http/client.py\"\", line 1032, in _send_output\\n  File \"\"http/client.py\"\", line 972, in send\\n  File \"\"docker/transport/unixconn.py\"\", line 43, in connect\\nPermissionError: [Errno 13] Permission denied\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \"\"requests/adapters.py\"\", line 449, in send\\n  File \"\"urllib3/connectionpool.py\"\", line 727, in urlopen\\n  File \"\"urllib3/util/retry.py\"\", line 410, in increment\\n  File \"\"urllib3/packages/six.py\"\", line 734, in reraise\\n  File \"\"urllib3/connectionpool.py\"\", line 677, in urlopen\\n  File \"\"urllib3/connectionpool.py\"\", line 392, in _make_request\\n  File \"\"http/client.py\"\", line 1277, in request\\n  File \"\"http/client.py\"\", line 1323, in _send_request\\n  File \"\"http/client.py\"\", line 1272, in endheaders\\n  File \"\"http/client.py\"\", line 1032, in _send_output\\n  File \"\"http/client.py\"\", line 972, in send\\n  File \"\"docker/transport/unixconn.py\"\", line 43, in connect\\nurllib3.exceptions.ProtocolError: (\\'Connection aborted.\\', PermissionError(13, \\'Permission denied\\'))\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \"\"docker/api/client.py\"\", line 214, in _retrieve_server_version\\n  File \"\"docker/api/daemon.py\"\", line 181, in version\\n  File \"\"docker/utils/decorators.py\"\", line 46, in inner\\n  File \"\"docker/api/client.py\"\", line 237, in _get\\n  File \"\"requests/sessions.py\"\", line 543, in get\\n  File \"\"requests/sessions.py\"\", line 530, in request\\n  File \"\"requests/sessions.py\"\", line 643, in send\\n  File \"\"requests/adapters.py\"\", line 498, in send\\nrequests.exceptions.ConnectionError: (\\'Connection aborted.\\', PermissionError(13, \\'Permission denied\\'))\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \"\"docker-compose\"\", line 3, in &lt;module&gt;\\n  File \"\"compose/cli/main.py\"\", line 81, in main\\n  File \"\"compose/cli/main.py\"\", line 200, in perform_command\\n  File \"\"compose/cli/command.py\"\", line 70, in project_from_options\\n  File \"\"compose/cli/command.py\"\", line 153, in get_project\\n  File \"\"compose/cli/docker_client.py\"\", line 43, in get_client\\n  File \"\"compose/cli/docker_client.py\"\", line 170, in docker_client\\n  File \"\"docker/api/client.py\"\", line 197, in __init__\\n  File \"\"docker/api/client.py\"\", line 222, in _retrieve_server_version\\ndocker.errors.DockerException: Error while fetching server API version: (\\'Connection aborted.\\', PermissionError(13, \\'Permission denied\\'))\\n[42012] Failed to execute script docker-compose```\",1643201306.284800,1643201321.284900,U02TVGE99QU\\nf5be1bf4-e1ed-4d5b-857d-35c66193ef0e,U02TVGE99QU,,,\"and when I try to get into the ny_taxy_postgres_data folder, I get this:\",1643201306.284800,1643201461.285200,U02TVGE99QU\\nb060d492-788a-499e-9e8c-88876113f697,U02TATJKLHG,,,<@U02UX664K5E> I plan to do the same. Try and build the same solution from scratch and just use the provided scripts as a reference or end goal.,1643200258.278900,1643201697.285600,U02TATJKLHG\\n79129e3e-be0a-4735-b0d7-46e6a9d0da52,,1.0,,\"<@U01AXE0P5M3> Just to clarify, the video on environment setup on GCP is for those having issues setting up locally, right?\\n\\nCan I skip the video if I was successful in setting up my environment?\",1643202079.287000,1643202079.287000,U02QS4BD1NF\\n0b7a9ec3-02c9-4630-aa20-000259e99a6f,U02QS4BD1NF,,,yes and yes!,1643202079.287000,1643203025.287200,U01AXE0P5M3\\n6550edb5-0c09-4437-932c-01530643147a,U02TP858APK,,,it helps thanks,1643196570.265400,1643203289.287400,U02QW395UNM\\n98e27630-3694-469c-87b7-03f793774331,U02CD7E30T0,,,\"yeap this is very cool, never used before wsl. After this I start liking windows:rolling_on_the_floor_laughing:\",1643192586.254600,1643203438.287600,U02UBV4EC8J\\nd2a6b721-3b9b-4a9c-92c9-3ee1a90f2721,U02CD7E30T0,,,I spent last 10 years working on Linux - but I start liking windows as well :smile:,1643192586.254600,1643203538.288900,U01AXE0P5M3\\n2446bc8b-8339-4eb7-becb-66e54d4b1714,U02TATJKLHG,,,Thank you for your feedback!,1643200258.278900,1643203572.290500,U01AXE0P5M3\\nd8ce818d-33b3-440f-b718-f38215bb4d9f,,9.0,,\"after running postgres for he first time , the folder ny_taxi_postgres_data is always empty , any help please ?\",1643203572.290700,1643203572.290700,U02QW395UNM\\n84DC11FE-146A-4292-9784-5E0DCE948D54,,5.0,,\"Silly question maybe, but what\\'s the benefit of using Airflow with Docker in a real production environment?\\n\\nIs the idea that you\\'d create your own custom image of Airflow, push to a private repo or something, then other members of your team can pull that image to their local machine and run a container?\",1643203746.293000,1643203746.293000,U02U34YJ8C8\\ncbee5ebb-c0e9-42e1-aeee-413715db0d90,U02QW395UNM,,,Can you paste the command that you are using to run the postgres docker image,1643203572.290700,1643203819.293100,U02TATJKLHG\\n8a8c1698-3981-42d6-9e21-d0fa0fd79722,U02U34YJ8C8,,,do you mean why we need to run Airflow using Docker (i.e. doing this docker compose stuff we did in the videos) or why do we need to use Docker for running airflow jobs?,1643203746.293000,1643203820.293300,U01AXE0P5M3\\n50da997c-2289-4d81-9e2e-bb4c4c0222d2,U02QW395UNM,,,\"docker run -it \\\\\\n  -e POSTGRES_USER=\"\"ichrak\"\" \\\\\\n  -e POSTGRES_PASSWORD=\"\"test\"\" \\\\\\n  -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n  -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n  -p 5432:5432 \\\\\\n  postgres:13\",1643203572.290700,1643204317.293700,U02QW395UNM\\nb8ff5c2c-b553-4fe9-b5a7-44d948becc22,U01UKRH6VGT,,,\"<@U01AXE0P5M3> yeah, I understand there might be a problem in the query.\\nCould you please specify the following: The particular question is what is meant by the\\xa0`largest average price`?\\nBecause I am not sure if I understand the question\",1643149862.204600,1643205471.294300,U01UKRH6VGT\\n79369CBF-072F-441A-8505-E1E7D0C1C3F0,U02U34YJ8C8,,,\"The latter, although understanding both would be good\",1643203746.293000,1643205659.295200,U02U34YJ8C8\\ne9f5e64f-cd17-4471-9fc5-eb4ac13e07fa,U02QW395UNM,,,<@U01AXE0P5M3>,1643203572.290700,1643205857.295400,U02QW395UNM\\n201751e6-fd4b-4463-9895-d8aa913e506c,U02QW395UNM,,,Did you ingest the data? And are you checking the folder contents using sudo?,1643203572.290700,1643205941.295600,U02GVGA5F9Q\\nc235f71a-80ca-4e5b-9641-8da0d89e68a1,U02U34YJ8C8,,,Because it makes your jobs self contained and you don\\'t need to install Python stuff on airflow workers like we do in the videos,1643203746.293000,1643206012.295800,U01AXE0P5M3\\nf803cfa0-4eb4-48f2-8722-25e0aceb83f2,U02U34YJ8C8,,,Like pyarrow for example,1643203746.293000,1643206026.296000,U01AXE0P5M3\\nebc93e3c-0eb1-48ea-acc3-07993b3d33d7,U02QW395UNM,,,aa no  how can i do thatt please ??,1643203572.290700,1643206066.296200,U02QW395UNM\\n88832c7c-a9d9-451b-a4d0-b1dd2ab059e3,U02TBTX45LK,,,Thank you both for the responses. I was able to figure it out and it was a mistake on my end. I saved my airflow folder within a folder called “Week 2: Data Ingestion” and it appears that the colon in the folder name caused issues,1643159253.225200,1643206217.296500,U02TBTX45LK\\n05f54b9d-ec4c-4e85-82ea-8d13fc1bdcf1,U01UKRH6VGT,,,\"For each pair, compute the average price. Then select the pair with the largest one\",1643149862.204600,1643206256.296700,U01AXE0P5M3\\n4d0aadfa-eb1c-4649-989c-ec5fab52cdc4,U02QW395UNM,,,Are you running this on windows? If so then can you add a `/` before `$(pwd)` like this -&gt; `-v /$(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\`,1643203572.290700,1643206345.296900,U02TATJKLHG\\n9b61bcf8-5be6-4dc5-8f74-d64170f6fa43,U02QW395UNM,,,it works :wink: thank you very much <@U02TATJKLHG>,1643203572.290700,1643206556.297100,U02QW395UNM\\nd4b30720-209b-40dd-b4a0-a18ac7dc8758,U02QW395UNM,,,Strange behavior that... This is a powershell feature? PWD always returns a complete and absolute path on Linux...,1643203572.290700,1643206681.297400,U02GVGA5F9Q\\n7181b37e-9d83-46d3-8b09-21807048cda9,,1.0,,\"`failed to solve: rpc error: code = Unknown desc = executor failed running [/bin/bash -o pipefail -e -u -x -c apt-get update -qq &amp;&amp; apt-get install vim -qqq]: exit code: 100`  Hey, is anyone else geting this error when they build the airflow docker container?\",1643207126.302300,1643207126.302300,U02TAA1LDQT\\n41BF0659-98B6-4BBE-85D7-C48B1F2F0C81,,2.0,,\"hi, a question from week 1: after successfully ingesting the data i try to do this: `$ docker build -t taxi_ingest:v001 .`, then I encounter the following: `error checking context: ‘can’t stat ‘/home/elina/data-engineering-zoom amp/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data ‘’.`\\n\\nwhat could be the solution for this blocker?\",1643207213.303900,1643207213.303900,U02DY0L6PHV\\n3f8ac5cd-92d9-4643-85e3-a0bf4f1cbca2,U02QW395UNM,,,<@U02GVGA5F9Q> i really don\\'t know :confused:,1643203572.290700,1643207326.304000,U02QW395UNM\\nD39B4E11-3625-4C0D-BA98-94A2B2BFCA4B,U02DY0L6PHV,,,Use sudo ,1643207213.303900,1643208111.304400,U02TBB0D0RG\\n8d540eb3-f1f8-4e39-9be8-f0ceb7bfd92b,,6.0,,\"Hello I have some questions about the week 2 `docker-compose.yaml` file in the `airflow`, which has some syntax I don’t quite understand.\\n\\n```version: \\'3\\'\\nx-airflow-common:\\n  &amp;airflow-common\\n...\\n  airflow-webserver:\\n    &lt;&lt;: *airflow-common\\n    command: webserver\\n    ports:```\\n1. What is the `&amp;` in front of the first  `airflow-common` doing?\\n2. What does the `&lt;&lt;` mean\\n3. Why is there a `*` before the second `airflow-common` \\nI’m familiar with docker-compose files, but I’ve never seen this syntax used. Thanks!\",1643208225.305900,1643208225.305900,U02TEERF0DA\\nd7598c41-2ea6-4ac8-b0dc-0d39fa898cbe,U02U34YJ8C8,,,Am learning,1643203746.293000,1643208246.306000,U02T0CYNNP2\\nefdd7976-3999-48cd-aa2d-c594cd489862,,5.0,,\"#ask I got this error when trying to terraform apply, googling it still don\\'t understand\",1643208872.312700,1643208872.312700,U02TA7FL78A\\ne6fdb2c2-3aa0-4ab4-963a-5c0daa6decef,U02TEERF0DA,,,\"With &amp;, &lt;&lt; and\\xa0*, You can reuse the yaml tag. Search for anchor and alias in yaml.\\n\\n<https://stackoverflow.com/questions/6651275/what-do-the-mean-in-this-database-yml-file>\",1643208225.305900,1643208930.313400,U0290EYCA7Q\\n68290a3f-9ed7-4c80-8b9e-3679f1802bf5,U02TEERF0DA,,,,1643208225.305900,1643209208.316200,U0290EYCA7Q\\n988fa6dc-44c1-427d-b867-0c73b7b909b7,U02TEERF0DA,,,Ah great. Thanks for this!,1643208225.305900,1643209330.317600,U02TEERF0DA\\n5dbea749-e531-426e-93cd-cefb350b68cd,U02TA7FL78A,,,Did you enable apis?,1643208872.312700,1643209334.317900,U0290EYCA7Q\\nb75f62aa-54da-4378-b5b3-b53a2b67a1dc,,4.0,,\"Hi All, I have few questions want to ask regarding to *DE Zoomcamp 2.3.1 - Setup Airflow environment with Docker.*\\n\\n1. In the Pre-Reqs stage, we need to adjust the docker engine. However, I cant set the resource due to using wsl2 backend in windows machine.\\nAnyone can guide through me how to adjust the resource?\\n\\n2. Correct me if I am wrong about the objective of command.\\n• docker-compose build: it\\'s running _*docker-compose.yaml*_ to download all the airflow service images.\\n• docker-compose up : running the container for all the airflow service images\\n3. After run the docker-compose up, I found that my machine memory almost hit 100%. Is there any other place we can setup the airflow?\",1643209351.318400,1643209351.318400,U02T697HNUD\\ne5618513-968d-4fc3-bcde-f0139f1e06d9,,3.0,,\"Two, hopefully quick, questions:\\n1. When is the last day to register for the course? \\n2. Will there be further guidance on the final project beyond what was in the first webinar?\",1643209456.319000,1643209456.319000,U02TNEJLC84\\n8868cc7d-1c90-44fb-bb1b-5ed2d2f5618c,U02T697HNUD,,,\"For the first question check the free memory with\\n```wsl.exe -d docker-desktop free -m```\\n<https://stackoverflow.com/questions/66172375/docker-desktop-is-using-12-gb-ram-to-run-one-container-with-24-mb-ram>\",1643209351.318400,1643209689.319300,U02CD7E30T0\\n460357f4-e7c0-4cd1-80b2-d643d9f33058,U01AXE0P5M3,,,See this video <@U02T697HNUD> for one of your questions :slightly_smiling_face:,1643190996.249500,1643209877.319600,U02CD7E30T0\\nd9e5a4e3-678b-473b-a6bf-a705c826056b,U01AXE0P5M3,,,\"<@U02CD7E30T0>Hi Luis, thanks for point me on this meme:joy: looks like everyone facing this issue HAHAHA\",1643190996.249500,1643210017.320200,U02T697HNUD\\nbcfb2f53-a913-4096-ba52-d02fb4dc4011,U02T697HNUD,,,\"Thanks for sharing Luis, will have a look on this later.\",1643209351.318400,1643210038.320600,U02T697HNUD\\nf4e63263-98f7-4d09-ac76-50ce2afbfaae,U02TA7FL78A,,,\"<@U0290EYCA7Q> yes, I have\",1643208872.312700,1643210304.320900,U02TA7FL78A\\nbd1c07fe-5233-455d-b697-1af75741424f,U02UE7NTLUU,,,i have the same problem to <@U02R4F43B0C> Were you able to figure out the next step?,1642465146.363400,1643210841.321400,U02V5EP18PQ\\nd50ea43a-7fd4-4edf-9f85-3a27ffa0a5e1,U02UE7NTLUU,,,\"<@U02U5SW982W> Hi Sandy, are you running on mac? I don’t know what’s the password for root\",1642465146.363400,1643210885.321600,U02V5EP18PQ\\nd83ef3b1-7fc7-4386-96ad-14230c4acc99,,7.0,,\"A small advice to all on week 2 :slightly_smiling_face:\\nBefore running the command \"\"docker-compose (...)\"\" watch both video 2.3.1 and 2.3.2\\n;)\",1643211560.323300,1643211560.323300,U02CD7E30T0\\nd820dfc2-7079-43a5-bf94-88a66f28a170,,8.0,,\"hey folks, after all my efforts to get off the ground in my ubuntu vm I\\'ve been failing over and over, so I switched back to windows and restarted. still trying to set up a local environment with docker and going through the docker setup again, but having volume issues. when I try to run the postgres container with my full filepath I\\'m just getting this:\\n``` docker: Error response from daemon: invalid mode: /var/lib/postgresql/data.```\\nI\\'ve created the volume and it\\'s there on docker desktop. I\\'ve checked for typos and I\\'m 99% sure that\\'s not it\",1643211867.326300,1643211867.326300,U02TVGE99QU\\n55ceb3bf-8841-4b34-a240-dced6c3bb298,U02TNEJLC84,,,Technically anyone can enroll up to the point when the project deadline passes,1643209456.319000,1643211868.326400,U01AXE0P5M3\\nabd61332-3b65-4572-9b78-0c76d66a7451,U02TNEJLC84,,,\"As for the project, we\\'re still figuring it out\",1643209456.319000,1643211889.326600,U01AXE0P5M3\\n17f347ba-5e70-4727-9a07-45b66768d050,U02T697HNUD,,,Alternatively you can try running it on a VM,1643209351.318400,1643212125.327100,U01AXE0P5M3\\nd4eaeb74-efce-46b9-bd5b-4bfaf596bdfa,U02H0GUC7ML,,,I\\'ve 8 gb of RAM. And running docker  info shows ~7.5 gb memory. Still unable to run airflow containers.,1643173811.236300,1643212208.327300,U02H0GUC7ML\\n373bdc17-f098-4763-a329-a0bcf84a1c53,U02TVGE99QU,,,Can you show the command you use?,1643211867.326300,1643212222.327500,U01AXE0P5M3\\n539731b3-b6c9-4016-a931-c4da144d0503,U02TVGE99QU,,,\"I\\'m not sure why it pastes like this into my terminal, but it\\'s copied from your command with the path changed:\\n```dan@DESKTOP-92OUFMB:/mnt/c/Users/meler/Documents/code/DE-zoomcamp/week-1-basics-setup/2-docker-sql$ docker run -it \\\\\\n&gt;   -e POSTGRES_USER=\"\"root\"\" \\\\\\n&gt;   -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n -e PO&gt;   -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n&gt;   -v c:/Users/meler/Documents/code/DE-zoomcamp/week-1-basics-setup/2-docker-sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n  -p 543&gt;   -p 5432:5432 \\\\\\n&gt;   postgres:13\\ndocker: Error response from daemon: invalid mode: /var/lib/postgresql/data.\\nSee \\'docker run --help\\'.```\",1643211867.326300,1643212346.328200,U02TVGE99QU\\n6ff4b61e-c83e-4ffb-b450-6a9a8d8f8077,U02TVGE99QU,,,Try replacing c:/ with /c/ or //c/,1643211867.326300,1643212451.329200,U01AXE0P5M3\\n153028c2-026c-45c8-a8c3-ec0a828ce8bc,U02TVGE99QU,,,replacing it with /c/ worked. Thank you Alexey,1643211867.326300,1643212511.330100,U02TVGE99QU\\ne388ca4f-1b4a-4bec-bece-d373f860dfdf,U02CD7E30T0,,,\"same <@U01AXE0P5M3> have been with linux for last 11+ years, actually recently I\\'ve traded my mac to my wife\\'s windows.\",1643192586.254600,1643212535.330300,U02UBV4EC8J\\ncc84282d-ec08-4da5-8678-264cc6b3d8f5,U02TVGE99QU,,,\"Oh wait. You\\'re on wsl. Wsl is actually a Linux, so you can use the $pwd thing\",1643211867.326300,1643212545.330500,U01AXE0P5M3\\nd5b850e4-5d31-4cfc-87f8-a73a5ca0192b,U02TVGE99QU,,,ahhh that\\'s handy to know as well,1643211867.326300,1643212572.330800,U02TVGE99QU\\nfa09c689-2e21-412c-9478-db9598d4711d,U02TVGE99QU,,,\"Dan, can you please check the faq and if your question is not there, add it?\\n\\n<https://docs.google.com/document/d/19bnYs80DwuUimHM65UV3sylsCn2j1vziPOwzBwQrebw/edit?usp=sharing|https://docs.google.com/document/d/19bnYs80DwuUimHM65UV3sylsCn2j1vziPOwzBwQrebw/edit?usp=sharing>\",1643211867.326300,1643212603.331100,U01AXE0P5M3\\nec952f0d-4a45-4109-9c42-13efbe1424ce,U02TNEJLC84,,,Got it and got it. Thank you <@U01AXE0P5M3>!,1643209456.319000,1643212654.331300,U02TNEJLC84\\nba4f68af-6fde-45ac-987a-dc56ea8b5e0d,U02TVGE99QU,,,oh wow I did not know this faq existed. will check shortly and add it if it\\'s not there,1643211867.326300,1643212672.331500,U02TVGE99QU\\n7618214c-5402-4e99-b56f-e8f12b0ab61e,,2.0,,\"Not a question, just sharing. Out of memory on my local machine… My poor old macbook air was not built for these tasks:smiling_face_with_tear:\",1643212981.333500,1643212981.333500,U021RS6DVUZ\\nac2167a6-ea64-4991-b17b-d9b6922261fa,U02CD7E30T0,,,Any particular reason?,1643211560.323300,1643212987.333600,U02V4412XFA\\ncfb1bee2-a65f-4a87-9f7f-09a3e63a1767,U02CD7E30T0,,,A small detail is missing at the first video and you will only understand in the second,1643211560.323300,1643213144.333800,U02CD7E30T0\\n6b577f07-a72d-47fb-843a-e58b4410ba99,U02T697HNUD,,,Most likely it will be plenty of available memory. I had 12 GB.,1643209351.318400,1643213206.334100,U02CD7E30T0\\n1fce0df2-178f-4eec-9b13-8f0f7a2e6d20,U02UE7NTLUU,,,It\\'s root,1642465146.363400,1643213353.334400,U01AXE0P5M3\\nE538F9CA-7D09-4740-BE9A-A5F46EC2653A,U02UE7NTLUU,,,\"I just figured it out myself lol I spent the whole morning trouble shooting and finally finished the first step setup docker and pgsql <@U01AXE0P5M3> since I just joined 2 days ago.. is it possible to delay hw submission? If not, totally fine, I will keep working and try to stay up to date\",1642465146.363400,1643213754.340400,U02V5EP18PQ\\nca20d073-d345-4b1f-aa9c-d6cfaed4b2a5,U021RS6DVUZ,,,Maybe renting a VM could be an alternative?,1643212981.333500,1643213795.340900,U01AXE0P5M3\\nabc9a10d-d24b-44c3-a46d-b03778a4213f,U02TATJKLHG,,,\"Thanks for the feedback. You have a good point, something that we can be mindful of in the upcoming weeks. For this week, we plan to give you homework in the way where you can try building these components on your own.\",1643200258.278900,1643213870.341200,U01DHB2HS3X\\n969b73cb-249c-4cc1-bc6a-8249a6c21526,,3.0,,\"Just wondering..the docker build-t taxi_ingest:v001 \\n\\nHow long does it take to build? Mine has been running for minutes now\",1643213978.342700,1643213978.342700,U02SZARNXUG\\n739f9da6-b66e-4a78-9f34-4de15c03337b,,1.0,,\"A bit lost on the local docker airflow container. I am on a Windows machine and in step 4. Set the Airflow user I just add an env varibale AIRFLOW_UID = &lt;my_user&gt;, though when I run docker-compose up I get an error. Plus, do we need to create those three folders indicated? If yes where? The setup instructions are bit complicated if not using Linux :slightly_smiling_face:\\n```Error response from daemon: unable to find user &lt;my_user&gt;: no matching entries in passwd file```\",1643214180.345400,1643214180.345400,U02UA0EEHA8\\nf9017cc7-fc09-4e12-b348-d45a4079e3ab,,29.0,,\"Why would I get this error? What do you recommend? Download the .csv again? I have it in the wrong directory anyway.\\n```Inserted another chunk..., took 11.328 seconds\\nInserted another chunk..., took 11.015 seconds\\nInserted another chunk..., took 10.973 seconds\\nInserted another chunk..., took 10.893 seconds\\nInserted another chunk..., took 10.900 seconds\\nInserted another chunk..., took 13.112 seconds\\nInserted another chunk..., took 12.937 seconds\\nInserted another chunk..., took 12.166 seconds\\nInserted another chunk..., took 12.828 seconds\\nInserted another chunk..., took 17.735 seconds\\nInserted another chunk..., took 13.152 seconds\\n\\n/Users/cris/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3364: DtypeWarning: Columns (6) have mixed types.Specify dtype option on import or set low_memory=False.\\n  if (await self.run_code(code, result,  async_=asy)):\\n\\nInserted another chunk..., took 13.214 seconds\\nInserted another chunk..., took 8.870 seconds```\",1643214414.346100,1643214414.346100,U02UBQJBYHZ\\n1fc7aa4e-6821-47fc-a292-5b7fc94a84d9,U0290EYCA7Q,,,There\\'s also Flyway - <https://flywaydb.org/> which the community edition is open source,1643044288.403300,1643214506.346200,U02V7ARKJ8M\\n4b109c1c-6153-4396-ae25-8c80f39143bc,U02UBQJBYHZ,,,\"that\\'s not really an error but a warning, there is a column with mixed data types and this slows down the data transfer. Nothing to worry about :slightly_smiling_face:\",1643214414.346100,1643214523.346500,U02UA0EEHA8\\n9e5d93b8-1458-4b20-af9f-e141e9c6a7a2,U02UE7NTLUU,,,\"We\\'ve delayed it once already, so I\\'m afraid it\\'s not a good idea to delay one more time. But it\\'s okay if you don\\'t submit the homework - the most important thing if you\\'re learning something new (hopefully you are!)\",1642465146.363400,1643214532.346700,U01AXE0P5M3\\na18fb3ad-a33f-4b46-802a-40d954da63a0,U02UBQJBYHZ,,,I ended up with only 1369765 rows.,1643214414.346100,1643214604.347000,U02UBQJBYHZ\\n184d6e04-a5e6-4660-b8a8-e7fc48eb8c63,U02UBQJBYHZ,,,that\\'s the correct number,1643214414.346100,1643214623.347200,U02UA0EEHA8\\na633e6eb-77aa-4916-9ab2-075fd1226149,U02UBQJBYHZ,,,Warning as we didn\\'t specify schema upfront.,1643214414.346100,1643214626.347400,U02V4412XFA\\n64fb1052-3f86-456f-ad15-65ff85d8b617,U02UBQJBYHZ,,,I thought it was more than 160000,1643214414.346100,1643214635.347600,U02UBQJBYHZ\\n777fc9b7-5fa3-4f75-be3e-f58d9dad7996,U02UBQJBYHZ,,,1600000,1643214414.346100,1643214640.347800,U02UBQJBYHZ\\n39204b46-4033-4aec-b2f3-edd612525908,U02UA0EEHA8,,,You can try using gitbash - that\\'s what I use and it worked fine,1643214180.345400,1643214667.348100,U01AXE0P5M3\\nb7e4b3b1-7552-4f51-9141-4b26c6718e58,U02UBQJBYHZ,,,I\\'ll take your word for it. :slightly_smiling_face:,1643214414.346100,1643214698.348300,U02UBQJBYHZ\\naa50d3c2-c389-4a4b-a2d5-8afb219a2ce2,U02UBQJBYHZ,,,It is :slightly_smiling_face:,1643214414.346100,1643214708.348500,U02CD7E30T0\\n582891b9-682c-4b82-995b-cf76c99b15f9,U02SZARNXUG,,,\"How many minutes? It should take a bit of time, so it\\'s fine\",1643213978.342700,1643214711.348700,U01AXE0P5M3\\nb400d4f7-073c-4a6b-aebd-83be09cf013b,U02UBQJBYHZ,,,\"This is an error, though may be inoffensive... To avoid it you have to use a proper code to iterate, instead of the `while True` loop.\",1643214414.346100,1643214746.348900,U02GVGA5F9Q\\na51a98fa-694b-4eb8-a5a0-0ef0258b8e16,U02UBQJBYHZ,,,On to pgAdmin.,1643214414.346100,1643214787.349100,U02UBQJBYHZ\\na21ed165-05b8-4a5a-b39e-9b723fa94bad,U02UBQJBYHZ,,,\"The first time it occurred to me it didn\\'t insert the last chunk, but afterwards it always completed all chunks.\",1643214414.346100,1643214816.349300,U02GVGA5F9Q\\nff5f1e50-f411-47e1-b125-d524309eee21,U02SZARNXUG,,,It got stuck on downloading pandas for a while..then some errors. Ran the command again and it\\'s on pandas as we speak,1643213978.342700,1643214866.349600,U02SZARNXUG\\nb2b625ff-b1ca-4722-ae16-c5b0e61d3eaf,U02SZARNXUG,,,Built now!:man_dancing: :man_dancing: :man_dancing:,1643213978.342700,1643215060.349900,U02SZARNXUG\\n0C6691D8-03F0-4F86-85DF-307797EEA75D,U02UE7NTLUU,,,Indeed! I’m super excited about this learning process. Also I feel extremely thankful that you and other lecturers and classmates are very involved and help each other resolve issues lol,1642465146.363400,1643215221.351800,U02V5EP18PQ\\n4c7875e9-2437-4523-a58e-2e0269339b43,U02CD7E30T0,,,\"Do you mean the dag python files?\\nedit: hmm maybe you mean the google cloud variables associated with the project and the bucket in the yaml\\nOr maybe not, I see that it is part of the setup instructions actually, I have to say that setup/tutorial is not easy to follow. So many concepts thrown and pre written files. I guess it is not too hard to copy and edit but, as you said, I suppose it needs to be done before following the procedure in the video.\",1643211560.323300,1643215335.352100,U02TC8X43BN\\nCC3DD107-6FB2-4E0C-8414-452440EDD652,U02UBQJBYHZ,,,\"Yeah if you change the while loop you won\\'t get this error/warning, if it bothers you \",1643214414.346100,1643215879.354100,U02U34YJ8C8\\n516ba11d-9b14-4da8-a802-9e0c3fc3cbf8,,1.0,,\"Does `docker-compose up airflow-init`  run the `airflow-init` service within the `docker-compose.yaml` file. And then `docker-compose up` runs all the services with the yaml file?\\n\\nAnd is the reason we have to run `docker-compose up airflow-init`  first because other services in the yaml file depend on this running before being started?\",1643216239.357900,1643216239.357900,U02U34YJ8C8\\ncc96dbd1-39d9-4a72-8d28-674844c51698,,1.0,,Is completing the assignment mandatory to continue the course/ work on the project ?,1643217904.359200,1643217904.359200,U02TWFZURD1\\n71d8ebf9-5ceb-4b8b-a9d3-4bcafc5d0617,U02TWFZURD1,,,No,1643217904.359200,1643217919.359300,U01AXE0P5M3\\nf8572751-a0ed-4dcc-8cbf-470d8bfc8000,,5.0,,Is the wednesday 22.00 CET a hard deadline or there is a flexibility?,1643218392.360500,1643218392.360500,U0308MF3KUH\\n510e9388-7c17-4f8b-b96f-5f96dd0e0551,U0308MF3KUH,,,\"We\\'ve already postponed it once, so we shouldn\\'t delay it one more time. What are the problems you have?\",1643218392.360500,1643218635.360600,U01AXE0P5M3\\nfe26de0d-8fc5-45b6-a3ea-45d118b1e1de,U0308MF3KUH,,,\"I logged in recently, had some issues with postgres on my windows, worked fine on my linux machine, but now i have to set up the gcloud part\",1643218392.360500,1643219296.361000,U0308MF3KUH\\nf71af4fb-7155-438b-ac8d-4ba51e1d5887,U0308MF3KUH,,,\"For most of the homework you don\\'t need it,  only local postgres. You can focus on that part first to answer the questions there and then setup gcp\",1643218392.360500,1643219391.361900,U01AXE0P5M3\\n3d750574-f494-47ed-bf9e-c9d1c2940466,,2.0,,\"Greetings from Kenya, hoping you\\'re all keeping safe and healthy! Good day/night ladies and gentlemen. I did it! I just submitted my half baked homework, reason being I\\'m tired. Week 1 was just hell, I found myself sleeping doing SQL, so I guessed two of the answers and submitted. You can do it as well, keep on pushing.\",1643219394.362100,1643219394.362100,U02UQLZ071B\\neb1a5392-a2a5-4b7e-b99c-83e2d077f3c3,U0308MF3KUH,,,\"Ok, i ll submit the sql part then, probably the teraform part will be missing, thanks Alexey\",1643218392.360500,1643219500.362200,U0308MF3KUH\\n5a2a74e1-2762-40ce-bd8f-56fb799a92e1,U0308MF3KUH,,,You\\'ll have a chance to catch up this week,1643218392.360500,1643220249.363700,U01AXE0P5M3\\nc3d58038-5f49-47a9-b0fc-6c6e955c9154,U02UQLZ071B,,,Nice job submitting your homework! It\\'s 3 am in South Korea right now and I was half sleeping doing the homework as well haha.,1643219394.362100,1643220250.364000,U0308865C0H\\na43cb6a4-9851-4d2a-bd3b-13d480ceb14a,,6.0,,\"Hello all when i run this code\\ndef main(params):\\n    user = params.user\\n    password = params.password\\n    host = params.host\\n    port = params.port\\n    db = params.db\\n    table_name = params.table_name\\n    url = params.url\\n    csv_name = \\'output.csv\\'\\n\\n    os.system(f\"\"wget {url} -O {csv_name}\"\")\\n\\n    engine = create_engine(f\\'postgresql://{user}:{password}@{host}:{port}/{db}\\')\\n\\n    df_iter = pd.read_csv(csv_name, iterator=True, chunksize=100000)\\n\\n    df = next(df_iter)\\n\\n    df.tpep_pickup_datetime = <http://pd.to|pd.to>_datetime(df.tpep_pickup_datetime)\\n    df.tpep_dropoff_datetime = <http://pd.to|pd.to>_datetime(df.tpep_dropoff_datetime)\\n\\n    df.head(n=0).to_sql(name=table_name, con=engine, if_exists=\\'replace\\')\\n\\n    <http://df.to|df.to>_sql(name=table_name, con=engine, if_exists=\\'append\\')\\n    while True:\\n        t_start = time()\\n\\n        df = next(df_iter)\\n\\n        df.tpep_pickup_datetime = <http://pd.to|pd.to>_datetime(df.tpep_pickup_datetime)\\n        df.tpep_dropoff_datetime = <http://pd.to|pd.to>_datetime(df.tpep_dropoff_datetime)\\n\\n        <http://df.to|df.to>_sql(name=table_name, con=engine, if_exists=\\'append\\')\\n\\n        t_end = time()\\n\\n        print(\\'inserted another chunk, took %.3f second\\' % (t_end - t_start))\\n\\n\\nif __name__ == \\'__main__\\':\\n    parser = argparse.ArgumentParser(description=\\'Ingest CSV data to Postgres\\')\\n\\n    parser.add_argument(\\'--user\\', required=True, help=\\'user name for postgres\\')\\n    parser.add_argument(\\'--password\\', required=True, help=\\'password for postgres\\')\\n    parser.add_argument(\\'--host\\', required=True, help=\\'host for postgres\\')\\n    parser.add_argument(\\'--port\\', required=True, help=\\'port for postgres\\')\\n    parser.add_argument(\\'--db\\', required=True, help=\\'database name for postgres\\')\\n    parser.add_argument(\\'--table_name\\', required=True, help=\\'name of the table where we will write the results to\\')\\n    parser.add_argument(\\'--url\\', required=True, help=\\'url of the csv file\\')\\n\\n    args = parser.parse_args()\\n\\n    main(args)\\n\\n------I got this error ----\\n```usage: ipykernel_launcher.py [-h] --user USER --password PASSWORD --host HOST\\n                             --port PORT --db DB --table_name TABLE_NAME --url\\n                             URL\\nipykernel_launcher.py: error: the following arguments are required: --user, --password, --host, --port, --db, --table_name, --url\\nAn exception has occurred, use %tb to see the full traceback.\\n\\nSystemExit: 2\\n\\n\\n/Users/sisu/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3452: UserWarning: To exit: use \\'exit\\', \\'quit\\', or Ctrl-D.\\n  warn(\"\"To exit: use \\'exit\\', \\'quit\\', or Ctrl-D.\"\", stacklevel=1)```\\n\",1643220253.364200,1643220253.364200,U02T8HEJ1AS\\nfbc07570-86ff-4a09-b231-91acc9cc4a08,U02UQLZ071B,,,I feel awful reading this :sweat_smile: sorry =),1643219394.362100,1643220288.364300,U01AXE0P5M3\\nf8663c08-6153-41e9-9c1a-6e96d6cdaca0,U02T8HEJ1AS,,,\"Please next time put the code inside the thread\\n\\nHow do you run it?\",1643220253.364200,1643220344.364600,U01AXE0P5M3\\n8d5bd97e-bde7-4626-9fd1-601a48fd77cb,U02T8HEJ1AS,,,<@U01AXE0P5M3> Jupyter not book.,1643220253.364200,1643220443.364900,U02T8HEJ1AS\\n4c72a969-8abc-49a9-a135-38b8343e735b,U02DY0L6PHV,,,<@U02DY0L6PHV> <https://github.com/sindresorhus/guides/blob/main/docker-without-sudo.md> this tutorial will help you not use sudo again :thumbsup:,1643207213.303900,1643220510.365100,U02SUH9N1FH\\nf0669ef9-f360-4a15-aab3-6bfafb704da1,U02T8HEJ1AS,,,<@U01AXE0P5M3> the upload worked and the ingest code i copied it from git hub,1643220253.364200,1643220531.365400,U02T8HEJ1AS\\nf30cac6f-f06b-4421-badc-08f0e686e2ec,,2.0,,I do not think I will be able to hand in the homework by the due date. What is the appropriate course of action,1643220825.366300,1643220825.366300,U02SXQ9L0FJ\\ne31c5317-6d52-47aa-9e6d-82f96b7155d2,U02UBQJBYHZ,,,<@U02UBQJBYHZ> did the ingest_data.py worked for you?,1643214414.346100,1643221006.366600,U02T8HEJ1AS\\nece34277-409b-4076-b6e5-eaab29710e1a,U02UBQJBYHZ,,,Yes it did and I noticed that the same error appeared on the video.,1643214414.346100,1643221148.367600,U02UBQJBYHZ\\na90499e7-9280-4c60-bf8f-70b7a12c3ac7,U02UBQJBYHZ,,,Or warning.,1643214414.346100,1643221178.368100,U02UBQJBYHZ\\n21337e6e-f9c0-4aba-872d-c1f835e6f0bc,U02UBQJBYHZ,,,\"<@U02UBQJBYHZ> this is my error\\n```usage: ipykernel_launcher.py [-h] --user USER --password PASSWORD --host HOST\\n                             --port PORT --db DB --table_name TABLE_NAME --url\\n                             URL\\nipykernel_launcher.py: error: the following arguments are required: --user, --password, --host, --port, --db, --table_name, --url\\nAn exception has occurred, use %tb to see the full traceback.\\n\\nSystemExit: 2\\n\\n\\n/Users/sisu/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3452: UserWarning: To exit: use \\'exit\\', \\'quit\\', or Ctrl-D.\\n  warn(\"\"To exit: use \\'exit\\', \\'quit\\', or Ctrl-D.\"\", stacklevel=1)```\\n\",1643214414.346100,1643221216.368300,U02T8HEJ1AS\\nd8301b54-a9d4-4658-8fd5-d620a5ed5e1a,U02UBQJBYHZ,,,Did you copy the code from git hub and run it on Jupyter notebook?,1643214414.346100,1643221255.368500,U02T8HEJ1AS\\nb4f88edf-67f0-445a-8b7a-116b81d81de8,U02UBQJBYHZ,,,No I typed the commands into the notebook while I watched the video.,1643214414.346100,1643221306.368700,U02UBQJBYHZ\\n380b1e98-faea-4e21-b3f5-743884131981,U02UBQJBYHZ,,,oh i will try that maybe the code on git hub is wrong?,1643214414.346100,1643221344.368900,U02T8HEJ1AS\\n7d9c50f9-7b95-4678-bb9b-22ff183a0f43,U02UBQJBYHZ,,,\"I would suggest post your question as a new post, and someone might have the answer for you.\",1643214414.346100,1643221346.369100,U02UBQJBYHZ\\n85b78623-f4b7-4302-8696-3bc5fbb73f2f,U02UBQJBYHZ,,,i did and nothing yet,1643214414.346100,1643221373.369300,U02T8HEJ1AS\\neb0b91d1-6ddc-4354-b7e3-032d59434bcc,U02SXQ9L0FJ,,,^^,1643220825.366300,1643221473.370000,U02SWHZKSDN\\n5ed21434-e777-4d5a-983b-6f9b4a602206,U02UBQJBYHZ,,,I have anaconda installed and I used the command jupyter notebook to start up the python environment. I had to make sure I updated jupyter-console because pgcli had broken it.,1643214414.346100,1643221492.370200,U02UBQJBYHZ\\n93377f0d-3ed7-4b23-88c3-d9961d909c15,U02UBQJBYHZ,,,pip install -U jupyter-console,1643214414.346100,1643221565.370400,U02UBQJBYHZ\\n67f590ad-6222-421c-b1ce-53ff52889191,U02UBQJBYHZ,,,the upload data worked with no issue,1643214414.346100,1643221573.370600,U02T8HEJ1AS\\n770efd1e-2f1d-4861-ae43-9e15e52c7a0d,U02UBQJBYHZ,,,\"I see, what I did was upload-data. I didn\\'t run ingest-data. I uploaded the data and then used pgAdmin to answer the questions for the homework. That\\'s as far as I am in the course. I was hung up all last week on the pgcli problem.\",1643214414.346100,1643221774.372600,U02UBQJBYHZ\\n33f5591e-ba21-491d-905d-7c29178b1231,U02UBQJBYHZ,,,Try posting again. I think the responses are faster today.,1643214414.346100,1643221822.372800,U02UBQJBYHZ\\n049c58e2-1024-4ec2-a81b-5c04b942e7b0,,5.0,,\"the upload-data.ipynb files worked but the ingest_data.py didn\\'t work for me on Jupyter\\n```usage: ipykernel_launcher.py [-h] --user USER --password PASSWORD --host HOST\\n                             --port PORT --db DB --table_name TABLE_NAME --url\\n                             URL\\nipykernel_launcher.py: error: the following arguments are required: --user, --password, --host, --port, --db, --table_name, --url\\nAn exception has occurred, use %tb to see the full traceback.\\n\\nSystemExit: 2\\n\\n\\n/Users/sisu/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3452: UserWarning: To exit: use \\'exit\\', \\'quit\\', or Ctrl-D.\\n  warn(\"\"To exit: use \\'exit\\', \\'quit\\', or Ctrl-D.\"\", stacklevel=1)```\\n\",1643221876.373900,1643221876.373900,U02T8HEJ1AS\\n476ffab7-f97a-4e32-828b-7f8905af0729,,7.0,,\"Just wanted to ask if it is possible to do an if statement inside of postgres and if so, how you would go about doing it? Eg. if a column value is null, how would you change it to \\'Unknown\\' (Something like\\nif NULL\\nthen \\'Unknown\\'\\nend if.\\nI know there are better ways to do this such as updating the table where it is NULL or pre-processing. Just curious\",1643221899.374200,1643221899.374200,U02T18VH90F\\n767b507d-17b9-4746-9417-429013994ad6,U02UBQJBYHZ,,,that was what confused me what is the difference between upload-data and ingest_data?,1643214414.346100,1643222046.374400,U02T8HEJ1AS\\n4655908c-0a7d-48dc-b60b-e8b759a5b76e,U02UBQJBYHZ,,,i can\\'t find the video for ingest_data too,1643214414.346100,1643222066.374600,U02T8HEJ1AS\\ne2485985-6e4e-4252-93b9-4b0de771b4ed,U02SXQ9L0FJ,,,relax and take it easy :slightly_smiling_face:,1643220825.366300,1643222077.374800,U01AXE0P5M3\\ne920263a-d4cb-461c-8c70-cc7d20bb43b5,U02T8HEJ1AS,,,\"i think its because the values are not saved there. just replace the user, password, host, port, db and table name with root, root, 5432, ny_taxi, yellow_taxi_data and it should work\",1643221876.373900,1643222097.375000,U02T18VH90F\\n827c07bc-5668-4237-99a4-3323c66212b2,U02T8HEJ1AS,,,you don\\'t need to put it to jupyter - check how I run this script in the video,1643220253.364200,1643222118.375200,U01AXE0P5M3\\n31428bb3-a08c-47e3-946d-d1db93cb68b6,U02T18VH90F,,,\"I googled \"\"sql replace null with string\"\" and found some answers\",1643221899.374200,1643222182.375400,U01AXE0P5M3\\nb2f17ece-c227-4ceb-8e8e-f38ce7368f43,U021RS6DVUZ,,,\"<@U021RS6DVUZ>. I have Macbook (the thin 13in macbook) and I am dreading this to happen. With each transaction, I check my storage capacity and at times 10GB just disappear in sec\",1643212981.333500,1643222195.375600,U02TB5NK4FL\\n91DC5452-22CB-40B9-B5B7-04BBCBC51164,U02T18VH90F,,,Look into CASE statement,1643221899.374200,1643222213.376100,U02U34YJ8C8\\nae6f7a9a-5af3-48e4-a8d3-db2e847b95f0,U02UBQJBYHZ,,,Could be it\\'s for running the python script with arguments. I\\'m just guessing here.,1643214414.346100,1643222288.376300,U02UBQJBYHZ\\n538a19ed-7706-4f59-8427-e7f42e7d7580,U02T18VH90F,,,\"```case\\nwhen statement1 then action1\\nwhen &lt;field&gt; is null then \\'Unknown\\'\\nwhen statement3 then action3\\nelse &lt;optional&gt;\\nend```\",1643221899.374200,1643222497.379100,U02UA0EEHA8\\n9c23bc58-f6c1-429d-825f-d2b255746240,U02T8HEJ1AS,,,<@U02T18VH90F> do i need to do both upload and ingest?,1643221876.373900,1643222499.379400,U02T8HEJ1AS\\n1c82b112-d110-4848-9ce2-64118f65cc90,,2.0,,\"What\\'s the best way for ingesting the zone data ? Make copies of the dockerfile and ingest file only this time with zone file and zone table name ? I\\'ve commented out the datetime reference in the ingest file, because the zone file doesnt have it, however when I run the ingest docker command, it keeps saying that the datetime reference is stil there? I\\'ve done a look to see if I have the docker volume for the ingest process running and it isn\\'t\",1643222531.380100,1643222531.380100,U02TMP4GJEM\\nb7205167-c63d-4745-87e0-9ca6e3d32e7f,U02T8HEJ1AS,,,<@U01AXE0P5M3> i can\\'t  find the ingest_data.py video.,1643220253.364200,1643222620.380300,U02T8HEJ1AS\\nffa5f5ca-7123-4b2f-bef4-76c1a62dece2,U02T8HEJ1AS,,,<@U02T8HEJ1AS> no i dont think so. the upload one is basically just run on jupyter where you do it line by line. The ingest script is basically doing the same thing just automated when you run the docker taxi images. One is just automated and the other manual-ish. You can check whether the dataset is uploaded into the database by checking on pgcli. If you run one after the other properly you would just erase the table on the database and create a new one due to the ifexist = \\'replace\\' part of the code,1643221876.373900,1643222909.383000,U02T18VH90F\\n4a0a508c-06dd-4491-96cd-512844a9167e,,5.0,,\"in homework 1 google form we have:\\n\"\"Your code (link to GitHub or other public code-hosting website). Remember: no code - no scores!\"\"\\n\\nhere we need to provide link to github project with sql queries for questions 3-6\\nright?\",1643222966.383600,1643222966.383600,U02UEESBNGK\\n5b2f58e6-314c-48c7-be86-be4b735314eb,,3.0,,\"Maybe I missed something, but is there homework posted for week 2?\",1643223040.384900,1643223040.384900,U02TEERF0DA\\n6c774b36-8e8d-48a2-965e-348bcc8f6df7,U02T18VH90F,,,<https://www.postgresql.org/docs/8.1/functions-conditional.html>,1643221899.374200,1643223113.385500,U0290EYCA7Q\\n64eb174f-be1e-4fc4-b264-dac1ad2abd2b,U02T8HEJ1AS,,,<@U02T18VH90F> Thank you. The ingest script is not working for me so i will leave it for now and move to the next one.,1643221876.373900,1643223126.385900,U02T8HEJ1AS\\n37f2c880-9eb2-43fc-9148-c540db81a80d,U02UEESBNGK,,,\"That\\'s all I did, hope it\\'s OK. I put the queries into the README.md that had the docker run commands in it.\",1643222966.383600,1643223191.387400,U02UBQJBYHZ\\n7b6fcad3-3edf-4ce6-8760-a98ed6a197f4,U02UEESBNGK,,,My github repo is a forked copy of the whole class repo.,1643222966.383600,1643223219.388000,U02UBQJBYHZ\\n93e754d5-239c-417a-8f8c-d6a64308ff49,U02TEERF0DA,,,You are too fast.,1643223040.384900,1643223222.388200,U0290EYCA7Q\\nb96d5b28-382c-4bc1-9ed6-338d62f9ee7f,U02TEERF0DA,,,Ha ok,1643223040.384900,1643223233.388700,U02TEERF0DA\\n8286c330-036e-4dfa-a7ac-9b569a60a56d,U02T8HEJ1AS,,,<@U01AXE0P5M3> no worries. Thank you,1643220253.364200,1643223234.388900,U02T8HEJ1AS\\n9b8033f7-d74d-4e4d-9ab1-06e63c789124,U02TEERF0DA,,,Not yet,1643223040.384900,1643223268.389400,U01AXE0P5M3\\nf7629151-bb13-4a77-8183-ecb0e0ae964d,U02UEESBNGK,,,Yes that\\'s right,1643222966.383600,1643223285.390100,U01AXE0P5M3\\n79e1ed57-53c2-48a0-bfbb-f5b6c6523b8f,U02UEESBNGK,,,Thanks a lot),1643222966.383600,1643223296.390600,U02UEESBNGK\\n5fc669fa-bee6-4812-bc11-fd95a844d348,U02TMP4GJEM,,,\"You don\\'t have to do it from docker, you can do it from the notebook like I did in the video\",1643222531.380100,1643223333.391600,U01AXE0P5M3\\n4B2B41F1-AF29-4EB9-8F14-2EC3BD4F5065,U02TMP4GJEM,,,\"I just updated the ingestion script, and also made sure to pass additional parameters when building the ingestion container. You\\'ll have to rebuild the image after updating the ingestion script though\",1643222531.380100,1643223453.393500,U02U34YJ8C8\\n7d016862-cb6e-4074-b2dd-938113545a0e,U02T8HEJ1AS,,,the data is there. Thank you,1643221876.373900,1643223617.393900,U02T8HEJ1AS\\n7e473d29-9c1d-4d7e-839e-e922b824a926,,2.0,,\"Hello. When I run \"\"docker-compose build\"\" in airflow folder I get error on Step 11/13\\n\\nWelcome to the Google Cloud SDK!\\nWARNING: You appear to be running this script as root. This may cause\\nthe installation to be inaccessible to users other than the root user.\\nERROR: (gcloud.components.list) Failed to fetch component listing from server. Check your network settings and try again.\\nERROR: Service \\'airflow-init\\' failed to build : Build failed\\n\\nWho knows what should I do?\",1643223995.397300,1643223995.397300,U02QL1EG0LV\\naec45237-6ead-40f6-87e1-7552b6df986f,U02U34YJ8C8,,,\"For recurrent runs, you can avoid running airflow-init explicitly. But for the first time, you would need to run it to initialize some config, and create a backend db\",1643216239.357900,1643225262.399500,U01DHB2HS3X\\n25c3907f-b46e-467c-87ab-418202ac6dd1,U02TEERF0DA,,,\"<@U02TEERF0DA>  I agree. Not a common standard that I\\'ve come across in docker-compose files either. This is an official quick-start template provided by Airflow, that we\\'ve used here, and as explained in the video and the notes, we did not want to spend time modifying around their Docker setup to build a slimmer custom version, or explain each section within Docker-compose, but rather wanted to focus more on the Airflow parts. You can also have a look at a custom no-frills version of this file in the extras directory.\\n\\n<@U0290EYCA7Q> Thanks for sharing! Now I learnt something from you :)\",1643208225.305900,1643225937.400800,U01DHB2HS3X\\na8dd8e6f-10ec-45dd-887e-f81605a782f8,U02QL1EG0LV,,,\"Ok. My bad, Icreated AIRFLOW_UID as not a user \"\"root\"\"\",1643223995.397300,1643225983.401100,U02QL1EG0LV\\n634af531-4b46-4ab4-b5c8-0d887b03da12,U02CD7E30T0,,,\"Are you referring to the hard-coded values to the env-vars of ProjectID &amp; Bucket? I just realized that mistake, better to create a .env file and not commit it to git\",1643211560.323300,1643226234.401500,U01DHB2HS3X\\nf345d470-e31c-4ca4-a91c-a67c014f5586,U02T18VH90F,,,To replace nulls you should use `coalesce`,1643221899.374200,1643226737.401900,U02GVGA5F9Q\\n91099f36-b9ec-4d3f-8758-2f6d3d33cd3c,U02T18VH90F,,,\"`select colaesce(column, value_if_column_is_null)`\",1643221899.374200,1643226796.402100,U02GVGA5F9Q\\n9ce59994-07ef-44ad-baf6-d2be7b849a67,U02UEESBNGK,,,\"I used a new repo, not the forked one that has too much information...\",1643222966.383600,1643226915.402400,U02GVGA5F9Q\\n78387ea6-d424-4eff-ba3f-8d7b93a6be88,,4.0,,\"using docker-compose to connect and I\\'m having some trouble again. I think it\\'s because of this:\\n```Attaching to 2-docker-sql-pgadmin-1, 2-docker-sql-pgdatabase-1\\n2-docker-sql-pgdatabase-1  | chmod: changing permissions of \\'/var/lib/postgresql/data\\': Operation not permitted\\n2-docker-sql-pgdatabase-1  | The files belonging to this database system will be owned by user \"\"postgres\"\".\\n2-docker-sql-pgdatabase-1  | This user must also own the server process.\\n2-docker-sql-pgdatabase-1  |\\n2-docker-sql-pgdatabase-1  | The database cluster will be initialized with locale \"\"en_US.utf8\"\".\\n2-docker-sql-pgdatabase-1  | The default database encoding has accordingly been set to \"\"UTF8\"\".\\n2-docker-sql-pgdatabase-1  | The default text search configuration will be set to \"\"english\"\".\\n2-docker-sql-pgdatabase-1  |\\n2-docker-sql-pgdatabase-1  | Data page checksums are disabled.\\n2-docker-sql-pgdatabase-1  |\\n2-docker-sql-pgdatabase-1  | fixing permissions on existing directory /var/lib/postgresql/data ... initdb: error: could not change permissions of directory \"\"/var/lib/postgresql/data\"\": Operation not permitted\\n2-docker-sql-pgdatabase-1 exited with code 1\\n2-docker-sql-pgadmin-1     | NOTE: Configuring authentication for SERVER mode.```\\nI can connect to pgadmin but not to the database\",1643227187.403500,1643227187.403500,U02TVGE99QU\\nb2ed7caf-ab9a-4642-8c1e-ab77f0c1ee72,,4.0,,Have all the videos for week 2 been posted? or are there more coming?,1643227204.403800,1643227204.403800,U02T9550LTU\\n81f08800-5f31-4a06-a8b5-e26110f72943,U02T9550LTU,,,There will be a few more,1643227204.403800,1643227260.403900,U01AXE0P5M3\\n4e6a1c6b-5451-4bca-ad05-c05224448925,U02TVGE99QU,,,You can remove the folder and let docker create it with the right permissions,1643227187.403500,1643227357.404300,U01AXE0P5M3\\n1555ed04-7b28-4163-8052-be7dde9e5bc6,,2.0,,If I submit week 1 homework today but later than 22:00 CET what happens?,1643227557.405200,1643227557.405200,U01SXD1FU9W\\ne052d525-1fa6-4747-bbde-23f60a3b3173,U02TVGE99QU,,,do you mean the mounted folder ny_taxi_postgres_data?,1643227187.403500,1643227728.405300,U02TVGE99QU\\n9cb8c5a7-6d5e-4c3f-9864-6830215c1a1b,,1.0,,\"btw, will we have a public leader board after submitting first week hw?\",1643227813.406000,1643227813.406000,U01UKRH6VGT\\n57f63eb1-d6de-4d93-83cf-0ad114421202,,1.0,,\"Prior to this course,  networking has always been an issue for me, expecially cloud in environment. I\\'m beginning to get a hang of it. I\\'m so excited:smiley:\",1643228405.409200,1643228405.409200,U02SZARNXUG\\nf9bee7ea-56c3-42f0-9c3a-4245789ca42f,U01SXD1FU9W,,,You can try and tell us what happens =),1643227557.405200,1643228496.409300,U01AXE0P5M3\\na1635c27-3d6a-4efd-8964-4e9a3c0c9c78,U01UKRH6VGT,,,Eventually,1643227813.406000,1643228508.409500,U01AXE0P5M3\\n34f42466-fdf2-4942-90f1-257cabdf0f09,U01SXD1FU9W,,,:eyes:,1643227557.405200,1643228592.409900,U01SXD1FU9W\\n6ca55a10-625b-4ce7-9b7e-e85a678f4c75,,,,in gcloud is Viewer Role same as Action Viewer? cos i did not see Viewer instead i saw Action Viewer for roles,,1643229388.413000,U02UKLHDWMQ\\nc17011ac-8bf9-48cc-b553-45fe8c888ef8,U02TVGE99QU,,,yep,1643227187.403500,1643229511.414400,U01AXE0P5M3\\ne38a7a2e-72d6-4190-bac6-de70085dfb99,,4.0,,\"hello everyone,\\n\\nI already did the Question 2 for the homework of week 1 alongside the video. I tried to do it again and get the error:\\n\\n```Error: googleapi: Error 409: You already own this bucket. Please select another name., conflict```\\nAm I suppose to copy this message to the assignment?\",1643229517.414600,1643229517.414600,U02RSAE2M4P\\n1d6f6bdb-2ded-408a-8ae3-eb285be3df85,U02RSAE2M4P,,,yep that works as well,1643229517.414600,1643229559.415100,U01AXE0P5M3\\n924abb29-599a-45e4-a1c1-ebc236fce8ca,U02RSAE2M4P,,,\"that\\'s the output of terraform apply, right? if yes, it works :smiley:\",1643229517.414600,1643229587.415800,U01AXE0P5M3\\n09b5a0b9-b68c-4a31-a082-396d9bbef558,,13.0,,\"Has anyone else ran into issues running “docker-compose up” for week 2? No matter how long i wait, the items that are unhealthy do not change.\",1643229595.416000,1643229595.416000,U02TBTX45LK\\n63b6fe3c-4820-4f5e-ad33-53dd56b2f3da,U02TBTX45LK,,,did you give it enough RAM like Sejal suggested?,1643229595.416000,1643229630.416500,U01AXE0P5M3\\n58495023-a3ee-4359-8902-ad0dac595b30,U02TBTX45LK,,,\"Yes, I currently have it set at 6gb on docker\",1643229595.416000,1643229798.416700,U02TBTX45LK\\n08fe8ed8-d5eb-4816-86b6-9f1d0a84f664,,15.0,,\"I have been unable to claim the $300 free credit from google cloud.\\nWho else is experiencing this and and how can I resolve?\",1643229926.418000,1643229926.418000,U02V90BSU1Y\\nfff02b81-4721-44ed-9bfe-fa5a2958c360,U02RSAE2M4P,,,yes,1643229517.414600,1643229935.418100,U02RSAE2M4P\\nD378A961-9AFC-4DEE-A683-81B2E4176C47,U02V90BSU1Y,,,Same here…. It’s requesting I verify my payment ,1643229926.418000,1643230021.419500,U02TZ1JCVEC\\ne719a4d3-3edc-446c-b670-e0e2d9689a07,U02V90BSU1Y,,,Which card kind of card are you using,1643229926.418000,1643230059.420200,U02UKLHDWMQ\\nf9e1ef67-0737-4b8b-b465-dcbe6c7f641b,,2.0,,\"So I started up the class with an empty directory and ran all the docker commands and created the database in that directory. Then yesterday I forked the class repo and initialized my own repository. I see that the database is in .gitignore. Can I just copy the database from one directory to the other, or do I have to start from some other point?\",1643230159.421800,1643230159.421800,U02UBQJBYHZ\\n009f7ece-5da4-47fa-8305-49975248d531,U02UBQJBYHZ,,,You should be able to move it,1643230159.421800,1643230215.421900,U01AXE0P5M3\\nf16db1a5-3043-4593-9a83-3b90a0b98223,,3.0,,I\\'ll be recording a video now and might accidentally forget to close the form at 22:00,1643230267.422900,1643230267.422900,U01AXE0P5M3\\nd482f71c-863d-4223-be74-26a4cddb12ed,U02T9550LTU,,,should we wait until all have been uploaded? or what can we start in the meantime? <@U01AXE0P5M3>,1643227204.403800,1643230279.423100,U02T9550LTU\\n704f133f-a020-48ea-882f-62885b08a93b,U02UBQJBYHZ,,,Awesome!,1643230159.421800,1643230414.423600,U02UBQJBYHZ\\n1aa1d9b3-4c3c-4e79-8c0e-b9e34aa1bf94,U02T9550LTU,,,You can start,1643227204.403800,1643230519.423800,U01AXE0P5M3\\n4a8a68eb-35fe-47a5-85d3-f82e01843ce5,,5.0,,\"Hello, the ny_taxi_postgres_data folder is empty after running\\n``` winpty docker run -it\\\\   \\n        -e POSTGRES_USER=\"\"root\"\" \\\\ \\n        -e POSTGRES_PASSWORD=\"\"root\"\"   \\n        -e POSTGRES_DB=\"\"ny_taxi\"\"   \\n        -v /c/Users/chide/onedrive/desktop/docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n       -p 5432:5432   postgres:13```\",1643230715.425600,1643230715.425600,U02RZEKRBHD\\n9aeb1a03-a21d-44e5-96c0-351683515285,U02RZEKRBHD,,,\"If you are having issues, I suggest only watch the docker videos. But skip to docker-compose to set everything up.\",1643230715.425600,1643230775.426000,U02ULBM39B4\\nE3448340-C340-4CFC-BE4B-1241B3D1E36E,U02V90BSU1Y,,,<@U02UKLHDWMQ> Visa,1643229926.418000,1643231142.429200,U02TZ1JCVEC\\n0de94fee-f1a4-4ede-aa96-b0e6be5ffc5e,,1.0,,\"I have a few obligations at work that came up and will need to postpone being active in the course, is it okay if I can stay enrolled in the slack channel even though I won\\'t be active for a few weeks?\",1643231513.431200,1643231513.431200,U02UUCT5G2F\\nfc01b134-d390-4f3c-9ca5-832b713b40ac,U02TBTX45LK,,,\"Me too. I set 7 GB RAM, still got unhealthy and keep restarting\",1643229595.416000,1643231560.431400,U02UNB4G739\\n08029559-73da-4206-ba94-9c352744a6fb,U02RZEKRBHD,,,\"The command you posted only _creates_ a directory which is then linked to a directory in the postgres image. You need to run the python ingestion script after this, and then you’ll see data in the directory\",1643230715.425600,1643232506.432700,U02TEERF0DA\\nd6a27c19-747c-4ae1-a9a4-ac2607f14df2,U02RSAE2M4P,,,you can use `terraform show` to get the same info as when you issued the `apply`,1643229517.414600,1643232701.433000,U02GVGA5F9Q\\n95545956-25D2-4C22-97A6-4F288DFFB58B,U02TBTX45LK,,,\"Does it still work, even though it says unhealthy?\",1643229595.416000,1643232735.433600,U02U34YJ8C8\\n352cbebd-422e-4847-af28-eca7c2ca248e,U02T2DX4LG6,,,\"If you haven\\'t fixed this yet-\\n\\n• run docker network ls =&gt; which will give you a list of networks you have created\\n• Then find the network that is used by the pgadmin+pgcli, usually from the top of the terminal output when you have run docker compose\\n• Then use that in the network argument. For example: --network=here_is_the_networ\",1643061773.488300,1643232892.433900,U02QZN0LSBT\\n0091D5A1-F088-4FA8-9D3C-2C5380ABCFA5,,,,\"Seeing all the messages, I need to get back to work and start week 2! FOMO (fear of missing out) is kicking in! \",,1643233006.435700,U02UX664K5E\\n409b2317-d842-4cbe-99b3-e1283303b84f,U02V90BSU1Y,,,\"Yeah, you have to enter your card details, Google will make a small transaction (cents) to verify the card...\",1643229926.418000,1643233140.436100,U02SY5JKNBZ\\nCC098A72-0EF0-4955-9E83-C485C19E699A,U02V90BSU1Y,,,That happened ,1643229926.418000,1643233404.437700,U02TZ1JCVEC\\n61006582-A79B-4A95-9A19-120C01038365,U02V90BSU1Y,,,But I’m still unable to access it ,1643229926.418000,1643233421.438400,U02TZ1JCVEC\\nD06875FB-85A1-4DBE-8AFC-8AD547342873,U02V90BSU1Y,,,Says I need to verify payment method ,1643229926.418000,1643233429.438800,U02TZ1JCVEC\\n9769fa43-8b16-49a9-92c0-9d08175e1899,U02UUCT5G2F,,,\"Of course! This community is always open, even when there are no ongoing courses\",1643231513.431200,1643233628.439700,U01DHB2HS3X\\n0c60efc6-2d34-4400-8da7-660fb3e792bd,U02V90BSU1Y,,,I remember it took few days for the verification,1643229926.418000,1643233666.440000,U0308MF3KUH\\n2745fb05-89b8-4626-8fd7-ede58ddaafe6,U02TBTX45LK,,,\"no, i am unable to access the webserver in my browser\",1643229595.416000,1643233675.440200,U02TBTX45LK\\ndd8fae84-fa51-40cd-93b3-a70921cbd8a3,U02TBTX45LK,,,\"Sorry to hear this. :/ I faced a similar problem too, and had to wait about 15-20 mins the first time.\\nI\\'ll see if I can trim down the unwanted services from the official quick-start template, and use a leaner version to save memory. But it\\'ll take some time &amp; trials. If someone wants to give it a try with the \"\"no-frills\"\" version in the extras/ directory, and help me set it, would be great!\",1643229595.416000,1643233875.440400,U01DHB2HS3X\\na634f6f5-f4da-4e31-87d8-76478ff5af65,U02T9550LTU,,,\"The main ones are all up, to finish the workshop with Airflow+GCP. A few trailing ones are WIP.\",1643227204.403800,1643234120.441000,U01DHB2HS3X\\n4a82856b-3777-4180-90f0-c66785deb73b,,6.0,,\"My dag job failed at the load_to_gcs_task, please any solution to this\",1643234608.442300,1643234608.442300,U02T0CYNNP2\\ne6377ad7-345a-4fee-b104-9f3710e10cb1,U02TBTX45LK,,,Thanks <@U01DHB2HS3X>! Ill try my best to use the “no-frills” version and report back if that fixes everything,1643229595.416000,1643234668.442800,U02TBTX45LK\\n5A849E65-98C1-4880-8C69-4D5B6D3CBC6D,,1.0,,Has answers been published for the first homework?,1643234704.443400,1643234704.443400,U02U6DR551B\\n4fe72347-e93e-449e-9359-6dfea2266c98,U02CD7E30T0,,,\"Kjndly explain better <@U02CD7E30T0> my third task in airflow failed during the job run, wasnt able to load in gcp storage\",1643211560.323300,1643234939.443700,U02T0CYNNP2\\n919da272-7120-4ea2-9462-311d34bdb5a1,U01AXE0P5M3,,,\"I think I submitted at 22:00, Hopefully I got in. Is there a way to know if my homework was submitted? <@U01AXE0P5M3>\",1643230267.422900,1643235194.443900,U02RSAE2M4P\\n25885DD5-FDF3-4438-A384-D9BDE6C57EB7,U02V90BSU1Y,,,\"I had an issue when signing up to claim my 300$ free account. When creating the GCP account , the screen froze when I was almost done signing up. They blocked the account. I think I got an email with the notification m. They prompted me to send them a photo ID card and credit card. They unblocked my account the following day \",1643229926.418000,1643236835.451700,U02TB5NK4FL\\n94f05d1f-f47f-4691-8ae3-83291c0d1620,U02U5SW982W,,,I really liked <@U02QZN0LSBT>’s notes in Notion. <https://www.notion.so/Week-1-Introduction-f18de7e69eb4453594175d0b1334b2f4>,1643179603.242900,1643237200.452000,U02SQQTC5NV\\n2d195780-b030-4385-9db8-d8ce37d172fd,,7.0,,Greetings has any one\\'s google cloud trial account lost access?(i am unable to set up a new one for projects atm...i activated it 8 days ago and now i do not have access.) Any suggestions?,1643237452.453700,1643237452.453700,U02SFFC9FPC\\n8d833fa9-e58e-4a93-90d2-6a8292f21ef3,U02TBTX45LK,,,\"Just to confirm, did you run in your airflow folder:\\n```mkdir -p ./dags ./logs ./plugins\\necho -e \"\"AIRFLOW_UID=$(id -u)\"\" &gt; .env```\\nThe unhealthy error might not have anything to do with it not working. I can see from googling that this is an issue affecting other people.\",1643229595.416000,1643237507.453800,U02U34YJ8C8\\na8a67a51-ebc5-4878-8cc9-fa21d3f0ae4b,,2.0,,\"hi everyone\\nI still confused about this Pre-Reqs\\n&gt; You may need to upgrade your docker-compose version to v2.x+\\nI try to install docker-compose version 2.2.3 but docker-compose not running well\\n\\nis ok if using version\\ndocker-compose version 1.29.2\",1643237781.454600,1643237781.454600,U02SQQYTR7U\\n2269bd32-c80b-4432-92f8-689ed13fc097,U02U5SW982W,,,And also <@U02BVP1QTQF>\\'s notes @ <https://github.com/ziritrion/dataeng-zoomcamp/tree/main/notes> cf <https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642713863457200>,1643179603.242900,1643237836.454700,U02SQQTC5NV\\n1a528e67-19c2-4023-83d1-09a60d3b277b,U02TBTX45LK,,,yup! all folders and the .env files were created,1643229595.416000,1643238784.455600,U02TBTX45LK\\n6882a576-bd8b-410f-80af-fb811897a256,U01AXE0P5M3,,,\"The second to last solution for the sql query question ought to be south not north, kindly review <@U01AXE0P5M3>\",1643230267.422900,1643239183.455900,U02T0CYNNP2\\nb7fed81c-7a13-4c78-aeca-1753484b7bbc,,2.0,,will there be homework every week?,1643239580.456200,1643239580.456200,U02UGA597HS\\n16bf67c7-b75c-460a-a8d8-86327ab979ae,,,,Missed the deadline but happy I completed week1 HW questions :smile: . on to week 2,,1643239771.457100,U02TWFZURD1\\nc7ab040e-3913-421b-8959-5c33dd6cc723,,3.0,,Was on the last question :disappointed: Forget to check time . Missed the deadline enitrely. Sucks :disappointed:,1643240547.458200,1643240547.458200,U02QZN0LSBT\\n1689E6BA-2B88-4294-BAC4-60749DAE33D4,,1.0,,Where can we see scores for the homework,1643240896.458700,1643240896.458700,U02U6DR551B\\n0fe47537-6acb-43bd-b49a-4775f395dbbd,U02TBTX45LK,,,\"Update: i was able to run the build command using the no-frills yaml file. However, when running the docker-compose up command i am getting\",1643229595.416000,1643241602.458800,U02TBTX45LK\\naf52af99-8e3b-48e4-997c-fc4f2e3c9652,U02TBTX45LK,,,```airflow.exceptions.AirflowConfigException: error: cannot use sqlite with the LocalExecutor```,1643229595.416000,1643241619.459000,U02TBTX45LK\\ne57dd4e5-60ae-4be5-b574-1821dd67ceaa,U02T2DX4LG6,,,\"Yes , it was the network name. Thank you all for you help :slightly_smiling_face:\",1643061773.488300,1643243069.459200,U02T2DX4LG6\\nea5ac5c2-08e4-4a6c-808b-bde409a617d9,U02RZEKRBHD,,,Okay thanks,1643230715.425600,1643244129.459500,U02RZEKRBHD\\na6169738-b2be-4c08-aca5-98a8ff863009,U02H0GUC7ML,,,\"Faisal, I have 16GB of RAM on Ubuntu 20.04 and I didn\\'t have an issue.\",1643173811.236300,1643254001.471300,U02QGTCGJUD\\n78bca8c5-71fd-4944-8bd4-6600bfa8d0e0,U02H0GUC7ML,,,Max container size for me was webserver and flower... totally all containers use around 3GB,1643173811.236300,1643254066.471500,U02QGTCGJUD\\n33617d90-89ca-4a20-9381-9b0e0c8e5e53,U02TP858APK,,,\"I had the same issue too, so for the rest of the course DE always uses terminal inside VS code instead of git bash on windows? Or is there another solution so I can use git bash on windows?\",1643196570.265400,1643255083.472200,U02SHV7RJTW\\n3079CF24-3BA9-4B3F-A13B-50EF724F7E7B,U02T0CYNNP2,,,\"<@U02T0CYNNP2> I am also facing similar problem ..my error in log is as follows \\n“\\ngoogle.auth.exceptions.DefaultCredentialsError: File /.google/credentials/google_credentials.json was not found. “ \\n\\nCan u please confirm error in your log file ? \\n\",1643234608.442300,1643256331.474900,U02AGF1S0TY\\n6af9fb5a-77ed-46e9-8394-7ae539fb3b80,,3.0,,\"HI All, just wondering if anyone knows where we might share our notes for Week 2? There was a section in GitHub for Week 1 but not sure about Week 2? Was going to put mine up there - just in case it helps someone (like others helped me).\",1643256979.476800,1643256979.476800,U02U5SW982W\\na3020102-e6b8-49c0-90ba-f7c1b99608e4,U02H0GUC7ML,,,<@U02QGTCGJUD> I\\'m using Ubuntu as guest os. But it has 8gb ram and 2 core cpu. How can I set docker engine default memory limit to 6gb? Any idea?,1643173811.236300,1643257067.476900,U02H0GUC7ML\\n84125305-f2bc-44d0-855a-2879ee1989ca,U02UGA597HS,,,Hi <@U02UGA597HS> I do believe there is supposed to be homework every week but it seems like we might all be running a little behind. I haven\\'t seen a posting yet for week 2 homework (ro that might be just wishful thinking).,1643239580.456200,1643257120.477100,U02U5SW982W\\n354bc9fb-5fc6-4fa1-916a-fb7e27292533,U02SFFC9FPC,,,HI <@U02SFFC9FPC> you can set up another account. This has been asked and answered before on another thread (do a search in slack on this channel and you should find it). I too was in the same boat and didn\\'t want to have to fork out for a study course.,1643237452.453700,1643257248.477700,U02U5SW982W\\nf369a30f-4f9b-43b4-b6a4-0de9bae70cec,U02QZN0LSBT,,,Hi <@U02QZN0LSBT> - did you see Alexey\\'s post that he possibly won\\'t close the form at 22:00. Just put it in anyway and see how you go. At least this is how I understood his message.,1643240547.458200,1643257385.478000,U02U5SW982W\\n2f29efbc-2b59-43a5-b2a0-f7fdd950cfe9,U02H0GUC7ML,,,ok I am using a physical OS.,1643173811.236300,1643257423.478200,U02QGTCGJUD\\n3e2baabe-0a62-407a-bfcb-50e6f9cd4fed,U02H0GUC7ML,,,have you seen this page. I have no idea if it would help. <https://docs.docker.com/config/containers/resource_constraints/>,1643173811.236300,1643257494.479100,U02QGTCGJUD\\ne88e44e7-f034-4fc3-8b36-834cc2a043a6,U02U6DR551B,,,Hi <@U02U6DR551B> There is a week 1 solution for homework here <https://www.youtube.com/watch?v=HxHqH2ARfxM&amp;list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&amp;index=21> for the SQL.,1643234704.443400,1643257614.481400,U02U5SW982W\\n8800EFED-C4DF-4A92-82F3-D4313FC98F94,U02H0GUC7ML,,,Maybe you can try the opposite and reduce airflow requirements? <https://stackoverflow.com/questions/52060390/airflow-scheduler-out-of-memory-problems|https://stackoverflow.com/questions/52060390/airflow-scheduler-out-of-memory-problems>,1643173811.236300,1643257665.483200,U02QGTCGJUD\\n56f2d235-ea7c-41e8-b8ac-d3b5d77e0e41,,5.0,,\"Hi everyone, I got all the SQL answers right for the week 1 homework, but realized that my queries were basic and inefficient. I have a basic knowledge of SQL from Youtube courses but haven\\'t written complex queries. How should I learn to write such queries? Any guidance from <@U01AXE0P5M3> <@U01DHB2HS3X> <@U01DFQ82AK1> and other SQL gurus here would be greatly appreciated!\",1643257694.483700,1643257694.483700,U02UB8XDCHJ\\nba2d7b84-a770-490a-9a6c-48aa32891b57,U02QZN0LSBT,,,\"Whoops I just saw that a solution has been released for Homework week 1 SQL so maybe not. But at least you got to the end. I think that in itself is a major achievement. Also, there aren\\'t a huge amount of points just for Week 1 (out of all our weeks that we have to do).\",1643240547.458200,1643257724.483800,U02U5SW982W\\n2729d54b-bcc0-4ba0-bcb0-75534b91109d,U02TP858APK,,,<@U02SHV7RJTW> I don\\'t think that should be an issue. It just happened with pgcli so more of a pgcli issue than bash I guess. Bash works fine. Also we won\\'t be using pgcli going forward. It was just shown as one of the ways you can connect to postgres.,1643196570.265400,1643258607.484100,U02TATJKLHG\\n03d710f4-3b1c-449e-b779-dc3327705151,U02TP858APK,,,\"<@U02TATJKLHG> got it, thank you so much\",1643196570.265400,1643258800.484500,U02SHV7RJTW\\n5be97a2a-32d0-4b4c-b348-04859189bf05,U02UB8XDCHJ,,,\"<@U02UB8XDCHJ>\\nNot a SQL Guru by any means and was somewhat in a similar situation like you at some point in time so I can share what helped me.\\n\\nYou can try <http://pgexercises.com|pgexercises.com> or <http://hackerrank.com|hackerrank.com> to get better at SQL queries. There are some complex questions as well.\\n\\nAs to getting even better at it, look at Dimensional Modelling and create a dimensional model let\\'s say of a Spotify like service or any other dataset that you can find. Then write queries to create those tables. Those queries can get very interesting.\\n\\nI did the same with the million songs dataset, you can check out my Databricks notebook here - <https://bit.ly/3AExzyv>\",1643257694.483700,1643260252.484800,U02TATJKLHG\\nf5598bdf-6ae8-41d1-a6df-cf0ee3cacced,U02UB8XDCHJ,,,\"Hi! There is a great resource /r/dataengineering, and there are tons of resource for SQL improvement: <https://dataengineering.wiki/Tools/SQL>\",1643257694.483700,1643261070.485600,U02U5L97S6T\\nbb599784-8967-421e-958a-09885ed38979,U02V90BSU1Y,,,\"It happened to me as well. In your notifications (and also in google pay), they will ask to upload photos of  government id and credit/debit card. Although they said 48 hours, within 3-4 hours, it was done (at least for me)\",1643229926.418000,1643261331.486000,U02HB9KTERJ\\ne06ad6a1-4ae2-4e67-b89d-6ad61e99fb43,U01AXE0P5M3,,,North for me!,1643230267.422900,1643261375.486200,U02HB9KTERJ\\n3568d6a6-988b-4e1b-ab4f-9999f76683f1,U02T18VH90F,,,alright thanks everyone!,1643221899.374200,1643262460.486500,U02T18VH90F\\nd4715d59-69c3-4a95-ad7e-0868b8225bfd,U02U5SW982W,,,\"<@U02BVP1QTQF> just created it with his PR, and you can do the same for week 2\",1643256979.476800,1643264532.486900,U01AXE0P5M3\\ncde2094a-8678-4678-8382-d20b56044bb0,U02U6DR551B,,,There\\'ll be a spreadsheet with scores soon,1643240896.458700,1643264572.487100,U01AXE0P5M3\\nf8c24b4d-3331-4a63-aad0-15b0c2872cc0,,4.0,,\"I have enabled billing for my project, but I am not sure why I am getting this error when I run `terraform apply`. The error is mentioned in the thread to this message.\",1643265061.489000,1643265061.489000,U02T96HEARK\\nf061c689-6a19-46b9-8771-f1b056b70b70,U02UB8XDCHJ,,,<https://www.youtube.com/playlist?list=PLroEs25KGvwzmvIxYHRhoGTz9w8LeXek0>,1643257694.483700,1643265148.489100,U0290EYCA7Q\\n4ce64cfc-2a46-494e-a00a-f646014d4496,U02T96HEARK,,,\"```Error: Error updating Dataset \"\"projects/xxxxxx-xxxx-xxxxx/datasets/trips_data_all\"\": googleapi: Error 403: Billing has not been enabled for this project. Enable billing at <https://console.cloud.google.com/billing>. The default table expiration time must be less than 60 days, billingNotEnabled\\n│ \\n│   with google_bigquery_dataset.dataset,\\n│   on <http://main.tf|main.tf> line 45, in resource \"\"google_bigquery_dataset\"\" \"\"dataset\"\":\\n│   45: resource \"\"google_bigquery_dataset\"\" \"\"dataset\"\" {```\\nI have not mentioned the project name (xxxxxx-xxxx-xxxxx) so that it doesn\\'t overlap with someone else\\'s project.\",1643265061.489000,1643265250.489900,U02T96HEARK\\nB27FDC46-DDC7-4A72-AD9B-A35EA16DAEC3,U02V90BSU1Y,,,\"Alright, thanks everyone! I guess I’ll wait for their verification \",1643229926.418000,1643265261.490300,U02TZ1JCVEC\\n28740260-2d59-4dcd-ae31-db95f11d39a2,U02T0CYNNP2,,,Yes i have solved the issue,1643234608.442300,1643266003.492600,U02T0CYNNP2\\n3D446C55-255E-4EF3-8B75-E1B1A86E9509,,1.0,,There will be feedback for homework codes? Or just scoring?,1643266019.492800,1643266019.492800,U02MEN1AVQV\\n5f674757-a2be-4217-b911-0728221c1054,U02T0CYNNP2,,,\"Under the volume @ The docker-compose.yaml file add this\\n\\n- ~/.google/credentials/:/.google/credentials:ro\",1643234608.442300,1643266131.492900,U02T0CYNNP2\\n1f1c4f91-3187-4042-a4e3-7708e66fdcbc,,6.0,,Can anyone suggest some good command line reference to supplement this course?,1643266612.494000,1643266612.494000,U02VA225CG4\\n6683b5fb-193e-4777-88fc-75688971876e,U02MEN1AVQV,,,There will be no feedback on your code. But feel free to discuss it here and ask for feedback if you need,1643266019.492800,1643267523.494100,U01AXE0P5M3\\nfb76067e-25f9-4b36-8bf0-422ddee31e7a,U02VA225CG4,,,if you wish to learn about Linux Shell and Scripting you can check out <https://practice.geeksforgeeks.org/batch/linux-shell-script>,1643266612.494000,1643267531.494300,U02T96HEARK\\n1e600d64-5cb3-4ec8-acee-f976fe5e829a,U02T96HEARK,,,Have you enabled all other apis?,1643265061.489000,1643267596.494700,U01AXE0P5M3\\n506c938c-4f50-44ec-adf3-ed59f4a672e0,U02VA225CG4,,,This is a free course and it also teaches you the basics required when using command line,1643266612.494000,1643267616.494900,U02T96HEARK\\n6fa9545c-71bd-40b1-ace6-29e11951a6e3,U02UGA597HS,,,We still haven\\'t prepared it :see_no_evil: you\\'ll get it soon,1643239580.456200,1643267662.495100,U01AXE0P5M3\\n4f49c1b2-223f-4ee6-93f2-68ee7c779838,U02VA225CG4,,,<https://www.katacoda.com/courses/operating-systems>,1643266612.494000,1643268731.495500,U0290EYCA7Q\\nfc23b0f1-b786-438b-a3dc-b459eaff1167,U02VA225CG4,,,This should cover mostly all commands.,1643266612.494000,1643268960.495800,U0290EYCA7Q\\ne4ba83ec-756e-4d5e-af11-3f09dae646d7,U02T96HEARK,,,what helped for me was - to enable billing and afterwards `terraform delete` and all commands again!,1643265061.489000,1643268974.496200,U02V24WAZRN\\n72ae3172-c25e-4fa0-ae74-4bd5574ccf83,U02U5SW982W,,,Thanks Alexey - I can but try.,1643256979.476800,1643269784.496400,U02U5SW982W\\ne9de7900-266d-4f22-a6ae-957d86c0ce59,U01AXE0P5M3,,,\"Just a thought, can we keep two separate channels for the course discussion.\\none for doubts/errors/course-related issues and the other for general stuff about the course.\",1642488983.400100,1642494209.410200,U0254S545D5\\naad4eff7-8b3d-4857-a922-b876bc2c874e,U02RTJPV6TZ,,,\"trying to run postgress, i dont see the repository they are complaining about\",1642488820.397100,1642495587.410800,U02RTJPV6TZ\\n00840563-bf29-4f7e-82ea-dedd8c5d49f7,U02RTJPV6TZ,,,\"Dell@JW MINGW64 /e/DOCKER/week_1_basics/2_docker_sql\\n$ winpty docker run -it \\\\\\n&gt;         -e POSTGRESS_USER=\"\"root\"\" \\\\\\n&gt;         -e POSTGRESS_PASSWORD=\"\"root\"\" \\\\\\n&gt;         -e POSTGRESS_DB=\"\"ny_taxi\"\" \\\\\\n&gt;         -v e:/DOCKER/week_1_basics/2_docker_sql/ny_taxi_postgres_data: /var/lib/postgresql/data \\\\\\n&gt;         -p 5432:5432 \\\\\\n&gt;     postgress:13\\ndocker: invalid reference format: repository name must be lowercase.\\nSee \\'docker run --help\\'.\",1642488820.397100,1642495592.411000,U02RTJPV6TZ\\n0460c4a8-b9f9-41a3-a55a-ca7a7e6b6b86,U02RTJPV6TZ,,,Have you tried googling this error?,1642488820.397100,1642495782.411300,U01AXE0P5M3\\nb68f47ef-66fd-4d45-8c0b-458c72d7fdc3,,18.0,,\"```Unable to find image \\'root:latest\\' locally```\\nError while running Postgres on Docker. Looks like it is searching for  `root` instead of `postgres:13:`\",1642496193.414600,1642496193.414600,U02T96HEARK\\nbefa3440-4277-431f-b488-3780aae9f360,U02T96HEARK,,,\"```docker run -it -e POSTGRES_USER= \"\"root\"\" -e POSTGRES_PASSWORD= \"\"root\"\" -e POSTGRES_DB= \"\"ny_taxi\"\"  -v C:/Users/SOUVIK PAN/Desktop/data-engineering-work-zoomcamp/Week 1/docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data -p 5432:5432 postgres:13```\\nI ran the above code in the directory where I earlier created my docker file and pipeline.py file.\",1642496193.414600,1642496284.414700,U02T96HEARK\\n83f2dde0-e2bf-4594-ad53-2d50612b301b,U02T96HEARK,,,Try removing spaces when setting env variables,1642496193.414600,1642496322.414900,U01AXE0P5M3\\n9282008b-bc86-43c6-bc6a-6e449e7ac3be,U02T96HEARK,,,\"Also you have a space in your path, maybe putting the v parameter in quotes is a good idea\",1642496193.414600,1642496385.415100,U01AXE0P5M3\\n044ee399-2768-447b-94f5-a118d9ffb217,U02T96HEARK,,,I\\'d actually suggest creating a directory in C without spaces in the name and use it. This space will give you a lot of troubles,1642496193.414600,1642496437.415300,U01AXE0P5M3\\n25327cb8-ce6f-4475-aa65-ad4b22d56e08,U02T96HEARK,,,\"```docker: invalid reference format: repository name must be lowercase.\\nSee \\'docker run --help\\'.```\\nYeah, you\\'re right. I think it is because of these spaces I am getting this error now.\",1642496193.414600,1642496704.415700,U02T96HEARK\\n801aa7a0-40e9-49e6-b699-1b1e1f9e48fa,U02T96HEARK,,,\"The space in the path is actually in the name of my device. :joy: I think I need to change that , it has caused trouble for me at many places earlier as well.\",1642496193.414600,1642496862.416000,U02T96HEARK\\nb5754a05-90b1-48c9-b373-7a66caf2626a,U02RTJPV6TZ,,,\"following same command steps you used on post gress , mine are failing\",1642488820.397100,1642497289.416800,U02RTJPV6TZ\\nbf281a86-f5dd-4770-8a63-bdfca969d15e,U02RTJPV6TZ,,,\"Dell@JW MINGW64 /e/DOCKER/week_1_basics/2_docker_sql\\n$ winpty docker run -it         -e POSTGRESS_USER=\"\"root\"\"         -e POSTGRESS_PASSWORD=\"\"root\"\"         -e POSTGRESS_DB=\"\"ny_taxi\"\"         -v e:/DOCKER/week_1_basics/2_docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data         -p 5432:5432     postgress:13\\nUnable to find image \\'postgress:13\\' locally\\ndocker: Error response from daemon: pull access denied for postgress, repository does not exist or may require \\'docker login\\': denied: requested access to the resource is d\\nenied.\\nSee \\'docker run --help\\'.\",1642488820.397100,1642497293.417000,U02RTJPV6TZ\\nf545324e-65b0-4cd2-adc5-a97b030af08a,U02T96HEARK,,,Maybe you could try to escape the space with backslash: `C:/Users/SOUVIK\\\\ PAN/Desktop...`,1642496193.414600,1642497332.417300,U02U546D36Z\\nba2b3723-8c4f-41a4-b6bc-2f37c3f4c549,U02T96HEARK,,,I tried using backslash but that is giving error,1642496193.414600,1642497986.417800,U02T96HEARK\\n11377af8-8c5a-4851-b3cc-ff77bc0c8f7a,U02T96HEARK,,,\"```docker: Error response from daemon: OCI runtime create failed: container_linux.go:380: starting container process caused: process_linux.go:545: container init caused: rootfs_linux.go:76: mounting \"\"/run/desktop/mnt/host/c/Users/SOUVIK PAN/Desktop/data-engineering-work-zoomcamp/Week 1/docker_sql/ny_taxi_postgres_data\"\" to rootfs at \"\"/var/lib/postgresql/data\"\" caused: mount through procfd: not a directory: unknown: Are you trying to mount a directory onto a file (or vice-versa)? Check if the specified host path exists and is the expected type.```\\nIs this error due to the space in my path or due to the path type being a windows type path instead of a linux type path?\",1642496193.414600,1642498096.418000,U02T96HEARK\\n575fa888-bcc8-4c19-8563-b7397c81047c,,2.0,,\"Because of this course, I started to learn and use wsl 2. Am I doing the right thing by using the installation methods for Ubuntu (the distro that I am using) then? For example, Google Cloud\\'s SDK\",1642498657.419800,1642498657.419800,U02T941CTFY\\ne1bf11ec-0557-49e9-90d4-596246b574c9,U02T96HEARK,,,Can you show the full command?,1642496193.414600,1642499346.421900,U01AXE0P5M3\\n2a00de7a-d2d9-481d-9b04-722577cad496,U02T941CTFY,,,\"Then Python, Google cloud sdk and terraform\",1642498657.419800,1642499391.422200,U01AXE0P5M3\\nfc89d429-89c4-4698-850f-fb9cdfccf41b,U02T941CTFY,,,Ubuntu is great!,1642498657.419800,1642499402.422400,U01AXE0P5M3\\neca892f4-3e5a-4271-a622-7a6213683724,U01AXE0P5M3,,,MacOS,1641913480.265900,1642499920.423500,U021RS6DVUZ\\n20717659-4028-44e9-9d66-5712677b492d,U02T96HEARK,,,\"```\\ndocker run -it -e POSTGRES_USER=\"\"root\"\" -e POSTGRES_PASSWORD=\"\"root\"\" -e POSTGRES_DB=\"\"ny_taxi\"\"  -v \"\"C:/Users/SOUVIK PAN/Desktop/data-engineering-work-zoomcamp/Week 1/docker_sql/ny_taxi_postgres_data\"\":\"\"/var/lib/postgresql/data\"\" -p 5432:5432 postgres:13```\\nThis is the command I used.\",1642496193.414600,1642500051.423700,U02T96HEARK\\n18a8db11-be0c-4869-a3a2-4281b4fa66ec,,5.0,,I have a silly question but it is annoying me. In the video dezoomcamp 01 01  at <https://youtu.be/EYNwNlOrpr0?t=1336|22:16> you use a shortcut to jump from f{day}\\' to the opening quote and add a f for the string interpolation and I cannot figure how that work on vsc linux.,1642500119.424300,1642500119.424300,U02TC8X43BN\\nc6abc81d-900e-4324-9a68-0007ddda355a,U02TC8X43BN,,,\"Not a silly question at all :slightly_smiling_face: That is a \"\"shorcut\"\" to add variables inside strings in Python.\\nImagine you have the variable\\ncourse = \\'data- engineer\\'\\n\\nThen if you do :\\nprint(f\\'I am doing a f{course} bootcamp\\')\\n\\nAnd the result is\\n\\'I am doing a data- engineer bootcamp\\'\",1642500119.424300,1642500665.424700,U02CD7E30T0\\n83a94887-ce98-4a2b-9bfb-f1c21f1fd6c6,U02TC8X43BN,,,\"No but I get what string interpolation is for, what I don\\'t get is which extension and which shortcut does he use so it\\n1: autocomplete the curly bracket,\\n2 let him jump to the opening quote after he inserted the variable to add the \"\"f\"\" to make the string interpolation work\",1642500119.424300,1642501281.427300,U02TC8X43BN\\nd189ce57-d74d-46a8-9dc3-ee101188266e,U02TC8X43BN,,,\"I’m unsure of how Alexey does it in the video, but you can find a keyboard shortcut reference here: <https://code.visualstudio.com/docs/getstarted/keybindings>\\n\\nYou can also add extensions to add Vim keybindings and the like\",1642500119.424300,1642501428.427600,U02BVP1QTQF\\nc28524d7-a6df-42d2-934c-6ad6ef4c142a,U02T96HEARK,,,\"Not saying it is the right way but have you tried : (cause for me on linux I had to remove the quote for the variable)\\n```docker run -it -e POSTGRES_USER=root -e POSTGRES_PASSWORD=root -e POSTGRES_DB=ny_taxi -v \"\"C:/Users/SOUVIK PAN/Desktop/data-engineering-work-zoomcamp/Week 1/docker_sql/ny_taxi_postgres_data\"\":\"\"/var/lib/postgresql/data\"\" -p 5432:5432 postgres:13```\",1642496193.414600,1642502860.429600,U02TC8X43BN\\nea0d63de-30b2-451d-815c-3223efb338fc,U02TC8X43BN,,,\"Ok so it is potentially a vim or sublime key binding.  I will try to figure it out later then, I was just wondering if it was a pre existing shortcut that everyone is using usually and that I was not aware of (not being that familiar with vsc).\",1642500119.424300,1642503113.429800,U02TC8X43BN\\ne1371120-c2ae-47cd-afc0-5784a89c77d1,U02TNEJLC84,,,\"I wonder if they check the name on the credit card. Is it really easy to set up a new free tiered account with a new email account? I have many, but assume one would have to use a brand new one...\",1642457024.345400,1642503601.430100,U02GVGA5F9Q\\nc3ba1c04-ab25-4b88-bcb1-2c3c1be8e466,,2.0,,\"Hi All,  Can anybody recommend me a book which could cover the system design aspect related to data engineering like the ones which we have in software development ? TIA\",1642504063.432400,1642504063.432400,U02ULC8DN5P\\n7ce3e1b5-cfd7-43da-b0ca-82453b3ac84b,U02T96HEARK,,,\"Put the quotes around the entire argument for -v. I.e. remove the quotes around  :\\n\\nAnd please make sure you don\\'t have spaces in your paths\",1642496193.414600,1642504160.432500,U01AXE0P5M3\\n39057143-db85-4de4-9d3d-972457a76e17,U02T96HEARK,,,Moving this to another directory should help,1642496193.414600,1642504181.432700,U01AXE0P5M3\\n8569e390-a149-49a5-801b-a648de3e9f48,U02TC8X43BN,,,\"You have different keybindings for Windows and Linux VS Code setups, or at least used to have...\",1642500119.424300,1642505146.433400,U02GVGA5F9Q\\n51bb256c-b8e4-47cf-960e-e64f096d8f3f,U02ULC8DN5P,,,\"I have not come across a book, but i did this course a while ago and covers some data intensive system design\\n<https://www.educative.io/courses/grokking-the-system-design-interview>\",1642504063.432400,1642505800.434200,U01DFQ82AK1\\nd082a211-0697-4e02-8bf2-fccea285d57d,,2.0,,It seems I\\'m late for the start of the course. Which message here should I start at here?,1642506201.436000,1642506201.436000,U02TBFKLTTN\\nd04d1f9b-8971-4491-8b22-85094909826d,U02TBFKLTTN,,,Check the topic for the course repo and check the stickies,1642506201.436000,1642506233.436100,U02BVP1QTQF\\nbe362756-4e2c-47db-adaa-88f3749c6ebe,U02ULC8DN5P,,,<@U02ULC8DN5P> 1) <https://www.amazon.com/System-Design-Interview-Mastering-Introduction-ebook/dp/B09DQ5RY26|https://www.amazon.com/System-Design-Interview-Mastering-Introduction-ebook/dp/B09DQ5RY26> 2) <https://www.amazon.com/System-Design-Interview-Fundamentals-Rylan/dp/B09BF9K98C/ref=mp_s_a_1_1?crid=1CKUQZJCOV0G4&amp;keywords=Rylan+liu&amp;qid=1642506448&amp;sprefix=rylan+liu%2Caps%2C262&amp;sr=8-1|https://www.amazon.com/System-Design-Interview-Fundamentals-Rylan/dp/B09BF9K98C/ref=mp_s_a_1_1?crid=1CKUQZJCOV0G4&amp;keywords=Rylan+liu&amp;qid=1642506448&amp;sprefix=rylan+liu%2Caps%2C262&amp;sr=8-1>,1642504063.432400,1642506488.436500,U02S848271C\\n2c6840c3-7238-4b4f-a653-a3401ac21a98,U02TBFKLTTN,,,\"Hey Sergey, in the repo you\\'ll find the material and the link to the youtube channel -&gt; <https://github.com/DataTalksClub/data-engineering-zoomcamp>\",1642506201.436000,1642506514.436800,U01B6TH1LRL\\nfc03cc9a-1e88-4a99-98dc-b72842696df2,U02RTJPV6TZ,,,you have a typo,1642488820.397100,1642506541.437100,U01AXE0P5M3\\ncf331333-62aa-420c-b003-6cd34979a153,U02TVGE99QU,,,I am using python 3.9 (anaconda),1642467396.368700,1642507752.438600,U02TVGE99QU\\naed04e2a-32ad-4d34-9ac5-d9aac0c92e82,U02TVGE99QU,,,\"According to your screenshot, it\\'s python 3.10\",1642467396.368700,1642507786.438800,U01AXE0P5M3\\n9fa836a3-e31d-48c6-aa53-bc68772c12e5,,33.0,,\"Hi <@U01AXE0P5M3>\\n\\nThe environment setup doesn\\'t work on MacOs, it keep return -v not found, any other thing that need to be done?\",1642507885.440700,1642507885.440700,U02TBKWL7DJ\\ncef4ff56-3bb5-4313-a129-880de37df4c7,U02TBKWL7DJ,,,\"I’m on MacOS and it’s running just fine on my end. I use fish rather than zsh though, but the only difference is that I don’t use the $ sign in front of (pwd).  You do seem to have an extra space right after -v\",1642507885.440700,1642508101.442900,U02BVP1QTQF\\nc7cedc99-a000-42ba-a3a5-efff8a57a7cf,U02TVGE99QU,,,That\\'s Jason\\'s screenshot. Mine is below,1642467396.368700,1642508116.443100,U02TVGE99QU\\nfc97e1e0-a476-4131-89df-83d5e6ec9ad4,U02TVGE99QU,,,\"When I try to connect using pgcli, I get this:\\n```Password for root: Version: 1.9.0\\nChat: <https://gitter.im/dbcli/pgcli>\\nMail: <https://groups.google.com/forum/#!forum/pgcli>\\nHome: <http://pgcli.com>\\nroot@localhost:ny_taxi&gt; Exception in thread completion_refresh:\\nTraceback (most recent call last):\\n  File \"\"/usr/lib/python3.9/threading.py\"\", line 954, in _bootstrap_inner\\n    self.run()\\n  File \"\"/usr/lib/python3.9/threading.py\"\", line 892, in run\\n    self._target(*self._args, **self._kwargs)\\n  File \"\"/home/dan/.local/lib/python3.9/site-packages/pgcli/completion_refresher.py\"\", line 68, in _bg_refresh\\n    refresher(completer, executor)\\n  File \"\"/home/dan/.local/lib/python3.9/site-packages/pgcli/completion_refresher.py\"\", line 110, in refresh_tables\\n    completer.extend_columns(executor.table_columns(), kind=\\'tables\\')\\n  File \"\"/home/dan/.local/lib/python3.9/site-packages/pgcli/pgcompleter.py\"\", line 204, in extend_columns\\n    for schema, relname, colname, datatype, has_default, default in column_data:\\n  File \"\"/home/dan/.local/lib/python3.9/site-packages/pgcli/pgexecute.py\"\", line 483, in table_columns\\n    for row in self._columns(kinds=[\\'r\\']):\\n  File \"\"/home/dan/.local/lib/python3.9/site-packages/pgcli/pgexecute.py\"\", line 478, in _columns\\n    cur.execute(sql)\\npsycopg2.errors.UndefinedColumn: column def.adsrc does not exist\\nLINE 7:                         def.adsrc as default```\\n``````\",1642467396.368700,1642508208.443600,U02TVGE99QU\\nf7d728be-6a7d-4e2c-ab6f-b669050a7bba,U02TVGE99QU,,,\"it then opens the pg command line but when I try to \\\\d the yellow_taxi_data db I get the following\\n```root@localhost:ny_taxi&gt; \\\\d yellow_taxi_data\\ncolumn c.relhasoids does not exist\\nLINE 2: ...                 c.relhasrules, c.relhastriggers, c.relhasoi...\\n                                                             ^\\n\\nTime: 0.004s```\",1642467396.368700,1642508276.443800,U02TVGE99QU\\n35d4ae39-5cef-4d9e-b4e3-0fab5e4bf5ef,U02TVGE99QU,,,\"which is apparently common when you install pgcli with apt, rather than pip (but I installed with pip)\",1642467396.368700,1642508321.444000,U02TVGE99QU\\n41f6803f-2e7a-4cc3-a2b2-1383426aa96e,U02TBKWL7DJ,,,\"<@U02BVP1QTQF>\\n\\nI\\'ve removed the extra space and I still get same error message of -v not found\",1642507885.440700,1642508587.444500,U02TBKWL7DJ\\n30cba24f-e5bd-4492-b5b9-60d7fc45940b,U02TBKWL7DJ,,,so you have just (pwd) ?,1642507885.440700,1642508662.444800,U02TBKWL7DJ\\nba3c30ec-5518-4071-9e63-600a854507ba,U02TBKWL7DJ,,,Did you create the ny_taxi_postgres_data folder?,1642507885.440700,1642508685.445100,U02BVP1QTQF\\nb2210ebc-f510-4041-952f-5cda93e3c87d,U02TBKWL7DJ,,,the instruction asumes that you have this subfolder in the directory you’re running the docker command from,1642507885.440700,1642508727.445300,U02BVP1QTQF\\n167308fc-ac48-4a42-81c9-a733f06dda9e,U02TBKWL7DJ,,,\"Yes, I have the ny_taxi_postgres_data folder in my directory\",1642507885.440700,1642508772.445500,U02TBKWL7DJ\\nb44728ba-b4d9-4f2a-ab76-4b6da4373e18,U02TBKWL7DJ,,,\"try running the command without any linebreaks and see if it helps. Remove the \\\\ signs and put everything in a single line. If that doesn’t work, then try removing the environment vars and check if Docker keeps complaining about the -v part\",1642507885.440700,1642508919.445700,U02BVP1QTQF\\n9b4020a7-f2aa-4619-90b0-cccde60053c0,U02TBKWL7DJ,,,\"Okay\\n\\nLet me try that.\\n\\nThanks <@U02BVP1QTQF>\",1642507885.440700,1642508950.446300,U02TBKWL7DJ\\n12b62b21-ba67-47ba-a45a-5a3c110c9c60,U02TBKWL7DJ,,,\"no worries, although I’m honestly not sure what the error is because the command looks okay to me\",1642507885.440700,1642508975.446700,U02BVP1QTQF\\n13a506f2-c456-4cb2-9dd8-7510ed2ac658,,3.0,,Is there any advantage to installing Docker as a VS Code extension?,1642508979.446900,1642508979.446900,U02UBQJBYHZ\\n8F481B6F-1E71-4E2D-88EA-5D3519C983B2,U02UBQJBYHZ,,,\"This provides features that will make your Docker container development experience easier and more efficient when setting up running, debugging, your containerized apps in Visual Studio Code. I feel it Just  makes it little easier ! \",1642508979.446900,1642509719.449000,U02AGF1S0TY\\n65c99c12-4cac-4e8b-9822-1d01ff6e7d9e,U02TBKWL7DJ,,,\"I\\'ve been able to fix it <@U02BVP1QTQF>\\n\\nAnother the challenge with mouting wasn\\'t working because I had whitespace character in my directory name, so I had to rename the directory without space and it worked.\",1642507885.440700,1642509955.449300,U02TBKWL7DJ\\nae5cc47b-1533-4e07-9b92-787aefa07653,U02TBKWL7DJ,,,\"Thanks for helping out.\\n\\nAnd I did as you suggested by removing the line break and putting everything in one line\",1642507885.440700,1642509977.449500,U02TBKWL7DJ\\nf8fbf960-37ad-41f2-9822-aef6f526bec7,U02UBQJBYHZ,,,\"Thanks! I\\'ve set it up this way before, but on a different computer.\",1642508979.446900,1642510013.449700,U02UBQJBYHZ\\n06051da8-8d0e-49a3-84f2-c4c3d0b2df74,U02TBKWL7DJ,,,\"right, I totally forgot that could be an issue. Whitespaces in folder and file names are almost always a bad idea, so getting rid of them will save you lots of headaches in the future :+1:\",1642507885.440700,1642510038.449900,U02BVP1QTQF\\nefc97d17-0358-49ad-b02e-04e349777df9,U02TBKWL7DJ,,,\"Thanks <@U02BVP1QTQF> . I will take note of that.\\n\\nHow were you able to access pgcli?  I\\'m kinda getting some error accessing  it after pip installing it\",1642507885.440700,1642510332.450300,U02TBKWL7DJ\\naf5175fc-a41a-49fe-a500-fa53205c1737,U02TBKWL7DJ,,,I tested in on linux and I also get an error for pgcli. but it actually works. is it the same for you?,1642507885.440700,1642510469.450600,U01AXE0P5M3\\n11e1a462-1f59-4cc0-901a-f5854c3ef199,U02TBKWL7DJ,,,\"I installed pgcli with brew rather than pip. If you used pip, make sure that it was installed to the Python environment you’re currently using. If you are not using a Python environment manager, I strongly suggest you look into Conda\",1642507885.440700,1642510483.450900,U02BVP1QTQF\\ncaaea240-eb51-4635-928f-ed22c51c3f85,U02TBKWL7DJ,,,\"<@U01AXE0P5M3>\\nThis is the error I\\'m getting when I try accessing the postregres.\\n\\n<@U02BVP1QTQF>\\n\\nI use pip following the tutorial\",1642507885.440700,1642510603.451300,U02TBKWL7DJ\\nb7c44fb3-a513-4e7e-b2ee-958c1fec79e5,U02TBKWL7DJ,,,\"Sorry, I honestly don’t know what could be the issue. Try installing it with brew maybe?\",1642507885.440700,1642510693.451800,U02BVP1QTQF\\ne24ae546-551f-4b1a-bf98-ead392480498,U02TVGE99QU,,,I managed to fix it by uninstalling and reinstalling pgcli. no clue why that worked,1642467396.368700,1642510766.452000,U02TVGE99QU\\n795f1723-a92c-450f-8286-3f04890c0a87,U02TBKWL7DJ,,,\"for what it’s worth, I’m currently on video 01 02, I’m using Conda and when I did `conda install sqlaclhemy` I actually had a dependency problem and had to run `conda install psycopg2` to solve it. I don’t think that’s the issue here but I see the library mentioned in your error.\\n\\nAre you using Python 3.9?\",1642507885.440700,1642510825.452300,U02BVP1QTQF\\n06a32343-6eaa-4a7c-b5de-f829b51b100e,,19.0,,\"If you have problems installing pgcli on linux or macos, try this:\\n\\n```conda install -c conda-forge pgcli\\npip install -U mycli```\\nIf `pip insatll pgcli` doesn\\'t work for you\",1642511128.453300,1642511128.453300,U01AXE0P5M3\\nf9dc4d20-34e8-4a7f-b5e0-924fb4f73af9,U01AXE0P5M3,,,That assumes you use conda/anaconda,1642511128.453300,1642511143.453400,U01AXE0P5M3\\nb8b21589-a9ae-42dd-9320-f9c55cef7320,,7.0,,Is there anyone having issues with Debit card (Europe) when registering GCP?,1642511203.454300,1642511203.454300,U02U88WH7D0\\n57ed1e5b-123e-4177-b359-ef67c090d06e,U02TBKWL7DJ,,,\"<@U02TBKWL7DJ>\\n\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642511128453300>\\n\\ntry this\",1642507885.440700,1642511289.454500,U01AXE0P5M3\\n77c3b286-c9e3-47c8-95d4-b8e791c85bbb,U02TVGE99QU,,,\"great, thanks! sorry for the confusion with the screenshot\",1642467396.368700,1642511329.454800,U01AXE0P5M3\\n062d95d6-d951-4a31-89a9-98095cb6a5e5,U01AXE0P5M3,,,\"Personally I had issue with psycopg2 and I had to follow a stackoverflow <https://stackoverflow.com/questions/65821330/error-failed-building-wheel-for-psycopg2-ubuntu-20-04-python-3-8-5-venv|solution> with:\\n```sudo apt-get install libpq-dev \\npip install psycopg2```\",1642511128.453300,1642511362.455000,U02TC8X43BN\\n9d1c74f2-9ecb-4879-9345-96cf0440acfa,U02TBKWL7DJ,,,\"Alright <@U01AXE0P5M3>\\n\\nI will do that and revert with the update\\n\\nThanks for prompt response and <@U02BVP1QTQF> for been very helpful\",1642507885.440700,1642511825.455600,U02TBKWL7DJ\\ndf577024-2cf9-453f-a80d-1d1ef56ed069,U02TBKWL7DJ,,,No worries. I hope you’re able to solve your issues. Enjoy the course and feel free to ask anytime.,1642507885.440700,1642511903.456000,U02BVP1QTQF\\naf9f090c-da9a-48d7-bb85-34dd8b251e09,U02U88WH7D0,,,I am from EU and with debit card. no issues at all,1642511203.454300,1642512397.456200,U02TCMEDTUL\\nf1a50a8d-7461-47b9-b0c1-ada57c7d985f,U02U88WH7D0,,,I imagined that something doggy is going on with my google account. I will try to get in contact with them. Thanks!,1642511203.454300,1642513547.458200,U02U88WH7D0\\n10A1D65B-6751-4B22-B848-95176578350B,,1.0,,<@U01AXE0P5M3> Any particular reason why GCS was chosen as the cloud service provider?,1642514520.460800,1642514520.460800,U02QK4ZV4UX\\n52b48510-c1ef-4e3e-af40-16450db0c998,U0205L73QNS,,,\"I am in CT, would anyone like to create our own group? Or can i join yours?\",1642447403.330800,1642514648.461100,U02STS0J2KS\\n051c6712-7fe1-4e16-bae4-d217879846da,U0205L73QNS,,,\"Hello everyone! Add me to the group, you\\'re interested. Currently working as a QA engineer.\",1642447403.330800,1642515313.461700,U02T2T1CZBN\\n9643fedb-211e-4f81-96d3-8bf6f1c69eb9,U02QK4ZV4UX,,,\"Yes\\n\\n• better integration with DBT\\n• $300 in free credits \",1642514520.460800,1642515869.462100,U01AXE0P5M3\\n2d9fb4cd-a5f2-4e56-a465-c337f784d896,U02U88WH7D0,,,\"Revolut virtual card - not working\\nRevolut real card - not working\\nPayPal - not working\\nyesterday evening  was unsuccessful:grimacing:\",1642511203.454300,1642515896.462300,U02UCHLH5RQ\\n0ca8e6b0-077c-47a5-844f-d3540d196763,U02UBQJBYHZ,,,nice one!. Didn\\'t know there was a docker extension for VS. Just installed it. Anything that makes me more efficient I love.,1642508979.446900,1642516460.462900,U02TMP4GJEM\\n36cf2793-a6ce-4190-8954-b4e729a7d222,,7.0,,\"<https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/homework.md>\\n\\nHomework for week 1. We\\'ll create the form for submitting the answers a bit later\",1642516682.463800,1642516682.463800,U01AXE0P5M3\\n216a5ff8-68f9-41d9-9dd7-ba558c683c3c,,1.0,,\"<https://medium.com/airbnb-engineering/data-infrastructure-at-airbnb-8adfb34f169c|https://medium.com/airbnb-engineering/data-infrastructure-at-airbnb-8adfb34f169c>\\n\\nFound this interesting, so I thought of sharing:sunglasses:\",1642517261.465300,1642517261.465300,U01MFQW46BE\\nbf8b5337-0c38-422e-a378-09ae2ec3f037,U01F6E6P45Q,,,Thanks!!,1642492381.406800,1642518066.466200,U01F6E6P45Q\\n0478bf9b-5566-4433-a015-36dd7832da4e,,8.0,,\"I keep getting the error message:\\n`connection to server at \"\"localhost\"\" (::1), port 5432 failed: FATAL: password authentication failed for user \"\"root\"\"` despite inputting the correct password `root`.\",1642518716.469700,1642518716.469700,U02SBTRTFRA\\n65def06d-ef86-4c6d-a038-b0dd56059bad,U02SBTRTFRA,,,\"`docker run -it \\\\\\n-e POSTGRES_USER=\"\"root\"\" \\\\\\n-e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n-e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n-v ... ny_taxi_postgres_data:/var/lib/postgres/data \\\\\\n-p 5432:5432 \\\\\\npostgres:13`\\n\\nThis eventually returns:\\n`... LOG: database system is ready to accept connections`, but the password authentication fails despite using the password: `root`\",1642518716.469700,1642519398.470000,U02SBTRTFRA\\n5198e730-97ec-4183-850a-d116e3c8114d,,22.0,,\"what am i doing wrong ? i keep getting this error\\n$ docker run -it -e POSTGRES_USER=\"\"root\"\" -e POSTGRES_PASSWORD=\"\"root\"\" -e POSTGRES_DB=\"\"ny_taxi\"\" -v c:/users/abhis/git/data-engineering-zoomcamp/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data -p 5432:5432 pos\\ntgres:13\\ndocker: Error response from daemon: invalid mode: /var/lib/postgresql/data.\\nSee \\'docker run --help\\'.\",1642520122.473300,1642520122.473300,U02RREQ7MHU\\n938d9833-c8a3-40e8-964a-c1be27ac5c02,U02RTJPV6TZ,,,\"hey much appreciated <@U01AXE0P5M3>, thanx for giving me a hand....lemme be more vigilant on \"\"TYPOs\"\"\",1642488820.397100,1642520618.475500,U02RTJPV6TZ\\n9fed33c5-9e5f-433d-bb83-653f9430b178,U02SBTRTFRA,,,Did you try with another passwords to check if the error is there or at the env variables you are assigning?,1642518716.469700,1642520859.475700,U02UR6MFW1F\\nce747536-f195-4d46-afa3-cf711b95111a,U02RTJPV6TZ,,,\"typos cleared....stuck again\\n\\n$ docker run -it  -e POSTGRES_USER=\"\"root\"\"  -e POSTGRES_PASSWORD=\"\"root\"\"  -e POSTGRES_DB=\"\"ny_taxi\"\" -v e:/zoomcamp/data_engineer/week_1_fundamentals/2_docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data  -p 5432:5432  postgres:13\\ndocker: Error response from daemon: invalid mode: \\\\Program Files\\\\Git\\\\var\\\\lib\\\\postgresql\\\\data.\\nSee \\'docker run --help\\'.\",1642488820.397100,1642521206.475900,U02RTJPV6TZ\\n423ae86b-97b7-49b3-aa36-d9737aec5337,U02U88WH7D0,,,\"Kazakhstan both VISA and MASTERCARD are rejected :(\\nUPD: .edu account gets rejected, for normal gmail everything is fine\",1642511203.454300,1642521893.476100,U02UECC4H6U\\nc424cc76-49ce-4a75-88ea-f0d12c4c128a,U02RREQ7MHU,,,\"You\\'re on Windows, I assume. You need to write the local path with backslash separator (\\'\\\\\\').\\n`c:\\\\users\\\\abhis\\\\git\\\\data-engineering-zoomcamp\\\\week_1_basics_n_setup\\\\2_docker_sql\\\\ny_taxi_postgres_data:/var/lib/postgresql/data`\\nThe -v argument represents `local_path:container_path`\",1642520122.473300,1642522099.476400,U02QH3TBA11\\n7c014696-c084-4fe9-bb35-f4052ef5e6b1,U02T96HEARK,,,\"Long story short, I messed up my windows while trying to resolve this issue. Then instead of relaunching windows OS on my system I thought of trying out Linux and I did all the installations and now things are working finally. The database system is ready to accept connections.\",1642496193.414600,1642522119.476600,U02T96HEARK\\n36f3a6e6-2566-429a-8a43-38990bdba007,U02T96HEARK,,,<@U02TC8X43BN> removing quotes wasn\\'t required on Linux for me.,1642496193.414600,1642522205.476900,U02T96HEARK\\n77b269d1-3815-46cf-8d06-52ee2edaa7b8,U02T96HEARK,,,\"```docker run -it -e POSTGRES_USER=\"\"root\"\" -e POSTGRES_PASSWORD=\"\"root\"\" -e POSTGRES_DB=\"\"ny_taxi\"\" -v \"\"/home/souvik/Desktop/data-engineering-work-zoomcamp/Week_1/docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data\"\" -p 5432:5432 postgres:13```\\nThis is the command I used.\",1642496193.414600,1642522257.477200,U02T96HEARK\\n0c00c5c2-1869-44d3-a537-c2c21c5772e4,U02SBTRTFRA,,,\"I had the same issue, and I tried to change the name of the user in POSTGRES_USER field and it works.\",1642518716.469700,1642522694.477400,U02QYKBJWN6\\n85366cd6-a4b8-4d39-b9cd-53b26fa273d2,U02RREQ7MHU,,,\"did that and i am still getting the same error\\n`$ docker run -it -e POSTGRES_USER=\"\"root\"\" -e POSTGRES_PASSWORD=\"\"root\"\" -e POSTGRES_DB=\"\"ny_taxi\"\" -v c:\\\\users\\\\abhis\\\\git\\\\data-engineering-zoomcamp\\\\week_1_basics_n_setup\\\\2_docker_sql\\\\ny_taxi_postgres_data:/var/lib/postgresql/data -p 5432:5432 postgres:13`\\n`docker: Error response from daemon: invalid mode: /var/lib/postgresql/data.`\\n`See \\'docker run --help\\'.`\",1642520122.473300,1642522696.477600,U02RREQ7MHU\\n1287192F-31F2-4FBB-851F-CE70B6CAFAD4,U01AXE0P5M3,,,<@U01AXE0P5M3> please when can we expect to see the remaining videos for week 1 (docker compose &amp; terraform)?? ,1642511128.453300,1642523247.479700,U02TBCXNZ60\\n377ef178-6589-4f6c-a9da-fdd33ff36a8d,U02RREQ7MHU,,,have you tried with WSL?,1642520122.473300,1642523247.479900,U02U2Q5P61Z\\n44a76064-a807-4891-b132-1d431add3741,U02RREQ7MHU,,,\"Dell@JW MINGW64 /e/zoomcamp/data_engineer/week_1_fundamentals/2_docker_sql\\n$ docker run -it  -e POSTGRES_USER=\"\"root\"\"  -e POSTGRES_PASSWORD=\"\"root\"\"  -e POSTGRES_DB=\"\"ny_taxi\"\" -v E:/zoomcamp/data_engineer/week_1_fundamentals/2_docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data  -p 5432:5432  postgres:13\\ndocker: Error response from daemon: invalid mode: \\\\Program Files\\\\Git\\\\var\\\\lib\\\\postgresql\\\\data.\\nSee \\'docker run --help\\'.\",1642520122.473300,1642523380.480200,U02RTJPV6TZ\\n16f680d3-65ef-4088-aa6a-ced797df3d3b,U02RREQ7MHU,,,same error,1642520122.473300,1642523385.480400,U02RTJPV6TZ\\n58f06adb-213a-4c1f-9f0f-78357a1cd1b2,U02RREQ7MHU,,,Where are you running the command? Is it git bash or WSL?,1642520122.473300,1642523437.480600,U02QH3TBA11\\n36437523-bd9e-4ad5-88bc-e3302304d484,U02RREQ7MHU,,,git basg,1642520122.473300,1642523443.480800,U02RREQ7MHU\\n1e9c0d98-561c-4d4b-888d-4a596e71c67a,U02RREQ7MHU,,,bash*,1642520122.473300,1642523449.481000,U02RREQ7MHU\\n442ddf75-30ed-49ba-a7bc-e210811fa950,U02RREQ7MHU,,,\"tried to do it in ubuntu but i got this error\\n`$ sudo docker run -it -e POSTGRES_USER=\"\"root\"\" -e POSTGRES_PASSWORD=\"\"root\"\" POSTGRES_DB=\"\"ny_taxi\"\" -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data -p`\\n `5432:5432 postgres:13`\\n`docker: invalid reference format: repository name must be lowercase.`\\n`See \\'docker run --help\\'.`\",1642520122.473300,1642523561.481800,U02RREQ7MHU\\ndb80b935-34a6-454b-b6ab-55fe9e1851aa,,8.0,,\"Hello all!! I have a weird issue when running pgcli (Windows 10 host machine)\\n```$ pgcli -h localhost -p 5432 -u root -d ny_taxi\\nPassword for root:root```\\nAfter the password prompt, it hangs out indefinitely and I need to kill the terminal to exit/do anything.\",1642523612.482900,1642523612.482900,U02U4G7U3GV\\na7058a0c-1c57-48ca-bba2-2936274322be,U02SBTRTFRA,,,\"I\\'ve now tried doing both; changing the POSTGRES_USER and POSTGRES_PASSWORD, and I\\'m still getting the same error.\",1642518716.469700,1642523659.483100,U02SBTRTFRA\\nf0f2ecf2-235e-4c1b-8921-1b0313c63710,U02U4G7U3GV,,,I searched online but I didn\\'t found anything helpful. I was trying to use another shell like psql but I haven\\'t figured how to use it yet on docker,1642523612.482900,1642523729.484600,U02U4G7U3GV\\n0b701c5c-762e-4f7b-b061-79a3c5d1fb52,U02RREQ7MHU,,,\"all the letter in my directory are all lower case except maybe  \"\"Users\"\" `abhishek@ABHISHEK:/mnt/c/Users/abhis/git/data-engineering-zoomcamp/week_1_basics_n_setup/2_docker_sql`\",1642520122.473300,1642523751.485200,U02RREQ7MHU\\n9bc9412a-eb3e-45c1-a161-6ea8f05ad286,,12.0,,\"```2022-01-18 16:34:48.569 UTC [34] FATAL:  password authentication failed for user \"\"root\"\"\\n2022-01-18 16:34:48.569 UTC [34] DETAIL:  Role \"\"root\"\" does not exist.\\n\\tConnection matched pg_hba.conf line 99: \"\"host all all all md5\"\"```\\nThis is the error I am getting when I use `pgcli -h localhost -p 5432 -u root -d ny_taxi`\",1642523808.486200,1642523808.486200,U02T96HEARK\\n895c032d-00e3-4d43-8af8-e31e760e0b6a,U02RREQ7MHU,,,\"<@U02RREQ7MHU>, the command you ran on ubuntu is missing `-e` before `POSTGRES_DB`\",1642520122.473300,1642523895.486500,U02QH3TBA11\\n8d94b1e1-38c7-4da2-997d-3e0873b44061,U02T96HEARK,,,\"When I enter password for root I get this error on that terminal as `FATAL:  password authentication failed for user \"\"root\"\"`\",1642523808.486200,1642523920.486700,U02T96HEARK\\n77bcd418-3877-436b-95dc-c3cac53d55de,,2.0,,\"Hey, I\\'m trying to install postgres:13 with the commands specified in dezoomcamp 01 02. But I keep getting the following error:\\n\\n\\n```(base) luna@luna-mi-notebook-horizon-edition-14:~/Documents/data-engineering-zoomcamp-own/week-1/2_docker_sql$ sudo docker run -it -e POSTGRES_USER=\"\"root\"\" -e POSTGRES_PASSWORD=\"\"root\"\" -e POSTGRES_DB=\"\"ny_taxi\"\" -v /home/luna/Documents/data-engineering-zoomcamp-own/week-1/2_docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data -p 5432:5432 postgres:13\\nThe files belonging to this database system will be owned by user \"\"postgres\"\".\\nThis user must also own the server process.\\n\\nThe database cluster will be initialized with locale \"\"en_US.utf8\"\".\\nThe default database encoding has accordingly been set to \"\"UTF8\"\".\\nThe default text search configuration will be set to \"\"english\"\".\\n\\nData page checksums are disabled.\\n\\ninitdb: error: directory \"\"/var/lib/postgresql/data\"\" exists but is not empty\\nIf you want to create a new database system, either remove or empty\\nthe directory \"\"/var/lib/postgresql/data\"\" or run initdb\\nwith an argument other than \"\"/var/lib/postgresql/data\"\".```\\nOS: Ubuntu 20.04\",1642523945.487300,1642523945.487300,U02HNGBQ1U7\\n647cea2b-38b5-479e-80f5-e6bc92e82696,U02T96HEARK,,,\"```docker run -it -e POSTGRES_USER=\"\"root\"\" -e POSTGRES_PASSWORD=\"\"root\"\" -e POSTGRES_DB=\"\"ny_taxi\"\" -v \"\"/home/souvik/Desktop/data-engineering-work-zoomcamp/Week_1/docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data\"\" -p 5432:5432 postgres:13```\\nThis is the command I used to run postgres\",1642523808.486200,1642523986.487400,U02T96HEARK\\n0b51b4f3-5a3c-40d7-b34a-c9ee9f7817a0,U02T96HEARK,,,\"hi, I ran into a similar issue, I think because I’ve played around with Postgres before and set my root-user-name to something else. When using that name for “root” in setting up the docker and after -u it worked for me.\",1642523808.486200,1642524028.487600,U02SE2PSSTC\\nc2c0fa8c-6f6d-488d-a836-271fd32ccc96,U02RREQ7MHU,,,\"yep that worked, it downloaded postgres 13 locally and gave me an error at the end tho\\n`fixing permissions on existing directory /var/lib/postgresql/data ... initdb: error: could not change permissions of directory \"\"/var/lib/postgresql/data\"\": Operation not permitted`\",1642520122.473300,1642524078.487800,U02RREQ7MHU\\n032dda57-f6b0-477d-b68b-19726fa4f431,U02RREQ7MHU,,,\"here\\'s the whole code\\n `sudo docker run -it -e POSTGRES_USER=\"\"root\"\" -e POSTGRES_PASSWORD=\"\"root\"\" -e POSTGRES_DB=\"\"ny_taxi\"\" -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data`\\n `-p 5432:5432 postgres:13`\\n`Unable to find image \\'postgres:13\\' locally`\\n`13: Pulling from library/postgres`\\n`a2abf6c4d29d: Pull complete`\\n`e1769f49f910: Pull complete`\\n`33a59cfee47c: Pull complete`\\n`461b2090c345: Pull complete`\\n`8ed8ab6290ac: Pull complete`\\n`495e42c822a0: Pull complete`\\n`18e858c71c58: Pull complete`\\n`594792c80d5f: Pull complete`\\n`8ce2d192c320: Pull complete`\\n`00cfe308d793: Pull complete`\\n`4c4a326c1cb3: Pull complete`\\n`c9f2a6fdb326: Pull complete`\\n`9c1f873a68ce: Pull complete`\\n`Digest: sha256:a63b1bd5dff73a9c1851a0f97e4c593a6b6e2cde6671811b1fa9d10d7e86b658`\\n`Status: Downloaded newer image for postgres:13`\\n`chmod: changing permissions of \\'/var/lib/postgresql/data\\': Operation not permitted`\\n`The files belonging to this database system will be owned by user \"\"postgres\"\".`\\n`This user must also own the server process.`\\n\\n`The database cluster will be initialized with locale \"\"en_US.utf8\"\".`\\n`The default database encoding has accordingly been set to \"\"UTF8\"\".`\\n`The default text search configuration will be set to \"\"english\"\".`\\n\\n`Data page checksums are disabled.`\\n\\n`fixing permissions on existing directory /var/lib/postgresql/data ... initdb: error: could not change permissions of directory \"\"/var/lib/postgresql/data\"\": Operation not permitted`\",1642520122.473300,1642524137.488300,U02RREQ7MHU\\n6ccac964-fd70-46e3-9765-c5bd09cc9b53,U02HNGBQ1U7,,,\"I just ran into this issue. When mounting the volume, the folder on the host side must be empty. If your CSV or notebook files are in it, initialisation will fail\",1642523945.487300,1642524146.488500,U02BVP1QTQF\\n5428bb16-bf7d-415a-9d51-4bf34c73c8e6,U02HNGBQ1U7,,,\"Once you successfully run the container for the first time, it will be filled with the database’s files, but you must not modify them nor add anything to that folder\",1642523945.487300,1642524198.488900,U02BVP1QTQF\\ne8f9da50-ea3e-4387-9732-feba80fda07e,U02T96HEARK,,,<@U02SE2PSSTC> Isn\\'t root-user-name same as POSTGRES_USER?,1642523808.486200,1642524452.489200,U02T96HEARK\\n716241d7-ce8e-43bf-a7c5-0773fdde72f4,U02T96HEARK,,,\"yes, sry that is what I meant\",1642523808.486200,1642524517.489600,U02SE2PSSTC\\ne5a4070c-7772-4259-94e1-46c92d7757f9,U02RREQ7MHU,,,change persmissions to the path,1642520122.473300,1642524526.489800,U02U2Q5P61Z\\nf7772c58-41c4-4598-a269-671b2cfc69df,U02RREQ7MHU,,,lookup chmod,1642520122.473300,1642524529.490000,U02U2Q5P61Z\\na533171f-8d6e-4a62-90a9-41b004bf2ec2,U02U4G7U3GV,,,Im using gitbash,1642523612.482900,1642524556.490200,U02U4G7U3GV\\n01cbca0d-1423-4fb0-8fcc-3b9914e8d52d,U02T96HEARK,,,\"Yeah, but in the command that I ran I had POSTGRES_USER=\"\"root\"\". But still I am getting the error.\",1642523808.486200,1642524777.490600,U02T96HEARK\\n82bcd051-2ede-4cb3-9e03-9546e06849c7,U02SBTRTFRA,,,I have the same error,1642518716.469700,1642525421.491100,U02T96HEARK\\nce4d6e01-b1e5-4f56-9682-a46e6cefa4b4,U02T96HEARK,,,I think im having the same issue as you <@U02T96HEARK>,1642523808.486200,1642525665.492400,U02U4G7U3GV\\nd468072b-32e8-4e31-9d5d-1b6cafacb5e3,U02U4G7U3GV,,,\"I used the windows command prompt and was able to advance further, just to get the same error as the one below\\n```connection to server at \"\"localhost\"\" (::1), port 5432 failed: FATAL:  password authentication failed for user \"\"root\"\"```\",1642523612.482900,1642525718.492600,U02U4G7U3GV\\n7d6f0f86-40c4-4d32-b6ec-6931f491fc2f,U01AXE0P5M3,,,Soon. Actually for most of the homework the videos that we already released are sufficient,1642511128.453300,1642525737.492800,U01AXE0P5M3\\n599923b2-b1da-4698-9d8a-8d1ca68f8018,U02T96HEARK,,,\"Yay! Happy to hear it! \\n\\nYes with Linux it definitely will be simpler\",1642496193.414600,1642525839.493100,U01AXE0P5M3\\n726d0ac5-261d-4789-805d-d83b1dd129e1,U02UE7NTLUU,,,It looks like changing the port command worked and deleting my local volume to re-initialize the db.,1642465146.363400,1642525926.493300,U02UE7NTLUU\\nd443b9a2-2cc8-4778-b166-bffdc6c4f045,U02UE7NTLUU,,,\"```docker run -it \\\\\\n    -e POSTGRES_USER=\"\"root\"\" \\\\\\n    -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n    -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n    -v &lt;localPath&gt;:/var/lib/postgresql/data \\\\\\n    -p 5431:5432 \\\\\\n    postgres:13```\",1642465146.363400,1642526016.493500,U02UE7NTLUU\\n86d91716-6d25-4c25-bb8f-d4d656e5de34,U02RTJPV6TZ,,,Can you try this? <https://stackoverflow.com/a/53097168>,1642488820.397100,1642526121.493700,U01AXE0P5M3\\n3cf403b5-6de8-420a-8915-fad0cb09415c,U02RTJPV6TZ,,,Do you use docker toolbox?,1642488820.397100,1642526331.494200,U01AXE0P5M3\\n8c07c5d3-eaa3-4c29-9539-ea001d793bed,U02T96HEARK,,,I had to set POSTGRES_USER=“tjanif” which is the name of my root user. I am not fully sure why (I don’t have a CS background :sweat_smile: ),1642523808.486200,1642526465.494700,U02SE2PSSTC\\n06df01ce-5a65-48d9-b199-80213dc15b0f,,3.0,,\"Hey everyone :wave::skin-tone-2:\\n\\nI’m currently struggling with adding more roles to the service account …\",1642526499.495400,1642526499.495400,U02UNQNMH7B\\nd3373c25-522f-4226-bd81-d6a299f3b820,U02UNQNMH7B,,,\"I created it with just viewer\\n<https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/2_gcp_overview.md#initial-setup>\\nand now I don’t see where I can grant it further access :woman-facepalming::skin-tone-2:\",1642526499.495400,1642526531.495500,U02UNQNMH7B\\ne218d9c0-052b-425d-88e4-9a3f0166db92,U02T96HEARK,,,\"I had this same issue. What worked for me was deleting the local volume and then remapping the port to 5431 before rerunning the docker command.\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642526016493500?thread_ts=1642465146.363400&amp;cid=C01FABYF2RG>\",1642523808.486200,1642526987.496200,U02UE7NTLUU\\n,USLACKBOT,4.0,tombstone,This message was deleted.,1642527158.497600,1642527158.497600,USLACKBOT\\n70d5af6e-c0a2-4195-8a12-9375791f3ee7,U02SBTRTFRA,,,\"Had the same issue. What worked for me was remapping the port, deleting my local volume, and rerunning the docker command.\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642526016493500?thread_ts=1642465146.363400&amp;cid=C01FABYF2RG>\",1642518716.469700,1642527260.498200,U02UE7NTLUU\\n9228a2ec-e936-44e1-bb15-365d8ee10956,USLACKBOT,,,\"`winpty docker run -it \\\\`\\n\\xa0`-e POSTGRES_USER=\"\"root\"\" \\\\`\\n\\xa0`-e POSTGRES_PASSWORD=\"\"root\"\" \\\\`\\n\\xa0`-e POSTGRES_DB=\"\"ny_taxi\"\" \\\\`\\n\\xa0`-v \"\"C:/Users/LENOVO/Desktop/zoomcamp-test/ny_taxi_postgres_data\"\":\"\"/var/lib/postgresql/data\"\" \\\\`\\n\\xa0`-p 5432:5432 \\\\`\\n`postgres:13` This is the command is used and it worked smoothly for me. You can modify the dir name and try the same. :slightly_smiling_face:\",1642527158.497600,1642527261.498400,U02QGA57GRY\\n279fa96c-ffea-42f9-9723-54999a51c1c0,U02T96HEARK,,,I ended up referring back to the postgres docker documentation and got it work using this: docker run -it --name postgres13 -e POSTGRES_PASSWORD=“postgres” -v $(pwd)/nytaxi:/var/lib/postgresql/data -p 5432:5432 -d postgres,1642523808.486200,1642527306.498700,U02UC0G6FB5\\n63e42f40-60dd-4ce2-8fa6-f48cf6cf4a68,U02T96HEARK,,,\"This assigns the default posgres user a password of postgres. I had never installed postgres previously, but was having the same issue as others. This resolved it and allowed me to connect. I had to do a docker system prune --all and then remove the local postgres volume manually.\",1642523808.486200,1642527405.499100,U02UC0G6FB5\\n27af78e9-e125-4f48-8f0d-583f0806986f,U02UNQNMH7B,,,\"Maybe this will help?\\n\\n<https://docs.google.com/document/d/e/2PACX-1vSZapy7gIj0TP-EFzub2OpAlAkuifGEVJ4XpkA1RvxZ45NjiQi29b6OhLuetdXXHWAn2lbbKxnbzMdd/pub>\",1642526499.495400,1642527465.499300,U01AXE0P5M3\\n6325c3c3-dcbd-47a3-a137-d72cbe83c50e,USLACKBOT,,,\"Just curious, why do you need winpty?\",1642527158.497600,1642527508.499900,U01AXE0P5M3\\n082783e5-bb7b-437f-ba06-7dc8ee9e7b92,U02U4G7U3GV,,,can you try starting posgres with a different port? e.g. 5431,1642523612.482900,1642527616.000800,U01AXE0P5M3\\nbf858559-6a4c-4d45-8033-510b26bd199c,U02U4G7U3GV,,,You\\'ll need to change mapping from -p 5432:5432 to -p 5431:5432,1642523612.482900,1642527638.001200,U01AXE0P5M3\\n57e490b2-053e-42f2-963b-d96a0797c233,U02RREQ7MHU,,,\"<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642526121493700?thread_ts=1642488820.397100&amp;cid=C01FABYF2RG>\\n\\ndo you use docker toolbox?\",1642520122.473300,1642527688.001500,U01AXE0P5M3\\n7800a79c-570a-4be0-ab6a-6a777ee7176c,U02U4G7U3GV,,,\"Thanks a lot, that resolved the issue. It seems its conflicting with my own postgres instance? maybe.\",1642523612.482900,1642527874.003600,U02U4G7U3GV\\n4937937e-d071-43d8-82fb-ce8d2f771a48,U0205L73QNS,,,please add me too. I am interested too. thanks,1642447403.330800,1642527893.004000,U02SSA2SL95\\nc03b2845-a1f7-4cb8-aeb4-942518745fea,USLACKBOT,,,\"getting the invalid mode error\\n`docker run -it -e POSTGRES_USER=\"\"root\"\" -e POSTGRES_PASSWORD=\"\"root\"\" -e POSTGRES_DB=\"\"ny_taxi\"\" -v c:\\\\users\\\\abhis\\\\zoomcamp\\\\week_1\\\\`\\n`ny_taxi_postgres_data:/var/lib/postgresql/data -p 5432:5432 postgres:13`\\n`docker: Error response from daemon: invalid mode: /var/lib/postgresql/data.`\\n`See \\'docker run --help\\'.`\",1642527158.497600,1642527932.005000,U02RREQ7MHU\\n1e0fa758-596a-4513-a071-cbc6cd71e287,,1.0,,\"Hiiii everyone! :wave:\\nThis gem just popped up in my feed today, thought of sharing this fantastic video with y\\'all! :star-struck:\\nFantastic animations and its so relevant for everyone taking this course.\\nLike Data Engineering in a Nutshell!! :nerd_face:\\n<https://youtu.be/qWru-b6m030>\",1642528031.007100,1642528031.007100,U01QGQ8B9FT\\nb86bcd87-ceca-4608-9446-9e757adcd7ed,U02UNQNMH7B,,,\"Hi <@U02UNQNMH7B>, we have also uploaded a video yesterday explaining everything in a step-by-step fashion: <https://youtu.be/Hajwnmj0xfQ?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&amp;t=1044>\\nPlease watch from 17:24 for granting IAM roles. Hope this helps\",1642526499.495400,1642528105.007400,U01DHB2HS3X\\n73e16af0-9629-4078-aab1-9f52c4d43322,U02SBTRTFRA,,,<@U02UE7NTLUU> It worked. Thank you.,1642518716.469700,1642528850.008100,U02SBTRTFRA\\n5cb2ebf2-97b0-49cb-ba53-83ef5df6aeb9,U01AXE0P5M3,,,When will be the deadline for submission of week 1 homework,1642516682.463800,1642528901.008300,U02T9JQAX9N\\n523c0ebd-30e9-4251-86bc-c2e539651322,,1.0,,\"I found this interesting, so thought to share\\n<https://landscape.cncf.io/>\",1642528945.008800,1642528945.008800,U0254S545D5\\n4f180de0-4b3d-4a08-95a6-6fcb13e7b7d2,U02SBTRTFRA,,,\"Awesome, glad to hear! It was a pretty frustrating issue hah\",1642518716.469700,1642529346.009400,U02UE7NTLUU\\n49c6e50a-9bd7-4756-97fe-604765eeb469,,12.0,,Anyone who found any solution for postgres:13 installation and mounting path on windows?,1642529404.010000,1642529404.010000,U02QGA57GRY\\n5a971d0e-0ec2-43eb-b38a-108db74df269,U02QGA57GRY,,,\"``` winpty docker run   -e POSTGRES_USER=\"\"root\"\"   -e POSTGRES_PASSWORD=\"\"root\"\"   -e POSTGRES_DB=\"\"ny_taxi\"\"   -v \"\"C:/Users/LENOVO/Desktop/zoomcamp-test/sql_test/ny_taxi__postgres_data\"\":/var/lib/postgresql/data   -p 5431:5432 postgres:13```\\nThis is the command I used..\",1642529404.010000,1642529522.010200,U02QGA57GRY\\ne6e35bea-ba4e-43b4-9afc-cc335408f2ab,U01AXE0P5M3,,,\"In question 2 which output are we supposed to copy, the one after running terraform plan?\\nIn question 5, by district name, do you mean the zone name.\\nIn question 6, do you want the id or the zone name (District name) and should both names be valid?\",1642516682.463800,1642529676.010600,U02TC8X43BN\\nac922225-e335-4dd2-a770-5bdcd09edd74,U02QGA57GRY,,,\"This is how I did mine:\\n```winpty docker run \\\\\\n  -e POSTGRES_USER=\"\"root\"\" \\\\\\n  -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n  -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n  -v \"\"c:/Users/Osmani Diaz/DataScience/Data Engineering/data-engineering-zoomcamp/docker_sql/ny_taxi__postgres_data\"\":/var/lib/postgresql/data \\\\\\n  -p 5431:5432 \\\\\\npostgres:13```\",1642529404.010000,1642529772.010900,U02U4G7U3GV\\nFD4984FB-7F39-4E50-93ED-8B1704D74141,U01QGQ8B9FT,,,\"Thank you so much for sharing this! I agree with the comments, this video deserves an award, such a beautiful vulgarisation of DE under 15 minutes. \",1642528031.007100,1642529893.012400,U02UX664K5E\\nc4df370f-3d73-4e3c-8f7e-de4ac3cb2e23,U02QGA57GRY,,,This is what I\\'m getting :disappointed:,1642529404.010000,1642530095.012700,U02QGA57GRY\\n17e3aa7f-ca51-4f20-a651-67794cab91fc,USLACKBOT,,,One error occurs When I try to run docker commands without winpty on git bash...,1642527158.497600,1642530737.013500,U02QGA57GRY\\n52beeee6-0b78-48f4-b979-a2ecaab7f04b,U02QGA57GRY,,,there is a /Git/ folder before /var/ . Seems weird because you didnt have it in the command,1642529404.010000,1642531242.015100,U02T2JGQ8UE\\ne5f6d82f-acae-44a0-a136-4bae9b18e069,U02QGA57GRY,,,Try creating a new folder and running your docker image there. I seem to remember someone in another docker class I had had a similar issue.,1642529404.010000,1642531368.015600,U02TNEJLC84\\n108d9a1e-9cd5-4682-9cd7-2e636d1f5169,U02QGA57GRY,,,<https://nickjanetakis.com/blog/setting-up-docker-for-windows-and-wsl-to-work-flawlessly|Setting Up Docker for Windows and WSL to Work Flawlessly>,1642529404.010000,1642531370.015800,U02TNEJLC84\\n72b02ad2-a6cc-4fed-9555-84caec62ff76,,8.0,,\"Hello!\\n\\nAfter doing:\\n```terraform init```\\nI get the following error:\\n```(base) &lt;&gt; Downloads % terraform plan -var=\"\"project=&lt;id&gt;\"\"\\n╷\\n│ Error: No configuration files\\n│ \\n│ Plan requires configuration to be present. Planning without a configuration would mark everything for destruction, which is normally not what is desired. If you would like to\\n│ destroy everything, run plan with the -destroy option. Otherwise, create a Terraform configuration file (.tf file) and try again.```\",1642532204.017900,1642532204.017900,U029DM0GQHJ\\n1aff98b0-4115-461a-a5fd-01a64577bca4,U029DM0GQHJ,,,Are you sure you didn\\'t have terraform plan? This looks like output of `terraform -plan`,1642532204.017900,1642532260.018000,U02QPBZ3P8D\\nb4836b8e-8ba6-4d85-a935-1dc5fd06f6e4,U029DM0GQHJ,,,\"You need both main and variables files in your folder downloaded from the repo, I think.\",1642532204.017900,1642532377.018400,U02TC8X43BN\\n104adcbf-5323-4f4e-b6d2-040f709692fb,U02U4G7U3GV,,,Most likely,1642523612.482900,1642532441.018600,U01AXE0P5M3\\na1b3e870-063b-4445-a780-86d470c562cc,U029DM0GQHJ,,,<@U02TC8X43BN> yep..,1642532204.017900,1642533051.019400,U029DM0GQHJ\\n94636c4b-36e7-4d45-8aa5-cfca3b47a750,U02RTJPV6TZ,,,\"Thanx alot I just removed \"\":\"\" and then added a \"\"/\"\"  before E, it worked well thnx\",1642488820.397100,1642533738.019900,U02RTJPV6TZ\\n9f612a26-425c-46ce-b26b-b2795229877e,U02QGA57GRY,,,\"Add a \"\"/\"\" before the \"\"C\"\" and then remove the \"\":\"\" it will work fine in windows <@U02QGA57GRY>\",1642529404.010000,1642533901.020400,U02RTJPV6TZ\\ne8c17564-1748-44d6-bd3f-73621d0b6fea,U02QGA57GRY,,,\"```-v /C/Users/LENOVO/Desktop/zoomcamp-test/sql_test/ny_taxi__postgres_data\"\":/var/lib/postgresql/data   ```\",1642529404.010000,1642534056.021200,U02RTJPV6TZ\\n31436b75-260b-4974-bea0-865ae9f18f92,,6.0,,\"```Hello everyone, I ran the following command successfully -   winpty docker run -it -e POSTGRES_USER=\"\"root\"\" -e POSTGRES_PASSWORD=\"\"root\"\" -e POSTGRES_DB=\"\"ny_taxi\"\" -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data -p 5432:5432 postgres:13 ; But nothing being created in ny_taxi_postgres_data folder. Can some one help me with it?```\",1642534057.021400,1642534057.021400,U02V4412XFA\\nf62d9fb3-e364-471c-95fb-f3e5c571be63,U02QGA57GRY,,,that above should be able to work in your windows machine,1642529404.010000,1642534082.021500,U02RTJPV6TZ\\nb2c36f19-a9c1-4526-ab9d-686360235038,U02V4412XFA,,,try to put some data there,1642534057.021400,1642534184.022300,U01AXE0P5M3\\n65b709d1-f8b9-4504-b4b7-d357645f5921,U02V4412XFA,,,Do you mean data in ny_taxi_postgres_data folder?,1642534057.021400,1642534329.022800,U02V4412XFA\\n,,25.0,,\"It seems that some of you have this issue\\n\\nThis is the solution that worked for <@U02RTJPV6TZ> :\\n\\n> I just removed \"\":\"\" and then added a \"\"/\"\"\\xa0before E, it worked well thnx\",1642534337.023000,1642534337.023000,U01AXE0P5M3\\nd609e03a-09d5-4848-9647-9dc070da4229,U02V4412XFA,,,\"no, to the database\",1642534057.021400,1642534343.023100,U01AXE0P5M3\\n3f2cd635-10b9-4143-9120-fb648b0ac9de,U01AXE0P5M3,,,i.e. it should be `/e/zoomcamp/...`,1642534337.023000,1642534375.023300,U01AXE0P5M3\\nbcd7af14-cf8a-416d-9711-a997409b271b,U01AXE0P5M3,,,\"Not sure if this issue happens with docker desktop, but apparently it happens with docker toolbox\",1642534337.023000,1642534411.023500,U01AXE0P5M3\\n65718fe0-31fc-4bcc-b662-60712f26d27f,U02V4412XFA,,,Sorry but I\\'m unable to catch how it should be done,1642534057.021400,1642534698.023900,U02V4412XFA\\n52feaebe-a6c1-4189-b103-28bf2e9389e5,U02V4412XFA,,,Is it part of ingestion(later part of the video)?,1642534057.021400,1642534766.024100,U02V4412XFA\\nf9db3d29-5d45-4415-a7db-e9539e0b6c58,,6.0,,\"hey all!\\nHas anyone completed their setup in normal Windows system for GCP and Terraform?\",1642534914.025300,1642534914.025300,U02SC4Y6X0U\\n7fedded5-426f-4590-b8d5-18ebb2189c88,U02RREQ7MHU,,,\"omg thank you soo much it finally worked, i have been suck on this for almost 2 days, also after this i was having some troubles with the password so i changed my port to 5431 as you said in another thread and changed the password to admin, now it works fine.\\nthe code that worked for me(i had to add\"\" \"\" on both sides of the directory and add extra / on both sides of the directory ) -\\n `docker run -it -e POSTGRES_USER=\"\"root\"\" -e POSTGRES_PASSWORD=admin -e POSTGRES_DB=\"\"ny_taxi\"\" -v \"\"/c/users/abhis/git/zoomcamp/week_1/ny_taxi_postgres_&lt;data://var/lib/postgresql/data&gt;\"\" -p 5431:5432 postgres:13`\",1642520122.473300,1642535077.025500,U02RREQ7MHU\\n7891ad7c-3703-4668-97fc-c34397fcc1d7,U02RREQ7MHU,,,and fyi i am using docker desktop,1642520122.473300,1642535184.026000,U02RREQ7MHU\\nbfd3089c-2c35-4f81-8ed3-e639b94c523c,U02QGA57GRY,,,try the solutions given here it worked for me <https://stackoverflow.com/a/53097168>,1642529404.010000,1642535263.026200,U02RREQ7MHU\\n4825f4bf-2d43-4d57-93f8-4d06fdcad04b,,1.0,,\"to any one still stuck with postgres:13 installation try these solutions, thanks <@U01AXE0P5M3>\\n<https://stackoverflow.com/a/53097168>\",1642535362.027400,1642535362.027400,U02RREQ7MHU\\n199b63c4-8d8d-4312-a7ef-fb92e1153181,U02V4412XFA,,,Yes,1642534057.021400,1642535791.027600,U01AXE0P5M3\\n69fbda90-67c2-438f-bb97-48039d59c4fb,U02QGA57GRY,,,Will give it a try!:smiley:  Thanks <@U02RTJPV6TZ> and <@U02RREQ7MHU>.,1642529404.010000,1642535972.028100,U02QGA57GRY\\n505f8705-564b-44b4-b65c-5c2053c3e157,U02SC4Y6X0U,,,I\\'m unable to connect SDK to Python( in Anaconda) :(,1642534914.025300,1642537207.029600,U02SC4Y6X0U\\n9745C878-A56B-4B37-80A5-9ACAF9A00AFF,U01AXE0P5M3,,,Btw u can use datagrip for postgres or dbeaver,1642511128.453300,1642537886.030700,U02TRHFHDH7\\n6f701b88-2a53-4f38-8126-1e063ce1c548,U02TBKWL7DJ,,,\"<@U01AXE0P5M3>\\n\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642511128453300>\\n\\nI\\'ve tried running this and I was able to install the pgcli, but when tried connecting to the db using :\\n\\n`$ pgcli -h localhost -p 5432 -u root -d ny_taxi`\\n\\nI keep getting this error\",1642507885.440700,1642538453.031200,U02TBKWL7DJ\\n625f77c0-396c-4ebc-8b07-1b119fd5f530,U02TBKWL7DJ,,,Can you please copy paste the error message instead of making a screenshot?,1642507885.440700,1642538586.032100,U01AXE0P5M3\\n6c1333cd-9017-4ce5-914c-3c1952485faf,U02TBKWL7DJ,,,Also have you tried Googling this error?,1642507885.440700,1642538630.032300,U01AXE0P5M3\\n63ccc857-0e46-410b-84dd-1eb74303e5a4,U02SC4Y6X0U,,,have you tried the instruction from the course repo?,1642534914.025300,1642538726.032500,U01AXE0P5M3\\n75b083ad-69dc-4d30-9d21-ffc127f96622,,1.0,,If we are going to mention the course on LinkedIn is there a particular individuals or organization we should tag?,1642539721.034000,1642539721.034000,U02TNEJLC84\\nc458b1a5-8f91-4d52-ac85-41977f722609,U029DM0GQHJ,,,so should i clone the repo and run the terraform command from there with my gcp project id ?,1642532204.017900,1642540651.034200,U02TMP4GJEM\\ne6d1ce52-297c-46da-ad7f-c23d8c4b4cc0,U029DM0GQHJ,,,i ask because i\\'m having the same issues,1642532204.017900,1642540696.034400,U02TMP4GJEM\\nb7997570-a822-4de6-8323-766ee0312df6,,1.0,,Hello guys! What do you think based on the syllabus: all pieces of knowledge which we are going to get during the course will be enough to find an entry-level job in the USA market without actual experience?,1642540976.035900,1642540976.035900,U02UCDETTFU\\nf48cd965-df28-47fc-82a1-1b2cb5b01aa0,,4.0,,\"Hello. I get such an error when I try to execute function  <http://df.to|df.to>_sql\\n\\nAttributeError: \\'Engine\\' object has no attribute \\'cursor\\'\",1642541151.036900,1642541151.036900,U02QL1EG0LV\\n0b4c6778-99c7-4859-9b86-66ac90da9247,U02UCDETTFU,,,\"I don\\'t know how the market is in the US, but I would suggest, after this course, you complement your knowledge with more personal projects, explore more tools and go in depth in the most important topics.\",1642540976.035900,1642541203.037000,U02U4G7U3GV\\na4480299-470c-4652-96bc-18d6b3dcb7bb,U02QL1EG0LV,,,Could you provide a code snipped?,1642541151.036900,1642541254.037200,U02TNEJLC84\\nc8cd14d3-c81a-44f3-b982-56ffac50f7ef,U02QL1EG0LV,,,\"Not sure but maybe you put\\n```engine.cursor()```\\ninstead of\\n```engine.connect()```\\n?\",1642541151.036900,1642541336.038000,U02TNEJLC84\\nb2050ec5-881f-4b40-9e1e-b2f6d736727f,,3.0,,\"Hello everyone, need some help, after running `docker build -t test:pandas .`  I got the following error :\\n`failed to solve with frontend dockerfile.v0: failed to create LLB definition: the Dockerfile cannot be empty` . Any idea what might be wrong?\",1642541435.038900,1642541435.038900,U02UX664K5E\\n0f2c67b1-299e-4369-b346-d02108acb4f4,U02TNEJLC84,,,I would tag <http://DataTalks.Club|DataTalks.Club>,1642539721.034000,1642541518.039000,U02UX664K5E\\n288048ac-a12e-44b9-aec4-1482a8c9d567,U02UX664K5E,,,Maybe you forgot save your Dockerfile,1642541435.038900,1642541559.039200,U02QL1EG0LV\\neacb7a45-ef39-4b6e-bb2a-cc9d6e7d0c10,U02UX664K5E,,,\"Oh damn, you are right, thank you!\",1642541435.038900,1642541591.039400,U02UX664K5E\\nf149552d-632b-41ef-bb65-ecb2e2921249,U02QL1EG0LV,,,I didn\\'t know what it was. I\\'ve just restarted my cells. And now it is working :grinning:,1642541151.036900,1642542007.039900,U02QL1EG0LV\\n7493ac85-191e-465f-89ec-750280b26d13,U0254S545D5,,,A bit of information overflow... don\\'t you think?,1642528945.008800,1642543061.041200,U02FLPS4GSJ\\n611d73e6-bb27-472c-aaf2-c696a0a194ba,,,,\"Have a question regarding the GCP installs.\\n_<https://cloud.google.com/sdk/docs/quickstart#deb|Quickstart: Getting started with Cloud SDK>_\\nIn the 4th section \"\"Optionally, install any of the following additional components\"\" there is a list of software.\\n`google-cloud-sdk-app-engine-python`\\n`google-cloud-sdk-app-engine-python-extras`\\n`google-cloud-sdk-app-engine-java`\\n`google-cloud-sdk-app-engine-go`\\nQ. How much did anyone here install of this huge list. I did the first two then decided to hold off. Any thoughts?\",,1642543317.044200,U02FLPS4GSJ\\n145340CE-7D96-490B-8E07-43AEE3AFC2EE,,1.0,,\"Hi, I need someone to walk me through the installs in on MacOS please. \",1642543574.046000,1642543574.046000,U02T9VA6VJS\\n38b350cf-62aa-42bb-ae6d-3861f22b774e,U01MFQW46BE,,,\"I really like the airbnb engineering blog! Their pieces are always super informative, and they certainly have a very challenging task\",1642517261.465300,1642543890.046900,U01B6TH1LRL\\n8391e5ec-cc42-4f87-a181-5f482452b651,,12.0,,\"I know many of you have problems configuring your env, so I recorded a tutorial for setting things up on a virtual machine\\n\\n<https://youtu.be/ae-CV2KfoN0|https://youtu.be/ae-CV2KfoN0>\\n\\n\\nThis should work for any OS you use\",1642544154.050000,1642544154.050000,U01AXE0P5M3\\n1a2da500-3c4a-4ba1-af35-1e57ad4a7fbe,U02QL1EG0LV,,,Sweet!,1642541151.036900,1642544165.050300,U02TNEJLC84\\n2f866c8a-0ae1-4ea1-b557-a90f5d2e79ec,U02TNEJLC84,,,<@U02UKBMGJCR> Passed the payment information. It was needed.,1642457024.345400,1642544213.050700,U02TNEJLC84\\n21c1cb0b-5785-43d0-bbfb-d45c27772363,,4.0,,<@U01AXE0P5M3> is anaconda makes life easier or can regular python with virtual env be used as well?,1642544480.053500,1642544480.053500,U02UBV4EC8J\\n4ca123e3-8d52-47bd-a8cd-a63fc10109f6,U02UBV4EC8J,,,To me- yes,1642544480.053500,1642544565.054700,U01AXE0P5M3\\n3ad2a356-a210-4d85-bc03-0f0e699d93ba,U02UBV4EC8J,,,Regular Python with venv can be totally used if you know how to use it,1642544480.053500,1642544601.054900,U01AXE0P5M3\\n9e448bdd-d53f-4135-a0d2-4a3da6445a59,U02UBV4EC8J,,,\"sounds good, thanks!\",1642544480.053500,1642544659.055300,U02UBV4EC8J\\n25625fa0-f05b-4877-ad6c-009054445671,,3.0,,Hi all — I\\'m stuck with pgcli not responding when I go to input the password to the postgres db (details in reply),1642544670.055500,1642544670.055500,U02SM3LKD2B\\n757879e9-fc96-401d-af41-1981aa992fa6,U02SM3LKD2B,,,\"I am able to successfully connect to the db using pgcli and am prompted for a password, but when I try to type the password (root) and hit enter, nothing happens and I\\'m not able to do anything (e.g., ctrl + D does nothing) unless I exit the git bash window and restart). I\\'ve tried googling and haven\\'t found any similar examples of this problem.\",1642544670.055500,1642544683.055600,U02SM3LKD2B\\nb0f7f37e-2fcd-492b-b39c-9cf79c7ea054,U02SM3LKD2B,,,\"I was having the same issue. What I did was change the port mapping in the script, since that might be caused by a conflicting instance of postgres on your local machine.\\nSomething like this\\n```-p 5431:5432```\",1642544670.055500,1642545092.056000,U02U4G7U3GV\\n03fb3d06-a32d-4c12-bc02-bc95c67c29f7,,2.0,,\"Hi everyone, please I\\'m having issues running docker desktop...\\n\\nI keep getting \"\"Docker desktop failed to start\"\"\\n\\nI need help with a way out please.\",1642545252.057900,1642545252.057900,U02QS4BD1NF\\ne82e90e0-cdee-459a-8793-add18a92f5f6,U02QS4BD1NF,,,\"Difficult to know what could be causing the issue but I also faced the same issue and here what I suggest:\\n\\n1. Install Linux Kernel update package from here: <https://docs.microsoft.com/en-us/windows/wsl/install-manual#step-4---download-the-linux-kernel-update-package>\\n2. Rerun Docker\\nIf it doesn\\'t work, try this:\\n1. Uninstall Docker Desktop\\n2. Restart computer\\n3. Install Docker Desktop (make sure to install it with admin rights)\",1642545252.057900,1642545541.058000,U02UX664K5E\\n7be3bc4a-6743-4000-aa9b-38406fc85640,U02SM3LKD2B,,,Oops — I just saw your earlier post. Didn\\'t mean to create a duplicate. Thank you!!,1642544670.055500,1642546415.062900,U02SM3LKD2B\\n9e442d38-d68a-4d67-95b9-e731ab7ac0bc,,5.0,,\"Hi Everyone, how to open visual Studio from terminal from local? I saw that on youtube channel Alexey just type code. and then the Visual studio opened. I am not able to to do that. For background I just downloaded the visual studio and I have mac with intel chip.\",1642546559.066600,1642546559.066600,U02R77VC12N\\n860309af-c188-4fd3-8695-58dc7433a868,,7.0,,\"Need help with some theory, silly questions but I\\'m totally new to this and the type that needs to understand the workings behind what I do.\\n\\n1. From my understanding when we execute `docker build -t test:pandas .` , what we are doing basically is create a docker image named `test:pandas`  and the image settings would be pulled from Dockerfile. How the image knows which docker file settings to use? Is it because you include the docker file in the same directory where the docker image was created?\\n\\n2. In the beginning of the video Alexey ran `docker run -it ubuntu bash`  which is supposedly a docker image as well, how is this related to the `test:pandas` image? Is `test:pandas`  image within `ubuntu`  image?\",1642546712.068400,1642546712.068400,U02UX664K5E\\nc1f7da7a-8653-43e2-a09c-376a4b55d298,,,,\"anyone, what will be a QUOTA_PROJECT_ID?\",,1642546826.068700,U02UBV4EC8J\\n55e015f7-09d6-4ddf-b319-b18c3ca70831,U02R77VC12N,,,just type code and folder name or code . current folder,1642546559.066600,1642546857.068900,U02UBV4EC8J\\nB77C6AFC-BAE4-470B-805B-877FCA2D0C3F,U02R77VC12N,,,\"You may also need to set that up in VS code ..This link may help \\n<https://code.visualstudio.com/docs/setup/mac|https://code.visualstudio.com/docs/setup/mac>\",1642546559.066600,1642547015.070000,U02AGF1S0TY\\n3e6fb0cc-9dd6-4828-a0e3-c12125660127,U02UX664K5E,,,1) The . tells docker to look in the current directory for a file called Dockerfile,1642546712.068400,1642547489.076900,U02TNEJLC84\\n249675bb-d278-43ed-a43d-8a634502d573,U02UX664K5E,,,2) It\\'s related I believe to show a different way to run a container. Run means run a container from the image Ubuntu. If it can\\'t find it local it pulls the latest from Dockerhub. Or whatever repo Docker is set to.,1642546712.068400,1642547616.077200,U02TNEJLC84\\ne17c7625-3120-4897-98b7-388a389d81da,U02UX664K5E,,,\"1. Correct, you run the command from the same folder where the Dockerfile is contained, otherwise the command would fail. If your Dockerfile were named something else and/or was on a different folder, you could use instead `docker build -t test:pandas -f path/to/filename`\\n\\n2. Here’s a rundown of what’s happening:\\n\\n`docker run -it`: run docker in interactive mode. This is what you will use most of the time.\\n`ubuntu`: name of the image to run. If not found locally, docker will look up the image on docker hub. Ubuntu offers a basic image on <https://hub.docker.com/_/ubuntu> , so that’s the image you’re downloading and running.\\n`bash`: you’re telling docker to run bash as the container opens. I’ve actually tried runnin the docker run command without `bash` at the end and it doesn’t seem to make a difference, though.\\n\\nShameless plug: check out my gist for a quick Docker explanation and cheatsheet: <https://gist.github.com/ziritrion/1842c8a4c4851602a8733bba19ab6050#docker>\",1642546712.068400,1642547649.077400,U02BVP1QTQF\\nD76F0669-ABEA-406C-ADF5-472D416AA717,U02UX664K5E,,,\"(A) when u say docker build and include a . at the end it means that use the Dockerfile in the current directory \\n(B) docker run -it Ubuntu bash means , run a u u Ubuntu image ( it will fetch from repository if not found locally ) and as an entry point into the container open a bash terminal ..so when we execute this command a bash terminal is opened .\\nImage can be understood as a blueprint which is read only with all codes and dependencies and we can say container is an instance of a running image . We can have multiple containers running from same image ..all image will have a base image and Ubuntu is a base image . \\nAs far I remember test:pandas is using a python base image and not related to Ubuntu image \\n\",1642546712.068400,1642547651.077800,U02AGF1S0TY\\n79f7037d-c90f-4edd-9ac6-887d025e47ed,U02UX664K5E,,,\"Thank you to all of you, very well explained!\",1642546712.068400,1642547784.080400,U02UX664K5E\\n2b2024e4-110e-4110-9a64-82a7caf1ddfb,U02UX664K5E,,,\"<@U02BVP1QTQF> Your Docker terminology gist is very helpful, thanks again.\",1642546712.068400,1642548024.081400,U02UX664K5E\\ncc5ae5a3-0cbe-4e52-9dfb-41e716f93db6,,18.0,,\"Hi Errrybody. I have made some notes for the Week 1, Docker video. There is a whole lot of typos in there, I am too lazy to revise. It takes 3-4 times the video duration to make notes, so I am burnt out right now.\\nMake sure to drop a reaction to this message if you like it.\\nlet me know if you have any issue or if you want to see more of these notes. Thanks :)\\n\\n<https://www.notion.so/Week-1-Introduction-f18de7e69eb4453594175d0b1334b2f4>\",1642548032.081900,1642548032.081900,U02QZN0LSBT\\n0cb69b1b-467e-4962-9007-75b0e3cb6326,U02QZN0LSBT,,,Really good work :slightly_smiling_face: I was building my notes as well and you just made my life easier! I hope to return you the favor!,1642548032.081900,1642548385.082300,U02UX664K5E\\n534fb644-c402-42e1-aedc-15c9c24f61f2,U02QZN0LSBT,,,\"If the course organizers agree, you can create a pull request on the course repo and add a link to your notes so that other students can always access them easily rather than having to search in slack. You can take a look at the ML zoomcamp repo to see how other students shared their notes for each lesson, like here, for example: <https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/01-intro>\",1642548032.081900,1642548640.082700,U02BVP1QTQF\\n5e35494f-f043-45c1-b792-973e56cdca1b,U029DM0GQHJ,,,Answered my own question. Did it and it works. Did a `gsutil ls` to confirm that the GCP bucket had been created too. Thanks,1642532204.017900,1642548920.083400,U02TMP4GJEM\\ncfac126e-993f-4e9e-b709-defd9ab07cf3,U02UBV4EC8J,,,You can use update-alternatives too... Python is quite messy...,1642544480.053500,1642550203.084200,U02GVGA5F9Q\\na8b92e48-f9c3-407d-8e68-0efdbf36b8fd,U02R77VC12N,,,\"\"\"code\"\", but you must have $PATH properly setup\",1642546559.066600,1642550327.084400,U02GVGA5F9Q\\n7DE39CDB-10D3-47ED-ADD4-3A57E133B9BC,,6.0,,\"Haven\\'t set anything up yet, but am I likely to face any issues setting up my environment on an M1 Mac?\",1642551977.086500,1642551977.086500,U02U34YJ8C8\\n574c407a-8850-4074-bb95-9d235b0a8ebd,U02U34YJ8C8,,,\"So far for me no any issues with M1, I have setup docker and was able to run the containers.\",1642551977.086500,1642554209.087200,U02SEH4PPQB\\n2ae48de5-0ee3-45ce-b84f-75d390477dcf,,4.0,,\"Hi, who has been able to run pgcli?\\nAfter running and entering my password, the bash freezes indefinitely.\\nwhat do you suggest I do?\",1642556934.089100,1642556934.089100,U02T1BX1UV6\\n57df7483-66aa-4dca-a74a-da8e37428fb1,U02T1BX1UV6,,,Great. Thank you. I didn\\'t see this early enough. I got it working already,1642460502.352700,1642557021.089300,U02T1BX1UV6\\n8ab5efc9-2db5-4aac-8ad9-0eb66812831e,U02T1BX1UV6,,,I am using windows,1642460502.352700,1642557053.089500,U02T1BX1UV6\\n2dd8afe4-32a3-4a26-a290-99d06390dc09,U02U34YJ8C8,,,\"hmm, i\\'ve got an M1 and i remember having issues with DS libs like pandas and matplotlib. However, I pip installed those packages without a problem a week ago.\",1642551977.086500,1642560079.090800,U02S8K9JBD0\\n0decde7d-d641-4e9b-b0b9-901be86ff292,U029DM0GQHJ,,,<@U02TMP4GJEM> i had to clone the repo after having installed terraform and performing basic gcp setup as was instructed,1642532204.017900,1642563574.091800,U029DM0GQHJ\\nd9e0e8a4-a95f-409a-ac6e-8ef212a7e0f4,U029DM0GQHJ,,,then i changed the working directory to terraform and then performed all the steps and it worked like a charm,1642532204.017900,1642565041.092100,U029DM0GQHJ\\n0d5d7ccf-9d6e-497e-b448-9b9b54708a0d,U02U34YJ8C8,,,i have yet to setup docker,1642551977.086500,1642565112.092300,U029DM0GQHJ\\nfd0f8b01-d34c-4300-81dc-ca42ca1a586c,U02QZN0LSBT,,,This is some amazing work. God bless you. :raised_hands:,1642548032.081900,1642569595.093700,U02CER9BGRJ\\nf48d2151-b057-49a0-9528-9b0c979ca469,U02U88WH7D0,,,I don\\'t have access to international payment gateway and being student our bank doesn\\'t provide visa or master card that will work with these to get google credit access.. are there any alternative for those like me who don\\'t have any access to GCP. if yes what should i do,1642511203.454300,1642571649.096100,U02TAU2PR7U\\n8fdc4f13-4000-40a0-944d-230681c60aa9,U02T1BX1UV6,,,<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642527616000800?thread_ts=1642523612.482900&amp;cid=C01FABYF2RG>,1642556934.089100,1642571893.096300,U02T9550LTU\\n4575c2d5-e42e-4d47-8117-0fe230a8f080,U02T1BX1UV6,,,\"interestingly, this helped me connect on jupyter but not gitbash for queries. I used command prompt and it worked but on git bash it freezes. so try running it on command prompt\",1642556934.089100,1642571928.096600,U02T9550LTU\\n6736dfc1-3d76-478b-b41d-8035e0cc89e7,U02UQPA35DX,,,sama2,1642438962.240800,1642572322.097500,U02B5AM1ZT4\\nc3158425-ca8a-4393-b5ee-33938baadaae,U02R77VC12N,,,\"Note the space between code and the dot. I.e. it\\'s \"\"code .\"\"\",1642546559.066600,1642572905.098000,U01AXE0P5M3\\ne5002b4f-9c63-4ed0-9ac9-c1360d99ac86,,,,we can create gcp account using bank details instead of credit card right ? Because I didn\\'t have a card,,1642573767.099100,U028TUG4B9A\\nbee6e0bf-b481-4571-a86c-82bed49949ed,U02QZN0LSBT,,,thank you for this :slightly_smiling_face:,1642548032.081900,1642573879.099300,U02R6AMCCBE\\n4119eb36-7db2-4ca4-9864-67210317d68e,U02U88WH7D0,,,\"I dont have credit card, just debit card. Mastercard and visa, none of them allow it. Not only GCP but IBM cloud same issues.\",1642511203.454300,1642574010.099600,U02U88WH7D0\\nb2beca84-cdb2-4293-b8a9-a949cf8a7dbc,,4.0,,\"Sorry folks, I think somebody already asked the question but I can\\'t find the thread: is the class on Terraform already available? because in the one on youtube only the GCP setup part is presented\",1642574514.102300,1642574514.102300,U02QGJE2AUE\\n883a0a08-bc80-4518-a0fd-e88f8ceb6009,U02QGJE2AUE,,,We haven\\'t uploaded it yet. Soon =),1642574514.102300,1642574656.102400,U01AXE0P5M3\\nedc0a2b4-37b8-4722-9c60-1cacdcef9879,U02QZN0LSBT,,,Yes please send us a pull request!,1642548032.081900,1642574702.102600,U01AXE0P5M3\\nf3cd953e-1c08-45ee-a8be-882221675624,U01AXE0P5M3,,,No answers please! I\\'ll delete it here <@U02T01VH7C0>,1642516682.463800,1642574942.102900,U01AXE0P5M3\\n4177c59a-4f28-49a1-a4ea-c77a5690ed11,U01AXE0P5M3,,,\"<@U02TC8X43BN> \\n\\nQ2 yes, after\\nQ5 exactly \\nQ6 the name\\n\\nI\\'ll update the instructions to make it clearer\",1642516682.463800,1642575005.103300,U01AXE0P5M3\\nab602c1b-9ac0-4ea9-a4b1-beedcad4d156,U02T1BX1UV6,,,\"thanx <@U02T9550LTU> this helped, true git bash freezes after inserting root pwd....but CMD prompt accepts to go ahead at that point\",1642556934.089100,1642575153.103800,U02RTJPV6TZ\\n069fdfb8-747c-4260-bf77-848d0823abbb,U02QGJE2AUE,,,thanks Alexey!!! I am really looking forward to it :),1642574514.102300,1642575236.104100,U02QGJE2AUE\\n03524e98-b81b-468c-a013-c12d84b56f27,U02T96HEARK,,,\"I tried out all of these, but IDK how my original command now works perfectly after starting it from scratch. :joy:\",1642523808.486200,1642575457.104700,U02T96HEARK\\nb510b427-0a20-45d6-af40-a11d3367c47f,,,,\"For activating service account on cloud VM, I had to use `--key-file=$GOOGLE_APPLICATION_CREDENTIALS` on 368.0.0. <@U01AXE0P5M3>\\n\\n```(base) ahairshi@de-zoomcamp:~/.gc$ gcloud -v\\nGoogle Cloud SDK 368.0.0\\nalpha 2022.01.07\\nbeta 2022.01.07\\nbq 2.0.72\\ncore 2022.01.07\\ngsutil 5.6\\nminikube 1.24.0\\nskaffold 1.35.1```\\n```(base) ahairshi@de-zoomcamp:~/.gc$ gcloud auth activate-service-account -—key-file $GOOGLE_APPLICATION_CREDENTIALS\\nERROR: (gcloud.auth.activate-service-account) unrecognized arguments: -—key-file```\\n```(base) ahairshi@de-zoomcamp:~/.gc$ gcloud auth activate-service-account\\nERROR: (gcloud.auth.activate-service-account) argument --key-file: Must be specified.\\nUsage: gcloud auth activate-service-account [ACCOUNT] --key-file=KEY_FILE [optional flags]\\noptional flags may be  --help | --password-file | --prompt-for-password```\\n```(base) ahairshi@de-zoomcamp:~/.gc$ gcloud auth activate-service-account --key-file=$GOOGLE_APPLICATION_CREDENTIALS\\nActivated service account credentials for: [dtc-de-user@dtc-de-course-339017.iam.gserviceaccount.com]```\\n\",,1643098704.041700,U0290EYCA7Q\\n28868D53-25B2-4157-B8A0-8FFD3C796422,U02TB5NK4FL,,,\"<@U02U34YJ8C8>, thanks for asking. Deleting all the containers actually solved my issue. I faced another issue when trying to network both containers but I think it was due to the fact I was issue a different port tan 5432. I am now starting all over again to see if it works. I can’t count the number of hours I have already spent on week 1!!!!!\",1642975558.217600,1643101240.046700,U02TB5NK4FL\\n0467a1f7-1943-43b4-9dcc-2b0939edcf68,U02SQQTC5NV,,,Not needed unless you need to save the output of any of the modules generated,1643093658.036400,1643102035.047100,U01DHB2HS3X\\n,USLACKBOT,8.0,tombstone,This message was deleted.,1643102431.051000,1643102431.051000,USLACKBOT\\n38fb36a0-13b6-41c4-b5e5-2c23db48c233,USLACKBOT,,,I have also my Postgres container running. I have checked the transaction using port 5432 and I have got the following transactions. I do not know if I am supposed to kill all those transactions. The difficulty is I don’t know what each transaction relates to,1643102431.051000,1643103167.053600,U02TB5NK4FL\\ncb266157-a113-4be6-817c-d4114d34ec38,,1.0,,\"Hi <@U01AXE0P5M3>, what\\'s the python library to used to make a directory on your computer web like ? You did it in one of the videos\",1643103442.055500,1643103442.055500,U02TMP4GJEM\\nba37a303-c476-463b-b367-43c97cdcdba7,USLACKBOT,,,\"If you followed along with Alexey, then most likely you created a \"\"network\"\" for the containers to run on. I believe Alexey used \"\"pg-network\"\" when creating both the container for pgAdmin and the container for Postgresql.\",1643102431.051000,1643103535.055600,U02QPBZ3P8D\\ne78f1159-be0b-4f4a-b6c4-f015f5ffa8bc,USLACKBOT,,,This was the Dockerfile definition for pgAdmin,1643102431.051000,1643103595.055800,U02QPBZ3P8D\\n148c5c4d-d485-45b9-aec9-afa0cfd2b75f,USLACKBOT,,,And this was the definition for Postgresql,1643102431.051000,1643103626.056200,U02QPBZ3P8D\\nf491ffaa-3b8d-4a8f-a05f-cac9e757ebec,USLACKBOT,,,You can see that both definitions have `--network=pg-network`,1643102431.051000,1643103641.056600,U02QPBZ3P8D\\nf4961a43-1bbd-4178-b16f-2da556974632,U02TMP4GJEM,,,\"It\\'s\\n\\nPython -m http.server\",1643103442.055500,1643103708.056800,U01AXE0P5M3\\n533a0493-2eb2-4ba1-8715-af4556b22030,USLACKBOT,,,\"However, when connecting to the Server in pgAdmin, you need to specify the value in \"\"name\"\", so something like below\",1643102431.051000,1643103781.057000,U02QPBZ3P8D\\nc8062c54-b2d8-4dba-b6fb-916da94b3e17,USLACKBOT,,,Alexey started explaining this at 6:20 -&gt; <https://youtu.be/hCAIVe9N0ow?t=380>,1643102431.051000,1643103925.057600,U02QPBZ3P8D\\nc466dea9-adf3-426c-9a98-3fa299cdfa1d,USLACKBOT,,,<@U02QPBZ3P8D> thanks a lot. I will follow along again. I had already gone through the entire video. Then I had an issue and I had to go back and forgot that Alexey had this issue.,1643102431.051000,1643104839.058300,U02TB5NK4FL\\n13db9cd4-5695-4792-8349-e9ed6f5d4e0e,,4.0,,Hello. I\\'m having issues running the Jupyter notebook on vscode. Are we supposed to install jupyter in the container for it to run because it keeps wanting me to select a kernel?,1643105031.060500,1643105031.060500,U02QY444V8E\\nc42a5611-bf0b-4908-8bcb-6cae55ce0f3d,U02QY444V8E,,,\"In Jupyter, a kernel refers to the Python environment you want to use.\\n\\nAre you using Conda or any other Python environment manager? If so, I strongly recommend you do. You can check out the guide from the ML zoomcamp here: <https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/01-intro/06-environment.md> . I also have a gist with a conda cheatsheet here: <https://gist.github.com/ziritrion/8024025672ea92b8bdeb320d6015aa0d>\\n\\nOnce you set up conda or any equivalent, create a Python 3.9 env with all the needed dependencies (pandas, etc), restart VSCode and when trying to run cells of a notebook in VSCode, select the environment you created.\",1643105031.060500,1643105560.060600,U02BVP1QTQF\\na092f4d9-1a99-4302-a370-1b875c2f574a,U02QY444V8E,,,\"I\\'ve already set up conda a while back since I usually use jupyter notebooks but this time with Docker, no kernels shows up. Which is why I\\'m wondering if I have to install jupyter in the container for it to find the kernel\",1643105031.060500,1643106143.060900,U02QY444V8E\\n513aa8fe-8119-43e6-a078-95c6bf33a8b2,U02DY0L6PHV,,,You should check this solution <https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642693557370600?thread_ts=1642615746.189300&amp;cid=C01FABYF2RG>,1643055068.463500,1643106750.061400,U02S82E4N4S\\n5823944c-f8ed-4046-b2f1-7f427c333c46,U02QY444V8E,,,I just open the 2_docker_sql folder mauall through vscode instead of the terminal it seems to be working now,1643105031.060500,1643107095.061900,U02QY444V8E\\n6a8e6a17-1a28-4517-ae16-b9c4e464328d,U02QY444V8E,,,Thank you for the github link and help :ok_hand:,1643105031.060500,1643107132.062100,U02QY444V8E\\n9d4dfbc6-f905-45b0-a55a-e52ebc0a1a41,,7.0,,\"Hello everyone. I have been getting this error after I tried creating docker network and running this:\\n```docker run -it \\\\\\n  -e POSTGRES_USER=\"\"root\"\" \\\\\\n  -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n  -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n  -v /c/Users/44784/Documents/Projects/docker-example/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n  -p 5431:5432 \\\\\\n  --network=pg-network\\n  --name pg-database\\n  postgres:13```\\nIt doesnt seem to find my collection in the db\",1643107785.064400,1643107785.064400,U02TMEUQ7MY\\n6ff74ae7-c377-4f03-841c-f3342f98c743,U02TMEUQ7MY,,,Maybe you inserted it with a different name? Try \\\\dt to see what you have there,1643107785.064400,1643107895.064700,U01AXE0P5M3\\ne7c9ee6e-3200-45eb-8f7c-320801b13685,U02TMEUQ7MY,,,Not seeing anything here,1643107785.064400,1643108216.065100,U02TMEUQ7MY\\nF5E00C41-7E57-4C30-89C0-02D2E97F0198,U02TB5NK4FL,,,I spent about 15 hours on week 1 lol. It\\'s a lot. I have made some very rough notes on my GitHub if it helps. ,1642975558.217600,1643108545.067500,U02U34YJ8C8\\nDA3CBBA3-8893-4DE3-B202-5B26B85A4718,U02TMEUQ7MY,,,Did you run python script to load data? Or Jupyter notebook. ,1643107785.064400,1643108802.069200,U0290EYCA7Q\\n0b85c9d5-38a5-4cd8-9b18-473b0a77ad87,,1.0,,When we `pip install sqlalchemy` etc. is it best to install inside a virtual environment?,1643109691.071100,1643109691.071100,U02QY444V8E\\nf6bfe742-d59f-4b93-a3ef-ceb11293f917,,13.0,,\"winpty docker run -it\\\\-e POSTGRES_USER=\"\"root\"\"\\\\-e POSTGRES_PASSWORD=\"\"root\"\"\\\\-e POSTGRES_DB=\"\"ny_taxi\"\"\\\\-v c:/Users/Ahmad Abd Elhameed/data-engineering-zoomcamp/week_1_basics_sql/ny_taxi_postgres_data:/var/lib/postgresql/data\\\\-p 5432:5432 \\\\postgres:13\",1643109720.071600,1643109720.071600,U02U8AJV6E6\\nc9fcc740-4ae2-44c8-9b62-e33ac68c4460,U02U8AJV6E6,,,I could be wrong but maybe the problem is because there isn\\'t a space between each flag and a forward-slash so it assumes it\\'s one whole command. Try that out,1643109720.071600,1643110010.072200,U02QY444V8E\\n3defff2e-c390-4cfe-a6a2-8d647c622d21,U02TMEUQ7MY,,,\"Yes, I did\",1643107785.064400,1643110356.072400,U02TMEUQ7MY\\ncf73bb1b-e8fe-49be-bf3d-29c55f8433fa,U02TMEUQ7MY,,,And it was there initially,1643107785.064400,1643110370.072600,U02TMEUQ7MY\\nbff97233-6ec7-4c0e-927b-4ca196291c93,U02U8AJV6E6,,,do you mean that,1643109720.071600,1643110424.072800,U02U8AJV6E6\\n575bf5ac-10bd-41c0-9191-43d727347cba,U02U8AJV6E6,,,\"If it\\'s a single line, you don\\'t need \\\\. Just a space is enough.\",1643109720.071600,1643110743.073300,U0290EYCA7Q\\n43510369-28ba-4e92-9bae-2f66b677c69e,U02TMEUQ7MY,,,Why not try running it again?,1643107785.064400,1643110818.073500,U0290EYCA7Q\\n23fb72c5-2523-4ad6-b3eb-c71b6fd0814b,U02QY444V8E,,,Ideally yes,1643109691.071100,1643110897.073700,U01AXE0P5M3\\n6ca0596b-813e-47e6-a498-211b199b9c0b,U02U8AJV6E6,,,\"does it mean ,I should covert all letters to lower case or something else.\",1643109720.071600,1643111639.074100,U02U8AJV6E6\\nef7bb2f0-f51c-4868-a920-4a1c5dd38049,U02U8AJV6E6,,,Just remove \\\\,1643109720.071600,1643111815.074600,U0290EYCA7Q\\n90cff5f0-c86a-42fb-a002-aee081f45d6d,U02U8AJV6E6,,,removed all \\\\,1643109720.071600,1643111900.074800,U02U8AJV6E6\\ne53e8b44-fe56-4138-8a5f-0c9cd0350390,U02U8AJV6E6,,,there is no \\\\ in image above,1643109720.071600,1643111916.075000,U02U8AJV6E6\\neba42ce2-b67c-46a1-b380-3f5612b9b1ff,,3.0,,\"I\\'m currently trying to connect to Postgres with Jupyter and Pandas since I\\'m having issues with the pgcli. When I try connect to the sqlalchemy engine I get this error `OperationalError: (psycopg2.OperationalError) connection to server at \"\"localhost\"\" (::1), port 5432 failed: FATAL:  password authentication failed for user \"\"root\"\"` but looking at the terminal the db is running. Any ideas for this issue?\",1643113249.078000,1643113249.078000,U02QY444V8E\\n382b44f6-8082-4bce-908c-418d37b6ec10,U02QY444V8E,,,Could it be because you have a different instance of postgres running locally?,1643113249.078000,1643113594.078300,U01AXE0P5M3\\n6cec6a0d-785c-4925-b1c5-1250f94048cb,U02DY0L6PHV,,,still having the same issue for my ubuntu vm.,1643022678.324100,1643113975.079300,U02TVGE99QU\\nc5fb36d2-072a-45ee-987a-6f52dacd752d,U02QY444V8E,,,Not that I know of beside the one I\\'m currently using. I\\'ve just been shutting it down and running it over and over again. Is it possible to list all instances to check?,1643113249.078000,1643114115.079500,U02QY444V8E\\n4D2C23B2-4ABC-407F-9CAE-19A07EAD5F69,U02U8AJV6E6,,,Worked?,1643109720.071600,1643114366.082900,U0290EYCA7Q\\n6d5b7dd6-b683-4c69-beb6-19ba76c98adb,,3.0,,\"Hey guys, I am quite late with the homework and have a potentially silly question. Sorry for that. In the Postgresql part of the homework what is the procedure. Running Postgesql with docker, running pgcli, running the ingestion script to populate the yellow_taxi_table, running queries, ingesting the taxi zone lookup table (in analogy to the trips table), running queries on both tables. Right? And finally storing all this code (apart from the obvious one, shown in the video) in a publicly accessible repo.\",1643114484.084700,1643114484.084700,U02CGKRHC9E\\n0f0e8850-6c8a-492d-a821-07f888e92d18,,25.0,,Still having Docker and postgres problems and I don\\'t really want to move on until I can actually connect succesfully everytime. I\\'ll post the errors in this thread if anyone can please help,1643114613.085900,1643114613.085900,U02TVGE99QU\\n22bc35fe-66df-4490-82e1-8e3f0f992fe4,U02TVGE99QU,,,\"I succesfully run the pg container within the pg-network but only after I\\'ve removed the old container with the same name. when I run it, O first get this:\\n```docker: Error response from daemon: Conflict. The container name \"\"/pg-database\"\" is already in use by container \"\"...\"\". You have to remove (or rename) that container to be able to reuse that name.```\\nso I run\\n```docker container rm pg-database```\\nand then rerun the docker run -it command and I\\'m connected.\",1643114613.085900,1643114777.086000,U02TVGE99QU\\n2864333b-4b66-4094-8e8f-27c52c865b60,U02U8AJV6E6,,,Don\\'t use whitespaces in your repository\\'s name,1643109720.071600,1643114788.086200,U02QL1EG0LV\\n52aed7e9-b1e1-40ec-a609-cdb370cd9c9e,U02TVGE99QU,,,but then I can\\'t connect to the db via pgcli or pgadmin,1643114613.085900,1643114789.086400,U02TVGE99QU\\nf6b0f8c2-8f1c-4cf6-a4ab-0f52c1f9f087,U02TVGE99QU,,,\"for pgcli I get this:\\n```dan@ubuntu:~/Documents/DE-zoomcamp/week-1-basics-setup/2-docker-sql$ pgcli -h localhost -p 5432 -u root -d ny_taxi\\nserver closed the connection unexpectedly\\n\\tThis probably means the server terminated abnormally\\n\\tbefore or while processing the request.```\",1643114613.085900,1643114830.086800,U02TVGE99QU\\n262b2176-6ed3-43c0-82b7-b17a99309d1a,U02TMEUQ7MY,,,All right,1643107785.064400,1643114839.087200,U02TMEUQ7MY\\na168e085-de3f-4ec1-8ccd-f64b31865b4c,U02TVGE99QU,,,\"and for pgadmin, I run it like this:\\n```dan@ubuntu:~/Documents/DE-zoomcamp/week-1-basics-setup/2-docker-sql$ docker run -it \\\\\\n  -e PGADMIN_DEFAULT_EMAIL=\"\"<mailto:admin@admin.com|admin@admin.com>\"\" \\\\\\n  -e PGADMIN_DEFAULT_PASSWORD=\"\"root\"\" \\\\\\n  -p 8080:80 \\\\\\n  --network=pg-network \\\\\\n  --name pgadmin-2 \\\\\\n  dpage/pgadmin4```\\nBut I just can\\'t connect to localhost:8080\",1643114613.085900,1643114873.087500,U02TVGE99QU\\ne0665207-3f24-4059-9b10-a7c497580ff5,U02TVGE99QU,,,I\\'m on a Ubuntu VM with the firewall not running,1643114613.085900,1643114884.087700,U02TVGE99QU\\n7a5e0bf1-2525-4b04-bfc4-45521eae438e,U02TVGE99QU,,,If anyone can help. that\\'d be great. if not I may restart on my host machine (windows) and try again,1643114613.085900,1643114970.087900,U02TVGE99QU\\n301f5d55-5321-4c22-9bfa-22d4dc357c79,U02TVGE99QU,,,\"can you check which all containers are running\\n```docker ps```\\n\",1643114613.085900,1643115539.088100,U02QPTB3PU5\\ne913d70a-75d7-4fb5-bf5d-25a288090b71,U02TVGE99QU,,,<@U02QPTB3PU5> I stopped them all a couple of minutes ago,1643114613.085900,1643115633.088300,U02TVGE99QU\\n5dd20030-8969-41d4-8f03-e5bbbd967354,U02TVGE99QU,,,\"these are my current containers:\\n```dan@ubuntu:~/Documents/DE-zoomcamp/week-1-basics-setup/2-docker-sql$ docker container ls -a\\nCONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS                      PORTS     NAMES\\n9d84606cabdb   dpage/pgadmin4   \"\"/entrypoint.sh\"\"         28 minutes ago   Exited (0) 45 seconds ago             pgadmin-2\\nf4c222136439   postgres:13      \"\"docker-entrypoint.s…\"\"   48 minutes ago   Exited (0) 42 seconds ago             pg-database\\ndan@ubuntu:~/Documents/DE-zoomcamp/week-1-basics-setup/2-docker-sql$ docker ps\\nCONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES```\\nbut I have to remove them everytime I run because it won\\'t let me run a container with the same name twice\",1643114613.085900,1643115716.088500,U02TVGE99QU\\n647b9245-9029-4a3f-a47c-4df8da2f226b,U02TVGE99QU,,,\"If the container is already running, there would not be a need to run it again using\\n```docker run```\\nI\\'d suggest doing docker run once, check the status using \"\"docker ps\"\", and if the status is \\'Up x hours/minutes\\', we can confirm that the container is running\",1643114613.085900,1643116070.088700,U02QPTB3PU5\\na58a7eaa-4347-456d-97f8-98f9c2263685,U02TVGE99QU,,,\"once you confirm that it is running, we can try pgcli on another terminal\",1643114613.085900,1643116109.088900,U02QPTB3PU5\\n551fe5e7-98e0-45fd-90c7-ac587a9d6ba2,U02TVGE99QU,,,docker ps returns nothing as above,1643114613.085900,1643116146.089100,U02TVGE99QU\\nc98da543-6cb6-4ec6-a375-b1b1eefdfd94,U02TVGE99QU,,,\"```dan@ubuntu:~/Documents/DE-zoomcamp/week-1-basics-setup/2-docker-sql$ docker ps\\nCONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES```\",1643114613.085900,1643116188.089300,U02TVGE99QU\\n1c269649-be88-4409-9d60-a43fbf86406f,U02TVGE99QU,,,\"ok, now you should be able to run the following without errors, since no containers currently run. After running this, check docker ps.\\n\\n```dan@ubuntu:~/Documents/DE-zoomcamp/week-1-basics-setup/2-docker-sql$ docker run -it \\\\\\n  -e PGADMIN_DEFAULT_EMAIL=\"\"<mailto:admin@admin.com|admin@admin.com>\"\" \\\\\\n  -e PGADMIN_DEFAULT_PASSWORD=\"\"root\"\" \\\\\\n  -p 8080:80 \\\\\\n  --network=pg-network \\\\\\n  --name pgadmin-2 \\\\\\n  dpage/pgadmin4```\\n\",1643114613.085900,1643116508.089500,U02QPTB3PU5\\n258f9610-8f35-4c12-baad-5f1c324681c1,U02TVGE99QU,,,same issue again,1643114613.085900,1643116921.089800,U02TVGE99QU\\nb9c2518c-99c2-4af5-88c8-dc958da2d425,U02TVGE99QU,,,\"docker ps says I have no container running but I still get this :\\n```dan@ubuntu:~/Documents/DE-zoomcamp/week-1-basics-setup/2-docker-sql$ docker run -it \\\\\\n  -e PGADMIN_DEFAULT_EMAIL=\"\"<mailto:admin@admin.com|admin@admin.com>\"\" \\\\\\n  -e PGADMIN_DEFAULT_PASSWORD=\"\"root\"\" \\\\\\n  -p 8080:80 \\\\\\n  --network=pg-network \\\\\\n  --name pgadmin-2 \\\\\\n  dpage/pgadmin4\\ndocker: Error response from daemon: Conflict. The container name \"\"/pgadmin-2\"\" is already in use by container \"\"9d84606cabdba9ad3f9fe6004eabbc73085cac889582792af78e72efd5286748\"\". You have to remove (or rename) that container to be able to reuse that name.\\nSee \\'docker run --help\\'.```\",1643114613.085900,1643116948.090000,U02TVGE99QU\\n92ba2a71-32f7-447d-b910-c97f4514cb76,U02TVGE99QU,,,docker container ls -a says that I have two exited containers (not running) with those names and I can\\'t rerun other containers with the same names,1643114613.085900,1643117025.090200,U02TVGE99QU\\n3a7f0f5b-d9b5-4960-b0b9-e5ead19da6ea,U02TVGE99QU,,,\"can you try starting it through\\n```docker start pgadmin-2```\\nand then check\\n```docker ps```\",1643114613.085900,1643117186.090400,U02QPTB3PU5\\n48bc5c6f-a25d-4bc8-a578-8d0e36111ce3,U02TVGE99QU,,,I started both containers but still can\\'t get into pgadmin and pgcli is still giving me the same error,1643114613.085900,1643117354.090600,U02TVGE99QU\\na03e12e9-deea-4826-a5e2-0c96384384f3,U02TVGE99QU,,,\"docker ps gives this:\\n```dan@ubuntu:~/Documents/DE-zoomcamp/week-1-basics-setup/2-docker-sql$ docker ps\\nCONTAINER ID   IMAGE            COMMAND                  CREATED             STATUS          PORTS                                            NAMES\\n9d84606cabdb   dpage/pgadmin4   \"\"/entrypoint.sh\"\"         54 minutes ago      Up 26 seconds   443/tcp, 0.0.0.0:8080-&gt;80/tcp, :::8080-&gt;80/tcp   pgadmin-2\\nf4c222136439   postgres:13      \"\"docker-entrypoint.s…\"\"   About an hour ago   Up 2 seconds    0.0.0.0:5432-&gt;5432/tcp, :::5432-&gt;5432/tcp        pg-database```\",1643114613.085900,1643117377.090800,U02TVGE99QU\\ne70d512f-0326-4199-b16c-11fc03432e8b,U02TVGE99QU,,,but no difference. can\\'t find localhost:8080 and pgcli still the same,1643114613.085900,1643117395.091000,U02TVGE99QU\\n00aa1144-55b8-4b90-a291-ad938164654d,,4.0,,\"Hi All, sorry for asking the out of boundary question. May I ask what\\'s the software that Alexey using to write &amp; draft? I can see that this is quite useful for us to have online meeting with colleague/client:joy:\",1643118792.092800,1643118792.092800,U02T697HNUD\\ncf896f4a-ee2e-431b-82f0-e44884421ab3,U02QY444V8E,,,Check the network and the host.,1643113249.078000,1643118863.093300,U02KZNLAEAW\\na263600c-1158-4752-990c-e056c3259c00,U02T697HNUD,,,plus one to the question :blush:,1643118792.092800,1643120748.093900,U02ULQFCXL0\\n52fa1267-c535-4521-bc63-102ad114ce6c,U02TVGE99QU,,,\"Not sure I can help tbh. Try running `pip install -U mycli`  on the VM, then try pgcli again. Did you also install pgcli with `conda install -c conda-forge pgcli`and are you trying to access pgadmin locally?\",1643114613.085900,1643121248.094400,U02U34YJ8C8\\nff8006be-ab35-4a86-9d39-b232ae23c1ac,U02T697HNUD,,,\"I’m not 100% but I think it’’s Drawboard\\n\\n<https://www.drawboard.com/>\",1643118792.092800,1643121268.094600,U02BVP1QTQF\\nBBD7AAF2-EE20-49C2-A3E0-F7C91B6F0034,U02CGKRHC9E,,,\"Yes , that more or less summarizes the requirements for Postgres part of homework . You may use pgcli or run a pgadmin container ( just that it gives better GUI ) connected to Postgres database to run your queries .\",1643114484.084700,1643121810.098600,U02AGF1S0TY\\na74af3c8-bd4e-4c34-b16f-4119c1763599,U02U8AJV6E6,,,its the user name of my computer I can\\'t change it,1643109720.071600,1643123204.099200,U02U8AJV6E6\\n1d626c10-5b9d-4947-b375-c921bb44f3c6,U02U8AJV6E6,,,still not worked <@U0290EYCA7Q>,1643109720.071600,1643123235.099400,U02U8AJV6E6\\n,USLACKBOT,1.0,tombstone,This message was deleted.,1643123809.103900,1643123809.103900,USLACKBOT\\n62e14d10-747e-4212-a8b3-618e1d8a62b3,,4.0,,\"Hi :wave: I have a specific question about how a folder is populated.\\n\\nIn week 1, we’re asked to link a volume during the build process for `postgres`. In that command we specify `ny_taxi_postgres_data` as the volume, however no data actually exists in the folder. Later on that folder is populated with many `pg_*` files.\\n\\nSo my question is, at what part of the process does this file actually get filled with data / files? Is it when the `ingest_data.py` script is run or is it when we specify the `-v` flag in the docker build process?\\n\\nJust trying to understand the flow of data here. Thanks!\",1643124144.107600,1643124144.107600,U02TEERF0DA\\n3dc1964d-572b-4f8d-b905-ea7a8936f592,,2.0,,\"Hi all, where can I find the videos?\",1643124247.108100,1643124247.108100,U02V3508KST\\n1e3ba520-c381-4fbf-838f-209083eff590,U02V3508KST,,,<https://www.youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb>,1643124247.108100,1643124511.108200,U02NSF7RYP4\\n8f6fb653-043c-4af1-b3ff-75dbfae17e5d,U02TEERF0DA,,,\"My thinking here is, when a volume is initially mounted between my local machine and the docker container the folder is created and nothing is in it. However, when I ingest the data, postgres saves the data in the folder that is linked to `ny_taxi_postgres_data` . So when the container is spun down, that ingestion data and subsequent postgres files are saved locally to be used later.\\n\\nAm I thinking about this correctly?\",1643124144.107600,1643124646.108500,U02TEERF0DA\\n68c341f6-75e5-46cc-b6bb-0e427607d01c,U02V3508KST,,,Thank you!,1643124247.108100,1643124788.108700,U02V3508KST\\n81716c5a-b3e5-4ee7-bdcf-9bb559dbe58b,U02TEERF0DA,,,\"This is how I understood it as well, I tested this theory by killing all container and spinning them back up, the ingested data was still there.\",1643124144.107600,1643125027.109300,U02SUH9N1FH\\n46b557dc-f69a-48d1-834f-8916061cb7c2,,3.0,,I cannot find an existing thread for this but is there a video missing for this week? It seems like there should be a video 2.3.*?,1643125042.109600,1643125042.109600,U025P29E8D9\\n615bf63d-e014-469c-ac21-cf957d7da485,U025P29E8D9,,,\"I\\'m not sure, the playlist does say there\\'s a hidden video but not sure if it\\'s supposed to be visible\",1643125042.109600,1643125117.109700,U02SUH9N1FH\\nd441a940-fe0f-44e4-bc16-84279302b559,U02T697HNUD,,,\"It\\'s drawboard, yes. I have a windows tablet, but since it has arm64 architecture, most of the stuff doesn\\'t work there =)\",1643118792.092800,1643125277.109900,U01AXE0P5M3\\n3eae95be-4ed3-40ed-b61f-383abab8a345,U02TVGE99QU,,,\"<@U02TVGE99QU> I\\'ve had a similar issue, I solved it by killing the network created to accommodate pg-database &amp; pg-admin. Although the containers weren\\'t running docker was still picking their names. From memory I had to run `docker ps -aq` this displays the names of historic container, select the name of the network then use `docker kill &lt;container id&gt;`. Hope this fixes it for you mate\",1643114613.085900,1643125880.110200,U02SUH9N1FH\\n4ca09c00-772d-4992-8092-1a925b287504,U02CGKRHC9E,,,\"I used the pgadmin that I have installed on the host, but could just have used the containerized version. I couldn\\'t make it work with the settings persistence solution proposed...\",1643114484.084700,1643126041.110700,U02GVGA5F9Q\\n9c8a42e9-cabb-4482-8752-1f3726c06ada,,8.0,,\"Anyone managed to make the containerized pgamin settings persistent? The volume mounting solution caused troubled, so I ended giving up on it...\",1643126156.112200,1643126156.112200,U02GVGA5F9Q\\n470c0801-56b1-412a-b092-476943ab00ac,U02TEERF0DA,,,I ended up finding this article which helped me understand <https://blog.container-solutions.com/understanding-volumes-docker>,1643124144.107600,1643126173.112500,U02TEERF0DA\\n87c3ff98-bdeb-4f77-ab3a-bddfd820ed10,U02GVGA5F9Q,,,check the name of the db you\\'re mounting,1643126156.112200,1643126260.113000,U02U2Q5P61Z\\n01a39b48-f425-4c73-a30a-a674ba079c5e,U02GVGA5F9Q,,,and make sure they corresponding to the path that you\\'re passing to the -v parameter,1643126156.112200,1643126277.113200,U02U2Q5P61Z\\n37910459-8ad1-4283-aaf0-ca079d2bb132,U02TEERF0DA,,,\"He explains how to use docker volume with a demo\\n\\n<https://www.youtube.com/watch?v=G-5c25DYnfI>\",1643124144.107600,1643126285.113400,U0290EYCA7Q\\n2ea48ae4-3fd7-42aa-acd7-1aa96d6786d1,U02GVGA5F9Q,,,I had this problem with mysql and phpmyadmin (same setup as pgadmin and postgres),1643126156.112200,1643126300.113700,U02U2Q5P61Z\\n91bce014-77a5-4bc7-a460-e30fb1daa13f,U02GVGA5F9Q,,,\"<@U02GVGA5F9Q>\\nSee the last message on <https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup/2_docker_sql>\\n\\nBefore that, you need to create the folder `data_pgadmin` .\\n\\nAlso, run the following command\\n`sudo chown 5050:5050 data_pgadmin`\",1643126156.112200,1643127020.113900,U02HB9KTERJ\\n13082c2e-89bf-4899-9b5c-054b16ef2f25,U02GVGA5F9Q,,,\"Thanks to both! <@U02HB9KTERJ>, that ownership change solved it! The folder was already created by the image.\",1643126156.112200,1643127407.114300,U02GVGA5F9Q\\n94ce2bed-4efa-4264-a158-e845ef51eedf,U02TB5NK4FL,,,\"Hi Aaron, thanks for offered. It is true that without good notes you can get easily confused. So I had to review my notes, watch the videos in slow motion not to lose a single bit, and add screenshots to my notes. I have managed to finish the docker part of week 1. This has completely drained me\",1642975558.217600,1643127944.114700,U02TB5NK4FL\\nd967b5c9-b197-4d10-a8f7-1b764be6d1b4,U02GVGA5F9Q,,,Can you please add a PR to the readme mentioning the chmod thing?,1643126156.112200,1643128216.115100,U01AXE0P5M3\\n31e2527e-bcf6-46fb-9435-43a2a05f1589,U02SZARNXUG,,,Thanks guys...I\\'m on it now,1643053383.456400,1643128533.115400,U02SZARNXUG\\n1c057314-b460-492b-8845-30db6e588e7c,U02CGKRHC9E,,,\"Thank you <@U02AGF1S0TY>, <@U02GVGA5F9Q> !\",1643114484.084700,1643129204.116100,U02CGKRHC9E\\n22929aa7-499e-46a4-a44c-c634e0c69ba5,,6.0,,\":wave: I\\'ll drop out. Didn\\'t manage to populate my postgresql with the taxi rides. StopIteration. Too much time necessary, not possible besides work. Good luck for you!\",1643129468.117200,1643129468.117200,U01PU2VRBEZ\\n4A9DAFA8-D248-445A-AC24-AB7E92D7EC40,U02U34YJ8C8,,,<https://docs.portainer.io/v/ce-2.11/|https://docs.portainer.io/v/ce-2.11/> is a nice docker based GUI solution ,1642871725.398200,1643129658.117700,U02HLE69P19\\nb3b01915-997e-4cb1-a8d0-bfcc979c7944,,10.0,,\"hi !\\nWhat am I doing wrong, any suggestions  ?\",1643130156.118500,1643130156.118500,U02SNQ890A0\\nf008a3ed-0458-4205-b8dd-d5ec75cee15b,U01PU2VRBEZ,,,Thanks for feedback!,1643129468.117200,1643130238.118900,U01AXE0P5M3\\n80d20f34-1249-4e3a-a870-7dfbaf0c88bb,USLACKBOT,,,\"here is the error code which populates after an MSI install.\\n\\nCould not load file or assembly \\'NLog, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null\\' or one of its dependencies. The located assembly\\'s manifest definition does not match the assembly reference. (Exception from HRESULT: 0x80131040)\",1643123809.103900,1643130259.119100,U02SFFC9FPC\\n2a9a62fa-a028-4fac-83e6-e6bd8661282c,U01PU2VRBEZ,,,What was the main difficulty? running docker?,1643129468.117200,1643130262.119300,U01AXE0P5M3\\n8eed6a52-b9a7-42d0-8eac-6e714ce31eb4,,2.0,,\"*PLEASE HELP.*\\nDocker Desktop wasn\\'t not functional at the beginning of this course(i have been transcribing the code scripts and note taking for when the issue is rectified to run all content of week one). However when i attempted to uninstall to reinstall an update the client is now \"\"disabled\"\"(or unable to uninstall or open) . Moreover the client is now inaccessible to utilize, any suggestions or fixes known? *OS: Windows 10 64-bit Pro. (ubuntu and git bash are installed)* i have attempted to enable the WSL 2 backend, though nothing changed.\\nI should have inquired week one for assistance, merely was occupied with my academics. _Feel free to DM_\\n\\nhere is the error code which populates after an MSI install.\\n\\nCould not load file or assembly \\'NLog, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null\\' or one of its dependencies. The located assembly\\'s manifest definition does not match the assembly reference. (Exception from HRESULT: 0x80131040)\",1643130321.120900,1643130321.120900,U02SFFC9FPC\\na741e742-eb8a-49e4-90ca-ede97fa1c9c9,U02SNQ890A0,,,\"you also shouldn\\'t have a space in your -v flag\\n\\nand please don\\'t share screenshots - copy-paste your code here as text\",1643130156.118500,1643130379.121100,U01AXE0P5M3\\n5e5f6bfd-5b97-489c-a43f-7fca2a4946a5,,4.0,,\"Has anyone managed to connect to Postgres via PgAdmin deployed on GCP? I\\'ve forwarded the ports for both (5432 &amp; 8080) so I can connect to them as local host on my machine, but still can\\'t manage to connect to the DB. It\\'s not a big issue, I can still connect via pgcli, I was just curious if anyone managed to make it work.\",1643130741.125400,1643130741.125400,U02SUH9N1FH\\n304dc6d5-2cba-486d-9565-f54f72802d88,U02SUH9N1FH,,,\"I was able to open localhost:8080 on my local, and connect to db. Even jupyter lab worked.\",1643130741.125400,1643131445.126700,U0290EYCA7Q\\nC0315763-DAD4-4B89-8F61-12C4584C24CA,,7.0,,\"Is it ok to delete the VM instance created in the week1 ..I understand that if the VM instance is stopped there may still charges being applied for the storage part which will not happen in case of deletion .But considerable effort was spend in configuring the VM , I was just wondering whether this configuration will be required for any future exercises , In that case we can keep it in stopped status \",1643131687.132000,1643131687.132000,U02AGF1S0TY\\n508c2db8-8036-43fc-9a89-53386c7ff2ca,,,,I cant seem to get the data,,1643131737.132300,U02UKLHDWMQ\\n4d59f73b-6216-44b1-b661-701f96470075,,22.0,,\"sudo docker run -it \\\\\\n  --network=pg-network \\\\\\n  taxi_ingest:v001 \\\\\\n    --user=root \\\\\\n    --password=root \\\\\\n    --host=pg-database-2 \\\\\\n    --port=5432 \\\\\\n    --db=ny_taxi \\\\\\n    --table_name=yellow_taxi_trips \\\\\\n    --url=${URL}\",1643131741.132500,1643131741.132500,U02UKLHDWMQ\\nfd241fbb-6cf3-4586-a367-ce0b38241328,U02UKLHDWMQ,,,What\\'s the error?,1643131741.132500,1643131764.132700,U02QPBZ3P8D\\n364b0633-f076-49e4-96d2-834507305fd4,U02AGF1S0TY,,,Do You have steps saved somewhere? You could simply automate it.,1643131687.132000,1643131776.133000,U0290EYCA7Q\\n967ec857-a0f8-4e34-b6b4-149ca77184aa,U02UKLHDWMQ,,,\"We would need to see the Python code. Can you post it here, in the thread?\",1643131741.132500,1643131798.133300,U02QPBZ3P8D\\n198DD6BA-F3C3-4BD1-8609-02B382DDC8DB,U02AGF1S0TY,,,<@U0290EYCA7Q> can u elaborate how ? A bash script ? ,1643131687.132000,1643131839.134500,U02AGF1S0TY\\nb2defd73-3c18-485e-b015-1a2c84f62089,U02UKLHDWMQ,,,\"Traceback (most recent call last):\\n  File \"\"/app/ingest_data.py\"\", line 19, in &lt;module&gt;\\n    df = pd.read_csv(\"\"yellow_tripdata_2021-01.csv\"\", nrows=1000)\\n  File \"\"/usr/local/lib/python3.9/site-packages/pandas/util/_decorators.py\"\", line 311, in wrapper\\n    return func(*args, **kwargs)\\n  File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\"\", line 680, in read_csv\\n    return _read(filepath_or_buffer, kwds)\\n  File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\"\", line 575, in _read\\n    parser = TextFileReader(filepath_or_buffer, **kwds)\\n  File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\"\", line 933, in __init__\\n    self._engine = self._make_engine(f, self.engine)\\n  File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\"\", line 1217, in _make_engine\\n    self.handles = get_handle(  # type: ignore[call-overload]\\n  File \"\"/usr/local/lib/python3.9/site-packages/pandas/io/common.py\"\", line 789, in get_handle\\n    handle = open(\\nFileNotFoundError: [Errno 2] No such file or directory: \\'yellow_tripdata_2021-01.csv\\'\",1643131741.132500,1643131893.134900,U02UKLHDWMQ\\ndff46f36-66a1-4cd7-8525-3b01422a0b4d,U02AGF1S0TY,,,You may need it for week 2 and weeks afterwards,1643131687.132000,1643131982.135200,U01AXE0P5M3\\ne2a18898-4917-4712-9e0b-87a4256e5e43,U02AGF1S0TY,,,<@U02AGF1S0TY> Yes.,1643131687.132000,1643132076.135400,U0290EYCA7Q\\n350F525B-2FF8-48A8-807C-FC64667B19F3,U02AGF1S0TY,,,<@U01AXE0P5M3> Thanks .Noted ,1643131687.132000,1643132145.136200,U02AGF1S0TY\\nf9cb106b-92c6-4959-b10e-572984547fc8,U02SNQ890A0,,,\"docker run -it -e POSTGRES_USER=\"\"root\"\" -e POSTGRES_PASSWORD=\"\"root\"\" -e POSTGRES_DB=\"\"ny_taxi\"\"-v c:/users/mahmut094/desktop/mydockerbuild/ny_taxi_postgres_data: /var/lib/postgresql/data -p 5432:5432 postgres:13\\ndocker: invalid reference format.\\nSee \\'docker run --help\\'.\\n<@U01AXE0P5M3>\",1643130156.118500,1643132169.136800,U02SNQ890A0\\nDCD3D11C-0933-4B2C-B691-0D12ED3AECC9,U02AGF1S0TY,,,\"<@U0290EYCA7Q> Thanks , worth exploring \",1643131687.132000,1643132183.137300,U02AGF1S0TY\\n9349fa43-ef23-4ee6-b534-cd8e214d49bf,U02UKLHDWMQ,,,I have a feeling it from my wget,1643131741.132500,1643132292.137500,U02UKLHDWMQ\\n334d59ad-878a-4e60-8d66-25805a2039d4,U02SNQ890A0,,,<https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/2_docker_sql/README.md#running-postgres-with-docker|https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/2_docker_sql/README.md#running-postgres-with-docker>,1643130156.118500,1643132300.137700,U01AXE0P5M3\\n7a7065f9-62c5-41a0-9e95-fe7135ed265d,U02SNQ890A0,,,\"windows said me that repository name must be lowercase, if my folder in Desktop, and word Desktop writed with first upper case, is it mean that i need move to another space ?\\n<@U01AXE0P5M3> please could you answer this question, it my last question (((\",1643130156.118500,1643132365.138000,U02SNQ890A0\\neebd4f6b-8614-4209-8853-3566174f54ac,U02UKLHDWMQ,,,Do you use the same Python code as in the repo or different?,1643131741.132500,1643132372.138200,U01AXE0P5M3\\n8a7a771d-3592-46d0-be7a-cc9589b4e58b,U02SNQ890A0,,,Make sure there\\'s a space before -v,1643130156.118500,1643132416.138400,U01AXE0P5M3\\nfd64d051-2537-4a6b-9851-88d8f7004058,U02SNQ890A0,,,And no space inside. Check the github repo and copy the command from there to make sure there are no typos,1643130156.118500,1643132461.138600,U01AXE0P5M3\\n95086b39-061d-403a-99b4-f6e8615d9b12,U02AGF1S0TY,,,\"I just added ssh key using gcloud cli\\n\\n`gcloud compute os-login ssh-keys add \\\\ --key-file=\"\"/Users/***/.ssh/gcp.pub\"\" \\\\ --project=\"\"dtc-de-course-******\"\"`\",1643131687.132000,1643132474.138800,U0290EYCA7Q\\n86214dbe-7440-4b31-8080-5d96d0139f18,U02UKLHDWMQ,,,I used the docker run first,1643131741.132500,1643132526.139200,U02UKLHDWMQ\\nac446576-7d6c-4373-b284-49cb5e2ded15,U02UKLHDWMQ,,,that was after building the docker,1643131741.132500,1643132547.139400,U02UKLHDWMQ\\ndac662f6-c8ce-494c-a462-63885ef0ddb0,U02UKLHDWMQ,,,is it -o or -O in your wget?,1643131741.132500,1643132562.139600,U0290EYCA7Q\\n5267371c-90c1-438f-bcb6-22fcab814988,U02UKLHDWMQ,,,it is -O,1643131741.132500,1643132617.139800,U02UKLHDWMQ\\n6e292f6c-8206-41f9-9a65-b3afd37b125f,U02UKLHDWMQ,,,\"this is the error when i used python run\\nTry `wget --help\\' for more options.\\nTraceback (most recent call last):\\n  File \"\"/home/simeon/data-engineering-zoomcamp/week_1_basics_n_setup/my_docker_sql/ingest_data.py\"\", line 67, in &lt;module&gt;\\n    main(args)\\n  File \"\"/home/simeon/data-engineering-zoomcamp/week_1_basics_n_setup/my_docker_sql/ingest_data.py\"\", line 27, in main\\n    df_iter = pd.read_csv(csv_name, iterator=True, chunksize=100000)\\n  File \"\"/home/simeon/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\"\", line 311, in wrapper\\n    return func(*args, **kwargs)\\n  File \"\"/home/simeon/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\"\", line 586, in read_csv\\n    return _read(filepath_or_buffer, kwds)\\n  File \"\"/home/simeon/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\"\", line 482, in _read\\n    parser = TextFileReader(filepath_or_buffer, **kwds)\\n  File \"\"/home/simeon/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\"\", line 811, in __init__\\n    self._engine = self._make_engine(self.engine)\\n  File \"\"/home/simeon/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\"\", line 1040, in _make_engine\\n    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\\n  File \"\"/home/simeon/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\"\", line 51, in __init__\\n    self._open_handles(src, kwds)\\n  File \"\"/home/simeon/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\"\", line 222, in _open_handles\\n    self.handles = get_handle(\\n  File \"\"/home/simeon/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\"\", line 702, in get_handle\\n    handle = open(\\nFileNotFoundError: [Errno 2] No such file or directory: \\'output.csv\\'\",1643131741.132500,1643132724.140000,U02UKLHDWMQ\\n38431f64-ea23-48ad-89c4-939f18fe1bf5,U02UKLHDWMQ,,,please paste os.system() line,1643131741.132500,1643132762.140200,U0290EYCA7Q\\nc085824d-a70a-4701-98b9-8bfa5060fc34,U02UKLHDWMQ,,,and try running it manually first - wget command,1643131741.132500,1643132794.140400,U0290EYCA7Q\\n81c69d63-e0ff-41ea-8097-0f2e28a723a6,,1.0,,Does anybody know something about new week 2 videos?,1643133019.143200,1643133019.143200,U02T65GT78W\\nd1e175b7-2352-431f-8a17-185d6b838447,U02UKLHDWMQ,,,\"os.system(f\"\"wget {url} -O {csv_name}\"\")\",1643131741.132500,1643133157.144000,U02UKLHDWMQ\\n61d3d58c-dff0-43ba-8d73-4daa3ad6591f,,7.0,,I presume no one is able to assist with the issue i posted above? <@U01AXE0P5M3>,1643133164.144400,1643133164.144400,U02SFFC9FPC\\n334a112d-648a-451c-a6da-2fe507fda1f5,U02UKLHDWMQ,,,,1643131741.132500,1643133277.144600,U02UKLHDWMQ\\n10da841d-88d6-4c08-b44d-c7c522c19797,U02UKLHDWMQ,,,\"personally, I used `df = pd.read_csv(url)` and then saved the dataframe to disk with `<http://df.to|df.to>_csv(path, index=False)` Then just used the rest of the process with getting table headers, generating schema and sending it to postgres etc.\",1643131741.132500,1643133354.145000,U02QPBZ3P8D\\n8d40fac2-e395-4e41-b661-7fef043747c0,U02UKLHDWMQ,,,\"This might unblock you, for the moment\",1643131741.132500,1643133379.145200,U02QPBZ3P8D\\nada16f75-55e3-4599-b5d3-e8e33bbcbc4a,U02SUH9N1FH,,,\"Fair enough, I\\'ll try again with my local posgres service off. GCP forwarded port 5432 to 5433 locally. I didn\\'t think this would be an issue but I\\'ll try again nonetheless. Jupyter worked fine by the way. Thanks mate :thumbsup:\",1643130741.125400,1643133485.145600,U02SUH9N1FH\\n9d4e2095-7713-4837-8832-37b4fd50ec3f,U02SFFC9FPC,,,I´m sorry but I do not know how to solve your issue. I´d suggest going the VM route so that you can continue the course regardless of the status of your local environment,1643133164.144400,1643133879.145800,U02BVP1QTQF\\ne1a407c3-5ffc-463c-bc9e-e59e9bce3a35,U02SFFC9FPC,,,\"This error looks very particular to your system, I can\\'t personally think what the problem is. Have you tried googling the error? Maybe there\\'s a thread about it on stack overflow.\",1643133164.144400,1643133881.146000,U02SUH9N1FH\\n60207332-7a3e-464e-a824-47f84440a251,U02UKLHDWMQ,,,is path a variable path,1643131741.132500,1643134020.146600,U02UKLHDWMQ\\n287b15d5-077a-443e-88ad-ccfb73a3fb1d,,5.0,,\"hello all, I’m looking for study buddies. I locate in San Francisco (PST), anyone interested in forming a study group?\",1643134255.147500,1643134255.147500,U02V5EP18PQ\\n214f0a87-f5e1-4bd0-99ea-4ffa47f906fc,U02UKLHDWMQ,,,\"no, it\\'s just the disk path to where you want to save the file\",1643131741.132500,1643134350.147600,U02QPBZ3P8D\\n77465ed6-2096-43af-8f4a-3c72d78e2222,U02UKLHDWMQ,,,I think you can just replace `path` with `csv_name`,1643131741.132500,1643134398.147900,U02QPBZ3P8D\\n3acaa472-efe9-403e-8aa0-dcb10dc618af,U02SFFC9FPC,,,\"try to use VM on GCP, not local machine, i think is resolve problem\",1643130321.120900,1643134443.148100,U02ULQFCXL0\\nde4e6bcb-4f4a-4ed0-97cc-35c712ef5d9e,U02UKLHDWMQ,,,\"So it should look like `<http://df.to|df.to>_csv(csv_name, index=False)` and it will save the .csv file in the same directory\",1643131741.132500,1643134445.148400,U02QPBZ3P8D\\n7c6cfe1d-95f7-4e02-b15e-cac1aa3998c6,U02SNQ890A0,,,\"thank you)\\n<@U01AXE0P5M3>\",1643130156.118500,1643134481.148700,U02SNQ890A0\\n2091dab1-3f34-44ad-b41d-bf620cdd0aed,U02UKLHDWMQ,,,thanks guys,1643131741.132500,1643134528.148900,U02UKLHDWMQ\\n6ca91519-881f-4f44-9b31-d605ac1565a4,U02UKLHDWMQ,,,i solved it. I closed the terminal and open another terminal then build the docker again,1643131741.132500,1643134615.149100,U02UKLHDWMQ\\n09ee7d8e-6ba8-40d9-a264-34fe1a009996,U02V5EP18PQ,,,\"hi, i am interested\",1643134255.147500,1643134692.149300,U02T64KSX2S\\n81ad3554-57e7-4864-b6ef-a7db9c090b55,U02SFFC9FPC,,,\"indeed i have researched a myriad of solutions, with zero success unfortunately. Furthermore its an open query to anyone whom might understand the issues.\",1643133164.144400,1643134778.149600,U02SFFC9FPC\\n774dbfbb-3d2a-4993-9222-201e5d848333,U02V5EP18PQ,,,Shall you be searching for physical or virtual study partners?,1643134255.147500,1643134841.149900,U02SFFC9FPC\\n8b0773e2-13ed-4987-9889-9dd11e55108f,U01PU2VRBEZ,,,\"StopIteration occurs after it populates the record. It might make you worried, but if you check the database, the count changes.\",1643129468.117200,1643135938.150200,U02HB9KTERJ\\n8bb6b738-747b-4684-879e-5779e7daebca,U02V5EP18PQ,,,virtual,1643134255.147500,1643136553.150600,U02V5EP18PQ\\ne8ededdd-1bb2-48dc-b8cc-611331cfdc3b,U02V5EP18PQ,,,I\\'m down :slightly_smiling_face:,1643134255.147500,1643136576.151200,U02U2Q5P61Z\\nee1e58d2-f00a-48b2-9344-1c56a0e082cf,U02V5EP18PQ,,,Please join <#C0305878UD9|de-course-study-group> if you are interested,1643134255.147500,1643136625.152300,U02V5EP18PQ\\ndddd8e79-d616-47e2-9070-e29d9a54707e,U02SNQ890A0,,,Did it work?,1643130156.118500,1643136778.154500,U01AXE0P5M3\\n23c38815-8493-46ca-bf54-cb39703b4b72,,9.0,,\"please is this the right command for installing docker compose in Linux\\n\\n `sudo curl -L \"\"<https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname> -s)-$(uname -m)\"\" -o /usr/local/bin/docker-compose`\",1643136845.156500,1643136845.156500,U02UKLHDWMQ\\nd32910c5-509f-41d7-a4a5-825e5d345af1,,,,\"For anyone who is still stuck at week1 homework and with docker run commands (for whatever reason, namely environment issues): move on to create a VM and use docker-compose.\\nThe previous steps to that are meant to show the concepts and some internals of Docker; the exact purpose of the docker-compose.yaml file is to avoid having separate services, i.e. it makes it redundant to try the previous efforts. If the concepts are new to you, don\\'t worry, you\\'ll grasp it soon enough, now is time to move on and you will understand things better without the pressure :slightly_smiling_face:\\nDon\\'t give up now, just follow the last steps and use your SQL skills, you are almost there!\",,1643136932.157600,U02UMV78PL0\\n7d6d903e-018b-4f7c-9080-8dc033e923df,U02UKLHDWMQ,,,\"You can go to their github and download any latest release \\n\\nI show how to do it in the configuring the Vm video\",1643136845.156500,1643136940.157700,U01AXE0P5M3\\na80f2be4-7cab-4e08-9f04-56cb779e7130,U02UKLHDWMQ,,,But I think what you have here will work,1643136845.156500,1643136961.157900,U01AXE0P5M3\\n97cafb9a-0562-4183-ae72-0c99e55953b5,U02SNQ890A0,,,\"<@U01AXE0P5M3> , I too have same trouble and git dont help me also i`m dont knew what to do... can you help me?\\n\\n$ winpty docker run -it\\\\\\n&gt;   -e POSTGRES_USER=\"\"root\"\" \\\\\\n&gt;   -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n&gt;   -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n&gt;   -v C:\\\\Users\\\\User\\\\Desktop\\\\datasciense courses\\\\my project\\\\PROgRAM\\\\ny_taxi_postgress_data\\\\var\\\\lib\\\\postgresql\\\\data  \\\\\\n&gt;   -p 5432:5432 \\\\\\n&gt;   postgres:13\\nUnable to find image \\'coursesmy:latest\\' locally\\ndocker: Error response from daemon: pull access denied for coursesmy, repository does not exist or may require \\'docker log\\nin\\': denied: requested access to the resource is denied.\\nSee \\'docker run --help\\'.\",1643130156.118500,1643137245.158300,U02U14G4W0L\\n8389b2aa-e2b6-4966-8207-0e1888e4b0d7,U02UKLHDWMQ,,,\"OK, thanks\",1643136845.156500,1643137271.158500,U02UKLHDWMQ\\nf6e2eaa9-e73f-46ec-811f-754e24d0c049,,5.0,,I wanted to ask more about `terraform destroy`. Why would we want to destroy infrastructure that we are still working on? Can we still get it back after destroying it,1643137595.161000,1643137595.161000,U02T9JQAX9N\\nba3fb5c5-5938-4ad2-8f68-81b695b9e620,U02T9JQAX9N,,,\"Sometimes, not all terraform changes can be applied as a diff. Sometimes a destroy and recreate if your infrastructure has a new definition. *No, you can\\'t get it back after destroying it*.\",1643137595.161000,1643137757.161100,U02QPBZ3P8D\\nbc274e44-74d0-4059-b357-1b1bf9f95117,U01PU2VRBEZ,,,\"<@U01PU2VRBEZ>, the very first time I ran the jupyter notebook\\'s ingest script I had different behaviors between my Ubuntu and my WSL2 Ubuntu. On the former, StopIteration didn\\'t flush the last chunk of records, resulting in less than expected ingested records. But when I ran it after containerization it didn\\'t exhibit that odd behavior and worked flawlessly...\",1643129468.117200,1643137830.161400,U02GVGA5F9Q\\nf88216aa-efaa-4536-89bd-13383f369e28,U02SNQ890A0,,,\"<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1643132461138600?thread_ts=1643130156.118500&amp;cid=C01FABYF2RG|https://datatalks-club.slack.com/archives/C01FABYF2RG/p1643132461138600?thread_ts=1643130156.118500&amp;cid=C01FABYF2RG>\\n\\nThat should help\",1643130156.118500,1643137863.161600,U01AXE0P5M3\\n732f1c40-74d9-4df3-b925-ac685d5ed705,U01PU2VRBEZ,,,\"It always happens because the code is not very well behaved, but it shouldn\\'t stop you! :wink:\",1643129468.117200,1643137881.161900,U02GVGA5F9Q\\n4c669295-44f8-43af-a5aa-0222158c011b,U02UKLHDWMQ,,,not working,1643136845.156500,1643138115.162700,U02UKLHDWMQ\\n5de4d314-f209-4689-ad4e-734b6d2c2709,U02UKLHDWMQ,,,docker-compose: command not found,1643136845.156500,1643138118.162900,U02UKLHDWMQ\\n8e68b874-07ad-4db1-87c9-ffb0ef555132,U02SFFC9FPC,,,\"My system hardware is not the issue here, the process which is analogous to any system running a windows 10 64bitOS is prone to this error, my inquiry is where in the process of installation may I have made a mistake which has dominoed to the current status?\",1643133164.144400,1643138181.163100,U02SFFC9FPC\\n3d408a0e-5614-4c95-b6ec-82b2c5d099f6,U02T9JQAX9N,,,\"So if I use the storage bucket and bigquery warehouse as an example, If I run `terraform destroy`, they are gone forever\",1643137595.161000,1643138283.163500,U02T9JQAX9N\\n49c72a3e-5aca-494f-b406-7466b0eda406,U02UKLHDWMQ,,,Please check the video,1643136845.156500,1643138655.163800,U01AXE0P5M3\\n408fc276-e154-46a0-9395-bdcdbf92a6bf,U02T9JQAX9N,,,\"Forever, usually you have a prompt asking you if you are sure you want to destroy.\",1643137595.161000,1643139055.164100,U02QPBZ3P8D\\n92761dc1-13c7-417d-a10e-24fd2a0024e4,U02UKLHDWMQ,,,can\\'t find the video,1643136845.156500,1643139944.164400,U02UKLHDWMQ\\n1f0f053e-cf63-4acb-9c63-22668afe699b,U02T9JQAX9N,,,Thanks..,1643137595.161000,1643140007.164700,U02T9JQAX9N\\n916a6aab-066f-433b-b00c-4542003cef0b,U02UKLHDWMQ,,,<https://youtu.be/ae-CV2KfoN0|https://youtu.be/ae-CV2KfoN0>,1643136845.156500,1643140598.165200,U01AXE0P5M3\\n52fd19e5-9464-4000-a374-73c4b9538761,,10.0,,\"Hello all\\nI am the VM part and i have run my containers using `docker-compose up -d`\\n\\nbut when i check the containers using `docker ps`\\ni only see one containers running, and thats the `pgadmin`\",1643140697.167100,1643140697.167100,U02QKMCV39R\\n7083de01-438f-4092-aee2-8991d7c6cf8a,U02UKLHDWMQ,,,thanks,1643136845.156500,1643140987.167200,U02UKLHDWMQ\\nee41e417-1387-4f71-a20a-33d5761e64b2,U02QKMCV39R,,,\"Run docker ps -a. This will list all running and exited containers. \\nGet the container id for pgdatabase\\nRun docker container logs &lt;container id&gt;\",1643140697.167100,1643141134.167400,U0290EYCA7Q\\ndf29d88a-0d3e-49e1-bf73-9fad727e74ff,U02QKMCV39R,,,Did that but just pgadmin running,1643140697.167100,1643141196.167600,U02QKMCV39R\\n101fdbc7-41f5-428f-920b-d70e32e6b219,U02TVGE99QU,,,nah no dice. I may just make a new VM and try to redo it more carefully as I don\\'t want to fall behind at this point. Thanks for your help everyone,1643114613.085900,1643141411.167800,U02TVGE99QU\\na1b4c83e-81e4-49e8-a389-d1c01f02268f,U02QKMCV39R,,,Check your yaml file,1643140697.167100,1643141414.168000,U02GVGA5F9Q\\n868e48f1-5a79-4d47-859b-de8f06c8ac15,U02QKMCV39R,,,\"done that, everything okay\",1643140697.167100,1643141580.168200,U02QKMCV39R\\n696f47fa-f1a6-4004-8779-2555be751928,U02TVGE99QU,,,\"No worries mate, best of luck with the rest :thumbsup:\",1643114613.085900,1643141905.168400,U02SUH9N1FH\\na6abe786-1ccd-456c-8328-82e6148ba782,U01PU2VRBEZ,,,\":pray: Thanks for your concerns!\\n:hammer_and_wrench: <@U01AXE0P5M3> Docker ran fine. <@U02HB9KTERJ> After populating with this script the database remained empty. <@U02GVGA5F9Q> Containerization did not help either. The error came along.\\n:bow: After spending several hours on the technics not working as expected on Windows computer with WSL2 I\\'ll quit. And give it another go another time with a Mac! \\n:four_leaf_clover: Wish you all success and good experiences with this wonderful course. \\n:clinking_glasses: I\\'ll enjoy the free evening!\",1643129468.117200,1643142098.168600,U01PU2VRBEZ\\nb383bf47-afe8-43f8-b905-5d9de1a9bbfb,U02QKMCV39R,,,\"Docker-compose down\\nRunning it again without -d option\",1643140697.167100,1643142210.168800,U0290EYCA7Q\\n9d944796-6dc2-4329-a44e-f5d222850b1f,U02QKMCV39R,,,\"And do you see postgres and pgadmin images on the disk?\\n\\ndocker images ls\",1643140697.167100,1643142302.169000,U0290EYCA7Q\\nc0324def-d086-40a6-b7a6-4764499787fc,U02QKMCV39R,,,\"I recall having lots of images and containers from all the coding along and experiments. There was a moment when I had to prune everything, inclusive the network we created.\",1643140697.167100,1643142456.169200,U02GVGA5F9Q\\ne46a517e-f487-4c1f-b629-018626445cab,,16.0,,\"OK this may be a dumb question, but I\\'ve just started watching video 1.3.2, Creating GCP Infrastructure with Terraform, and <@U01DHB2HS3X> has this extensive directory structure and files that I don\\'t have. What did I miss?\",1643142576.170900,1643142576.170900,U02UBQJBYHZ\\na6916b0d-d4be-4164-9383-ac945c12d11c,U02UBQJBYHZ,,,\"I don’t think you need to worry about that. Just make sure you’re in `data-engineering-zoomcamp/week_1_basics_n_setup/1_terraform_gcp/terraform` when making the changes, and you should be fine\",1643142576.170900,1643142751.171000,U02TEERF0DA\\na5ac9028-d3b0-4707-a453-3a194cd63927,U02UBQJBYHZ,,,\"There\\'s no such thing as a dumb question :slightly_smiling_face: I think Sejal\\'s style was slightly different to Alexey\\'s, she just ran through the terraform config file explaining what each config does instead of coding along. The terraform config file is on github though if you need it.\",1643142576.170900,1643142797.171200,U02SUH9N1FH\\n20d809df-c050-42c2-8e67-c3179561e670,U02T9JQAX9N,,,\"You probably don\\'t want to destroy the infra that you\\'re still working on. But when you\\'re done, you just do destroy and everything is deleted\",1643137595.161000,1643142842.171500,U01AXE0P5M3\\nff3f8306-2009-468b-b955-a1f7e97e47d0,U02UBQJBYHZ,,,\"yes that\\'s the problem, I don\\'t have a file system setup like that at all.\",1643142576.170900,1643142916.171700,U02UBQJBYHZ\\ndbe699b5-e99c-4377-9b05-55ce5ffb3348,U02UBQJBYHZ,,,I think I must have missed a previous video.,1643142576.170900,1643142946.171900,U02UBQJBYHZ\\nefa57ecc-78bf-42cc-8493-f9088663d3da,U02UBQJBYHZ,,,Maybe you need to `git pull`?,1643142576.170900,1643142968.172300,U01AXE0P5M3\\n2d503182-2896-4061-97a2-cee81220f4b6,U02UBQJBYHZ,,,Pull the whole repository?,1643142576.170900,1643142993.172500,U02UBQJBYHZ\\n61dc71f2-68f3-4ac2-803a-6d56a52f9f62,U02UBQJBYHZ,,,Yes,1643142576.170900,1643143026.172700,U01AXE0P5M3\\n3324b10c-ef52-4339-a5a8-0d7533100afb,U02UBQJBYHZ,,,\"Do you already have it? If not, you need to clone it\",1643142576.170900,1643143045.172900,U01AXE0P5M3\\n0de8f3c3-0c26-4288-98c9-a2abac515c7a,U02UBQJBYHZ,,,No I just go to the repository and look for the video links. So now you know the depths of my cluelessness. :slightly_smiling_face:,1643142576.170900,1643143088.173100,U02UBQJBYHZ\\nf743e570-48d3-4571-b9d4-1aabe0c657e8,U02QKMCV39R,,,\"not seeing any images when using `docker images ls`\\nbut i can see containers when i use `docker ps`\",1643140697.167100,1643143130.173300,U02QKMCV39R\\n88ae411a-1009-4366-b9fc-75a593c0be0b,U02QKMCV39R,,,\"It\\'s actually docker image ls\\n\\n<https://docs.docker.com/engine/reference/commandline/image/|https://docs.docker.com/engine/reference/commandline/image/>\",1643140697.167100,1643143342.173700,U0290EYCA7Q\\ne134eb6b-be0b-47ca-bedd-760f18bdb174,U02UBQJBYHZ,,,Yea probably we should have mentioned it explicitly,1643142576.170900,1643143379.174000,U01AXE0P5M3\\nA8CAE82C-AD4A-4896-AB92-D7CBF0A12A8F,U02UBQJBYHZ,,,I just copied the files from the Zoomcamp repo. Don\\'t think you missed anything ,1643142576.170900,1643143466.175100,U02U34YJ8C8\\n14429245-e073-4fdb-bef4-973c845c1a81,U025P29E8D9,,,\"~I think it\\'s missing or video 2.4 is actualy 2.3 and we are missing the real 2.4 videos~\\n\\nI just refreshed my browser tab and 2.3 is there!\",1643125042.109600,1643143486.175300,U02TC704A3F\\na5f44242-ab7c-4cbb-beb6-c945b9c5dda6,U02UBQJBYHZ,,,I have it now.,1643142576.170900,1643143631.175500,U02UBQJBYHZ\\n7053e11a-28de-48d9-993b-9bd369635b33,U02QKMCV39R,,,yes i see postgres and pgadmin on the image,1643140697.167100,1643143716.175900,U02QKMCV39R\\n505dbed7-bf29-4433-98c1-c003f0f6edef,,,,\"This keeps happening when i want to do docker-compose,\\nbut when i run separately it works fine\",,1643144372.176900,U02QKMCV39R\\nC428A838-25CA-4162-B8F8-70EE3988C0A8,,3.0,,\"Can we do a tutorial in which we set up terraform from scratch?\\nRather than using the files from the repository?\",1643144410.178100,1643144410.178100,U02U6DR551B\\nc58655d0-3415-4c78-af1b-75296bcac0c7,U02U6DR551B,,,<https://learn.hashicorp.com/terraform|https://learn.hashicorp.com/terraform>,1643144410.178100,1643145147.178500,U02ULQFCXL0\\n1d829db5-fd2f-4c1e-8dd7-25239dc75b15,U02V2G3NK8U,,,\"Yes, we extended it to Wednesday! And we will release another video explaining the queries that day\",1643086171.025400,1643145859.179000,U01B6TH1LRL\\n93EEFF45-157B-4B4B-9CAE-9B79F2FEA061,U02U6DR551B,,,\"You could, although I think the purpose of the terraform bit was to setup our infrastructure correctly for the following weeks - so I\\'d just make sure the code ultimately ends up the same as what\\'s in the vids\",1643144410.178100,1643146035.181300,U02U34YJ8C8\\n42fa2882-0e13-451b-9720-5aa932fdb5bf,U02SFFC9FPC,,,You\\'re trying to use the msi to install on WSL2? I\\'d google for the Ubuntu install with WSL2. That\\'s the route I followed with my second machine and it ran flawlessly!...,1643133164.144400,1643146428.181500,U02GVGA5F9Q\\n758d9a38-a3a5-4275-93b9-85c90a068899,,2.0,,\"I\\'ve been following the course videos and noticed that video 1.4.1 - Setting up the Environment on GCP (<https://www.youtube.com/watch?v=ae-CV2KfoN0>)  says that we can use that VM for the course. Is it mandatory or would it be recommended to create the VM ourselves or can we just use our local machine for the course, <@U01AXE0P5M3> ?\",1643147315.183400,1643147315.183400,U02QPBZ3P8D\\n1c48f1ab-8778-4926-a0e2-c44214082630,U02QPBZ3P8D,,,\"you don\\'t have to do it - it\\'s mostly for people who have troubles setting up things locally\\n\\nbut it\\'s also convenient because the internet is good there, so downloading things / uploading to gcp is way faster\",1643147315.183400,1643147392.183600,U01AXE0P5M3\\n507ec679-7bb4-4345-98fe-dd8ddac7b9be,U02QPBZ3P8D,,,I saw how many people in this channel had problems setting things up and that was the main motivation for this video,1643147315.183400,1643147460.183800,U01AXE0P5M3\\na6ef63e8-e059-460d-93cd-059a2805dad6,,11.0,,\"Hello everyone,\\n\\nIn the DE Zoomcamp 1.4.1 video, I can\\'t seem to find the PORTS option on my VS Code. Tried searching the internet but can\\'t find anything helpful. I only see PROBLEMS OUTPUT DEBUG CONSOLE TERMINAL. Anyone knows what I need to do?\\n<https://www.youtube.com/watch?v=ae-CV2KfoN0&amp;list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&amp;index=14&amp;ab_channel=DataTalksClub>\",1643147637.185000,1643147637.185000,U02RSAE2M4P\\n6b257083-5e42-422e-bf3a-22257501e0b6,U02RSAE2M4P,,,\"<https://code.visualstudio.com/docs/remote/ssh#_forwarding-a-port-creating-ssh-tunnel>\\n\\nmaybe this will help?\",1643147637.185000,1643147984.186500,U01AXE0P5M3\\n5B788CE2-8A17-4127-936A-F51D4E6D6CA7,U02RSAE2M4P,,,Did you enable ssh extension and connect to host using ssh ? ,1643147637.185000,1643148047.186800,U02AGF1S0TY\\n460e84b1-b1c9-43cf-89cd-5af1ea1f569a,U02RSAE2M4P,,,<@U02AGF1S0TY> yes I did.,1643147637.185000,1643148648.187200,U02RSAE2M4P\\n4DB95003-E929-48EE-B69E-C7A5C2E2E01F,U02RSAE2M4P,,,Did ur HostName de-zoomcamp show up in drop down when you were connecting ? ,1643147637.185000,1643148762.189000,U02AGF1S0TY\\nC6FE6748-1C9B-431D-BE7E-E5E7F43BAFEA,U02RSAE2M4P,,,And you can see the folders inside VM on vs code after connecting ?,1643147637.185000,1643148814.190000,U02AGF1S0TY\\n076b19ed-6679-4f7f-8675-5af760dab6d4,U02UBQJBYHZ,,,Works! I understand it and i have a bucket on us-east4. :slightly_smiling_face:,1643142576.170900,1643148835.190200,U02UBQJBYHZ\\n1991205c-43a3-4910-889c-d2edb9967370,U02RSAE2M4P,,,<@U01AXE0P5M3> works perfectly. Thanks for the link.,1643147637.185000,1643148838.190400,U02RSAE2M4P\\n3ee33ea0-2542-40c9-91a7-43eda7fe7fff,U02RSAE2M4P,,,Thanks <@U02AGF1S0TY> works now.,1643147637.185000,1643148851.190600,U02RSAE2M4P\\n6F2F5678-8B3D-40F8-8923-03A9F9F2830A,U02RSAE2M4P,,,Anything you did in particular ? ,1643147637.185000,1643148888.191100,U02AGF1S0TY\\n1a27c6e8-9be8-45f4-bffc-1fde2422efbf,U02RSAE2M4P,,,I followed the link Alexey shared and forwarded the ports as the instruction specified.,1643147637.185000,1643149106.191900,U02RSAE2M4P\\n62728fce-9d42-4da5-af58-c6d2299ecc75,U02RSAE2M4P,,,And I also noticed I had to stop using some of the ports locally first for them to be assigned remotely.,1643147637.185000,1643149150.192800,U02RSAE2M4P\\n1633757B-01EF-41AD-8684-E38D7C248A1B,U02RSAE2M4P,,,Thanks for the inputs .,1643147637.185000,1643149230.194300,U02AGF1S0TY\\n9ef41f3f-971d-4fa0-b216-5a82e2874b6d,U02UBQJBYHZ,,,\"Just had a chance to look at this thread. Glad to read everyone’s answers and that you managed it :sweat_smile: Also, keep coming up with questions like these. It helps us improve our material for the upcoming weeks :slightly_smiling_face:\",1643142576.170900,1643149753.201700,U01DHB2HS3X\\n3fee6b7e-6b69-4570-9766-cb2597266da5,,5.0,,\"Hello,\\nI have a question regarding Question #6: Most expensive locations (title from `howemork.md` ) | Most expensive route (title from `google forms` )\\n```What\\'s the pickup-dropoff pair with the largest average price for a ride (calculated based on total_amount)? Enter two zone names separated by a slashFor example:\"\"Jamaica Bay / Clinton East\"\"If any of the zone names are unknown (missing), write \"\"Unknown\"\". For example, \"\"Unknown / Clinton East\"\".```\\nCould you please point me on how to get one of the options we were provided in google form?\\nThe particular question is what is meant by the `largest average price`?\\nI have a few different solutions, but all of them lead me to the same result: I always end up with the most expensive route - `Lenox Hill East / Upper East Side North` which is technically has the largest average price - only one trip has this top expenses.\\n\\nI would share my current code, to provide my thoughts and the way of my thinking but I\\'m not sure if it is legal. In case it\\'s fine I\\'d be glad to share it, <@U01AXE0P5M3> may I share the code or better not?\\n\\nI\\'ve searched over the channel, was trying to find if someone has asked the related question. This is the only result <https://datatalks-club.slack.com/archives/C01FABYF2RG/p1643018016296700> which is not useful for me\",1643149862.204600,1643149862.204600,U01UKRH6VGT\\n2e73c04b-ee5b-4ca0-944f-17b2d8fafae2,,4.0,,\"Question about the homework: I\\'ve followed along with the videos and wrote the scripts by hand and I\\'ve finished watching all the videos and I am noticing that maybe the goal was to just clone the repo initially and use that as a base for running all the commands. Anyway, regarding the homework, there\\'s a question at the end saying that *\"\"Your code (link to GitHub or other public code-hosting website). Remember: no code - no scores!\\xa0**\\n\\nShould I answer with the link of my current repo &amp; implementation, or is it mandatory to use the code you provided? I\\'m a little confused about what code you\\'re expecting to see. Do you want SQL queries for some questions or .. ?\\n\\nPS: My repo is 95% the same code as yours and it has a very extensive README.md with all the notes and things I learned along the videos. <@U01AXE0P5M3>\",1643149891.205300,1643149891.205300,U02QPBZ3P8D\\nf106766b-e968-4ff7-aabc-c58e39d75f4b,U025P29E8D9,,,\"Hi, like mentioned in Week 2\\'s intro session yesterday, we were delayed in uploading section 2.3, i.e. the airflow workshop, and we’re in the process of updating the material now (some of which is already there).\",1643125042.109600,1643149911.205400,U01DHB2HS3X\\nf22a1803-b899-4412-a441-92f35a2cd1f3,U02T65GT78W,,,\"Hi, like mentioned in Week 2\\'s intro session yesterday, we were delayed in uploading section 2.3, i.e. the airflow workshop, and we’re in the process of updating the material now (some of which is already there).\",1643133019.143200,1643149960.205800,U01DHB2HS3X\\n91b0a8ec-8a47-4e9b-8afc-c0115f7fe593,U02U6DR551B,,,I have added reference links in the readme’s to other code-along tutorials,1643144410.178100,1643150035.206100,U01DHB2HS3X\\n86F64979-872B-4454-9BB9-CA9F2FAEED1F,U02QPBZ3P8D,,,Its the sql code that you used to answer questions need to be uploaded ,1643149891.205300,1643151787.207900,U02UM74ESE5\\n50329b1c-fd40-46fd-a72d-5b911b4354a6,U02T2DX4LG6,,,Fixed it ! Thank you <@U02U34YJ8C8> that was it :slightly_smiling_face:,1642787354.152800,1643152877.208500,U02T2DX4LG6\\n8135805d-0681-4e2e-bd28-e68f658a2325,U02QPBZ3P8D,,,I just put a modified `homework.md` of my work and code in a <https://pastebin.com/|pastebin> link that expires in a few weeks. I did not want to go through the effort of making a whole new repo,1643149891.205300,1643153856.209900,U02TEERF0DA\\n39e0b51d-a6bd-4e94-9288-d117ce22ceed,,2.0,,\"Anyone having problems with Airflow Webserver? When I go to 0.0.0.0:8080 it shows ERR_ADDRESS_INVALID, like there\\'s nothing there. I already tried to change ports in my docker-compose file and still doesn\\'t works.\",1643155277.212700,1643155277.212700,U02TC704A3F\\n9b3b5513-4938-4610-8721-efe2c2e66d06,,1.0,,\"i think i did everything right but getting this error\\n\\nERROR: The Compose file \\'./docker-compose.yaml\\' is invalid because:\\nUnsupported config option for services: \\'pgadmin\\'\",1643156229.215500,1643156229.215500,U02UKLHDWMQ\\ndd4612ab-88f8-42a6-8f63-e6be3354d4d8,,5.0,,\"I started the course yesterday, so I\\'m late trying to watch all the videos and implement everything.\\nAfter I built the docker data ingestion image and created the network, when I execute\\n`docker run -it \\\\`\\n  `--network=pg-network \\\\`\\n  `taxi_ingest:v001 \\\\`\\n    `--user=root \\\\`\\n    `--password=root \\\\`\\n    `--host=pg-database \\\\`\\n    `--port=5433 \\\\`\\n    `--db=ny_taxi \\\\`\\n    `--table_name=yellow_taxi_trips \\\\`\\n    `--url=${URL}`\\nThe CSV file is not downloaded.\\nThe terminal freezes with this message: \"\"--2022-01-26 00:17:33--  <http://172.24.208.1:8000/yellow_tripdata_2021-01.csv>\\nConnecting to 172.24.208.1:8000...\"\"\\n\\nDoes anyone know how to solve this?\\n<@U01AXE0P5M3>\",1643156346.217000,1643156346.217000,U030F0YHDAM\\n6cdc06ec-6583-40e0-80b4-d4ee0b047217,U02TC704A3F,,,\"If anyone else run into this problem, is more simple than I\\'m proud of.\\n\\nGo for <http://localhost:8080> not to 0.0.0.0:8080.\",1643155277.212700,1643156450.217100,U02TC704A3F\\nfb5d4301-352f-4de0-93a4-25cf3e43d943,U030F0YHDAM,,,this ${URL} is a variable. What value did you assigned to it?,1643156346.217000,1643156518.217300,U02TC704A3F\\n4de91196-f499-4881-8be2-9dc9efbb35c4,U02UBQJBYHZ,,,\"Just wondering...I didn\\'t pull the githuv repo... my intention is to build mine as the course progresses. So I created an empty repo and made a new directory for terraform, created the main to file and copied the setup into it. I hope this won\\'t hinder something later on??\",1643142576.170900,1643156528.217600,U02SZARNXUG\\nded35df3-335c-4a29-85d0-b87ad394f91c,U030F0YHDAM,,,the problem is that you\\'re assigning a local IP (Alexey\\'s machine) to the URL environment variable. You\\'ve got to use the previous setting of that variable in the readme file. Must point to the cloud version.,1643156346.217000,1643156750.218000,U02GVGA5F9Q\\nb8f43c69-b644-4962-bc45-d53fc47b4c19,U030F0YHDAM,,,\"Thanks, it worked!\\nI probably somehow copied the wrong URL address.\\nWhen I changed to AWS S3 correct address, it started the download!\",1643156346.217000,1643157015.218200,U030F0YHDAM\\n4c7f3a22-e8be-4c99-943c-12324e969a52,U02UKLHDWMQ,,,\"Solved, I had to insert version: \"\"2\"\" at thr top of my yaml file\",1643156229.215500,1643157228.218400,U02UKLHDWMQ\\n623331bc-0b71-4c8c-8545-ca76d6f7f30a,U030F0YHDAM,,,I copied the command from README.md and didn\\'t pay attention,1643156346.217000,1643157703.219600,U030F0YHDAM\\n24461b9a-257d-466e-ae27-9a258d03a7df,,8.0,,Only pending the terraform part in my hw.:disappointed:  But I haven\\'t figure out how to fix this. Please help me :pray: I get this error:,1643158900.221700,1643158900.221700,U02T2DX4LG6\\nc1fc4f23-6ef2-4cec-a6a6-762f3a1b25f5,U02T2DX4LG6,,,Should I give create access to the service account then? I get this error when I use terraform apply,1643158900.221700,1643158938.221800,U02T2DX4LG6\\n76d609f8-dd84-4127-85c5-c447a33b4b8f,,3.0,,\"I have been experiencing issues with the volumes when trying to run “docker-compose up airflow-init”. I am seeing this error: Error response from daemon: invalid volume specification: ‘/Users/justinpak/Desktop/Python/Study/Data Engineering Zoomcamp/Week 2: Data Ingestion/airflow:/sources:rw’.\\n\\nThis specific issue is occurring due to the volume section under the airflow username and password but it also looks like an issue for the ./dags, ./logs, and ./plugins as well. It appears that the build command ran without any issues. Has anyone else experienced this/ knows that went wrong?\",1643159253.225200,1643159253.225200,U02TBTX45LK\\nef407e34-4b1f-4e41-ad8b-c5feb865e6a0,U02T2DX4LG6,,,Have you added the name of your project to the <http://variables.tf|variables.tf> file?,1643158900.221700,1643160154.226500,U02U55EC4EA\\n6bdb2ace-0091-4be9-98cb-fb55ec40353f,U02T2DX4LG6,,,I was having the same issue and that solved the problem,1643158900.221700,1643160167.226700,U02U55EC4EA\\n0257b81e-895a-4988-9152-1668d15a1883,U02TBTX45LK,,,<@U02TBTX45LK> not sure if this helps. I got mine up and running and noticed I don\\'t have the `:rw` following my sources in the `docker-compose.yaml` file.,1643159253.225200,1643167696.227700,U02UAFF1WU9\\n078d2a93-87c2-4c44-b425-89120a55deea,U01UKRH6VGT,,,I’m on the same boat as you. Not sure if we should pick the second most expensive route then,1643149862.204600,1643169682.228600,U02UB19EN2G\\n4491f79d-0434-46ee-b59f-2bd11e7d91e6,U02T2DX4LG6,,,\"<@U02U55EC4EA> thank you. I am not, I am passing the project id in run time.  .Did you add an additional variable for project name?\",1643158900.221700,1643170201.228800,U02T2DX4LG6\\nbdcda2d8-9d07-423d-8537-ae2623c73d93,U02T2DX4LG6,,,\"Also, make sure that the service account you created has \"\"Admin\"\" rights to create bigquery buckets and datasets\",1643158900.221700,1643170497.229100,U02QPBZ3P8D\\n85fe17e9-1a7a-415e-a790-e307fc664624,U02T2DX4LG6,,,\"Yes, I added it as follows in the <http://variables.tf|variables.tf> file\\n```variable \"\"project\"\" {\\n  description = \"\"dtc-de\"\" &lt;----your project name\\n}```\",1643158900.221700,1643170556.229300,U02U55EC4EA\\n738a7c86-50b3-489c-a1ec-d7ee825aaba8,U02T2DX4LG6,,,\"Watch it around 17:10 or before, if you didn\\'t set up the service account.\\n<https://youtu.be/Hajwnmj0xfQ?t=1033>\",1643158900.221700,1643170607.229500,U02QPBZ3P8D\\ndc67aafd-27f3-4320-8395-b0def84aa25f,U02T2DX4LG6,,,\"Thankyou <@U02QPBZ3P8D> and <@U02U55EC4EA>. Yes, I didn’t had the storage object admin permission, that fixed it!\",1643158900.221700,1643171038.229800,U02T2DX4LG6\\nB2ED0C90-7DA2-4083-BE65-0C555A1A2A20,,2.0,,Is anyone from Indonesia interested in virtual study group? :flag-id: ,1643173164.231400,1643173164.231400,U02H6BH5VSP\\n6273d9b8-1bd5-4965-8353-c34a206a9c86,U02H6BH5VSP,,,wah sangat berminat bang,1643173164.231400,1643173425.231600,U02T8ANTJGM\\nb506d4c8-6de7-4534-96f9-84373e062498,,12.0,,\"Anyone tried running airflow containers on Ubuntu? As mentioned in video, if there are issues with memory, web-server restarts. I see error \"\"pid ... terminated due to signal 9\"\" which means memory issue.\\n\\nI tried using mem_limit and mem_reservation specifically for webserver service, it still fails?\",1643173811.236300,1643173811.236300,U02H0GUC7ML\\n30c9dc18-7868-4114-90ac-aea2bfea59ce,U02H0GUC7ML,,,,1643173811.236300,1643173862.236500,U02H0GUC7ML\\nf1869f37-b872-423a-9969-bcd013d83c07,U02TC704A3F,,,Or 127.0.0.1:8080,1643155277.212700,1643174525.237100,U02QL1EG0LV\\n0d64ae7e-c843-4ab5-8e3c-88d39eb9492f,U02TTSXUV2B,,,\"Hi Divya, I have not changed <http://main.tf|main.tf>. But if this is the first time you are running terraform plan, you would be adding al the resources\",1643075363.020200,1643174958.237400,U01DFQ82AK1\\n134938ff-0b0c-4414-b621-178b91bf2c9f,,,thread_broadcast,\"I realaized the problem with video. Not sure how did it happen, but will fix that in future vidoes. For now you can find the commands in  video description\\n<https://www.youtube.com/watch?v=VhmmbqpIzeI>\",1643075363.020200,1643176424.237700,U01DFQ82AK1\\n3721fe76-08d0-492a-949f-c236a66c105a,U01UKRH6VGT,,,Please send me your query and I\\'ll run it against my database,1643149862.204600,1643177231.238300,U01AXE0P5M3\\nbac47976-359d-47c1-a1d9-e3b2a3c3882f,U030F0YHDAM,,,\"Oh right, it should be updated\",1643156346.217000,1643177280.238500,U01AXE0P5M3\\ne0f5fc66-9c16-417d-9d80-24378183940d,U02UBQJBYHZ,,,It shouldn\\'t,1643142576.170900,1643177315.238700,U01AXE0P5M3\\n020added-a444-4a81-a270-89cd0155a3a9,U02QPBZ3P8D,,,\"That works. Just as a side note, creating a repo is the same level of efforts =)\",1643149891.205300,1643177382.238900,U01AXE0P5M3\\n16e8543e-7e47-4d4d-9d88-77d4a767a948,U02H0GUC7ML,,,<@U01AXE0P5M3> <@U01DHB2HS3X> can you please help with this? How can I set memory limit for docker engine on Ubuntu?,1643173811.236300,1643178466.239400,U02H0GUC7ML\\nf8fd6514-2329-4650-be5d-192383a6420e,,4.0,,\"I just wanted to say a big, big, big thank you :raised_hands: for all of those who contributed their notes for Week 1. After having taken some for myself I know how time-consuming they are - so, so slow. Also, for people who pointed out their own particular struggles in their notes (technical and otherwise) and what worked for them this was really helpful. I hope you continue to share - it is very useful for people like me trying to remember what went on. Everything is a bit of a blur right now and my mind is mush.\",1643179603.242900,1643179603.242900,U02U5SW982W\\n33373014-bf08-4af3-9082-b6bd0617465c,,,,Hello everyone,,1642406796.126300,U02U2U0N1JS\\n3e4878cd-3caa-4c62-a8c4-0aaa13237c53,U01MNATQ03T,,,\"For this course, it will be GCP.\\nI\\'ll advise you to follow through with GCP. first. later you can transform knowledge to AWS.\\nThey are both Cloud platform.\\nbut IAC configurations syntax should be dissimilar.\",1642369980.061600,1642406990.126500,U02RUUJ2TV5\\nbeaab9b0-17e2-4e69-aa09-85450b885906,,,,Hi!,,1642407054.127000,U02TUV99NLF\\n5f393477-f8e2-4119-8f8f-84fb65996f53,U02Q7JMT9P1,,,\"Per session is between 1-1.5 hrs.\\nfor tomorrow it will be 1.5 hrs.\\ncheck the GitHub link\\n<https://github.com/DataTalksClub/data-engineering-zoomcamp>\",1642368547.058700,1642407071.127100,U02RUUJ2TV5\\nfb0e84af-1cfb-4b3e-a3c2-6dc73ecc33ef,,,,hello everyone! I am Shidhanta from India. Currently still studying but hoping to pick up a fresher DE job after this course.,,1642407599.128800,U02UZBJ2Q6L\\nd4ee5fcc-214f-4380-8eb7-7ca8f77dfc97,U02Q7JMT9P1,,,<@U02RUUJ2TV5> thanks!,1642368547.058700,1642407743.128900,U02Q7JMT9P1\\na822c101-f82e-4303-9cda-005719e38de2,,,,Hello Folks!! This is Sriniketh from India. An sophomore looking to learn a lot. Looking forward for this amazing course.,,1642408614.130700,U02A83NJTFY\\n901df756-84a7-4348-ac3d-8df61ee3d858,,,,\"Hello everyone ,I\\'m Ichrak Naceur from Tunsia , Im a data scientist and  i look forward to this course .\",,1642409575.132300,U02QW395UNM\\n340f3cd2-6e85-40be-937e-3987a041a395,U01MNATQ03T,,,Thanks <@U02RUUJ2TV5>,1642369980.061600,1642410261.133400,U01MNATQ03T\\nAC9D9CED-1747-469E-BB2A-12A65143F566,U02U6DR551B,,,Thanks for clearing out!,1642402844.116000,1642410351.133900,U02U6DR551B\\n7F12D0FD-F995-4245-9257-E14707B1D7DA,,,,Please cover topic on noSQL databases as well if you can. Many data engineers are required on this skill.,,1642410436.135700,U02U6DR551B\\n561fb8e7-c9c1-4553-b8a9-37fedb5cc4c7,,,,\"Hi Folks. I am Nurlan, data scientist from Kazakhstan, working in petroleum industry\",,1642410481.136500,U02UETQQ1DF\\n2bbe2022-b04e-49eb-8af3-3b2ff3aa7847,,3.0,,\"<@U01AXE0P5M3> and  having issues installing docker.\\nmy system is Windows 10 Pro.\",1642411265.139800,1642411265.139800,U02RUUJ2TV5\\nc3ef9346-e4e2-4af6-a59c-68cbfd2262d1,,2.0,,What OS is recommended and best for this course?,1642411366.140200,1642411366.140200,U02TUV99NLF\\n702d3d4a-00f0-4ec6-ac8e-127856695e68,U02RUUJ2TV5,,,May be needs some updates?,1642411265.139800,1642411440.140500,U02TUV99NLF\\nd061fddd-63ce-47fa-ac1c-3d87bffb8233,,,,Hi all. This is Varun. Aspiring data professional. Always wanted to learn data engineering and this seems like a great place to begin. Let\\'s connect - <https://www.linkedin.com/in/varun-nayyar-ml|https://www.linkedin.com/in/varun-nayyar-ml>,,1642411805.143400,U02JE8J1W8P\\n2b05066e-cc4e-4c60-ac2e-052168293f4e,U02RUUJ2TV5,,,\"yes doing that already.\\nthanks, <@U02TUV99NLF>\",1642411265.139800,1642412499.144300,U02RUUJ2TV5\\n5dafba02-a6e7-4c9e-ae57-c0a4a668318b,,,,He everyone! I\\'m Edgar. I\\'m web-analyst from Russia. <https://www.linkedin.com/in/edgarlakshin/>,,1642414590.146400,U02U3GB5T9C\\n995823fa-0789-4c0b-8336-474485ac18c7,,,,\"Hellooooo Everyone, I am Teshali from London, UK.   Excited to be here. Veryyy Excited that I’ve been checking <@U01AXE0P5M3>’s profile continuously  the last few days  for any updates :smile: Happy learning everyone :grinning:\",,1642415097.149300,U02UZVB01PA\\n91ef1c85-8ba3-4710-8685-1a7adc604968,U02T9JQAX9N,,,Thanks everyone.. I\\'ve been able to get docker up and running..,1642270989.467100,1642415263.149500,U02T9JQAX9N\\n1c692533-1084-44d7-9744-88f2658ef447,U02QGA57GRY,,,Is the playlist on the youtube channel,1642339856.499600,1642415661.150000,U02T9JQAX9N\\ncd07e36b-fdca-45d2-aaab-6fd6e38d1a19,U02RUUJ2TV5,,,Did it help?,1642411265.139800,1642416431.152100,U01AXE0P5M3\\n4c5623e6-38d4-458d-b033-6d4e4aeb194f,,50.0,,\"Hi, after watching the videos that are on the youtube playlist, I was unable to run this docker build command, even after I did exactly the same thing on the video (I think).\\nAny help?\",1642416911.154000,1642416911.154000,U02T1BX1UV6\\n5501bbf5-a586-48a2-a7e9-2c2a6abd1dc1,U02T1BX1UV6,,,Instead of screenshot  can you please copy the code? It\\'s very hard to read from a screenshot,1642416911.154000,1642418583.154600,U01AXE0P5M3\\n4a756853-857c-400e-979b-1194f7645bde,U02T1BX1UV6,,,Alright,1642416911.154000,1642419140.155800,U02T1BX1UV6\\n6e48daac-bdef-4701-90e6-abb142ecd6bd,U02T1BX1UV6,,,\"FROM python:3.9.9\\n\\nRUN pip install pandas\\n\\nWORKDIR /app\\n\\nCOPY pipeline.py pipeline.py\\n\\nENTRYPOINT [ \"\"bash\"\" ]\\n\\nThe code above is my dockerfile\",1642416911.154000,1642419314.156000,U02T1BX1UV6\\n33d1d34f-5aee-4a41-ade9-e60ea214acaf,U02T1BX1UV6,,,\"import pandas as pd\\n\\nprint(\\'job finished successfully\\')\\n\\nThis is the pipeline.py\",1642416911.154000,1642419480.157300,U02T1BX1UV6\\na25b2e2f-714f-4403-9eaa-33d591a0c79a,,,,\"Hi everyone!\\nI have enrolled for the course but it seems there was no email invitation and the first session is already over. Can someone help with the planning and link for the further session.\",,1642419495.157800,U0254S545D5\\n32591b20-d492-4230-a06f-0486a98d36f2,U02T1BX1UV6,,,The error,1642416911.154000,1642419607.157900,U01AXE0P5M3\\n598a28e1-bbc6-434b-9898-9bd10767ca10,U02T1BX1UV6,,,failed to solve with frontend dockerfile.v0: failed to read dockerfile: open /var/lib/docker/tmp/buildkit-mount617124908/Dockerfile: no such file or directory,1642416911.154000,1642419660.158100,U02T1BX1UV6\\n86a251a5-4148-4435-aac2-fc935fb4992c,U02TUV99NLF,,,\"Windows, Macos or Linux - all should work\",1642411366.140200,1642419678.158300,U01AXE0P5M3\\n22dfb836-3eae-4f69-8ccd-fd60a95cb309,U02TUV99NLF,,,The best would be Linux probably,1642411366.140200,1642419711.158700,U01AXE0P5M3\\nc1e69bfc-9bb1-45ef-bc6b-29241def6ac9,,,,\"Greetings! Shall be the zoom links be shared here or another channel when each meeting occurs throughout the course?\\n<@U01AXE0P5M3>\",,1642419917.160000,U02SFFC9FPC\\n300fccb6-71f4-489c-8a41-a06f025c7ed8,U02T1BX1UV6,,,What does the direcory contains? Is there an extension on a Dockerfile?,1642416911.154000,1642420198.161400,U02RYUWG4CQ\\n778f8845-c79b-4fd7-acc0-aa7797d07a78,U02T1BX1UV6,,,\"The directory contains the dockerfile, pipeline.py\",1642416911.154000,1642420781.162100,U02T1BX1UV6\\n7b6c0337-d289-425c-8003-7da0803ff4a1,,,,i don\\'t know why i keep getting this docker issues <https://docs.docker.com/desktop/windows/troubleshoot/#virtualization>,,1642420939.162900,U02TNA9H75E\\nba53eddf-7e8d-4824-a9d4-f49ea0e768c8,,,,Hardware assisted virtualization and data execution protection must be enabled in the BIOS.,,1642420947.163200,U02TNA9H75E\\n856ba411-c962-4dfc-bbae-51670eb435ea,U02T1BX1UV6,,,\"No, there is no extension on the dockerfile\",1642416911.154000,1642421147.163400,U02T1BX1UV6\\n5a52061c-a5a9-4fd0-aa86-1a0dbaedc4c9,,3.0,,is there a meeting now as my google calendar tell me that ?,1642421181.164000,1642421181.164000,U02T76JHS4S\\na764a31f-8f95-48a3-bafb-e428dd7eb55f,U02T1BX1UV6,,,\"You should name it Dockerfile, with capital letter\",1642416911.154000,1642421310.164100,U02RYUWG4CQ\\ndbf0b7ee-e184-49a1-afde-a005e39b1b62,U02T1BX1UV6,,,ok. I will try that,1642416911.154000,1642421557.164300,U02T1BX1UV6\\n96766007-0fd2-42f5-98f4-0613f50c33b9,U02T1BX1UV6,,,The name is already as Dockerfile,1642416911.154000,1642421585.164600,U02T1BX1UV6\\n017df4d4-b318-457f-9406-7d7ea67b61c8,U02T1BX1UV6,,,\"Try to change your Dockerfile. Add\\nRUN mkdir -p usr/src/app/\\n\\nCOPY . usr/src/app/\\n\\nCMD [\"\"python\"\", \"\"pypline.py\"\"]\",1642416911.154000,1642421817.165000,U02QL1EG0LV\\ne8932307-1067-4639-8e54-756b18643850,U02T1BX1UV6,,,Alright. I will try that,1642416911.154000,1642421844.165200,U02T1BX1UV6\\n1e435086-3a32-43ca-9730-7367413bd094,U02T1BX1UV6,,,\"I think Docker can\\'t find workdir, because you don\\'t create it\",1642416911.154000,1642422027.165500,U02QL1EG0LV\\nc723df9d-32b6-4bf6-b202-3a638fa9a5b5,U02T76JHS4S,,,Today at 17:00 CET,1642421181.164000,1642422230.165900,U02QL1EG0LV\\n04554190-26ad-4428-bc03-2ceb5a39bd06,,12.0,,Hi everyone. Is there a way to bypass the GCP credit card requirement to use the free trial? I can\\'t add my credit card in my country!,1642422345.167200,1642422345.167200,U02QL21SR4J\\ned3c1224-75d6-40e1-9034-4c442a08f1ae,U02T1BX1UV6,,,Make sure you\\'re actually in the same directory where the dockerfile is,1642416911.154000,1642423464.168100,U01AXE0P5M3\\n3cef6d15-5480-4c4a-b6fc-a3875ce53506,U02T1BX1UV6,,,\"And what does this line do in your file?\\n\\nCOPY pypline.py pypline.py\",1642416911.154000,1642424131.168500,U02QL1EG0LV\\naed92b76-d6e7-4218-b339-c5c5d0612181,U02QL21SR4J,,,There\\'s no way to bypass :kissing:,1642422345.167200,1642424627.170100,U02QGA57GRY\\n3da0652b-c8be-4a2a-b3b6-67e0944c4a32,,4.0,,\"Hello everyone, I\\'m a bit confused. Are we expected to go through the videos currently uploaded before the first session taking place today at 5 pm CET ?\",1642424671.171100,1642424671.171100,U02TWFZURD1\\n75405d7e-70b2-4bb2-b8d7-7c290fd2a049,U02TWFZURD1,,,The videos are quite introductory so I am seeing already :),1642424671.171100,1642424761.171200,U02CD7E30T0\\n5b5dde32-50e9-4123-9126-218a23a6e6f0,U02TWFZURD1,,,It\\'s better to go through them. <@U01AXE0P5M3> has clearly explained the docker concepts :relieved:,1642424671.171100,1642424811.171400,U02QGA57GRY\\n30ac2145-9703-4b9b-b6bf-4d32c598179b,U02T76JHS4S,,,Will it be a Zoom meeting? I have not received any Zoom invitation so far,1642421181.164000,1642425227.172400,U02UD80N6TE\\n0919cb79-a7ab-4037-8d37-d2c2d7999969,U02TWFZURD1,,,No you\\'re not expected to go through them before 5 pm. That\\'s the materials for this week,1642424671.171100,1642425741.172800,U01AXE0P5M3\\n1e53d8c3-8786-4587-bd87-66a292224072,U02QL21SR4J,,,Maybe you can get a virtual card?,1642422345.167200,1642425768.173000,U01AXE0P5M3\\nf2073800-4a2a-4671-a6ad-5d5f73fb660b,U02T76JHS4S,,,It\\'ll be in YouTube. You\\'ll get a link 10 minutes before the start,1642421181.164000,1642425810.173200,U01AXE0P5M3\\n36e57118-39d2-41a6-9344-e6bb8316a9b5,U02TWFZURD1,,,Thanks! will practice patience till 5 pm then :slightly_smiling_face:,1642424671.171100,1642426116.174200,U02TWFZURD1\\n7369fabb-dbcf-456c-b7e7-3ae5f585ea63,U02QL21SR4J,,,how /where can one get a virtual card,1642422345.167200,1642426998.175300,U02RTJPV6TZ\\na66447b0-2c81-4701-ae7c-8dd0672b3014,U02QL21SR4J,,,\"It probably depends on the country where you live, but if you Google \"\"virtual bank card\"\", you\\'ll probably find some options\",1642422345.167200,1642427090.175800,U01AXE0P5M3\\nf46f5d34-f99e-453e-85e4-fa8daf5ce0f3,,,,\"Hello, am yet to get the link\",,1642427935.177900,U02U5G0EKEH\\n8b381619-0f17-40e0-9323-cff795c286d4,,,,Can somebody send a link to today\\'s meeting please?,,1642428138.179000,U02V0TZKJBA\\n7834510a-b906-4ad5-9328-25b2b719ce24,,,,\"Hi, would apprecyate a link also\",,1642428215.180500,U02U5QC338V\\n8d4ba3aa-8a65-4a39-ae77-db31dd6ac18b,,4.0,,\"It\\'ll be streamed to our YouTube channel: <http://youtube.com/c/DataTalksClub|http://youtube.com/c/DataTalksClub>\\n\\nYou\\'ll get the link to the actual stream 10 minutes before the start\\n\\nYou can also subscribe to it and enable notifications\",1642428218.180700,1642428218.180700,U01AXE0P5M3\\n434b1341-c579-431a-97c1-7ae54669ec3c,U01AXE0P5M3,,,\"Sorry, timezones are hard =)\",1642428218.180700,1642428246.181500,U02V0TZKJBA\\ne572c990-3010-453b-944a-fcff552b3522,,2.0,,&gt; Did you guys all set up  terraform +gcp and docker before 5 pm?,1642428255.182300,1642428255.182300,U02U5QC338V\\nd0785cf8-5973-45cc-b8dc-5cb74c8984f1,U02U5QC338V,,,You don\\'t have to do it before 5 pm today,1642428255.182300,1642428279.182800,U01AXE0P5M3\\n2350f9d3-6a1b-42d9-b43a-4b70b49637e2,U02U5QC338V,,,\"Good to know, thanks\",1642428255.182300,1642428291.183000,U02U5QC338V\\n84d51ac9-e6e1-49df-a3f2-bddea2bc695b,,2.0,,I hate having to add my credit card to create a google cloud account :disappointed:,1642428379.183900,1642428379.183900,U02U5QC338V\\n73d8b73b-9186-49c5-9cf5-5925f9fdd798,,3.0,,We won\\'t be using AWS for this course?:thinking_face:,1642428414.184500,1642428414.184500,U02JE8J1W8P\\n52836ab8-0ce5-4ee4-89b0-1ed0083dac9c,U02JE8J1W8P,,,\"That\\'s correct,  we won\\'t\",1642428414.184500,1642428426.184600,U01AXE0P5M3\\n97a63e39-d16b-4501-bd67-68b321044862,U02JE8J1W8P,,,\"Its not AWS for a couple of reasons:\\n\\nThere\\'s no easy way to integrate dbt with athena (Snowflake would be too expensive) \\n\\nAnd there\\'s no 300$ free credits =)\",1642428414.184500,1642428500.185000,U01AXE0P5M3\\n5165019c-6542-46ef-b162-e05ed7430a3b,U02U5QC338V,,,You\\'ll be able to do most of the course locally without GCP except the big query part,1642428379.183900,1642428568.185300,U01AXE0P5M3\\n144f3673-79ca-4b1c-8ed4-20c2b720fd7a,U02U5QC338V,,,That makes me feel a little bit better,1642428379.183900,1642428740.185700,U02U5QC338V\\nb17900ee-1621-4934-ae1a-7e910344616a,U01AXE0P5M3,,,\"<@U01AXE0P5M3>, Will be possible to watch stream record later?\",1642428218.180700,1642429157.186600,U02TVSVB1D5\\na1f078ce-8a83-46e2-9c36-a5da231a112d,U01AXE0P5M3,,,Yes,1642428218.180700,1642429206.186800,U01AXE0P5M3\\nbe973a77-6c76-4605-8578-a7aac2be6bef,U01AXE0P5M3,,,Thanks:raised_hands:,1642428218.180700,1642429428.187300,U02TVSVB1D5\\n1ff0a9c5-d698-438f-ad69-d4a4cc9f9f97,U02JE8J1W8P,,,\"Cool no probs \\nIt\\'ll be fun\",1642428414.184500,1642429550.187500,U02JE8J1W8P\\n65ac581c-d684-4fd0-aabd-92dac0e700c2,U02QL21SR4J,,,<@U01AXE0P5M3> many thanx for bringing up the virtual card suggestion now am good to go and wont missout,1642422345.167200,1642431868.191000,U02RTJPV6TZ\\n2d734c78-ed23-4b51-ae05-71c20842c86a,U02T1BX1UV6,,,Thanks guys. I got it to work. I wasn\\'t in the right directory,1642416911.154000,1642432233.191600,U02T1BX1UV6\\n1995b48c-77a1-4d00-9ec2-150beaf9b44f,U02QL21SR4J,,,\"Well, In my country we\\'ve to give credit card details + I\\'d details as a proof to proceed :sweat_smile:\",1642422345.167200,1642433063.192500,U02QGA57GRY\\n61B0F2FC-973F-4DB4-8EEC-7B0AA4F0F87E,,11.0,,Greetings from :flag-ca: sending you tons of snow :smile: ,1642433181.192700,1642433181.192700,U02UX664K5E\\n0211d7a3-fed7-4542-8ba8-fb9c8292d599,U02UX664K5E,,,T\\'es au QC aussi? Are you also in Quebec?,1642433181.192700,1642433354.193600,U02QK0E49L2\\n0aa993bd-a5c8-44bc-961c-78e88463d343,U02UX664K5E,,,Oui! :slightly_smiling_face: à Brossard et toi?,1642433181.192700,1642433418.193900,U02UX664K5E\\ne868b73b-43d5-440e-8632-300a88282ba3,U02UX664K5E,,,on voit rien dehors LMAO,1642433181.192700,1642433565.194400,U02U2Q5P61Z\\nf358e1c4-8d99-4b3b-ac7a-43c4422b3118,U02UX664K5E,,,\"Cool, Same here from Helsinki!\",1642433181.192700,1642434006.196800,U02QAG0GMGE\\n03e10027-c95a-4012-b918-142b5d23abef,U02UX664K5E,,,Wow! looks amazing,1642433181.192700,1642434239.198300,U02TMP4GJEM\\nabcaf4c5-4a91-44b7-8f7c-4f059315df10,U02T9JQAX9N,,,\"For WSL2 the recommendation is to install on WSL2 and consume on both windows and linux, from linux. I did this and the first tests were successful.\",1642270989.467100,1642434312.198700,U02GVGA5F9Q\\n159cb939-ee63-47ad-b66a-0445d3970e5b,U02UX664K5E,,,Cool! Couldn\\'t have it at Indonesia :smile:,1642433181.192700,1642434551.200300,U02TES0M8LV\\n306713a8-77c0-4ab9-b856-8168052665b1,,,,\"We\\'re starting soon!\\n\\nStream: <https://www.youtube.com/watch?v=bkJZDmreIpA>\\nQuestions: <https://app.sli.do/event/mUDF6XhziMothMFuBPpVVo>\",,1642434638.201100,U01AXE0P5M3\\n41617d6c-c9a6-4b20-a424-46248624bece,U02UX664K5E,,,I also live in Canada and I cant even go get my morning coffee!!,1642433181.192700,1642434655.201300,U02U4ULM3HB\\nfe7a5467-6f76-4cbb-b151-1bdfcad0dbcc,U02UX664K5E,,,in Toronto,1642433181.192700,1642434671.201800,U02U4ULM3HB\\n6f8ba896-d17f-4822-aba7-b11819bc87eb,,,,I have a 30m appointment right at 16:00 GMT that I\\'m trying to move...,,1642434721.203300,U02GVGA5F9Q\\n2122695b-517a-4d34-9853-f3394a8df533,U02QL21SR4J,,,\"Try the Eversend app, you can create virtual cards\",1642422345.167200,1642434936.204500,U02C20G38TB\\n642e5f47-eac4-4391-aeeb-7fd0cae31bc6,,,,Thank you so much for organizing this course!,,1642435735.208100,U02CPBEH42W\\nCA403200-ECE0-43FA-813C-D7E7E7F9DD4D,,2.0,,\"Hi <@U01AXE0P5M3> . I’m so excited to watch the live course. I have a question for you. After completing this course, can we apply for data analyst roles that also have some requirements related to data engineering?\",1642435973.210600,1642435973.210600,U02QHQU3LF7\\ned37f5e8-a943-48d8-9868-ca2287f4914b,,,,I\\'m so excited right now:heart:,,1642436049.211200,U02T91CL14Y\\n72f5534e-d8c2-4b78-b100-4fa2c7de15a5,,8.0,,Has anyone installed Terraform on Mac M1?,1642436643.212800,1642436643.212800,U02U97ZNLJD\\na1f2f94a-d9ea-444a-a3ee-7cb3376e9142,U02U97ZNLJD,,,Yeah I just installed using brew. Seems to be working fine :crossed_fingers:,1642436643.212800,1642436759.213700,U01EKHDMRGT\\ne79ac59c-bbce-40a6-993d-0357f01c24c7,,,,\"<@U01AXE0P5M3> Could you pls land the list of Books which DE should \"\"drink\"\" as your collegue said. Much appreciated!\",,1642436892.215700,U02Q7PJ8PP1\\nf264ca61-7fe7-46b3-83ba-2ed645cf3a41,U02SFFC9FPC,,,So you\\'re like Sheldon :wink: Sounds very exciting :slightly_smiling_face:,1641275845.206400,1642436939.216400,U02Q7JMT9P1\\n7d49394d-442a-450e-be1e-fe266d1c5b0b,,,,\"*Book recommendations from the live-stream:*\\n1. Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems\\n2. Database Internals: A deep-dive into how distributed data systems work\",,1642436982.217000,U02TWFZURD1\\n8752f270-72fd-40fe-802a-632a1513a895,,3.0,,\"1. ...\\n2. ...\\n3. Data Pipelines with Apache Airflow\",1642437059.218000,1642437059.218000,U02ULQFCXL0\\n96ebe72a-724d-4634-9409-618de1d34288,U02ULQFCXL0,,,:point_up: this one is a solid read. Best Airflow resource that I have come across.,1642437059.218000,1642437160.219100,U02S8K9JBD0\\nd3463787-81cc-4a3d-a482-55ff0e03be01,U02U97ZNLJD,,,Enumerate the steps please.,1642436643.212800,1642437217.220500,U02T9RTGJLT\\n3bddbd9b-5558-4c84-858e-d8c45b899278,,,,\"Books mentioned:\\n• <https://www.amazon.de/-/en/Martin-Kleppmann/dp/1449373321>\\n• <https://www.amazon.com/Database-Internals-Deep-Distributed-Systems/dp/1492040347>\",,1642437223.220700,U01MYN6S6TE\\n8e27f25d-4dee-4ffe-b180-f3a4cf88d2c8,U02ULQFCXL0,,,<https://www.manning.com/books/data-pipelines-with-apache-airflow>,1642437059.218000,1642437321.222300,U02ULQFCXL0\\ncdb7c856-f4fa-4032-be9d-cb5a464ab3a2,U02SFFC9FPC,,,\"I myself have a funky scientific career. I did my masters in optical engineering working on optical image (analogous) processing. I moved quite far for my PhD, doing research in the implementation of microwaves and RF plasmas in heterogeneous catalysis. After PhD I wanted to do more \\'physical\\' research and did a 9-year post doc in surface science and synchrotron research.\",1641275845.206400,1642437325.222600,U02Q7JMT9P1\\nfafcdfb9-d741-43e0-9599-d901dbbe41da,U02U97ZNLJD,,,\"```brew tap hashicorp/tap\\nbrew install hashicorp/tap/terraform```\\n\",1642436643.212800,1642437344.222900,U02U97ZNLJD\\nb4d3e38c-a699-4c3d-9117-c6debd2f6fa2,U02ULQFCXL0,,,Amazing book for sure :),1642437059.218000,1642437344.223100,U02U2Q5P61Z\\n547093bb-be69-4fa1-bc51-510e45b19b1d,U02U97ZNLJD,,,I messed with my xcode and now I have to reinstall developer tools from scratch.,1642436643.212800,1642437375.223700,U02U97ZNLJD\\nc467df07-b98f-4982-83e1-69f5b678c281,,4.0,,\"American here. I was disturbed to hear that some students say they are unable to use GCP due to US Sanctions on their country? I ask because I had no idea this was the case. In any case, my question/suggestion would be to subscribe to a VPN provider and use that to subscribe. Is that a possible workaround?\",1642437561.227400,1642437561.227400,U02U8SBFWD8\\n85fe1f3e-57b6-4776-b5e9-9b6524d8b74c,U02U97ZNLJD,,,\"Yeah you can also follow the steps here: <https://learn.hashicorp.com/tutorials/terraform/install-cli>\\n\\nYou can go to the tab Homebrew on OS X in the install terraform section\",1642436643.212800,1642437595.227500,U01EKHDMRGT\\n83ac4fa8-5496-498b-b407-fd99b04cbabe,U02U8SBFWD8,,,\"This last summer I was doing a remote course with many Iranian students. Iran has been facing US sanctions for a while but they all managed to access Google Colab, so I believe they were all using VPN’s or some other means.\",1642437561.227400,1642437913.228900,U02BVP1QTQF\\na27edb12-1127-4be6-983b-43188318ec3e,,3.0,,\"Today is about introduction and questions, right? The lectures are already uploaded in the Youtube play list\",1642437958.230000,1642437958.230000,U01KKK6NB45\\n5e986a48-417c-4577-9c11-fd73665f00d1,U01KKK6NB45,,,yep,1642437958.230000,1642438002.230200,U02ULQFCXL0\\n889bf9bf-a061-42ef-8395-b2a6647e2327,U01KKK6NB45,,,<https://www.youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb>,1642437958.230000,1642438003.230400,U02ULQFCXL0\\n37d7a36c-badb-4a20-bd58-b52cf0f63635,U01KKK6NB45,,,thanks for the link,1642437958.230000,1642438021.230700,U01KKK6NB45\\n5d385397-5612-4c9f-9b06-4b9489009e69,,1.0,,Another highly recommended read: Data Pipelines Pocket Reference (Book by James Densmore) - <https://www.amazon.com/Data-Pipelines-Pocket-Reference-Processing/dp/1492087831>,1642438185.232100,1642438185.232100,U02TTRBDA1M\\n74f76f3b-487c-42f4-9153-bbb9619bbf71,,,,<@U02TTRBDA1M> got the data pipelines book. Highly recommended,,1642438632.234400,U02TMP4GJEM\\n1fb013b0-5b6c-4379-9a2b-b19223530c0a,,,,<@U02AT04LWL9> :wave:,,1642438678.236000,U02ULQFCXL0\\nc319b851-14b4-4591-979d-6b575c0cba5c,,,,hello from India,,1642438705.236700,U02U43UTK8W\\n45ca4a94-5f9b-4eaf-baed-c6d81cfcce99,,,,hello from singapore! excited to work and learn  using this course!,,1642438888.239700,U02U90VRYLS\\n2361b9da-cafd-401c-88f4-e1bf17c6c53f,,2.0,,Hello from Indonesia. Very excited to learn about DE,1642438962.240800,1642438962.240800,U02UQPA35DX\\n3d9a9977-1ae1-4c4c-8bd2-dd1bd7f7d802,,8.0,,For the first week do we have homework? <@U01AXE0P5M3>,1642439007.242900,1642439007.242900,U02R9627FLM\\n5ce2c0f7-0f6e-4331-8b44-c38a97bf6f47,U02R9627FLM,,,That was my question too. Maybe github has the answer.,1642439007.242900,1642439033.244500,U02UBQJBYHZ\\nc718cdf3-7246-486d-ac71-284615220577,,1.0,,\"I was wondering an answer to this question from sli-do:\\n```After the course, should i prepare for a certification? like Cloudinary, databricks```\",1642439044.245400,1642439044.245400,U02U2FJ456J\\n001b82f8-20a1-4b36-84fb-dfdc4c4a80b6,U02R9627FLM,,,\"We do, but we haven\\'t prepared it yet\",1642439007.242900,1642439071.246000,U01AXE0P5M3\\n32eeb6ca-e2bc-498b-82a0-07a43075f644,U02R9627FLM,,,Stay tuned!,1642439007.242900,1642439081.246300,U01AXE0P5M3\\n445abd78-81df-4b3b-97cc-e4558fe967e7,U02QHQU3LF7,,,\"Hey <@U02QHQU3LF7> the course is oriente dto prepare you for data engineer roles, so although it will cover some parts of what a more technical data analyst would do you\\'ll find that you need extra preparation for the dat analyst role.\",1642435973.210600,1642439121.247500,U01B6TH1LRL\\ne86deb98-e3e6-4cd6-83dc-8e7487466e37,,5.0,,I\\'m finishing my Master in CS this year and I was wondering about what a Data Engineer does and what responsibilities does it have in a business/company!,1642439158.249000,1642439158.249000,U02U546D36Z\\n59a33f16-434e-4653-ba9b-8b2475484c0f,,,,Can I learn programming as I\\'m learning this data engineering  course( I\\'m currently doing a python programming course),,1642439172.249400,U02SE3ZKQVA\\nbf2759c9-9bbc-496b-9e0f-4e0b261bcd61,U02R9627FLM,,,so we should check this channel daily for homeworks? or you will send it by email?,1642439007.242900,1642439179.249900,U02T6RMCF7X\\n2fe2da52-4007-46a0-910f-670bc9983673,,5.0,,\"I did not get it. I supposed we were having a course today, Docker, terraform...\",1642439182.250200,1642439182.250200,U02QDG5PCVB\\n70a7560e-2128-4f7a-b463-04646b992d04,U02QDG5PCVB,,,are we expected to go through the videos in the youtube channel ourselves?,1642439182.250200,1642439197.250700,U02QDG5PCVB\\n6fe78f05-39d9-4e74-a6fd-d00fc8286af0,U02R9627FLM,,,I guess videos for first week not ready either :grinning:,1642439007.242900,1642439207.251100,U02R9627FLM\\ne0a558cb-8f15-4672-b981-f6b9b0456441,U02R9627FLM,,,but we have 4 videos in playlist to learn material for hw :smiley:,1642439007.242900,1642439229.251900,U02ULQFCXL0\\n9050efda-0e1d-47a8-bd3c-dedd426a0016,U02QDG5PCVB,,,\"The course is done offline, we post the videos in the youtube channel and once a week we have a call for questions. Today was the introductory call <https://youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb>\",1642439182.250200,1642439246.252800,U01B6TH1LRL\\nd3b1c5cf-ea98-4cdf-a5a2-37a03e02f33b,,,,Thanks a lot for today indeed :smiley:,,1642439263.253800,U01HSHSKV6J\\nb879a954-0646-4abf-9f54-10f732b38156,U02R9627FLM,,,Here\\'s the link to the playlist <https://youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb>,1642439007.242900,1642439271.254200,U01B6TH1LRL\\n07065e05-5f36-4395-94a4-9433899d2252,U02QDG5PCVB,,,\"<@U01B6TH1LRL> Ok, so for next week we have to watch the videos and do the homework. Is the homework published already or will it be posted later this week?\",1642439182.250200,1642439303.256000,U02QPBZ3P8D\\n81ccacf0-2234-4cd6-87fa-a30f72914686,U02R9627FLM,,,\"yes, we\\'re still working on the videos\",1642439007.242900,1642439339.257800,U01AXE0P5M3\\n1682d697-8948-4c48-a948-569121530746,U02QDG5PCVB,,,\"The homework is not published yet, we\\'ll post about that soon\",1642439182.250200,1642439347.258100,U01B6TH1LRL\\nBBC696B7-466F-40C0-9DD2-336A730CAFA2,U02U97ZNLJD,,,Hey Yh I have,1642436643.212800,1642439794.268700,U02T91CL14Y\\nD2CEADD4-0CB1-4D85-9CD8-12360E87DA09,U02U546D36Z,,,\"DEs are basically responsible for getting, transforming, storing, and delivering any data that other internal teams in a company need to use. Crucial role for any data-driven company (and nowadays most companies rely on data in order to make decisions)\\n\\nAs an example: analytics team are using various marketing platforms (google ads, LinkedIn ads, etc). But they need all of that information to be stored in one common easy-to-access place. And they need that info to be prepared daily in order to build dashboards, metrics, etc. Here comes the DE who writes a script (a process that is executed based on a certain schedule) to download info from all of the sources, cleans it, and puts it into the company DB. \\n\\nHowever, it regularly depends on a company: some DEs are also expected to know devops, some are expected to know data modeling, database internals, etc. A lot of room for growth but also for ambiguity. All in all, it\\'s a very promising career because there are not enough knowledgeable DEs but the requirements can vary (but the core idea and responsibility is always the same. The one I mentioned in the beginning)\",1642439158.249000,1642439809.269500,U02Q2B9P15M\\nE84F5C25-71A8-4CBD-865D-BA36E2F721A4,U02U97ZNLJD,,,I had to update my xcode ,1642436643.212800,1642439815.269900,U02T91CL14Y\\n3B544E2A-FEC5-4DB1-B2DA-830E38C8A75A,U02U97ZNLJD,,,\"After the update you can now run brew tap hashicorp/tap\\n```brew install hashicorp/tap/terraform```\\n\",1642436643.212800,1642439864.270600,U02T91CL14Y\\nca2ee21e-abba-4bb1-a356-60b5e2ae7093,U02U2FJ456J,,,\"Wel, Certification normally mean that you are professional in some area. Course promise to cover the fundamentals. So - if you would like to get a certification after course then you’d should prepare for it, yes. And it will probably take much longer then the course itself  :woman-shrugging:\",1642439044.245400,1642439998.270900,U01MYN6S6TE\\na23a6270-a701-4a6c-a5dd-968f6efe00b4,,2.0,,\"Hi everyone. Instructors, thank you for today.\\nI am a bit confused about the viewing order for the videos in the Youtube playlist. Would it be possible to add an index so that we know where to start and go on from there?\",1642440106.273200,1642440106.273200,U02V1SRRHAL\\n3bf0135c-4f31-4fd1-9a1a-23800b0b7515,U02V1SRRHAL,,,\"I’m currently watching the videos. The first video in the playlist has been updated to be today’s office hours, which makes sense. Then there’s the short video about GCP which you can watch anytime, I think. The rest of the videos are indexed and you can watch them in order.\",1642440106.273200,1642440269.275200,U02BVP1QTQF\\nfbc70f70-0a73-4665-bc10-30ece098d79e,,,,\"Question: I heard there was a collection of google forms for weekly homework, have they been posted yet?\",,1642440283.275600,U02SC84ABQU\\n8d749753-d167-4b4a-9f00-8d456776d50f,U02V1SRRHAL,,,Thank you so much. I guess it\\'s already in order then. :slightly_smiling_face:,1642440106.273200,1642440327.276200,U02V1SRRHAL\\n,,,,\"oops, nevermind! found my answer :sweat_smile:\",,1642440389.276500,U02SC84ABQU\\na64c7681-19bb-44a8-b2b0-67dc1d861904,,,,This could be helpful.,,1642440411.277000,U0290EYCA7Q\\nc6ce5f1a-f1b1-4ad0-9bfb-7adb008795f2,U02U546D36Z,,,<@U02Q2B9P15M> Thank you very much for the clear response! Much appreciated!,1642439158.249000,1642440805.282700,U02U546D36Z\\nA996CA5F-8403-4892-8331-0F8852D240BA,U02QHQU3LF7,,,\"Hi <@U01B6TH1LRL> thank you for your answer. I recently did a diploma in data analysis. So I have knowledge about python, sql, tableau, excel and power BI. I have been looking for data analyst job roles on LinkedIn. Some employers (at least in India) wants candidates to have knowledge about data warehousing, ETL pipeline, cloud etc., So that’s the reason I asked this question. \",1642435973.210600,1642440884.284300,U02QHQU3LF7\\ndc82eb78-d611-466d-a357-7cebde1b07df,,5.0,,\"A bit confused myself. Was today\\'s live stream \"\"Week 1\"\",  is there a video we should watch on the YouTube Channel should we just read through the github or is next week \"\"Week 1\"\" covering the topics below?\",1642440893.284600,1642440893.284600,U02TNEJLC84\\n0208bcbb-7e6e-42f4-96ca-12715a156b0a,U02TNEJLC84,,,All lesson videos are in a YouTube playlist: <https://www.youtube.com/watch?v=bkJZDmreIpA&amp;list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb> (edited with proper link),1642440893.284600,1642440953.285200,U02BVP1QTQF\\nbaca1e14-bbac-44a6-a59b-8f83cd4e63d1,U02TNEJLC84,,,Just wait for a day. They will upload.,1642440893.284600,1642440979.286100,U0290EYCA7Q\\nA7052D8C-0ED3-4DBA-A428-7D28ED772B7D,U02U546D36Z,,,Glad I helped:+1:,1642439158.249000,1642441009.287100,U02Q2B9P15M\\nb0e7e0ea-2102-4558-9f90-5c0887f821b6,U02UX664K5E,,,Hi from Montreal :slightly_smiling_face:,1642433181.192700,1642441051.287900,U02U5G6MXJ8\\nbb5d828f-883c-4428-9e99-416ace00c357,,2.0,,\"Hi I missed the live session for today. can someone tell me where I can find the homework and submission page? Also, can we still submit the previous homework?\",1642441051.288100,1642441051.288100,U02S0U4RM2B\\n222931af-2b71-46ac-bc72-c4fc27b223a2,U02TNEJLC84,,,\"as of now, the playlist contains today’s stream (intro to the course), a short explanation of GCP and 3 videos for week 1. I haven’t finished watching so I’m unsure if there will be more, but the github repo will be updated with homework later this week\",1642440893.284600,1642441099.288700,U02BVP1QTQF\\nb195c6ec-db2d-4087-b1b9-d4549b2d8007,U02UX664K5E,,,\"Hi Majed, nice to meet you!\",1642433181.192700,1642441109.288900,U02UX664K5E\\n8b0ca156-196d-4ce7-bdb9-c044623d8ce6,U02TNEJLC84,,,the repo’s readme contains all the links: <https://github.com/DataTalksClub/data-engineering-zoomcamp>,1642440893.284600,1642441155.289700,U02BVP1QTQF\\n5476231f-76f2-4f61-be62-e5b997b3d103,U02S0U4RM2B,,,\"Homework is not published yet, it will be done soon as said in today\\'s session\",1642441051.288100,1642441196.290400,U029JQURC81\\n7560d758-ac29-41b3-a620-fef359b0ee1e,U02TNEJLC84,,,Thank you! Can\\'t believe I missed that.,1642440893.284600,1642441610.292900,U02TNEJLC84\\n8632dbe7-1ab1-4d99-84cf-83aa99a9acd3,,,,Thanks for the session today. It was a great start.,,1642441704.294700,U02T991SQHH\\n37bd6658-0431-43b7-8819-d628f1881019,,2.0,,The course git repo someone?,1642441999.299800,1642441999.299800,U02JE8J1W8P\\n821059a8-75ae-4e1e-9c11-9bd38b008e30,U02JE8J1W8P,,,<https://github.com/DataTalksClub/data-engineering-zoomcamp> here it is!,1642441999.299800,1642442061.300700,U02U546D36Z\\n71728077-93a3-429d-9b5b-33557a69c269,,,,\"The live session today was just amazing! :grin: Glad to have such amazing instructors <@U01AXE0P5M3> <@U01DFQ82AK1> <@U01DHB2HS3X> <@U01B6TH1LRL> . A nice ice breaking session, with the required course structure, (Zooming) into various important topics in the further weeks, followed by a patient QnA session! \\n\\nLooking ahead! :rocket:\",,1642442081.300900,U02SC4Y6X0U\\na1be22b3-fdcb-4036-9374-ef1a50c9667b,U02JE8J1W8P,,,Thanks Greg,1642441999.299800,1642442100.301000,U02JE8J1W8P\\n942da303-9c82-431b-a825-5a60842b6cbd,U02QDG5PCVB,,,\"There are some videos already uploaded for Week 1 (i.e. this week), that you can access via our YouTube playlist “DE Zoomcamp”. Some more of this week will be uploaded tonight/tomorrow.\\nThe goal of Week 1 is setting up your GCP infra with Terraform, as well as local Postgres env with Docker. Based on the instructions, you should have your environment ready by the end of this week.\\n\\nAlso, as explained during the intro call today, this is a self-paced course (similar to the ML Zoomcamp one), which means there will be recorded videos and textual instructions for you to go through every week, starting today.\",1642439182.250200,1642442233.301500,U01DHB2HS3X\\n526b3885-9144-4998-b85b-f4c333fd938b,U02U8SBFWD8,,,\"Thanks for bringing that to our notice, and sorry to hear that. As explained in the intro call today, we’ll be preparing exercises in a way, that it can also run on your local environment such as using Postgres, instead of GCP. You’ll receive more instructions on the videos when they’ll be aired\",1642437561.227400,1642442453.302800,U01DHB2HS3X\\n21B9DF39-184B-4698-9342-0AF431B2A641,,3.0,,\"Hi there!\\nDo we clone zoomcamp repo on our local environment or self create?\",1642442567.305000,1642442567.305000,U02TPNLQYDV\\nfa899291-0c9a-44cc-96a3-c10df4c88d1c,U02T1BX1UV6,,,\"docker run -it \\\\\\n\\xa0 -e POSTGREST_USER=\"\"root\"\" \\\\\\n\\xa0 -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n\\xa0 -e POSTGREST_DB=\"\"ny_taxi\"\" \\\\\\n\\xa0 -v c:/Desktop/data-engineering-zoomcamp-main/data-engineering-zoomcamp-main/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data:var/lib/postgresql/data \\\\\\n\\xa0 -p 5432:5432 \\\\\\n\\xa0 postgres:13\",1642416911.154000,1642443149.306900,U02T1BX1UV6\\n587c35d9-6aac-4994-a70e-ae365f2f391c,U02T1BX1UV6,,,Hello <@U01AXE0P5M3> I Couldn\\'t get the postgres serve running,1642416911.154000,1642443203.307900,U02T1BX1UV6\\n9a39dc9e-14eb-44f0-8193-9403a4131eb0,U02T1BX1UV6,,,\"docker: Error response from daemon: invalid volume specification: \\'/run/desktop/mnt/host/c/Desktop/data-engineering-zoomcamp-main/data-engineering-zoomcamp-main/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data:var/lib/postgresql/data\\': invalid mount config for type \"\"bind\"\": invalid mount path: \\'var/lib/postgresql/data\\' mount path must be absolute.\\nSee \\'docker run --help\\'\\n\\nThis is the error\",1642416911.154000,1642443231.308600,U02T1BX1UV6\\n5C2C1569-1106-44C4-B2F6-B37D79FC6B57,,3.0,,\"If you work on Mac and use zsh instead of bash.\\nYou can some problem with exac gcloud command.\\nYou need open .zshrc file and add few command.\\n————————————-\\n# The next line updates PATH for the Google Cloud SDK.\\nsource /Users/&lt;your_username&gt;/google-cloud-sdk/path.zsh.inc\\n\\n# The next line enables zsh completion for gcloud.\\nsource /Users/&lt;your_username&gt;/google-cloud-sdk/completion.zsh.inc\",1642443404.310800,1642443404.310800,U02TPNLQYDV\\nC1E0E27B-4CC5-4AD1-942D-1766C923D9BD,,,,\"Open zshrc file\\nvim ~/.zshrc\",,1642443445.311300,U02TPNLQYDV\\n449a9bfb-21fa-4876-9903-5bdf2b95d56b,U02T1BX1UV6,,,\"docker run -it \\\\\\n\\xa0 -e POSTGRES_USER=\"\"root\"\" \\\\\\n\\xa0 -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n\\xa0 -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n\\xa0 -v c:/Desktop/data-engineering-zoomcamp-main/data-engineering-zoomcamp-main/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data:var/lib/postgresql/data \\\\\\n\\xa0 -p 5432:5432 \\\\\\n\\xa0 postgres:13\",1642416911.154000,1642443584.311600,U02T1BX1UV6\\nd321f74d-8592-422d-ae34-2e110914cb97,U02T1BX1UV6,,,\"Corrected the spelling errors, but it\\'s still the same error\",1642416911.154000,1642443605.311800,U02T1BX1UV6\\na7b417ed-ce6a-499d-aeb5-e0f88ea72957,,17.0,,\"docker: Error response from daemon: invalid volume specification: \\'/run/desktop/mnt/host/c/Desktop/data-engineering-zoomcamp-main/data-engineering-zoomcamp-main/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data:var/lib/postgresql/data\\': invalid mount config for type \"\"bind\"\": invalid mount path: \\'var/lib/postgresql/data\\' mount path must be absolute.\\nSee \\'docker run --help\\'\\n\\ndocker run -it \\\\\\n\\xa0 -e POSTGRES_USER=\"\"root\"\" \\\\\\n\\xa0 -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n\\xa0 -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n\\xa0 -v c:/Desktop/data-engineering-zoomcamp-main/data-engineering-zoomcamp-main/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data:var/lib/postgresql/data \\\\\\n\\xa0 -p 5432:5432 \\\\\\n\\xa0 postgres:13\\n\\nThis is the syntax I ran that gave the error above\\n\\nAny help?\",1642444157.313800,1642444157.313800,U02T1BX1UV6\\nb6448208-65e2-4205-ad1c-a2bb5a71dbe4,U02T1BX1UV6,,,Add slash before var,1642444157.313800,1642444216.314200,U01AXE0P5M3\\nd7816293-b85c-48f6-b5ac-e9eb16f43f86,U02TPNLQYDV,,,Both work,1642442567.305000,1642444250.314700,U01AXE0P5M3\\nee5931e0-6a77-4e0f-a737-841d43eaa826,U02TPNLQYDV,,,\"Assuming the course follows the same structure as the ML zoomcamp (and apparently it does), then you can have your own repo built from scratch for homework submissions. Cloning the repo is useful for submitting pull requests if you’re inclined to do so but otherwise I don’t think it should be necessary\",1642442567.305000,1642444252.314900,U02BVP1QTQF\\n89b63e1d-0a57-42f8-b9d5-2d46cee06008,U02TPNLQYDV,,,\"Oops, ninja’d by Alexey\",1642442567.305000,1642444266.315200,U02BVP1QTQF\\ne868d85e-3007-4055-a540-25f99df9d80a,U02T1BX1UV6,,,\"Thank you, it worked\",1642444157.313800,1642445094.318500,U02T1BX1UV6\\n961ba847-33d3-41bd-a322-f6b9d39ea2ee,U02T1BX1UV6,,,\"However, I didn\\'t see the database in my VS.code editor under the folder i mapped\",1642444157.313800,1642445124.319200,U02T1BX1UV6\\n66769381-946d-4353-abd9-c01f6c7def8e,,,,\"A couple of people approached me asking to create a separate channel for announcements only. Since we\\'re on a free slack, it\\'s not possible, because we already have <#C01BQC114P2|announcements>\\n\\nBut we can use pinned messages for that. I just pinned the video from today\\n\\nYou can also pin other messages that you think are important\",,1642445200.320600,U01AXE0P5M3\\nae449ca5-4a88-4c26-b4a1-f17f935dd23b,U02T1BX1UV6,,,<@U01AXE0P5M3>,1642444157.313800,1642445642.322600,U02T1BX1UV6\\nc55c8153-2a15-4e10-a6f4-847b8198b8dd,U02T1BX1UV6,,,Are you sure you have the correct path?,1642444157.313800,1642445840.323300,U01AXE0P5M3\\n25b487f2-9057-4825-b850-fc28f189295b,U01AXE0P5M3,,,<@U01AXE0P5M3> I discovered the course has already started very accidentally. Don\\'t you make some email announcement by intent?,1642377725.070900,1642445870.323600,U01UKRH6VGT\\nbd242a85-badc-41d2-9691-9422bfffb65b,U01AXE0P5M3,,,Not sure I follow. What do you mean?,1642377725.070900,1642445923.323900,U01AXE0P5M3\\ndf8d6288-f4fc-46ed-ac9a-5ba193cfd1fc,U01AXE0P5M3,,,\"I mean I did register to the course on the first days of January, but after that, no hear :slightly_smiling_face:\",1642377725.070900,1642445927.324100,U01UKRH6VGT\\na5a0a71b-2dc3-458a-93b1-75573676450e,U01AXE0P5M3,,,Maybe you unsubscribed from our mailing list?,1642377725.070900,1642445958.324300,U01AXE0P5M3\\n9d6deb67-7eb3-4c7c-b209-81ccbbd5b637,U01AXE0P5M3,,,\"<@U01AXE0P5M3> I thought there would be official email that course has started, please join or smth :smile:\",1642377725.070900,1642445962.324500,U01UKRH6VGT\\n32143fdf-3584-4497-9f44-df8685a8db17,U02T1BX1UV6,,,Let me double check,1642444157.313800,1642445966.324700,U02T1BX1UV6\\n98a5b103-0a27-49de-982a-8d03839d1de4,U01AXE0P5M3,,,\"I sent a few emails, e.g. one of them: <https://mailchi.mp/datatalks.club/dezoomcamp-1-3?e=[UNIQID]|https://mailchi.mp/datatalks.club/dezoomcamp-1-3>\",1642377725.070900,1642446082.325100,U01AXE0P5M3\\n82c7c7dd-2733-4d10-ad48-00edcd88acff,U01AXE0P5M3,,,\"<@U01AXE0P5M3> yeah, most likely I\\'ve unsubscribed from the general newsletter, that is the issue and I thought to get the notification despite that fact, maybe from a different newsletter. Tbh, I don\\'t really know how it works :smile:\",1642377725.070900,1642446231.325400,U01UKRH6VGT\\n4fcfa43f-5e02-490f-b489-caa415affcba,U02T1BX1UV6,,,\"The files belonging to this database system will be owned by user \"\"postgres\"\".\\nThis user must also own the server process.\\n\\nThe database cluster will be initialized with locale \"\"en_US.utf8\"\".\\nThe default database encoding has accordingly been set to \"\"UTF8\"\".\\nThe default text search configuration will be set to \"\"english\"\".\\n\\nData page checksums are disabled.\\n\\ninitdb: error: directory \"\"/var/lib/postgresql/data\"\" exists but is not empty\\nIf you want to create a new database system, either remove or empty\\nthe directory \"\"/var/lib/postgresql/data\"\" or run initdb\\nwith an argument other than \"\"/var/lib/postgresql/data\"\".\",1642444157.313800,1642446265.325700,U02T1BX1UV6\\n8a377c7b-8b49-4029-aa2f-062252104c21,U02T1BX1UV6,,,This is the new error I am getting,1642444157.313800,1642446281.326100,U02T1BX1UV6\\n81060ce4-e37d-4fc5-a1e8-dc83a6f8c8d9,U01AXE0P5M3,,,\"It\\'s too expensive to maintain multiple mailing lists... but since you\\'re in slack already, you won\\'t miss anything\",1642377725.070900,1642446346.326700,U01AXE0P5M3\\necb61bbb-58c9-4017-bd96-c8ecafeafd3d,U01AXE0P5M3,,,\"yeah, that\\'s true :stuck_out_tongue:\\nit\\'s nice having all the things in one place\",1642377725.070900,1642446421.327000,U01UKRH6VGT\\n3e2c8d1d-43a4-4ae3-badd-3f5d3ea9df57,,2.0,,JFYI: the link to `images/architecture` on the page <https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup> does not work,1642446426.327300,1642446426.327300,U01UKRH6VGT\\nc3ebb4a3-bda4-487a-8f88-68cfc2e2eb5b,U01UKRH6VGT,,,<https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/images/architecture/arch_1.jpg>,1642446426.327300,1642446920.328200,U02V24WAZRN\\nc52aedd7-d0a0-4be6-8bc2-97b5eddf458d,U02U8SBFWD8,,,\"We can create the GCP account but we missed the free 300$ charge, since we can\\'t have a credit card. About using vpn, because it changes the server randomly, it\\'s a bit risky to use for aws or azure or google cloud. But we still try to find a way to work, never give up:woman-raising-hand:\",1642437561.227400,1642447182.328800,U02DM3BP884\\n3c4a4258-00da-453e-92a0-87adc44fccfe,,20.0,,I am on PST if anyone on the same timezone wants to work together on homework and project. I look to dedicate around 5 hours per week on the course,1642447403.330800,1642447403.330800,U0205L73QNS\\n4f5f5f71-7420-40e9-a4f2-6adef4af7508,U02T1BX1UV6,,,<@U01AXE0P5M3>,1642444157.313800,1642447481.330900,U02T1BX1UV6\\n57b31ba9-9d48-4d76-b24c-2c34c6c60050,U0205L73QNS,,,i have experience using some of the technologies in this course but would love to revisit them and drill them down through new exercises/projects. Preferably someone with some experience as well,1642447403.330800,1642447510.331100,U0205L73QNS\\n4100EAB5-C1ED-4946-9A13-5D86B9CA8278,U0205L73QNS,,,I am in! I am also in PST time zone,1642447403.330800,1642448077.335400,U02U6DR551B\\nC6B9DD59-0707-4AC6-9119-656EF3F3DCFD,U0205L73QNS,,,\"Hi!\\nHow you want to work together?\\nI have some experience in Python, ETL, and database.\\nNow I’m working as Data Analyst.\\nI’m from Moscow.\",1642447403.330800,1642448090.336000,U02TPNLQYDV\\n224AE0A0-81A6-4DBA-B4BF-2B811651BEA4,U0205L73QNS,,,Should we make a telegram group?,1642447403.330800,1642448127.336900,U02U6DR551B\\nc54874bf-47f5-4d7d-8000-b44f8ffb3a28,U0205L73QNS,,,Sounds good!,1642447403.330800,1642449071.338300,U02S848271C\\n57b1bcf2-0324-4386-9f17-7473fe91dd96,U0205L73QNS,,,does telegram support code snippets? i have discord as well <@U02U6DR551B>,1642447403.330800,1642449641.338800,U0205L73QNS\\n13f4cb49-64f6-4b62-9c08-ebc5e73ee3af,U02T1BX1UV6,,,\"<https://stackoverflow.com/questions/62697071/docker-compose-postgres-upgrade-initdb-error-directory-var-lib-postgresql-da>\\n\\nmight help?\",1642444157.313800,1642449867.339100,U02TJ5ED9MJ\\n580e1d60-f72d-43d0-b043-772ea0bcffb8,U02T1BX1UV6,,,Make sure you map the correct directory,1642444157.313800,1642450029.339500,U01AXE0P5M3\\nc66a616b-2844-48d3-80e3-729e7621b147,U02T1BX1UV6,,,\"Alright, I will try it again\",1642444157.313800,1642450251.339700,U02T1BX1UV6\\n5719A1FD-9C0B-4CEE-8807-5AA7654DC314,U0205L73QNS,,,I am not sure about code snippets. Or we can make a channel on slack as well.,1642447403.330800,1642450342.340800,U02U6DR551B\\n643f0066-9a50-4fa4-81e9-bedbfdcf8628,U0205L73QNS,,,\"You can attach a file there, on Telegram or you use *```* to paste your code. I have also found this option: <https://github-wiki-see.page/m/python-telegram-bot/python-telegram-bot/wiki/Code-snippets>\",1642447403.330800,1642451781.341700,U02S848271C\\naffc3d16-4bef-44dd-bb2f-1bd21b39ebd2,U0205L73QNS,,,hello i am working now as a data scientist . so i want to share with you,1642447403.330800,1642451849.341900,U02UCMYFDT4\\n62c6aa4e-95e6-4b4a-b995-2f8f2a3b30c8,U0205L73QNS,,,\"<@U02U6DR551B> lets look into slack, i dont wanna be opening up more apps. too many distractions\",1642447403.330800,1642453919.343100,U0205L73QNS\\n2eeadab4-28ac-4a56-bd13-4a4ee9336a79,U0205L73QNS,,,\"Slack is a good option too, we can just open a channel here and if needed, we can always lock it.\",1642447403.330800,1642454118.343300,U02S848271C\\neb5a1754-6cce-48ad-a573-e76cb8fbe92e,U0205L73QNS,,,I am also interested. Can you add me to the group?,1642447403.330800,1642455600.344100,U02T2DX4LG6\\n4f9c7dff-6663-455c-a205-f699656fdee4,,5.0,,A year or so ago I used my free GCP Credits for another course. Anyone else in the same situation/have a work around for it?,1642457024.345400,1642457024.345400,U02TNEJLC84\\n4adc0c84-b6a2-4a40-bf1e-2d3b5013b3c3,U0205L73QNS,,,Kindly add me to the group. Am interested,1642447403.330800,1642457535.346300,U02T9P895E2\\n2E493892-64D5-4494-871C-3669A2120842,U01UKRH6VGT,,,Created a pull request with a fix,1642446426.327300,1642458286.346900,U02U83LUCGY\\n106BD8CC-C812-4984-80E0-621B2C970298,U02TNEJLC84,,,Create a new account for this course?,1642457024.345400,1642458320.347500,U02U83LUCGY\\nc8458335-ad23-4316-b419-2141c0937ce0,U02TNEJLC84,,,<@U02U83LUCGY> Face palm! Thank you friend. Thought it wouldn\\'t let me due to Google emails being tied to phone numbers. New account created and got the credits. Ready to roll! Thank you again!,1642457024.345400,1642459013.348600,U02TNEJLC84\\nf5fa588a-0660-494d-b20a-0f393c640a55,U0205L73QNS,,,I\\'d be interested as well. I\\'m in EST.,1642447403.330800,1642459019.349000,U01L83DLHRC\\n3f3837be-01d8-4c6a-9326-a260a9494948,,3.0,,\"I registered using this link <https://airtable.com/shr6oVXeQvSI5HuWD> but didn’t get confirmation mail or show successful registration, is that normal?\",1642459039.349500,1642459039.349500,U02U5CCFBRU\\nac449384-05cd-43b9-907d-e00b5d67754b,U02U5CCFBRU,,,I believe it’s unnecessary; the important thing is that you’re already in this slack channel. When submitting your homework make sure that you use the same email adress you use for logging in to slack.,1642459039.349500,1642459138.349700,U02BVP1QTQF\\nce9670e2-23d0-495a-a02c-4d241d7f0450,U0205L73QNS,,,\"I am interested too, please do add me as well :slightly_smiling_face:\",1642447403.330800,1642459488.350200,U02RGTFKC3B\\n53fc0a46-6736-45d8-b67c-ee71ca3b195a,,,,I am having issues installing pgcli,,1642460444.352400,U02T1BX1UV6\\n0663e50c-4951-4a49-8423-2f1fbb84fe77,,4.0,,\"ERROR: Failed building wheel for setproctitle\\nFailed to build psycopg2 setproctitle\",1642460502.352700,1642460502.352700,U02T1BX1UV6\\nf2376815-d9f0-4861-acf7-ec1a8adba570,,,,There errors are so long..but those are the last two lines,,1642460518.353100,U02T1BX1UV6\\n13ef7fe5-244a-4d74-b7db-2f6a7138bef8,U02T1BX1UV6,,,\"I have been able to retrieve the database, thank you\",1642444157.313800,1642460554.353200,U02T1BX1UV6\\n836a364e-6f28-43ce-8134-1201d32854ee,U02T1BX1UV6,,,But I can\\'t install pgcli,1642444157.313800,1642460561.353400,U02T1BX1UV6\\nd10dfee0-e1cd-45c2-88d5-3e1f1aa0b7e4,,2.0,,Hi guys. Are there 5 videos (webinar included) in this week? I see 1 video was deleted from playlists. is that alright?,1642460697.354800,1642460697.354800,U02RBKL48KE\\n99e73941-473b-4a12-8a6c-c193eabd60ef,U02RBKL48KE,,,<@U01AXE0P5M3>,1642460697.354800,1642460712.354900,U02RBKL48KE\\n85771dee-b3f5-46d9-822a-ce12f397cd73,U0205L73QNS,,,\"I’m in PST and am interested, could you please add me to the group as well?\",1642447403.330800,1642460713.355100,U02U8CB58G3\\ne906410d-0558-4686-9255-099ab59c4ce7,U02RBKL48KE,,,\"Hi, check <https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642439007242900?thread_ts=1642439007.242900&amp;cid=C01FABYF2RG|this>\",1642460697.354800,1642461627.356000,U02QP6JM83U\\n2b061102-ea84-495e-929e-6f5213200598,,2.0,,\"Hi guys, is youtube playlist already listed in order?\",1642463665.357500,1642463665.357500,U02RA8F3LQY\\n83a07704-18d3-46f8-9449-99447bf286dd,U02RA8F3LQY,,,It appears to be as far as I\\'ve seen. <https://www.youtube.com/watch?v=2JM-ziJt0WI&amp;list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&amp;index=4>,1642463665.357500,1642464769.359500,U02TNEJLC84\\n088eeb9f-5099-4674-a725-f28a24cd9296,,15.0,,\"Hi all, I am having trouble connecting to my postgrs db. I am getting an authentication error for the password, but I am using the same as what is set in the in docker run command. I am on windows, and have other postgres versions set up locally. Anyone have an idea of where I am going wrong?\\nDocker command\\n```$ docker run -it \\\\\\n&gt;     -e POSTGRES_USER=\"\"root\"\" \\\\\\n&gt;     -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n&gt;     -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n&gt;     -v localPath:/var/lib/postgresql/data \\\\\\n&gt;     -p 5432:5432 \\\\\\n&gt;     postgres:13\\nPostgreSQL Database directory appears to contain a database; Skipping initialization\\n\\n2022-01-18 00:08:30.769 UTC [1] LOG:  starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\\n2022-01-18 00:08:30.769 UTC [1] LOG:  listening on IPv4 address \"\"0.0.0.0\"\", port 5432\\n2022-01-18 00:08:30.769 UTC [1] LOG:  listening on IPv6 address \"\"::\"\", port 5432\\n2022-01-18 00:08:30.777 UTC [1] LOG:  listening on Unix socket \"\"/var/run/postgresql/.s.PGSQL.5432\"\"\\n2022-01-18 00:08:30.812 UTC [27] LOG:  database system was shut down at 2022-01-18 00:06:01 UTC\\n2022-01-18 00:08:30.849 UTC [1] LOG:  database system is ready to accept connections```\\nIn separate terminal pgcli command with auth error\\n```$ pgcli -h localhost -p 5432 -u root\\nPassword for root:\\nconnection to server at \"\"localhost\"\" (::1), port 5432 failed: FATAL:  password authentication failed for user \"\"root\"\"```\",1642465146.363400,1642465146.363400,U02UE7NTLUU\\n6450ef35-032c-4e36-a2b6-34a264961e33,U02T1BX1UV6,,,I am also having issues install pgcli in Ubuntu (Windows11 WSL),1642444157.313800,1642465314.363700,U02TTRBDA1M\\n485cace4-c531-4811-8b2f-825b460dba7b,,4.0,,All - I am also having issues getting off the ground during the Docker setup. Experiencing errors connecting to postgres via pgcli,1642465470.364900,1642465470.364900,U02TTRBDA1M\\n35f25040-c193-45e9-ade5-b87d5b43323c,U02RA8F3LQY,,,Okay thanks <@U02TNEJLC84>,1642463665.357500,1642465804.365100,U02RA8F3LQY\\na87edd51-1c33-42aa-9497-7014ad851dee,U02TNEJLC84,,,<@U02TNEJLC84> did you passed the payment information verification or it was not needed?,1642457024.345400,1642466249.365400,U02UKBMGJCR\\n31e28c4f-1fa4-4296-9ce4-b32bfc610e68,,,,\"Hi everyone\\n\\nPeople asked about using a alternative for GCP.\\n\\nI hope this image is helpful for everyone\",,1642466646.366600,U02TC704A3F\\nd8b4ce00-0309-4a4e-bdaf-e16b12c6427f,U02TTRBDA1M,,,Also having issues here. what\\'s going on with yours?,1642465470.364900,1642466831.366900,U02TVGE99QU\\ne7b0244f-db36-482b-9a82-c86420d11d18,,,,\"Having some pgcli issues. from googling it appears that there is some kind of version mismatch between my pgcli and postgres or something, but I\\'m not totally sure how to troubleshoot this. I am a beginner to intermediate linux user\",,1642467380.368200,U02TVGE99QU\\n789d6ae6-6125-4656-8705-82741cdf9fa2,,11.0,,this is what I\\'m getting when trying to \\\\d the yellow_taxi_data table:,1642467396.368700,1642467396.368700,U02TVGE99QU\\n60677671-f4ab-4b4d-b377-d50146e88813,,,,,,1642467418.368800,U02TVGE99QU\\n24ea03b3-3771-44cb-be4f-e8fde07ac2fa,U02TTRBDA1M,,,\"<@U02TTRBDA1M> I found a workaround to login that doesn\\'t use pgcli. I haven\\'t finished the rest of the video though so I am not sure if we need pgcli elsewhere...\\n\\nI used `docker ps` to view the name of postgres db, and then connect to it via bash with   `docker exec -it &lt;container name&gt; bash`.  Then log in using psql.  `psql -U root ny_taxi`\",1642465470.364900,1642468218.370200,U02UE7NTLUU\\n22e6f68b-bdad-46db-93ec-bcc167bee370,U02T1BX1UV6,,,\"are you using macos?\\nI the psycopg2 error.\",1642460502.352700,1642468907.370700,U02CPH3FR33\\n4c6631e4-a8cb-4d33-b85e-de0892aeeea8,U02T1BX1UV6,,,Running `brew install postgresql` solved it for me,1642460502.352700,1642468944.370900,U02CPH3FR33\\nd808b4bd-c9c2-461e-8b5b-4eb846ad8220,U02UQPA35DX,,,semoga bisa sharing bareng mas,1642438962.240800,1642469969.371300,U02SQQYTR7U\\n861ab73c-2fdd-495d-a7fa-ee2c6464b5bf,U02TTRBDA1M,,,<@U02UE7NTLUU> Thank you for the input. I am also curious if pgcli will come back to haunt or if we can get by with psql,1642465470.364900,1642470627.371900,U02TTRBDA1M\\n6a45e2dc-190b-49e5-88b9-30959a6fba24,U02TVGE99QU,,,Here is my latest error. What version of python are you using and what OS are you on?,1642467396.368700,1642470770.372200,U02TTRBDA1M\\nfc94e416-e374-492c-b710-2d73c2f83427,U02TTRBDA1M,,,\"Haha it already has! In a way anyways, I am now stuck trying to connect to the db with sqlalchemy and getting the same auth error.\",1642465470.364900,1642471065.372600,U02UE7NTLUU\\n19F9F108-93E0-4095-BFF7-9AC570420FA6,U02T1BX1UV6,,,\"I used following to install pgcli on Ubuntu in my WSL\\nSudo apt update\\nSudo apt install pgcli \",1642444157.313800,1642473389.375200,U02AGF1S0TY\\n9F1A6313-05CD-4400-8CB8-B469FA788468,U02U5CCFBRU,,,Same here. No emails. :(,1642459039.349500,1642473610.375700,U02TB5KPR50\\n2f7602ec-3ea5-44e6-ac42-4d4a5746901e,,1.0,,Does anyone know what is the approximate size of the <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/dataset.md|Taxi Rides NY dataset> that we are going to use in this course?,1642474457.377300,1642474457.377300,U02T96HEARK\\ne6714f11-183d-429f-bf68-4c92a575d1dc,,2.0,,\"I’m not sure if anyone has mentioned this yet, but at the end of *dezoomcamp 01 03* video, <@U01AXE0P5M3> you mentioned of producing the docker commands in a YAML file. Was this portion posted?\",1642475076.380200,1642475076.380200,U02UAHJHJ20\\n9aafe816-0694-4ead-9ce1-3ed8ebb25874,,2.0,,I just finished watching the video on “Introduction to Terraform concepts and GCP Pre-Requisites” <https://youtu.be/Hajwnmj0xfQ>. It seems to end before Terraform is introduced / put into action. Am I missing something - or perhaps there is another video coming?,1642475429.381400,1642475429.381400,U02U6F9PKJS\\n06841c9f-33f0-4fec-a112-e5b0b9190915,U02UAHJHJ20,,,I think you can simply use .yml ...,1642475076.380200,1642477190.382100,U02QGA57GRY\\nf378e919-5af5-4bb1-91de-d6775edd8b39,U02U6F9PKJS,,,I believe that is still pending,1642475429.381400,1642478519.382500,U02SMBGHBUN\\n81c450f5-da9c-45fb-a102-088c231c438d,U02U6F9PKJS,,,,1642475429.381400,1642478522.382700,U02SMBGHBUN\\n8c3ea57e-102f-4d99-8442-3818223d13a8,,,,\"Hello I am Shivashis from California, USA\",,1642480979.384000,U02TFH3Q3KL\\n218498d4-1e76-4fc1-bf46-a698e650030e,,,,Thanks for conducting this course. Looking forward to learning Data Engineering concepts,,1642481009.384500,U02TFH3Q3KL\\n6d9020c0-ccd5-4c0e-a330-278b8d5eccb1,U02UAHJHJ20,,,Not yet posted,1642475076.380200,1642483277.385300,U01AXE0P5M3\\n319d3247-2391-48ea-b436-c4f0c5fdd60e,U02U5CCFBRU,,,I do this manually and didn\\'t have a chance to add you to the mailing list yet. But indeed being in slack is sufficient,1642459039.349500,1642483573.387900,U01AXE0P5M3\\n20bb6172-5c21-4377-a3de-155592e44d18,U02UE7NTLUU,,,\"The issue is probably the other postgres. Try to use a different port, e.g. 5431\",1642465146.363400,1642483714.388300,U01AXE0P5M3\\n579ebe19-5212-46a3-8838-2a8ac4d46881,U02UE7NTLUU,,,\"Maybe you have another password for root. Try to change password\\nType in your terminal\\n\\nsudo -u postgres psql\\n\\nALTER USER root WITH PASSWORD \\'new_password\\';\",1642465146.363400,1642483731.388700,U02QL1EG0LV\\n44df1742-85a8-494c-9252-21707cb8512e,,1.0,,\"<@U01AXE0P5M3> it would be of great help if you could share the VS Code extensions that you use. In the video, you glided through the whole code very smoothly but it took quite some time for me to code through the whole thing. :sweat_smile:\",1642483733.388900,1642483733.388900,U02T96HEARK\\n4c68d4dd-ae34-4583-90cd-9af31cf2b50d,U02TVGE99QU,,,Can you try it with Python 3.9? That\\'s what I used,1642467396.368700,1642483856.389200,U01AXE0P5M3\\n8f15ba8a-6db9-425e-b933-eb14dc675725,U02T96HEARK,,,For the first week it\\'s 100 mb. But we\\'ll use a lot more than that in the coming weeks,1642474457.377300,1642483905.389400,U01AXE0P5M3\\n67023aa1-698e-47e6-8d01-0d9fa47e57a1,U02T96HEARK,,,Docker extension will be helpful,1642483733.388900,1642483953.389600,U01AXE0P5M3\\ndcbae18b-c8a9-44d2-9ff7-8d9d4a685259,,3.0,,Hi. What about this week\\'s homework?,1642486684.391800,1642486684.391800,U02QL1EG0LV\\n4ad68fde-799c-483d-84d3-c25bb13ae370,U02QL1EG0LV,,,We\\'ll publish it today,1642486684.391800,1642487119.393400,U01AXE0P5M3\\n3f4f80c0-dca3-430c-ad9a-e09ca48e854f,,,,\"For those who use plain Windows - instructions for setting up google cloud SDK and terraform\\n\\n<https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/windows.md>\",,1642487144.394300,U01AXE0P5M3\\n6d1b1fed-5dcc-48ff-a5e7-7e5492368fdb,,,,Is there a preferred way to structure a data engineering project on GitHub? Something similar to Cookie-Cutter for DS projects or just a structure? Asking for both - the current tutorial practice(following along) as well as for the final project.,,1642487211.395200,U01QGQ8B9FT\\nE8C39189-F2A8-410E-8BC0-02A76ECD71B8,U02QL1EG0LV,,,<@U01AXE0P5M3> where would you publish it?,1642486684.391800,1642488281.396200,U02U6DR551B\\n01e835f0-a243-41bf-86c1-9b2a0d1c84c1,,,,\"Dell@JW MINGW64 /e/DOCKER/pyhton-image\\n$ winpty docker build -t test:pandas .\\n\\n[+] Building 4.5s (4/4) FINISHED\\n =&gt; [internal] load build definition from Dockerfile                       0.0s\\n =&gt; =&gt; transferring dockerfile: 103B                                       0.0s\\n =&gt; [internal] load .dockerignore                                          0.0s\\n =&gt; =&gt; transferring context: 2B                                            0.0s\\n =&gt; ERROR [internal] load metadata for <http://docker.io/library/pyhton:3.9|docker.io/library/pyhton:3.9>        4.4s\\n =&gt; [auth] library/pyhton:pull token for <http://registry-1.docker.io|registry-1.docker.io>              0.0s\\n------\\n &gt; [internal] load metadata for <http://docker.io/library/pyhton:3.9|docker.io/library/pyhton:3.9>:\\n------\\nfailed to solve with frontend dockerfile.v0: failed to create LLB definition: pu\\nll access denied, repository does not exist or may require authorization: server\\n message: insufficient_scope: authorization failed\\n\\nDell@JW MINGW64 /e/DOCKER/pyhton-image\",,1642488624.396600,U02RTJPV6TZ\\nf24799a9-18aa-4461-948a-e51ebdc2f51a,,,,\"trouble, help\",,1642488696.396900,U02RTJPV6TZ\\n521181fe-9989-456f-a651-f320873d00d8,,14.0,,,1642488820.397100,1642488820.397100,U02RTJPV6TZ\\n1f39eefa-76ea-4be5-9298-4b0bee3a212c,,2.0,,\"To keep this channel more organized, I\\'d like to ask you for a few favors:\\n\\n• Please use threads. When you have a problem, first describe the problem shortly and then put the actual error in the thread - so it doesn\\'t take the entire screen\\n• Instead of screenshots, it\\'s better to copy-paste the error you\\'re getting in text. Use ``` for formatting your code. For me personally it\\'s very difficult to read text from screenshots \\n• You don\\'t need to tag us when you have a problem. We will see it eventually.\\nHappy learning!\",1642488983.400100,1642488983.400100,U01AXE0P5M3\\ne0412313-baf2-4281-b078-6736b56b111c,U02RTJPV6TZ,,,you have a typo,1642488820.397100,1642489087.400400,U01AXE0P5M3\\nf7fc2f59-a4ba-4b60-8b08-87c33e79c6ec,U02QL1EG0LV,,,github and announce it here,1642486684.391800,1642489128.400600,U01AXE0P5M3\\n86e6125e-076d-4704-a923-2dc1d01d3eb7,U02RTJPV6TZ,,,thanx av arrested it,1642488820.397100,1642489698.402100,U02RTJPV6TZ\\n82009b15-8be9-478d-a008-7b81942d8a1f,U01AXE0P5M3,,,*Question about the installation of Terraform*; there are three versions out there and which one is suitable <https://prnt.sc/26f1qwl> for the course?,1641566368.013200,1642491067.403700,U02S848271C\\n1bc1445b-c003-47e0-8957-62302fd29181,U02S0U4RM2B,,,<https://www.youtube.com/watch?v=bkJZDmreIpA>,1642441051.288100,1642492061.405000,U02UBC4SE6N\\n560BA0BD-CFE8-412C-94CF-A4C8B54C938B,,2.0,,Hey all :) I missed the session yesterday! Will it be on YouTube?,1642492381.406800,1642492381.406800,U01F6E6P45Q\\na31280fd-c424-44cf-9ae6-4d2638e5830b,U01F6E6P45Q,,,<https://www.youtube.com/watch?v=bkJZDmreIpA&amp;list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&amp;index=1>,1642492381.406800,1642492522.407300,U02TB5VPF9N\\n09d6d27c-408e-4eae-94b6-fa8d682fc71d,USLACKBOT,,,DM,1643356664.702239,1643356997.896989,U01AXE0P5M3\\nc5c14129-6bf7-4d8c-84bf-25f1b1ef1acd,,5.0,,\"Hello Everyone,\\nI\\'m stuck when I\\'ve finished creating a network, and try again to config postgres with docker, and the result is an error like this, is there a solution for this?\",1643360269.815949,1643360269.815949,U02SHV7RJTW\\n34243874-c75c-4d32-8738-745ba17b758f,U02SHV7RJTW,,,\"\"\"$ winpty docker run -it -e POSTGRES_USER=\"\"root\"\" -e POSTGRES_PASSWORD=\"\"root\"\" -e POSTGRES_DB=\"\"ny_taxi\"\" -v /c:/de-zoomcamp/data-engineering-zoomcamp/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data -p 5432:5432 postgres:13\\n\\nPostgreSQL Database directory appears to contain a database; Skipping initialization\\n\\n2022-01-28 08:49:20.680 UTC [1] LOG:  starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\\n2022-01-28 08:49:20.681 UTC [1] LOG:  listening on IPv4 address \"\"0.0.0.0\"\", port 5432\\n2022-01-28 08:49:20.681 UTC [1] LOG:  listening on IPv6 address \"\"::\"\", port 5432\\n2022-01-28 08:49:20.695 UTC [1] LOG:  listening on Unix socket \"\"/var/run/postgresql/.s.PGSQL.5432\"\"\\n2022-01-28 08:49:20.722 UTC [27] LOG:  database system was shut down at 2022-01-27 18:00:48 UTC\\n2022-01-28 08:49:20.726 UTC [27] LOG:  invalid primary checkpoint record\\n2022-01-28 08:49:20.726 UTC [27] PANIC:  could not locate a valid checkpoint record\\n2022-01-28 08:49:20.726 UTC [1] LOG:  startup process (PID 27) was terminated by signal 6: Aborted\\n2022-01-28 08:49:20.726 UTC [1] LOG:  aborting startup due to startup process failure\\n2022-01-28 08:49:20.729 UTC [1] LOG:  database system is shut down\"\"\\n\\n\\n\"\"PS C:\\\\de-zoomcamp\\\\data-engineering-zoomcamp\\\\week_1_basics_n_setup\\\\2_docker_sql&gt; pgcli\\nPassword for Deatari RA:\\nconnection to server at \"\"localhost\"\" (::1), port 5432 failed: FATAL:  password authentication failed for user \"\"Deatari RA\"\"\\n\\nPS C:\\\\de-zoomcamp\\\\data-engineering-zoomcamp\\\\week_1_basics_n_setup\\\\2_docker_sql&gt; pgcli -h localhost -p 5432 -u root -d ny_taxi\\nPassword for root:\\nconnection to server at \"\"localhost\"\" (::1), port 5432 failed: FATAL:  password authentication failed for user \"\"root\"\"\"\"\",1643360269.815949,1643360354.086549,U02SHV7RJTW\\ncdbfdae2-a098-499a-b059-4ed38a6f0589,U02SHV7RJTW,,,<https://datatalks-club.slack.com/archives/C02V1Q9CL8K/p1643203354307200>,1643360269.815949,1643360413.698369,U02RA8F3LQY\\n530a215c-a6eb-495b-8862-26b1d029cb06,U02SHV7RJTW,,,Thank you so much fikri,1643360269.815949,1643360671.418229,U02SHV7RJTW\\n4823e6bc-1568-4489-b054-ac5892e40738,U02SHV7RJTW,,,i had this issue and i just  delete ny_taxi_postgres_data folder and re run the docker run command because i guess I guess my older command with different credentials was stored in users and the system confuse  . (if u change the credentials ),1643360269.815949,1643361151.458159,U02QW395UNM\\n1dc0481a-9669-418b-929a-8cd738f5ad23,U02SHV7RJTW,,,\"<@U02QW395UNM> got it, thank you so much\",1643360269.815949,1643362123.237299,U02SHV7RJTW\\nc5e8c5ad-bb16-45d6-96a7-8ec655433d57,,2.0,,\"hey everybody again :smile: I know this error was already reported, but I think followed the instructions through correctly, still when I run the DAG `data_ingestion_gcs_dag`  I get this error at the `local_to_gcs_task`\\n```google.auth.exceptions.DefaultCredentialsError: File /.google/credentials/google_credentials.json was not found.```\\nin the `docker-compose.yml`  I have in the environment the following variables:\\n``` GOOGLE_APPLICATION_CREDENTIALS: /.google/credentials/google_credentials.json\\n AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT: \\'google-cloud-platform://?extra__google_cloud_platform__key_path=/.google/credentials/google_credentials.json\\'```\\nand this volume added:\\n```    - ~/.google/credentials/:/.google/credentials:ro```\\nthe credentials are stored into\\n```\"\"C:\\\\Users\\\\&lt;my_user&gt;\\\\.google\\\\credentials\\\\google_credentials.json\"\"```\\nwhat should be the behavior here? should there be a folder/file created when I run `docker-compose build`  within the folder of my project? If yes, this is not happening, if not, could you clarify further? :slightly_smiling_face: Thanks\",1643362240.353579,1643362240.353579,U02UA0EEHA8\\nb91c8425-ec79-451f-b2f1-02eaf1c3b048,U02UA0EEHA8,,,You didn\\'t include the *HOME* directory  `GOOGLE_APPLICATION_CREDENTIALS: ~/.google/credentials/google_credentials.json`,1643362240.353579,1643362780.030019,U0290EYCA7Q\\nbbd060bc-8291-4c02-88f6-32f63a74e7a1,U0303JLGS23,,,\"Also, for the second question. In GCP, you pay for what you use. So be careful if you\\'ve set up any VMs, don\\'t keep it running\",1643309241.134000,1643362836.902089,U01DHB2HS3X\\n80b7d5d3-7ef4-417c-9a93-c328c363417f,U029DM0GQHJ,,,\"Did you try running it even with unhealthy containers up? Like I mentioned on another thread, we\\'re not using all the containers anyway. Once the webserver is healthy, you should be able to access localhost and proceed\",1643354276.441279,1643363199.643849,U01DHB2HS3X\\n6918c99a-e62e-4e00-8882-fa722c40f459,U02UA0EEHA8,,,\"Thanks, now it works!\",1643362240.353579,1643363448.623439,U02UA0EEHA8\\nf265c890-c660-4d57-870e-f275fa4c331f,U02Q9P0A0NA,,,Should I just redo the setup on GCP and try again?,1643299377.100400,1643363497.541359,U02Q9P0A0NA\\n981f307d-a220-4979-8fb6-9584e4dd3dd3,U02TBKWL7DJ,,,\"Hi <@U01HNUYV81L>; Mac already comes with zsh as the default shell since MacOS Catalina, but you can find out which shell you’ve got by opening a terminal and typing some nonsense command like `asdf` ; you should get an error message like `zsh: asdf: command not found`. If your Mac is pre-Catalina and you’ve upgraded the OS, it’s possible that your default shell is bash rather than zsh but it shouldn’t really matter at all.\\n\\nAll my mounting issues came down to typos, though. I never had permission issues on Mac. If you have any particular issues, try posting the logs here and the commands you used. I also suggest skipping the part of running `docker network` and `docker run` and going straight to `docker-compose` , because many people seem to be having issues that are somehow resolved when using `docker-compose`\",1642507885.440700,1643363872.663549,U02BVP1QTQF\\n182766a1-3a76-4245-8d00-39891d1d3597,U02UM74ESE5,,,\"FYI, you can still create folders starting with a period such as `.foldername` from a terminal, using the `mkdir .foldername`  command\",1643341570.337969,1643364173.350809,U02BVP1QTQF\\n87626913-bba8-496b-b74b-4a1492283aca,,3.0,,\"in video 1.4.1 he listed is directory with ls inside the ssh folder and got some files id_e........ that he got from his github,  i don\\'t understand that part how he got them\",1643364682.323409,1643364682.323409,U02UKLHDWMQ\\n78ecf60f-c47f-419c-8854-d96b087f4c23,U02UKLHDWMQ,,,\"<https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent|https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent>\\n\\nThis?\",1643364682.323409,1643364930.836159,U01AXE0P5M3\\n0fbc6efd-1407-4ef3-b7f9-69b67b286ce3,,3.0,,\"Any Idea about this error?? I set AWS access and secret key in the variable file. Was able to download the files using transfer service on the GCP console.\\n\\n```google_storage_transfer_job.s3-bucket-nightly-backup2: Creating...\\n╷\\n│ Error: googleapi: Error 403: The caller does not have permission, forbidden\\n│\\n│   with google_storage_transfer_job.s3-bucket-nightly-backup2,\\n│   on <http://main.tf|main.tf> line 75, in resource \"\"google_storage_transfer_job\"\" \"\"s3-bucket-nightly-backup2\"\":\\n│   75: resource \"\"google_storage_transfer_job\"\" \"\"s3-bucket-nightly-backup2\"\" {\\n│ ```\",1643365258.216129,1643365258.216129,U0290EYCA7Q\\ne4e9d3be-9048-4a3d-b909-daf6cd4102ab,,1.0,,hi everyone does week2 have homework?.....i checked and didnt see anything,1643365448.607149,1643365448.607149,U02RTJPV6TZ\\n59ab4012-ea4b-4388-8c90-125f65fd83c2,U02RTJPV6TZ,,,\"We didn\\'t publish anything yet, apologies for the delay\",1643365448.607149,1643365808.064099,U01AXE0P5M3\\n85ad7d6d-4cfe-4116-9215-34531994eab6,U02UKLHDWMQ,,,\"Also these are the files on my local computer, not remote\",1643364682.323409,1643365837.217069,U01AXE0P5M3\\n985fcc86-3b26-4e12-9dbe-8b5cffee3d79,,1.0,,Can we get an extension for the deadline for submitting week 2 homework?,1643365899.626459,1643365899.626459,U02QPBZ3P8D\\n3dc8ab03-d8b4-4ea8-ba24-06981e8196b5,U02QPBZ3P8D,,,Yes of course,1643365899.626459,1643365915.600339,U01AXE0P5M3\\nf2986e7a-fe1f-4296-b00f-47d35fc90e69,U029DM0GQHJ,,,i tried but the status for webserver wont turn healthy so no luck even after increasing the allocated memory,1643354276.441279,1643366293.892569,U029DM0GQHJ\\n7b87c89f-b6b2-43bb-86ab-d51743375c69,U02RTJPV6TZ,,,\"hi all, okay so i installed docker compose on my VM-instance, but when i do a \"\"docker build -t taxi_ingest_image:v001 .\"\" it returns error failed to know where am wrong, kindly help\\n(base) DELL@de-zoomcamp:~/Zcamp/week_1/docker_sql/docker_compose$ ls\\ndocker-compose.yaml  dockerfile  ny_taxi_postgres_data  pipeline.py  services.py  yellow_taxi_tripdataSCRIPT.py\\n(base) DELL@de-zoomcamp:~/Zcamp/week_1/docker_sql/docker_compose$ docker build -t taxi_ingest_image:v001 .\\nerror checking context: \\'can\\'t stat \\'/home/DELL/Zcamp/week_1/docker_sql/docker_compose/ny_taxi_postgres_data\\'\\'.\\n(base) DELL@de-zoomcamp:~/Zcamp/week_1/docker_sql/docker_compose$\",1641565510.010100,1643367145.628049,U02RTJPV6TZ\\n61655b49-4868-4cb8-96c7-835b3119e18e,U02RTJPV6TZ,,,,1641565510.010100,1643367201.963589,U02RTJPV6TZ\\n8888d542-b146-40e4-b7ac-e580c35382df,U02RREQ7MHU,,,\"*hi all, okay so i installed docker compose on my VM-instance, but when i do a \"\"docker build -t taxi_ingest_image:v001 .\"\" it returns error failed to know where am wrong, kindly help*\\n\\n(base) DELL@de-zoomcamp:~/Zcamp/week_1/docker_sql/docker_compose$ ls\\ndocker-compose.yaml\\xa0dockerfile\\xa0ny_taxi_postgres_data\\xa0pipeline.py\\xa0services.py\\xa0yellow_taxi_tripdataSCRIPT.py\\n(base) DELL@de-zoomcamp:~/Zcamp/week_1/docker_sql/docker_compose$ docker build -t taxi_ingest_image:v001 .\\nerror checking context: \\'can\\'t stat \\'/home/DELL/Zcamp/week_1/docker_sql/docker_compose/ny_taxi_postgres_data\\'\\'.\\n(base) DELL@de-zoomcamp:~/Zcamp/week_1/docker_sql/docker_compose$\",1642520122.473300,1643367375.431059,U02RTJPV6TZ\\nf45cae67-77b0-4404-9a4b-472a9266c09d,U02RREQ7MHU,,,,1642520122.473300,1643367389.870069,U02RTJPV6TZ\\n84e89f67-11b6-4de0-b8ee-16e82d6162d7,U029DM0GQHJ,,,I have the same problem,1643354276.441279,1643367481.355389,U02TVSVB1D5\\n36a6746a-773e-4514-b30a-06d7e9925f83,U02RREQ7MHU,,,\"<https://m.youtube.com/watch?v=tOr4hTsHOzU&amp;list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&amp;index=14&amp;t=2s|https://m.youtube.com/watch?v=tOr4hTsHOzU&amp;list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&amp;index=14&amp;t=2s>\\n\\nCheck this in the middle\",1642520122.473300,1643367629.760169,U01AXE0P5M3\\na8364086-e522-4860-85f5-395a2a1de917,U02RREQ7MHU,,,And can you please help a bit and add it to FAQ after you resolve this problem?,1642520122.473300,1643367662.926199,U01AXE0P5M3\\n1ba94aaf-2874-4d2d-b8eb-9c785cf5eb47,U02RTJPV6TZ,,,\"Hi Julius, it\\'s better to ask questions just in one place, not multiple\",1641565510.010100,1643367707.812579,U01AXE0P5M3\\na982d917-6037-4655-b4e7-d4e16439c9f6,U030PFH5CRX,,,<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1643314693152300?thread_ts=1643303881.118400&amp;cid=C01FABYF2RG|https://datatalks-club.slack.com/archives/C01FABYF2RG/p1643314693152300?thread_ts=1643303881.118400&amp;cid=C01FABYF2RG>,1643303881.118400,1643367749.033529,U01AXE0P5M3\\n5dd41975-61b8-42af-8805-a2415c991a34,,11.0,,\"Hello, everyone. `local_to_gcs_task` fails. I dont know why, checked everything. Honestly, digging into these problems teaches a lot. I tried to fix it on my on but couldn\\'t. Checked all the envs, paths but still can\\'t figure it out.\\n`airflow-airflow-webserver-1\\xa0 | 172.19.0.1 - - [28/Jan/2022:11:19:39 +0000] \"\"GET /object/task_instances?dag_id=data_ingestion_gcs_dag&amp;execution_date=2022-01-28T11%3A14%3A03.795060%2B00%3A00 HTTP/1.1\"\" 200 3019 \"\"<http://localhost:8080/graph?dag_id=data_ingestion_gcs_dag&amp;execution_date=2022-01-28+11%3A14%3A03.795060%2B00%3A00>\"\" \"\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36`\",1643369425.397059,1643369425.397059,U02QW0M1G9J\\n3ba751c0-1107-481e-a521-e922d5350dbb,U02QW0M1G9J,,,Did you check the logs on airflow ui?,1643369425.397059,1643369597.733619,U0290EYCA7Q\\n1ab09a93-1ee4-44e2-9977-1aa754c8f388,U02QW0M1G9J,,,Go to graph view -&gt; click on the task -&gt; view logs,1643369425.397059,1643369634.694399,U0290EYCA7Q\\nd8edcc37-e32e-4c05-8e5d-011956ccddc6,U02QW0M1G9J,,,\"<https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_2_data_ingestion/airflow/docker-compose.yaml#L63-L64>\\n\\nalso have you changed this?\",1643369425.397059,1643369637.998749,U01AXE0P5M3\\nd541daca-9dac-478f-a441-e2f948a883dd,,,,\"i keep getting exited with code 137 which upon checking i came to know that it wass related to OOM (out of memory), which I further investigated using docker inspect &lt;image-name&gt; and received OOMKilled: false, strange!\",,1643371093.970589,U029DM0GQHJ\\n95a112b5-1615-4b2d-baf8-b400e8d25eff,U02QW0M1G9J,,,\"<@U0290EYCA7Q> thanks, log helped me. I forgot to create folder inside .google and make typo in json name.\\nAlso, everytime I change yaml I haven\\'t rebuild image.\\nBut still log says im missing credentials.json.\\nWhat is the command to check directories inside container?\",1643369425.397059,1643371270.181629,U02QW0M1G9J\\nfa03013c-e854-4db3-ab4b-798b3f2a26ee,U02QW0M1G9J,,,Can you please paste the output for `echo $GOOGLE_APPLICATION_CREDENTIALS`,1643369425.397059,1643371362.729129,U0290EYCA7Q\\n08daddcc-d07e-4097-a7ad-6409d9aa5965,U02QW0M1G9J,,,Empty,1643369425.397059,1643371589.572939,U02QW0M1G9J\\n7c8a9101-57ff-4755-908d-04bc50a5e48e,U02QW0M1G9J,,,That\\'s the reason. You need to set it to google_credential.json,1643369425.397059,1643371643.766419,U0290EYCA7Q\\na02a6532-aa9a-4fcf-b7e7-87a131f42f48,U02QW0M1G9J,,,`export GOOGLE_APPLICATION_CREDENTIALS: ~/.google/credentials/google_credentials.json`,1643369425.397059,1643371671.472429,U0290EYCA7Q\\n10acb9be-7028-4074-a404-3883cc8c707a,,,,Did you try running with docker-compose-nofrills.yml? This might save memory.,,1643371773.905029,U0290EYCA7Q\\n29083580-e8ed-4966-82af-5d3f85398cc4,,11.0,,\"Think this will use sqlite for metadata, instead of postgres\",1643371828.797189,1643371828.797189,U0290EYCA7Q\\ncc94bf73-2ccf-4d49-8cd7-aa86adb29850,U0290EYCA7Q,,,how do i do that?,1643371828.797189,1643372185.120819,U029DM0GQHJ\\nb0ba00a1-f58b-4b8b-98ce-1f9bf8ad917f,U0290EYCA7Q,,,there is a compose file inside extras folder. Use it to build and run the container. Dont think we need to build again.,1643371828.797189,1643372247.642929,U0290EYCA7Q\\nd685e8cc-7317-4fda-a3ef-e9e172639c6a,U0290EYCA7Q,,,I obtained this error while I tried use docker-compose-nofrills.yml. *airflow.exceptions.AirflowConfigException: error: cannot use sqlite with the LocalExecutor*,1643371828.797189,1643373425.527749,U02TVSVB1D5\\n1c7e74bc-1034-4561-9d0a-574098f7eab7,U0290EYCA7Q,,,\"maybe, set postgres dependency alone, and try?\",1643371828.797189,1643373763.291319,U0290EYCA7Q\\n90e1d326-1a3d-4bad-918c-c5615e956a33,U02QW0M1G9J,,,<@U0290EYCA7Q> are these variables being erased every time I close terminal?,1643369425.397059,1643374116.189759,U02QW0M1G9J\\ncda2457c-dd62-42a0-be0e-165022880a68,U02QW0M1G9J,,,Yes. That\\'s the reason we use .bashrc / bash_profile files.,1643369425.397059,1643374168.374509,U0290EYCA7Q\\n399c8c65-ed10-41a8-bbc5-4da961a61760,U02QW0M1G9J,,,I miss this somewhere,1643369425.397059,1643374355.078049,U02QW0M1G9J\\n5556603c-31b1-4113-8834-d45315b989aa,U02UBV4EC8J,,,\"Parquet is the preferred format next to ORC. Delta Lake (not data lake) from Databricks has parquet as the default format. With Delta lake, ACID transactions can be achieved with Spark engine and much more.\",1643322602.141769,1643375237.262989,U02NSF7RYP4\\n74759b7c-3cc9-4e71-9470-e8faf1a8f6c1,U02UBV4EC8J,,,sounds great thanks guys!,1643322602.141769,1643375884.658619,U02UBV4EC8J\\n3a797abd-f58c-493c-9737-2570d150f143,U030PFH5CRX,,,\"I run docker in git bash and when I try to make the connection by doing \\'engine = create_engine(\\'<postgresql://root:root@localhost:5432/ny_taxi\\')\\'>, then I check the connection with engine.connect() and it throws me the error:\",1643303881.118400,1643376637.595249,U030PFH5CRX\\ncdb3f9d5-35a9-45b8-a271-d44cd1df8d96,U030PFH5CRX,,,,1643303881.118400,1643376726.195019,U030PFH5CRX\\n112cd400-2c7e-41a2-adba-05550b9f5337,U030PFH5CRX,,,Interesting - thanks. I though there was more in the error stack trace than that. How do you run postgres?,1643303881.118400,1643377027.985509,U01AXE0P5M3\\n18f8b02b-28ad-49db-b8d3-1aaa9359c1b0,U030PFH5CRX,,,\"Judging by the path, it seems you don\\'t use anaconda.\\n\\nI\\'d suggest using it (or miniconda), especially for windows. When it comes to packages with binary dependencies (such as psycopg2) on windows, versions coming from conda tend to work better\",1643303881.118400,1643377143.059509,U01AXE0P5M3\\ne95a2700-f2d2-4a62-85d0-f1885118279c,U030PFH5CRX,,,\"i run this  command in git bash, all Right?\",1643303881.118400,1643377197.684009,U030PFH5CRX\\n8d2ee873-05c6-4f66-b3de-b26e25efd9fc,U030PFH5CRX,,,\"looks right. I\\'d also suggest to put it to a path without spaces in it. For example \"\"c:/Users/ferna/git/de_zoomcamp\"\". Spaces in paths (esp. on windows) are always problematic\",1643303881.118400,1643377271.338639,U01AXE0P5M3\\n27c4f3e3-dcdc-4816-9c34-f83af7242fdb,,,,\"Data engineering is the apex of software engineering, we are combining data science, devops and cloud computing if i am correct.\\nThanks <@U01AXE0P5M3> <@U0290EYCA7Q> and the rest of you guys for putting this platform out for free.\",,1643378270.474849,U02T0CYNNP2\\nc68b6f8f-4a3d-4282-a8f1-5ae4488a3e86,U030PFH5CRX,,,\"I changed the file path and I still have the same problem, I also try to log in to the database with pgcli and it doesn\\'t do anything like in the photo\",1643303881.118400,1643378861.565489,U030PFH5CRX\\n57a4041c-6880-4462-b539-bb9f27228db8,U030PFH5CRX,,,running in miniconda would I have to do all the commands in that console?,1643303881.118400,1643378903.196889,U030PFH5CRX\\n8c805d03-9769-4efb-9c82-5406f5e9bc11,U030PFH5CRX,,,\"when you install it, check (using `which python` ) that you indeed use miniconda\\n\\nand then you can use gitbash as previously\",1643303881.118400,1643379579.227049,U01AXE0P5M3\\n3490ca2b-2f97-485b-a474-355a25a7f4f9,U030PFH5CRX,,,what do you see in the postres logs? is it actually running? (chekc with `docker ps`),1643303881.118400,1643379615.646129,U01AXE0P5M3\\n29d0c94e-ce1c-4a1d-96ae-a1a1c6c5017d,U030PFH5CRX,,,,1643303881.118400,1643379759.293269,U030PFH5CRX\\n36f8369c-d946-49a7-91b4-d2c9d6afb57e,U0290EYCA7Q,,,\"Ok, finally I hope I solve it. I just remove airflow-triggerer in the origin docker-compose file. I\\'m not sure that it\\'s the correct way, but seems container has started without errors\",1643371828.797189,1643379908.860669,U02TVSVB1D5\\n2d4e3880-66a6-4095-b68b-3dbc2245dfc0,U030PFH5CRX,,,i change localhost for 0.0.0.0 and give me other error:,1643303881.118400,1643380231.554939,U030PFH5CRX\\na0a58017-be6c-4d53-ab26-73a44056566f,U030PFH5CRX,,,\"yes 0.0.0.0 wont work, it should be localhost. What do you see in the logs in postgres?\",1643303881.118400,1643380338.516389,U01AXE0P5M3\\n6ee40323-57d6-41b7-af52-aacb9ab96cc5,U030PFH5CRX,,,but I have no idea why it might not be working for you. perhaps you can try using a cloud VM?,1643303881.118400,1643380451.929559,U01AXE0P5M3\\ne6295a76-03d7-4def-a4f4-ba615d202367,,4.0,,\"*Problem solved see thread.* I\\'m trying to recreate my gcp resources using Terraform (I destroyed them after week1) in order to run the airflow DAG. When I run terraform apply without changing anything from week 1, when it worked, I get an insufficient permissions error (full error in thread). Can anyone help with what permissions I need to adjust? I feel like it has something to do with the service account but I can\\'t figure it out\",1643380784.569359,1643380784.569359,U02TVGE99QU\\n83dbffff-9c04-4071-bddd-fc5a7eb2667a,U02TVGE99QU,,,\"```google_bigquery_dataset.dataset: Creating...\\ngoogle_storage_bucket.data-lake-bucket: Creating...\\n╷\\n│ Error: googleapi: Error 403: Provided scope(s) are not authorized, forbidden\\n│\\n│   with google_storage_bucket.data-lake-bucket,\\n│   on <http://main.tf|main.tf> line 21, in resource \"\"google_storage_bucket\"\" \"\"data-lake-bucket\"\":\\n│   21: resource \"\"google_storage_bucket\"\" \"\"data-lake-bucket\"\" {\\n│\\n╵\\n╷\\n│ Error: Error creating Dataset: googleapi: Error 403: Request had insufficient authentication scopes.\\n│ Details:\\n│ [\\n│   {\\n│     \"\"@type\"\": \"\"<http://type.googleapis.com/google.rpc.ErrorInfo|type.googleapis.com/google.rpc.ErrorInfo>\"\",\\n│     \"\"domain\"\": \"\"<http://googleapis.com|googleapis.com>\"\",\\n│     \"\"metadata\"\": {\\n│       \"\"method\"\": \"\"google.cloud.bigquery.v2.DatasetService.InsertDataset\"\",\\n│       \"\"service\"\": \"\"<http://bigquery.googleapis.com|bigquery.googleapis.com>\"\"\\n│     },\\n│     \"\"reason\"\": \"\"ACCESS_TOKEN_SCOPE_INSUFFICIENT\"\"\\n│   }\\n│ ]\\n│\\n│ More details:\\n│ Reason: insufficientPermissions, Message: Insufficient Permission\\n│\\n│\\n│   with google_bigquery_dataset.dataset,\\n│   on <http://main.tf|main.tf> line 47, in resource \"\"google_bigquery_dataset\"\" \"\"dataset\"\":\\n│   47: resource \"\"google_bigquery_dataset\"\" \"\"dataset\"\" {```\",1643380784.569359,1643380792.418539,U02TVGE99QU\\n22d2db32-e558-42c4-9ef1-4e62c50d7e87,U030PFH5CRX,,,\"okay,what vm cloud do you recommend?\",1643303881.118400,1643381710.418589,U030PFH5CRX\\neb16b292-d1f0-4c86-9230-90cce1860604,U0290EYCA7Q,,,<@U02TVSVB1D5> Thank you so much for sharing! I have been having so many issues with the docker-compose up but your suggestion seems to have worked! I hope deleting the triggerer doesnt come to bite us later,1643371828.797189,1643382005.186899,U02TBTX45LK\\n873C499C-7572-434E-A037-020FBE3B794B,U02TVGE99QU,,,\"Not really my area, but try going into GCP &gt; Service Accounts. Then update the roles for the service account you created. There should be 3 or 4 roles that we assigned in a previous video - one of them being BigQuery Admin.\",1643380784.569359,1643382022.707599,U02U34YJ8C8\\n5ec31f46-6131-46ba-bb5e-adb6b4d78e9e,U030PFH5CRX,,,Google Cloud. There\\'s a video in week 1 where I show how to set up everything,1643303881.118400,1643384246.473999,U01AXE0P5M3\\n43c306c0-9a19-46eb-b6e6-67c2f590e160,,4.0,,Good day everyone... Please i am about to start the Data Engineering class now and i was wondering if i can submit assignments for week 1...,1643384348.169029,1643384348.169029,U02SV2UV5GB\\n072a8437-ea88-4f02-a1bb-cff394b61c02,U02SV2UV5GB,,,\"No, the deadline is over we already published the solutions. But I\\'d encourage you to solve it yourself before checking the answers\",1643384348.169029,1643384454.439039,U01AXE0P5M3\\na2c135d3-f9b2-468c-9c93-1e0179c7c34b,,13.0,,\"Just started week 2. We are asked to rename our gcp-service-accounts-credentials file to google_credentials.json and to store in the `$HOME` directory. I don\\'t have any `$HOME` directory. The file is located within \\\\week1_basics_n_setup\\\\1_terraform_gcp I tried executing the suggested commands within that directory and it didn\\'t work. Sorry, I\\'m lacking some command line skills.\\n\\n```cd ~ &amp;&amp; mkdir -p ~/.google/credentials/\\nmv &lt;path/to/your/service-account-authkeys&gt;.json ~/.google/credentials/google_credentials.json```\",1643384587.773619,1643384587.773619,U02UX664K5E\\na333321a-439a-4e66-8a3d-7006fedded97,U02UX664K5E,,,You can replace ~ with any location,1643384587.773619,1643384737.302649,U01AXE0P5M3\\naea0b87f-e721-40d2-bd58-a5866f281c4a,U02UX664K5E,,,~ is a shortcut to your home,1643384587.773619,1643384754.816109,U01AXE0P5M3\\n952ce88a-163e-46f3-9dcb-1f00e668ade1,U02SV2UV5GB,,,Okay thanks,1643384348.169029,1643384819.413759,U02SV2UV5GB\\neaf0f4b3-1644-45a1-8411-84bb8058d66b,U02UKLHDWMQ,,,\"thanks, done with ssh and VM\",1643364682.323409,1643385057.738849,U02UKLHDWMQ\\n85f0ba8d-3086-4f78-b9c5-41f2cb9675da,U02UX664K5E,,,\"<@U01AXE0P5M3> :sweat_smile: Got it. The command basically renames the file for you, that\\'s all. Thanks!\",1643384587.773619,1643385290.923579,U02UX664K5E\\n324c5a71-1816-4c5a-b087-f7584dc7bbe2,U02UX664K5E,,,Exactly!,1643384587.773619,1643386050.547089,U01AXE0P5M3\\n7F1F3C56-1693-4626-B37C-2F0BB20ECD80,U02GVGA5F9Q,,,\"I\\'ve been stock in week 1 for 3days, it\\'s almost like a new challenge every new video, is anyone using Ubuntu and has been able to successfully complete week1, I really need your assistance \",1643126156.112200,1643386567.434079,U02SD163KD1\\n2df8e71a-b754-4827-b71a-96468a5cee35,,4.0,,\"Hi , I am not able to load files into gcs storage . The command to load files `blob.upload_from_filename(local_file)` fails with the error mentioned below. This represents the *local_to_gcs_task* section of the dag .\",1643386725.726689,1643386725.726689,U02Q7HGRB0F\\nddf2eeae-c290-4d0a-9dea-018b49317159,U0290EYCA7Q,,,<@U02TVSVB1D5> are you able to login at 0.0.0.0:8080?,1643371828.797189,1643386958.791259,U029DM0GQHJ\\n4b168e79-0a5e-4d60-9b81-422d7e38b794,U02GVGA5F9Q,,,I use ubuntu. Where are you stuck?,1643126156.112200,1643387368.779889,U01AXE0P5M3\\nbe383af6-9b89-4319-a22b-e40709b58fc9,U02Q7HGRB0F,,,does gcloud sdk work in your local env? it seems there\\'s some connection issues,1643386725.726689,1643387438.280159,U01AXE0P5M3\\n560eef3c-4b42-495a-b176-f84e1b7158e3,U0290EYCA7Q,,,your hack worked like a charm for me too,1643371828.797189,1643387546.343649,U029DM0GQHJ\\n80362f54-ddb5-4102-a2d1-169685fe5cf8,U0290EYCA7Q,,,any particular reason or any blog suggested to remove/comment it?,1643371828.797189,1643387587.552889,U029DM0GQHJ\\n86d191e1-302c-4a8c-b7d8-07685ec2ceba,U02TVGE99QU,,,that\\'s not it.. thanks though,1643380784.569359,1643387627.762299,U02TVGE99QU\\ncc6dd1f1-e7fe-40fe-879e-dc02b22a0638,U02Q7HGRB0F,,,\"Yes it does work in my local. Also i have logged into the docker container of the airflow web server and gcloud is working there too. I guess the python client `client = storage.Client()` is not able to assume the credentials.\\n\\nI also checked the file system of the docker container where the google credentials json file has been copied to `/.google/credentials:ro` in the docker container. I ahve also changed the env variables in the docker compose file\\n```    GCP_PROJECT_ID: \\'steam-sequencer-339109\\'\\n    GCP_GCS_BUCKET: \"\"dtc_data_lake_steam-sequencer-339109\"\"```\\n\",1643386725.726689,1643387984.670009,U02Q7HGRB0F\\nb8d5e37a-61f5-4f69-a062-46a30ca81b64,U0290EYCA7Q,,,I connected using localhost,1643371828.797189,1643388765.932869,U02TVSVB1D5\\n15e92e43-8012-4abb-9719-1971ca328c4d,U0290EYCA7Q,,,I think it\\'s working because u just delete some functions and so on. So I think that some part of image wouldn\\'t work at all. For example if u delete worker your tasks will not work fine,1643371828.797189,1643388946.398959,U02TVSVB1D5\\nbbf32e24-6a98-4074-844a-455df3226dd2,,25.0,,Stuck with the GCP credentials (local_to_gcs_task)... Anybody with this issue?,1643389675.008079,1643389675.008079,U02GVGA5F9Q\\n3a1e4025-0344-4c75-9a77-e91e77290e08,U02GVGA5F9Q,,,\"```*** Reading local file: /opt/airflow/logs/data_ingestion_gcs_dag/local_to_gcs_task/2022-01-28T17:05:01.424003+00:00/1.log\\n[2022-01-28, 17:05:18 UTC] {taskinstance.py:1032} INFO - Dependencies all met for &lt;TaskInstance: data_ingestion_gcs_dag.local_to_gcs_task manual__2022-01-28T17:05:01.424003+00:00 [queued]&gt;\\n[2022-01-28, 17:05:18 UTC] {taskinstance.py:1032} INFO - Dependencies all met for &lt;TaskInstance: data_ingestion_gcs_dag.local_to_gcs_task manual__2022-01-28T17:05:01.424003+00:00 [queued]&gt;\\n[2022-01-28, 17:05:18 UTC] {taskinstance.py:1238} INFO - \\n--------------------------------------------------------------------------------\\n[2022-01-28, 17:05:18 UTC] {taskinstance.py:1239} INFO - Starting attempt 1 of 2\\n[2022-01-28, 17:05:18 UTC] {taskinstance.py:1240} INFO - \\n--------------------------------------------------------------------------------\\n[2022-01-28, 17:05:18 UTC] {taskinstance.py:1259} INFO - Executing &lt;Task(PythonOperator): local_to_gcs_task&gt; on 2022-01-28 17:05:01.424003+00:00\\n[2022-01-28, 17:05:18 UTC] {standard_task_runner.py:52} INFO - Started process 177 to run task\\n[2022-01-28, 17:05:18 UTC] {standard_task_runner.py:76} INFO - Running: [\\'***\\', \\'tasks\\', \\'run\\', \\'data_ingestion_gcs_dag\\', \\'local_to_gcs_task\\', \\'manual__2022-01-28T17:05:01.424003+00:00\\', \\'--job-id\\', \\'33\\', \\'--raw\\', \\'--subdir\\', \\'DAGS_FOLDER/data_ingestion_gcs_dag.py\\', \\'--cfg-path\\', \\'/tmp/tmpz0m_apk4\\', \\'--error-file\\', \\'/tmp/tmpmqdaibd0\\']\\n[2022-01-28, 17:05:18 UTC] {standard_task_runner.py:77} INFO - Job 33: Subtask local_to_gcs_task\\n[2022-01-28, 17:05:18 UTC] {logging_mixin.py:109} INFO - Running &lt;TaskInstance: data_ingestion_gcs_dag.local_to_gcs_task manual__2022-01-28T17:05:01.424003+00:00 [running]&gt; on host 57babc3a3899\\n[2022-01-28, 17:05:19 UTC] {taskinstance.py:1426} INFO - Exporting the following env vars:\\nAIRFLOW_CTX_DAG_OWNER=***\\nAIRFLOW_CTX_DAG_ID=data_ingestion_gcs_dag\\nAIRFLOW_CTX_TASK_ID=local_to_gcs_task\\nAIRFLOW_CTX_EXECUTION_DATE=2022-01-28T17:05:01.424003+00:00\\nAIRFLOW_CTX_DAG_RUN_ID=manual__2022-01-28T17:05:01.424003+00:00\\n[2022-01-28, 17:05:19 UTC] {taskinstance.py:1700} ERROR - Task failed with exception\\nTraceback (most recent call last):\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1329, in _run_raw_task\\n    self._execute_task_with_callbacks(context)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1455, in _execute_task_with_callbacks\\n    result = self._execute_task(context, self.task)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1511, in _execute_task\\n    result = execute_callable(context=context)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py\"\", line 174, in execute\\n    return_value = self.execute_callable()\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py\"\", line 185, in execute_callable\\n    return self.python_callable(*self.op_args, **self.op_kwargs)\\n  File \"\"/opt/airflow/dags/data_ingestion_gcs_dag.py\"\", line 47, in upload_to_gcs\\n    client = storage.Client()\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/client.py\"\", line 128, in __init__\\n    _http=_http,\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/client.py\"\", line 318, in __init__\\n    _ClientProjectMixin.__init__(self, project=project, credentials=credentials)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/client.py\"\", line 266, in __init__\\n    project = self._determine_default(project)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/client.py\"\", line 285, in _determine_default\\n    return _determine_default_project(project)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/_helpers.py\"\", line 186, in _determine_default_project\\n    _, project = google.auth.default()\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/auth/_default.py\"\", line 459, in default\\n    credentials, project_id = checker()\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/auth/_default.py\"\", line 222, in _get_explicit_environ_credentials\\n    os.environ[environment_vars.CREDENTIALS]\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/auth/_default.py\"\", line 108, in load_credentials_from_file\\n    \"\"File {} was not found.\"\".format(filename)\\ngoogle.auth.exceptions.DefaultCredentialsError: File /.google/credentials/google_credentials.json was not found.\\n[2022-01-28, 17:05:19 UTC] {taskinstance.py:1277} INFO - Marking task as UP_FOR_RETRY. dag_id=data_ingestion_gcs_dag, task_id=local_to_gcs_task, execution_date=20220128T170501, start_date=20220128T170518, end_date=20220128T170519\\n[2022-01-28, 17:05:19 UTC] {standard_task_runner.py:92} ERROR - Failed to execute job 33 for task local_to_gcs_task\\nTraceback (most recent call last):\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/task/task_runner/standard_task_runner.py\"\", line 85, in _start_by_fork\\n    args.func(args, dag=self.dag)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/cli_parser.py\"\", line 48, in command\\n    return func(*args, **kwargs)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/cli.py\"\", line 92, in wrapper\\n    return f(*args, **kwargs)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py\"\", line 298, in task_run\\n    _run_task_by_selected_method(args, dag, ti)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py\"\", line 107, in _run_task_by_selected_method\\n    _run_raw_task(args, ti)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py\"\", line 184, in _run_raw_task\\n    error_file=args.error_file,\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py\"\", line 70, in wrapper\\n    return func(*args, session=session, **kwargs)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1329, in _run_raw_task\\n    self._execute_task_with_callbacks(context)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1455, in _execute_task_with_callbacks\\n    result = self._execute_task(context, self.task)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1511, in _execute_task\\n    result = execute_callable(context=context)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py\"\", line 174, in execute\\n    return_value = self.execute_callable()\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py\"\", line 185, in execute_callable\\n    return self.python_callable(*self.op_args, **self.op_kwargs)\\n  File \"\"/opt/airflow/dags/data_ingestion_gcs_dag.py\"\", line 47, in upload_to_gcs\\n    client = storage.Client()\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/client.py\"\", line 128, in __init__\\n    _http=_http,\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/client.py\"\", line 318, in __init__\\n    _ClientProjectMixin.__init__(self, project=project, credentials=credentials)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/client.py\"\", line 266, in __init__\\n    project = self._determine_default(project)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/client.py\"\", line 285, in _determine_default\\n    return _determine_default_project(project)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/_helpers.py\"\", line 186, in _determine_default_project\\n    _, project = google.auth.default()\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/auth/_default.py\"\", line 459, in default\\n    credentials, project_id = checker()\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/auth/_default.py\"\", line 222, in _get_explicit_environ_credentials\\n    os.environ[environment_vars.CREDENTIALS]\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/auth/_default.py\"\", line 108, in load_credentials_from_file\\n    \"\"File {} was not found.\"\".format(filename)\\ngoogle.auth.exceptions.DefaultCredentialsError: File /.google/credentials/google_credentials.json was not found.\\n[2022-01-28, 17:05:19 UTC] {local_task_job.py:154} INFO - Task exited with return code 1\\n[2022-01-28, 17:05:19 UTC] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check```\\n\",1643389675.008079,1643389688.695639,U02GVGA5F9Q\\n5f31143f-5ac7-45c5-8250-c72311ad67d6,U02GVGA5F9Q,,,\"```google.auth.exceptions.DefaultCredentialsError: File /.google/credentials/google_credentials.json was not found.```\\n\",1643389675.008079,1643389725.116069,U02GVGA5F9Q\\nceb5843b-6b6a-48e9-ac65-2a725fad4953,U02GVGA5F9Q,,,Can you bash to the worker container and check that the credentials file is there?,1643389675.008079,1643389738.631859,U01AXE0P5M3\\n46ce3706-566d-407b-bfa5-00cdc483c543,U02GVGA5F9Q,,,\"Yes, and the credentials aren\\'t there...\",1643389675.008079,1643390049.557809,U02GVGA5F9Q\\n1b276f12-f912-4046-8799-8a869fef1fd1,U02TVGE99QU,,,So what was happening was that when I set the environment variable to point at my credentials I didn\\'t make it permanent so when I logged out after week1 and logged back in I needed to reset the environment variable. I don\\'t know if this is the best thing to do but after some googling I just added the export GOOGLE_CREDENTIALS line from Sejal\\'s terraform notes to my .profile,1643380784.569359,1643390131.215189,U02TVGE99QU\\n4ecab6fd-7289-46a8-971f-c464f02ec384,U02GVGA5F9Q,,,\"volumes:\\n    - ./dags:/opt/airflow/dags\\n    - ./logs:/opt/airflow/logs\\n    - ./plugins:/opt/airflow/plugins\\n    - ~/.google/credentials/:/.google/credentials:ro\",1643389675.008079,1643390132.195099,U02GVGA5F9Q\\nc3896930-b071-4906-92ed-4ead50892ae4,U02GVGA5F9Q,,,hold on,1643389675.008079,1643390189.335819,U02GVGA5F9Q\\nf0cbdf61-c224-4ef9-8a78-0c9319cfd0c6,U02GVGA5F9Q,,,\"the folder is there, but is empty\",1643389675.008079,1643390231.763549,U02GVGA5F9Q\\n4e13f9d9-1e2e-4ca4-8066-50a5cbbb4f22,U02GVGA5F9Q,,,\"default@57babc3a3899:/opt/airflow$ ls -al /.google/credentials\\ntotal 8\\ndrwxr-xr-x 2 root root 4096 Jan 27 20:26 .\\ndrwxr-xr-x 3 root root 4096 Jan 28 17:02 ..\",1643389675.008079,1643390266.531139,U02GVGA5F9Q\\n82935436-8c2d-4266-8ec0-5d052fa0f84f,U02UX664K5E,,,\"<@U02UX664K5E>\\n$ cd ~&amp;&amp; mkdir -p ~/.google/credentials/\\n\\nUser1@DESKTOP-PD6UM8A MINGW64 ~\\n$ mv C:\\\\Users\\\\User1\\\\Downloads.json ~/.google/credentials/google_credentials.json\\nmv: cannot stat \\'C:UsersUser1Downloads.json\\': No such file or directory\\n\\nUser1@DESKTOP-PD6UM8A MINGW64 ~\\n$ mv &lt;C://Users//User1//Downloads.json&gt; ~//.google//credentials//google_credentials.json\\nmv: cannot stat \\'&lt;C://Users//User1//Downloads.json&gt;\\': No such file or directory\\n\\nUser1@DESKTOP-PD6UM8A MINGW64 ~\\n$ cd //.google//credentials//\\nbash: cd: //.google//credentials//: No such file or directory\",1643384587.773619,1643390354.801189,U02VBG59VQ9\\n95bb324a-7636-4bea-8f9d-ff46d3fec82e,U02UX664K5E,,,I am using Mingw64 on windows FYI,1643384587.773619,1643390397.620009,U02VBG59VQ9\\na5443f89-fd90-4f89-ba86-51b1ee6caa34,U02GVGA5F9Q,,,You have credential.json here on local? `~/.google/credentials`,1643389675.008079,1643390424.867689,U0290EYCA7Q\\n2fc346a1-ed6a-4240-b562-5905611bb3e5,U02GVGA5F9Q,,,\"[nleal@Dinux ~]$ ll .google/credentials\\ntotal 12\\ndrwxrwxr-x 2 nleal nleal 4096 jan 26 20:07 ./\\ndrwxrwxr-x 3 nleal nleal 4096 jan 26 20:04 ../\\n-rw-rw-r-- 1 nleal nleal 2316 jan 24 20:04 google_credentials.json\",1643389675.008079,1643390506.143989,U02GVGA5F9Q\\na9e49008-d64d-4dd9-8393-425885c25fb5,U02GVGA5F9Q,,,ah! I see! The name is wrong...,1643389675.008079,1643390563.105939,U02GVGA5F9Q\\n105e1bd0-ad83-4176-b39b-ba28e73d23be,U02UX664K5E,,,\"Hey <@U02VBG59VQ9>, I\\'m also using MINGW64, the commands I ran were:\\n`cd ~ &amp;&amp; mkdir -p ~/Desktop/DE-ZoomCamp/week1_basics_n_setup/1_terraform_gcp`\\n\\nAnd then:\\n`mv ~/Desktop/DE-ZoomCamp/week1_basics_n_setup/1_terraform_gcp/YOUR_KEY_FILENAME.json ~/Desktop/DE-ZoomCamp/week1_basics_n_setup/1_terraform_gcp/google_credentials.json`\",1643384587.773619,1643390594.083279,U02UX664K5E\\nf0da855f-b300-4b99-ae0e-90556f514f25,U02GVGA5F9Q,,,\"according to the error message, the name is correct - there must be another issue\",1643389675.008079,1643390657.453579,U02GVGA5F9Q\\n5e6d7813-5c98-4b72-b907-ddea786c32b5,U02UX664K5E,,,is that where it needs to be?,1643384587.773619,1643390778.761219,U02VBG59VQ9\\n7814550f-bda4-4cec-aede-f112967fd2e5,U02UX664K5E,,,<@U02UX664K5E>,1643384587.773619,1643390797.252259,U02VBG59VQ9\\n28a237d1-81e6-4eec-922d-ac9acd8e4a7e,U02UX664K5E,,,\"no, that is only an example, you can choose whatever location you want, I\\'m just following the same folder structure as the course to make things easier.\",1643384587.773619,1643390855.368949,U02UX664K5E\\n5598711f-2f45-4c42-843d-622960342e11,U02UX664K5E,,,<@U02UX664K5E> ok that is a good idea. It is in week2,1643384587.773619,1643390931.197479,U02VBG59VQ9\\n8c32049e-f099-4842-a015-996e8845c36e,U02UX664K5E,,,thank you,1643384587.773619,1643390937.182949,U02VBG59VQ9\\n17f16f09-dd67-42e2-b153-e292d0c739be,U02Q9P0A0NA,,,\"Apologies if I\\'m stating the obvious here, but did you change the project ID and credentials path before or after you ran `docker-compose build`? You will need to re-run the build if you did it after, as these arguments are baked in at build time.\",1643299377.100400,1643391092.865709,U02UXRH2SV6\\n9dd991eb-6a82-40a6-abc7-6a9540aa2e96,,2.0,,\"Hi all! I\\'m not receiving the correct postgres-like in my jupyter, any idea?\\n\\n```engine = create_engine(\\'<postgresql://root:root@localhost:5432/ny_taxi>\\')\\nengine.connect()\\n&lt;sqlalchemy.engine.base.Connection at 0x7fabb9f2d070&gt;\\n\\nprint(pd.io.sql.get_schema(df, name=\\'yellow_taxi_data\\', con=engine))\\nCREATE TABLE \"\"yellow_taxi_data\"\" (\\n\"\"VendorID\"\" INTEGER,\\n  \"\"tpep_pickup_datetime\"\" TIMESTAMP,\\n  \"\"tpep_dropoff_datetime\"\" TIMESTAMP,\\n  \"\"passenger_count\"\" INTEGER,\\n  \"\"trip_distance\"\" REAL,\\n  \"\"RatecodeID\"\" INTEGER,\\n  \"\"store_and_fwd_flag\"\" TEXT,\\n  \"\"PULocationID\"\" INTEGER,\\n  \"\"DOLocationID\"\" INTEGER,\\n  \"\"payment_type\"\" INTEGER,\\n  \"\"fare_amount\"\" REAL,\\n  \"\"extra\"\" REAL,\\n  \"\"mta_tax\"\" REAL,\\n  \"\"tip_amount\"\" REAL,\\n  \"\"tolls_amount\"\" REAL,\\n  \"\"improvement_surcharge\"\" REAL,\\n  \"\"total_amount\"\" REAL,\\n  \"\"congestion_surcharge\"\" REAL\\n)```\",1643392038.642349,1643392038.642349,U02910MJNBY\\n109bc86f-084e-434a-a5ed-458d2964207a,U02GVGA5F9Q,,,\"```--- a/week_2_data_ingestion/airflow/docker-compose.yaml\\n+++ b/week_2_data_ingestion/airflow/docker-compose.yaml\\n@@ -60,14 +60,14 @@ x-airflow-common:\\n     _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}\\n     GOOGLE_APPLICATION_CREDENTIALS: /.google/credentials/google_credentials.json\\n     AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT: \\'google-cloud-platform://?extra__google_cloud_platform__key_path=/.google/credentials/google_credentials.json\\'\\n-    GCP_PROJECT_ID: \\'pivotal-surfer-336713\\'\\n-    GCP_GCS_BUCKET: \"\"dtc_data_lake_pivotal-surfer-336713\"\"\\n+    GCP_PROJECT_ID: \\'ny-rides-alexey\\'\\n+    GCP_GCS_BUCKET: \"\"dtc_data_lake_ny-rides-alexey\"\"\\n\\n   volumes:\\n     - ./dags:/opt/airflow/dags\\n     - ./logs:/opt/airflow/logs\\n     - ./plugins:/opt/airflow/plugins\\n-    - ~/.google/credentials/:/.google/credentials:ro\\n+    - c:/Users/alexe/.google/credentials/:/.google/credentials:ro```\\nThis is what I needed to change\",1643389675.008079,1643392045.542719,U01AXE0P5M3\\nc2eb4389-ac65-48d1-a574-20eb7aa0b667,U02GVGA5F9Q,,,you removed the \\'~\\' and replaced it with the absolute path?,1643389675.008079,1643392143.476459,U02GVGA5F9Q\\n4d2b7108-ff2e-40f4-ba9d-0d3d3e145fcc,U02GVGA5F9Q,,,\"These are the environmental variables from docker. I dont see anyone named CREDENTIALS. Shouldn\\'t it be `GOOGLE_APPLICATION_CREDENTIALS` ?\\n\\n\\n```    os.environ[environment_vars.CREDENTIALS]\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/auth/_default.py\"\", line 108, in load_credentials_from_file\\n    \"\"File {} was not found.\"\".format(filename)```\\n```[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - _=/usr/bin/printenv\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - DEBIAN_FRONTEND=noninteractive\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - AIRFLOW__CORE__FERNET_KEY=\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - BUILD_ID=0\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - AIRFLOW__API__AUTH_BACKEND=***.api.auth.backend.basic_auth\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - AIRFLOW_UID=50000\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - PYTHON_BASE_IMAGE=<http://ghcr.io/apache/***/main/python:3.7-slim-buster|ghcr.io/apache/***/main/python:3.7-slim-buster>\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - PATH=/home/google-cloud-sdk/bin/:/home/***/.local/bin:/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - _MP_FORK_LOGLEVEL_=20\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - INSTALL_MYSQL_CLIENT=true\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - PYTHON_GET_PIP_URL=<https://github.com/pypa/get-pip/raw/3cb8888cc2869620f57d5d2da64da38f516078c7/public/get-pip.py>\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - GCLOUD_HOME=/home/google-cloud-sdk\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - LC_ALL=C.UTF-8\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - AIRFLOW_INSTALLATION_METHOD=apache-***\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://***:***@postgres/***\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - _MP_FORK_LOGFILE_=\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - PYTHON_GET_PIP_SHA256=c518250e91a70d7b20cceb15272209a4ded2a0c263ae5776f129e0d9b5674309\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - LC_CTYPE=C.UTF-8\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - RUNTIME_APT_COMMAND=echo\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - CELERY_LOG_LEVEL=20\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - PYTHON_PIP_VERSION=21.2.4\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - RUNTIME_APT_DEPS=       apt-transport-https        apt-utils        ca-certificates        curl        dumb-init        freetds-bin        gosu        krb5-user        ldap-utils        libffi6        libldap-2.4-2        libsasl2-2        libsasl2-modules        libssl1.1        locales         lsb-release        netcat        openssh-client        postgresql-client        rsync        sasl2-bin        sqlite3        sudo        unixodbc\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - LC_MESSAGES=C.UTF-8\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - SHLVL=1\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - _PIP_ADDITIONAL_REQUIREMENTS=\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - ADDITIONAL_RUNTIME_APT_COMMAND=\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - AIRFLOW_CTX_EXECUTION_DATE=2022-01-27T19:38:35.420106+00:00\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - AIRFLOW_PIP_VERSION=21.3.1\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - COMMIT_SHA=d69a42fa25c5b1e0006cb3b1c28ad9e49cad7a47\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - AIRFLOW__CORE__EXECUTOR=CeleryExecutor\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - AIRFLOW_CTX_DAG_ID=data_ingestion_gcs_dag\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - AIRFLOW_CTX_TASK_ID=print_passwd_task\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - GPG_KEY=########################\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - CELERY_LOG_REDIRECT_LEVEL=WARNING\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - AIRFLOW_VERSION_SPECIFICATION=\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - AIRFLOW_HOME=/opt/***\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - LANG=C.UTF-8\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - HOME=/home/***\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libstdc++.so.6\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - GUNICORN_CMD_ARGS=--worker-tmp-dir /dev/shm\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - GCP_PROJECT_ID=dtc-de-course-*******\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - PYTHON_SETUPTOOLS_VERSION=57.5.0\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - INSTALL_MSSQL_CLIENT=true\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - GOOGLE_APPLICATION_CREDENTIALS=/.google/credentials/google_credentials.json\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - _MP_FORK_LOGFORMAT_=[%(asctime)s: %(levelname)s/%(processName)s] %(message)s\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - AIRFLOW__CORE__LOAD_EXAMPLES=false\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - AIRFLOW_VERSION=2.2.3\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - AIRFLOW__CELERY__BROKER_URL=redis://:@redis:6379/0\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - PWD=/tmp/***tmpd8stvd2c\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - ADDITIONAL_RUNTIME_APT_DEPS=\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - AIRFLOW_USER_HOME_DIR=/home/***\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - AIRFLOW_CTX_DAG_RUN_ID=manual__2022-01-27T19:38:35.420106+00:00\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - CELERY_LOG_REDIRECT=1\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - GCP_GCS_BUCKET=dtc_data_lake_dtc-de-course-339017\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT=google-cloud-platform://?extra__google_cloud_platform__key_path=/.google/credentials/google_credentials.json\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - LANGUAGE=C.UTF-8\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - PYTHON_VERSION=3.7.12\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - HOSTNAME=750a21bdd7eb\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - DUMB_INIT_SETSID=0\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - AIRFLOW_CTX_DAG_OWNER=***\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://***:***@postgres/***\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:89} INFO - CELERY_LOG_FILE=\\n[2022-01-27, 19:38:42 UTC] {subprocess.py:93} INFO - Command exited with return code 0```\",1643389675.008079,1643392181.270119,U0290EYCA7Q\\nf45cd5ab-0074-428b-a2f3-a4ea324e9ca7,U02GVGA5F9Q,,,\"yes, I do that on windows automatically - you never know what it\\'ll do with `~`\",1643389675.008079,1643392186.853489,U01AXE0P5M3\\n3be9ea24-1f34-41d8-9a32-8cdfcd7be94c,U02GVGA5F9Q,,,\"Success!!!! Thanks Alexey! I did suffer from this issue before, but never thought could be it... And this is my Ubuntu machine, so it is not a Windows feature.\",1643389675.008079,1643392472.673109,U02GVGA5F9Q\\n8ab5e3d4-4219-44f6-8af0-c3542619fc73,U02Q7HGRB0F,,,\"Hi Alexy\\nI was able to solve it by downloading a fresh google credentials file from gcs. But still , i am not sure why the old credential file didn\\'t work. Is there any expiry period that you may be aware of ?\",1643386725.726689,1643392534.830949,U02Q7HGRB0F\\nC7763632-5BD9-42D8-8B0F-517CCE1EED82,U02910MJNBY,,,Try restart your kernel and try again ,1643392038.642349,1643392583.711049,U02TBCXNZ60\\n61fa7923-d697-4791-9d23-1d091ca40a8e,U02GVGA5F9Q,,,\"thanks <@U0290EYCA7Q>, the issue was an offending \"\"~\"\"...\",1643389675.008079,1643392592.159609,U02GVGA5F9Q\\n773b9a79-e6af-4936-b2fb-2aaeb3ab565c,,,,\"hi\\nafter commenting out the configuration as suggested in this message, i was able to login to airflow and trigger the DAG and could see all greens and of course the table in my gcp bucket:\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1643379908860669?thread_ts=1643371828.797189&amp;cid=C01FABYF2RG>\\n\\nwhat might be the potential consequence as we progress with the module?\",,1643394000.572069,U029DM0GQHJ\\nf11fd0cd-84e6-4a3d-b806-bacd5753fbf8,U02Q7HGRB0F,,,I\\'m not aware of any,1643386725.726689,1643394014.273109,U01AXE0P5M3\\n7f1e6bd8-bac9-479f-8ccb-634a7032cc77,U02GVGA5F9Q,,,I think in ubuntu it\\'s also safer to use `${HOME}` rather than `~`,1643389675.008079,1643394049.795649,U01AXE0P5M3\\nBCC88F73-902A-4CA6-8268-151C62A51D32,,1.0,,Is homework 2 posted yet?,1643394617.024309,1643394617.024309,U02U6DR551B\\n612bf0b5-7e77-4471-9bcb-3f59c599cd75,U02U6DR551B,,,Almost. You can take a look at the draft here: <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/week2-homework/week_2_data_ingestion/homework.md>,1643394617.024309,1643394657.268289,U01AXE0P5M3\\n26e97b0b-8dba-4279-b564-d681d5484f80,,2.0,,\"I can\\'t access postgres locally after adding the port this is the error\\n\\nTraceback (most recent call last):\\n  File \"\"/home/simeon/anaconda3/bin/pgcli\"\", line 5, in &lt;module&gt;\\n    from pgcli.main import cli\\n  File \"\"/home/simeon/anaconda3/lib/python3.9/site-packages/pgcli/main.py\"\", line 27, in &lt;module&gt;\\n    from prompt_toolkit import CommandLineInterface, Application, AbortAction\\nImportError: cannot import name \\'CommandLineInterface\\' from \\'prompt_toolkit\\' (/home/simeon/anaconda3/lib/python3.9/site-packages/prompt_toolkit/__init__.py)\",1643396262.711509,1643396262.711509,U02UKLHDWMQ\\n41b2061b-b6d5-4221-94e6-132b848a0646,,7.0,,\"Hi , I have just started out with the course and was looking at the docker video - 1.2.2\\nI have created the `test:pandas` image and specified pandas to be installed .\\nwhen I do `docker run -it test:pandas` and check the version of pandas i can see it\\nbut when I start the jupyter notebook , in the `upload_data.ipynb` notebook ,getting the error that `NO module named pandas` .\\nCan someone please suggest if I am missing out on something ?\",1643396374.451419,1643396374.451419,U02ULL45DNV\\n29378987-5c58-4873-9c5f-cda8a30a66ec,U02UKLHDWMQ,,,\"If you have troubles with pgcli, don\\'t spend too much time on it. It\\'s not essential for the course. There\\'s another video where I show how to do the same thing, but with pandas\",1643396262.711509,1643396798.319199,U01AXE0P5M3\\nc3ca1a6e-423d-4416-9ca7-bd5dcfdb4192,U02UKLHDWMQ,,,ok thanks,1643396262.711509,1643396821.869839,U02UKLHDWMQ\\n6c9f4065-3b98-4982-9b88-2d21042c1622,U02ULL45DNV,,,Did you maybe forget to save your Dockerfile?,1643396374.451419,1643396838.139669,U01AXE0P5M3\\n7eb5161c-1777-432e-b304-0c66b1c6ebf0,U02ULL45DNV,,,\"No , I have saved it\",1643396374.451419,1643396859.848049,U02ULL45DNV\\n8210185b-6148-4c13-a671-8b4305bd024a,U02ULL45DNV,,,Wait. Jupyter notebooks are different from your container. You need to pip install pandas on your host as well,1643396374.451419,1643396896.212279,U01AXE0P5M3\\n8d8b5c11-ef2d-4b0c-b1d2-f89876a233d6,U02ULL45DNV,,,\"are you running the upload_data.ipynb from your docker container or from your local computer\\n\\n\\nIf it is from your local computer, I will advise you install pandas with pip\",1643396374.451419,1643396967.912999,U02SPLJUR42\\n5f46cff9-13f8-4f9a-8cfe-c0a13c545428,U02ULL45DNV,,,\"I am running the file from local , and also have installed pandas on my host. Still not\",1643396374.451419,1643397467.532939,U02ULL45DNV\\ncec4be1b-ae26-4b30-bab8-532f414d68a3,,7.0,,\"So I just finished my notes for Airflow… I thought last week was dense but this week has been even more intense so far. I still need to watch the Data Transfer videos, but since the Airflow part is finished, I figured that it might come in handy for some of you during the weekend as a reference to complete the homework.\\n\\n<https://github.com/ziritrion/dataeng-zoomcamp/blob/main/notes/2_data_ingestion.md>\",1643397626.412989,1643397626.412989,U02BVP1QTQF\\n5f3c7ab5-7ebd-4ece-be33-aed5a66d414e,,4.0,,\"Also, I’ve got a question regarding Airflow’s UI. When we trigger a DAG, there’s a Run field with a default value of 25. I’m assuming that it will run the DAG 25 times but I honestly don’t fully understand how it works nor how we’re supposed to use it.\",1643397723.580519,1643397723.580519,U02BVP1QTQF\\n486a21d4-3601-41f0-8afb-c303eba92dd7,U02BVP1QTQF,,,\"Thanks Alvaro! \\n\\nWhat do you think about the density? Should it be less dense? Or it\\'s okay?\",1643397626.412989,1643397915.842729,U01AXE0P5M3\\n782e6ae3-da98-4cb9-b50b-3f694ac63162,U02ULL45DNV,,,is it okay to install pandas on directly jupyter notebook and  what difference does it make if any,1643396374.451419,1643397939.625999,U02ULL45DNV\\n94fb877c-941d-4692-8f5e-f9a820f7ffe8,,1.0,,please which service account json file Alexey was talking about that we need before we can run terraform,1643398799.967989,1643398799.967989,U02UKLHDWMQ\\n6e910f63-7b2a-4e90-a285-82148f566b7c,U02ULL45DNV,,,It\\'s okay,1643396374.451419,1643398862.918249,U01AXE0P5M3\\na82ec0cd-6dc3-449a-a1ee-077bba1b6411,U02GVGA5F9Q,,,\"<@U02GVGA5F9Q>\\n```File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/bucket.py\"\", line 1098, in path\\n    raise ValueError(\"\"Cannot determine path without bucket name.\"\")\\nValueError: Cannot determine path without bucket name.\\n[2022-01-28, 19:51:34 UTC] {local_task_job.py:154} INFO - Task exited with return code 1\\n[2022-01-28, 19:51:34 UTC] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check```\\n\",1643389675.008079,1643399750.447129,U02VBG59VQ9\\ne75eaff2-f729-454a-acd9-24a6562a9d3d,,,,\"Just a note, if anyone already had a certain version of Terraform installed on their local machine and that is going to cause issues with other projects they are working on there is something called <https://github.com/tfutils/tfenv|TfEnv> that allows you to switch between versions of Terraform.\\n\\nIf you have ever used pyenv or rbenv, this should be quite familiar.\\n\\nHomebrew can be used to install TfEnv:\\n```brew install tfenv```\\nNOTE! If you already have terraform installed with brew you’ll have to unlink it with `brew unlink terraform`\\n\\nTo install new versions of Terraform:\\n```tfenv install &lt;terraform_version_string&gt;```\\nTo swap the currently used version of Terraform:\\n```tfenv use &lt;terraform_version_string&gt;```\\nNow you’re better equipped to hop between projects :smiley:\",,1643399851.348779,U02TEERF0DA\\n6b5ebace-8e2e-45c9-9858-897803425f1b,,2.0,,\"any help is appreciated\\n```File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/bucket.py\"\", line 1098, in path\\n    raise ValueError(\"\"Cannot determine path without bucket name.\"\")\\nValueError: Cannot determine path without bucket name.\\n[2022-01-28, 19:51:34 UTC] {local_task_job.py:154} INFO - Task exited with return code 1\\n[2022-01-28, 19:51:34 UTC] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check```\\n\",1643399968.823989,1643399968.823989,U02VBG59VQ9\\nb23d8792-0d4c-4392-b08e-28e7e5abf764,U02GVGA5F9Q,,,Did you set your docker-compose.yaml file to point to your project and bucket?,1643389675.008079,1643399973.202269,U02GVGA5F9Q\\n7f76e85f-1160-4686-a5d0-59e05683dd44,U02GVGA5F9Q,,,\"GOOGLE_APPLICATION_CREDENTIALS: /.google/credentials/google_credentials.json\\n    AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT: \\'google-cloud-platform://?extra__google_cloud_platform__key_path=/.google/credentials/google_credentials.json\\'\\n    GCP_PROJECT_ID: \\'dtc-de-339219\\'\\n    GCP_GCS_BUCKET: \"\"dtc_data_lake_dtc-de-339219\"\"\",1643389675.008079,1643399977.751239,U02GVGA5F9Q\\n1bad89ad-e44c-486c-bd4c-960121191774,U02GVGA5F9Q,,,yes,1643389675.008079,1643399987.604389,U02VBG59VQ9\\n2ea2e886-9c93-40ea-9c82-a1121f0920c0,U02GVGA5F9Q,,,\"GCP_PROJECT_ID: \\'dezoomcamp22\\'\\n\\xa0 \\xa0 GCP_GCS_BUCKET: \"\"dtc_data_lake_dezoomcamp22\"\"\",1643389675.008079,1643400026.035239,U02VBG59VQ9\\nc3710049-c84b-4f8b-b7f6-04bc5c0a47e0,U02910MJNBY,,,\"You were right, thanks!\",1643392038.642349,1643400623.442929,U02910MJNBY\\n71d37879-c7a5-46d8-a689-e132b51e6865,U02GVGA5F9Q,,,I don\\'t know which task is complaining. but I suspect it\\'s the `local_to_gcs_task`. You set the other two variables I pasted here?,1643389675.008079,1643400640.193579,U02GVGA5F9Q\\n6c63eb8a-81de-4d1f-bd59-9fa5cf455cc4,,12.0,,\"Hey Everyone,\\n\\nGetting an error when running the first task in airflow. Details in thread\",1643400810.237809,1643400810.237809,U02UAFF1WU9\\nfd4788d5-3897-43e5-a8f0-0a7b61315191,U02UAFF1WU9,,,\"Log:\\n```PermissionError: [Errno 13] Permission denied: \\'/opt/airflowyellow_tripdata_2021-01.csv\\'```\\nI am on a mac and instead of using wget to download the CSV file, I am using pandas but getting an issue when using the `<http://pd.to|pd.to>_csv` method to save the file at the location specified in the video. Any ideas?\\n\\nmy code:\\n\\n```PROJECT_ID = os.environ.get(\"\"GCP_PROJECT_ID\"\")\\nBUCKET = os.environ.get(\"\"GCP_GCS_BUCKET\"\")\\n\\ndataset_file = \"\"yellow_tripdata_2021-01.csv\"\"\\ndataset_url = f\"\"<https://s3.amazonaws.com/nyc-tlc/trip+data/{dataset_file}>\"\"\\npath_to_local_home = os.environ.get(\"\"AIRFLOW_HOME\"\", \\'/opt/airflow/\\')\\nparquet_file = dataset_file.replace(\\'.csv\\', \\'.parquet\\')\\nBIGQUERY_DATASET = os.environ.get(\\'BIGQUERY_DATASET\\', \\'trips_data_all\\')\\n\\n\\ndef download_file(dataset_url):\\n\\n    df = pd.read_csv(dataset_url)\\n    df.to_csv(\\n        path_to_local_home + dataset_file)```\",1643400810.237809,1643400947.494769,U02UAFF1WU9\\n29adca94-10d9-49ab-b016-02c93e530462,U02SV2UV5GB,,,Where are the solutions published?,1643384348.169029,1643401020.366209,U02U809EAE7\\n8b64eded-7b4d-4429-adf5-8e4d95b56e95,U02SV2UV5GB,,,youtube channel and <#C02V1Q9CL8K|announcements-course-data-engineering>,1643384348.169029,1643401078.898749,U01AXE0P5M3\\n6f5ab2bc-b338-4909-b8ac-5e42fb02c387,U02UAFF1WU9,,,Have you set airflow user?,1643400810.237809,1643401452.594179,U01AXE0P5M3\\n0f16805e-391b-43b4-b5b4-ce27e3913178,U02UAFF1WU9,,,AIRFLOW_UID variable,1643400810.237809,1643401459.813249,U01AXE0P5M3\\n4e545f48-b86a-47f6-b9f7-9892cf208727,U02UAFF1WU9,,,\"i have it set in `.env` file &amp; it is referenced in the docker-compose.yaml\\n```##.env\\nAIRFLOW_UID=501```\\n```version: \\'3\\'\\nx-airflow-common:\\n  &amp;airflow-common\\n  # In order to add custom dependencies or upgrade provider packages you can use your extended image.\\n  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml\\n  # and uncomment the \"\"build\"\" line below, Then run `docker-compose build` to build the images.\\n  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.2.3}\\n  # build: .\\n  environment:\\n    &amp;airflow-common-env\\n    AIRFLOW__CORE__EXECUTOR: CeleryExecutor\\n    AIRFLOW__CORE__SQL_ALCHEMY_CONN: <postgresql+psycopg2://airflow:airflow@postgres/airflow>\\n    AIRFLOW__CELERY__RESULT_BACKEND: <db+postgresql://airflow:airflow@postgres/airflow>\\n    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0\\n    AIRFLOW__CORE__FERNET_KEY: \\'\\'\\n    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: \\'true\\'\\n    AIRFLOW__CORE__LOAD_EXAMPLES: \\'false\\'\\n    AIRFLOW__API__AUTH_BACKEND: \\'airflow.api.auth.backend.basic_auth\\'\\n    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}\\n    GOOGLE_APPLICATION_CREDENTIALS: /.google/credentials/google_credentials.json\\n    AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT: \\'google-cloud-platform://?extra__google_cloud_platform__key_path=/.google/credentials/google_credentials.json\\'\\n    GCP_PROJECT_ID: \\'erudite-coast-339015\\'\\n    GCP_GCS_BUCKET: \"\"dtc_data_lake_erudite-coast-339015\"\"\\n  volumes:\\n    - ./dags:/opt/airflow/dags\\n    - ./logs:/opt/airflow/logs\\n    - ./plugins:/opt/airflow/plugins\\n    - ~/.google/credentials/:/.google/credentials:ro\\n  user: \"\"${AIRFLOW_UID:-50000}:0\"\"```\",1643400810.237809,1643401525.337739,U02UAFF1WU9\\n955de651-85a2-4ef8-a54c-36f940b7a0a2,U02UAFF1WU9,,,You need to set it before running the build. It\\'s also specified in the docker file,1643400810.237809,1643401668.149879,U01AXE0P5M3\\ne76a85a2-178c-4f60-aeef-a7157c12c21b,U02UAFF1WU9,,,\"Oh if you have it in .env, then it\\'s okay\",1643400810.237809,1643401708.476189,U01AXE0P5M3\\n8d1a9083-caf0-4f1d-b18a-363d3c50cada,U02UAFF1WU9,,,\"Yeah. I followed the steps below:\\n\\n```Set the Airflow user:\\n\\nOn Linux, the quick-start needs to know your host user-id and needs to have group id set to 0. Otherwise the files created in dags, logs and plugins will be created with root user. You have to make sure to configure them for the docker-compose:\\n\\nmkdir -p ./dags ./logs ./plugins\\necho -e \"\"AIRFLOW_UID=$(id -u)\"\" &gt; .env\\nOn Windows you will probably also need it. If you use MINGW/GitBash, execute the same command.\\n\\nTo get rid of the warning (\"\"AIRFLOW_UID is not set\"\"), you can create .env file with this content:\\n\\nAIRFLOW_UID=50000```\\nthen ran:\\n1. `docker build .`\\n2. `docker-compose up airflow-init`\\n3. `docker-compose up`\\n\",1643400810.237809,1643401790.029949,U02UAFF1WU9\\n486e6350-3735-4d46-95e1-6eb57223529e,U02UAFF1WU9,,,try docker-compose build,1643400810.237809,1643401855.074749,U01AXE0P5M3\\n23e1dd11-11b6-4344-97b6-08f42760c2c9,U02UAFF1WU9,,,\"When I run docker-compose build, no output from the terminal. Is this expected?\\n\\nAlso, when running `docker-compose up airflow-init` i get the warning that the container is run using the root user.I assume this should be the user we specified in the env file though?\",1643400810.237809,1643402035.279269,U02UAFF1WU9\\n8ee4443b-e7d3-4bcb-828c-23134f003ebc,U02BVP1QTQF,,,Thanks Alvaro Navas,1643397626.412989,1643402038.169399,U02T0CYNNP2\\n498c1600-8aac-4d98-94e4-ff6d25a5ba8a,,15.0,,\"Where is this file\\n```    hostname=hostname, bucket_path=self.bucket.path\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/bucket.py\"\", line 1098, in path\\n    raise ValueError(\"\"Cannot determine path without bucket name.\"\")\\nValueError: Cannot determine path without bucket name.```\\n\",1643402199.685139,1643402199.685139,U02VBG59VQ9\\n23bb4c66-582e-4caa-acb6-04ced429510b,U02VBG59VQ9,,,<@U02VBG59VQ9> the bucket variable should be set in your `docker-compose.yaml` file,1643402199.685139,1643402279.014009,U02UAFF1WU9\\n8e9b614c-3dc6-438c-a11e-3cdd307f6651,,2.0,,\"I turned off postgres in the container after ingesting ny_taxi_data. When turned it on again, data was not there. How can I make this data to stay?\",1643402360.934499,1643402360.934499,U02910MJNBY\\n6ea65858-fda0-4f9a-a1d5-c8241fdb21c8,U02910MJNBY,,,that\\'s why we do volume mapping - it should stay. can you try the same with docker-compose?,1643402360.934499,1643402460.054699,U01AXE0P5M3\\n2833e80e-1da0-45ca-ba56-dbf88a16e565,U02UAFF1WU9,,,there should be some output - I think I had something,1643400810.237809,1643402499.324659,U01AXE0P5M3\\n0c7c9fff-56d0-43f5-9643-935940763858,,,,\"looking for the json file in gcloud what i found was the application_default credential json file, gce and logs\",,1643402608.695239,U02UKLHDWMQ\\nabf85cc8-47d1-4940-986d-ab1eb5a93fd3,U02VBG59VQ9,,,\"<@U02UAFF1WU9>\\nit is set already\\nAIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT: \\'google-cloud-platform://?extra__google_cloud_platform__key_path=/.google/credentials/google_credentials.json\\'\\n\\xa0 \\xa0 GCP_PROJECT_ID: \\'dezoomcamp22\\'\\n\\xa0 \\xa0 GCP_GCS_BUCKET: \"\"dtc_data_lake_dezoomcamp22\"\"\",1643402199.685139,1643402617.453309,U02VBG59VQ9\\n99edd56c-3c40-45a9-a968-13734f64a325,U02VBG59VQ9,,,\"<@U02VBG59VQ9> ok, then in your data ingestion you should declare the variable.\\n\\n`BUCKET = os.environ.get(\"\"GCP_GCS_BUCKET\"\")`\\n\\nthen in `upload_to_gcs`  it needs to be passed as a parameter. So in the airflow task it should be specified:\\n\\n```local_to_gcs_task = PythonOperator(\\n        task_id=\"\"local_to_gcs_task\"\",\\n        python_callable=upload_to_gcs,\\n        op_kwargs={\\n            \"\"bucket\"\": BUCKET,\\n            \"\"object_name\"\": f\"\"raw/{parquet_file}\"\",\\n            \"\"local_file\"\": path_to_local_home + parquet_file,\\n        },\\n    )```\",1643402199.685139,1643402823.773759,U02UAFF1WU9\\n8DCF7572-E9BA-4F7B-8E6F-BA18837D6EAA,U02BVP1QTQF,,,\"Great notes! Going to have a proper read through at some point.\\n\\nAlthough under the ETL section, you may have mixed up ETL and ELT. I think the former relates to DWs and the latter relates to DLs\",1643397626.412989,1643402974.663049,U02U34YJ8C8\\nedc01d05-1ce7-4efa-af3d-ddfd7851b593,U02VBG59VQ9,,,\"local_to_gcs_task = PythonOperator(\\n\\xa0 \\xa0 \\xa0 \\xa0 task_id=\"\"local_to_gcs_task\"\",\\n\\xa0 \\xa0 \\xa0 \\xa0 python_callable=upload_to_gcs,\\n\\xa0 \\xa0 \\xa0 \\xa0 op_kwargs={\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \"\"bucket\"\": BUCKET,\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \"\"object_name\"\": f\"\"raw/{parquet_file}\"\",\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \"\"local_file\"\": f\"\"{path_to_local_home}/{parquet_file}\"\",\\n\\xa0 \\xa0 \\xa0 \\xa0 },\",1643402199.685139,1643403185.461719,U02VBG59VQ9\\na0364f1c-9b11-43d6-ab32-a526282ce8ec,U02VBG59VQ9,,,where do i declare the variable,1643402199.685139,1643403209.568109,U02VBG59VQ9\\nfaa0014d-549f-4ff6-a181-297ba35a24c6,U02VBG59VQ9,,,<@U02UAFF1WU9>,1643402199.685139,1643403219.872069,U02VBG59VQ9\\n71889352-e52b-41c9-be38-d0e1af749af2,U02VBG59VQ9,,,\"<https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py>\\n\\nLine 15 is where the variable is declared\",1643402199.685139,1643403292.209419,U02UAFF1WU9\\n8b5580c8-847a-4e76-b15c-2869aabfdf50,U02VBG59VQ9,,,\"<@U02UAFF1WU9> it is already declared\\nPROJECT_ID = os.environ.get(\"\"dezoomcamp22\"\")\\nBUCKET = os.environ.get(\"\"dtc_data_lake_dezoomcamp22\"\")\",1643402199.685139,1643403395.239269,U02VBG59VQ9\\nbedc458d-02b8-4c66-a6cf-b228f97b3b7f,,3.0,,\"Hello All,\\nI am getting an error when at local_to gsk_task in airflow step.\\nI have set up\\nThe PROJECT_ID = os.environ.get(\"\"dezoomcamp22\"\")\\nBUCKET = os.environ.get(\"\"dtc_data_lake_dezoomcamp22\"\")\\nis step up in data_ingestion_gcs_dag.py file.\\n\\n____________________\\n```File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py\"\", line 2061, in _initiate_resumable_upload\\n    hostname=hostname, bucket_path=self.bucket.path\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/bucket.py\"\", line 1098, in path\\n    raise ValueError(\"\"Cannot determine path without bucket name.\"\")\\nValueError: Cannot determine path without bucket name.```\\n\",1643403530.745309,1643403530.745309,U02VBG59VQ9\\nd0023834-ddeb-4837-b564-93fc5f4d8e6e,,,,guys please am stuck is the json file same has the key we generated and copy in google cloud when we were setting up gcloud,,1643403664.043669,U02UKLHDWMQ\\nd6baa14a-6983-4d10-bad8-493bb174244f,U02VBG59VQ9,,,\"<@U02VBG59VQ9> i believe it needs to be:\\n\\n```PROJECT_ID = os.environ.get(\"\"GCP_PROJECT_ID\"\")\\nBUCKET = os.environ.get(\"\"GCP_GCS_BUCKET\"\")```\\nYou are telling the machine to get environment variables titled `dezooomcamp22` &amp; `GCP_GCS_BUCKET` when the variables are actually named in the code block above.\\n\\nMake sense?\",1643402199.685139,1643403743.091889,U02UAFF1WU9\\n53d463f4-2071-4986-bd1a-d51f62a4269c,U02VBG59VQ9,,,\"<@U02VBG59VQ9> see my comment in your other thread. you are calling the bucket name in your BUCKET variable. You need to specify the name of the environment variable, not the bucket naame\",1643403530.745309,1643403954.030739,U02UAFF1WU9\\n00b154e7-13bb-44a7-aa42-93aea9cba603,,4.0,,\"Has somebody figured out how to run a lightweight version of airflow via docker-compose? The \"\"official\"\" version is killing my system\\n\\nFor now, if you also have this problem and haven\\'t found a solution, I can recommend running it on a VM in GCP\",1643404097.008539,1643404097.008539,U01AXE0P5M3\\n76334dee-a0a4-473e-af06-8a7f689cc74a,U02VBG59VQ9,,,so i don\\'t have to mention my credential to the GCP <@U02UAFF1WU9>?,1643402199.685139,1643404132.658659,U02VBG59VQ9\\n0ee59d74-e49a-45a1-b2d7-f1ca14a4b9b5,U02VBG59VQ9,,,\"<@U02VBG59VQ9> Not in the python file. the credentials are stored as variables in the docker-compose.yaml file.\\n\\nLook at lines 63 &amp; 64: <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_2_data_ingestion/airflow/docker-compose.yaml|docker-compose.yaml>\\n\\nThe variables are titled GCP_PROJECT_ID &amp; GCP_GCS_BUCKET. Then in your python file, you need to get the *variables*.\",1643402199.685139,1643404321.395609,U02UAFF1WU9\\nd351dcc9-b11e-4be8-b57a-626160b50fd8,U02UAFF1WU9,,,\"build was commented out in the docker-compose.yaml.\\n\\nStill hitting an issue saying permission is denied when trying to save the CSV file though\",1643400810.237809,1643404725.540799,U02UAFF1WU9\\n56524cf4-b152-4d47-8717-c24c869fdda9,U02VBG59VQ9,,,\"now i got this error\\n```  \"\"error\"\": {\\n    \"\"code\"\": 403,\\n    \"\"message\"\": \"\"<mailto:dtc-de-usergs@dezoomcamp22.iam.gserviceaccount.com|dtc-de-usergs@dezoomcamp22.iam.gserviceaccount.com> does not have storage.objects.create access to the Google Cloud Storage object.\"\",\\n    \"\"errors\"\": [\\n      {\\n        \"\"message\"\": \"\"<mailto:dtc-de-usergs@dezoomcamp22.iam.gserviceaccount.com|dtc-de-usergs@dezoomcamp22.iam.gserviceaccount.com> does not have storage.objects.create access to the Google Cloud Storage object.\"\",\\n        \"\"domain\"\": \"\"global\"\",\\n        \"\"reason\"\": \"\"forbidden\"\"\\n      }\\n    ]\\n  }\\n}```\\n\",1643402199.685139,1643404770.265489,U02VBG59VQ9\\n897a6a5f-5343-4418-8c59-9924d0139780,U02VBG59VQ9,,,<@U02VBG59VQ9> you need to grant the service account the access. Thaat will be in google cloud platform. Go to IAM and grant  the `storage.objects.create` access,1643402199.685139,1643405094.990619,U02UAFF1WU9\\n4cd64f3c-c5fd-461b-bcba-813713dc63c3,,3.0,,\"Executed `docker-compose build` for Airflow and got the following error. The Dockerfile is in my airflow directory why it wasn\\'t found?\\n```=&gt; CANCELED [airflow_airflow-webserver internal] load build definition from Dockerfile                                                                                                                                \\n =&gt; =&gt; transferring dockerfile:                                                                                                                                                                                        \\nfailed to solve: rpc error: code = Unknown desc = failed to solve with frontend dockerfile.v0: failed to read dockerfile: open /var/lib/docker/tmp/buildkit-mount469670224/Dockerfile: no such file or directory```\",1643405309.258339,1643405309.258339,U02UX664K5E\\n63622336-7346-4a4a-9520-c11b17504084,U01AXE0P5M3,,,I didnt use the oficial version. I used <https://github.com/puckel/docker-airflow>,1643404097.008539,1643405449.345649,U02T01VH7C0\\ndd91154e-f31c-470d-83d4-d839f5551740,U02UX664K5E,,,\"<@U02UX664K5E> make sure you are in the correct directory. happened to me as well and I was one directory up.\\n\\nuse `ls` to see if the `Dockerfile` exists in your current directory\",1643405309.258339,1643405664.138749,U02UAFF1WU9\\ne9cfffd4-1555-44a0-8ce9-6aa5c6108fae,U02BVP1QTQF,,,\"<@U02U34YJ8C8> you’re absolutely right, I just changed it. Thanks!\\n\\nRegarding the density: it’s _*dense*_ :rolling_on_the_floor_laughing: . It took me 3 full days to parse all of the Airflow content into my notes. Your long video for ingesting to local Postgres was great because it explained additional things like idempotency and it also showed how to run 2 separate docker-compose files concurrently. Sejal’s videos were fantastic too; I missed the step-by-step component of your videos but I understand why you wouldn’t show how to create the GCP DAG because it would take several long videos and it’s a huge amount of work.\\n\\nFor me, the current density is okay because I can dedicate a large amount of time to the course, but I’m not sure how other people feel about this. If you’re setting this course as an actual bootcamp which takes many hours per day then I think that this amount of density is to be expected, but if I had a full-time job then I don’t think I could keep up with writing notes and I would just watch the videos.\",1643397626.412989,1643406185.156349,U02BVP1QTQF\\na215db12-e289-4a67-a26c-04e9d762c055,U02UX664K5E,,,\"Thanks for the help <@U02UAFF1WU9>, it was actually an issue with the name of my file. I had it as Dockerfile.dockerfile, that\\'s how VS Code saved the file, I think I should not specify it\\'s a Dockerfile when saving as it always add .dockerfile extension. If anyone runs into a similar issue, do `ls`  as suggested by <@U02UAFF1WU9> and make sure you see Dockerfile listed and not Dockerfile.dockerfile.\",1643405309.258339,1643406204.054189,U02UX664K5E\\nf1cc51f9-fe1b-4bfd-99bc-116cac6a212e,U02UX664K5E,,,\"Yep,  `Dockerfile` with no extension.\",1643405309.258339,1643406275.955629,U02UAFF1WU9\\nb91991e1-a00c-46a5-b07c-a01a67d43835,U02VBG59VQ9,,,i do have access,1643402199.685139,1643406413.441699,U02VBG59VQ9\\n74db8456-d87b-4bdb-b86e-51ac8fd83bad,,1.0,,\"Hey Everyone\\n\\nI\\'m curious has the solution for Week 1 assignment out to check our points?\",1643407044.254429,1643407044.254429,U02TBKWL7DJ\\nbcafaaa1-0323-4aaa-8809-5ef6cfef51c3,,2.0,,I have a question <@U01AXE0P5M3> do i have to set up  VM on gcloud cos i noticed in <https://youtu.be/ae-CV2KfoN0?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb> we are running terraform again and i remember we ran terraform in video 1.3.2. to set up our our cloud storage and Database.,1643407243.219729,1643407243.219729,U02UKLHDWMQ\\n7d1a90ee-2aaa-4741-a50e-250ad1b5b7f5,,3.0,,\"I am getting this error   while running docker compose up\\n`airflow-worker_1     | PermissionError: [Errno 13] Permission denied: \\'/opt/airflow/logs/scheduler\\'`\",1643407707.452239,1643407707.452239,U02SPLJUR42\\n628a5d1b-4c25-46e7-a006-f7d1f4b9db09,U02VBG59VQ9,,,link to the comment <@U02UAFF1WU9>,1643403530.745309,1643408212.598279,U02SPLJUR42\\ne33c5b39-d3f9-4a75-ae2d-dd356e44ea68,U02VBG59VQ9,,,\"<@U02SPLJUR42>\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1643403743091889?thread_ts=1643402199.685139&amp;cid=C01FABYF2RG>\",1643403530.745309,1643408274.889429,U02UAFF1WU9\\nfaba47e2-3ac0-4a3d-bd39-26a3aa752af8,U02SPLJUR42,,,FileNotFoundError: [Errno 2] No such file or directory: \\'/opt/airflow/logs/scheduler/2022-01-28\\',1643407707.452239,1643408287.552909,U02SPLJUR42\\nf4a2e386-fe2c-4992-aecd-369056e67d9b,U02910MJNBY,,,\"I found the error on the command that runs the docker, Thanks!\",1643402360.934499,1643408341.050089,U02910MJNBY\\nbe112309-8a52-4a3a-baab-c69b3bea4575,U02TBKWL7DJ,,,\"Yes it is out, check the playlist\",1643407044.254429,1643408378.457859,U02SPLJUR42\\ndced5993-3350-450b-86c5-dc7fc93bb836,,3.0,,\"hello everyone, I have two questions concerning the course content so far. It would be great if someone could help me to answer these. Is terraform a cloud agnostic alternative to AWS cloudformation templates? Or are there more differences advantages/disadvantages of using terraform? Which database would you use for timeseries data?\",1643408827.911599,1643408827.911599,U01T2HV3HNJ\\nb1e6d9b1-45c2-4825-88ba-73585cfa287a,,1.0,,\"Hi, quick question. At the end of the 1.3.2. video for creating GCP infrastructure using Terraform, should I have data in my BigQuery dataset ? I\\'ve created the dataset but don\\'t have any data\",1643409060.846469,1643409060.846469,U02TMP4GJEM\\n930ac0c2-b6c6-4b1c-9023-6b82e5359702,,6.0,,\"I\\'m working on the video 1.4.1 setting up the environment on google cloud. I\\'m using WSL, and having issues with the port forwarding via VS Code. I can access pgAdmin via localhost:8080 in my browser just fine, but when I try to access `pgcli -h localhost -U root -d ny_taxi` from the WSL terminal, I get a `Connection refused` error (full error in comments). Has anyone seen this issue? I know WSL sometimes has networking issues between the windows and linux sides and it seems like that might be the case here. thank you all.\",1643411270.076219,1643411270.076219,U02T9GHG20J\\n20bd08eb-89ad-4ed7-b10f-f41073655e6b,U02T9GHG20J,,,\"Error message:\\n```connection to server at \"\"localhost\"\" (127.0.0.1), port 5432 failed: Connection refused    Is the server running on that host and accepting TCP/IP connections?```\\n\",1643411270.076219,1643411320.118479,U02T9GHG20J\\n7cb7163a-faa2-45cd-a0ff-0344d4bb0750,,9.0,,\"Hey Everyone,\\n\\nOn task 3 of ingesting data with airflow, I am hitting an issue with the gooogle credentials path.\\n\\nError:\\n```google.auth.exceptions.DefaultCredentialsError: File /Users/steve/google/credentials/google_credentials.json was not found.```\\nThe exact path to my google credential file is in the `GOOGLE_APPLICATION_CREDENTIAL` variable below. I think I need to edit the AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT but not sure what exactly to.\\n\\n```GOOGLE_APPLICATION_CREDENTIALS: /Users/steve/google/credentials/google_credentials.json\\n    AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT: \\'google-cloud-platform://?extra__google_cloud_platform__key_path=/.google/credentials/google_credentials.json\\'```\",1643411641.340209,1643411641.340209,U02UAFF1WU9\\n7a83462e-299c-4a01-bd24-6cb429fbfa20,U02TC704A3F,,,\"Yes! But I was unable to connect using port 5432 in my computer for some reason. Along the project, port 7575 was working fine, I\\'m having problems only now using Postgres and Airflow connection\",1643340522.269329,1643411921.382599,U02TC704A3F\\n3bb2cffe-7dde-458d-aa5f-d6a6284373d3,U02UAFF1WU9,,,\"Sorry, I don\\'t have an answer for you, but I noticed our profile pictures were taken only 10 miles apart! :laughing:\",1643411641.340209,1643412076.585279,U02T9GHG20J\\n8301d22f-2a9b-4679-9fc7-dffbb530c7af,,2.0,,\"Hi everyone.\\n\\nI\\'m trying to dockerize the ingest_data.py file, but I\\'m getting an error:\\n```$ docker build -t taxi_ingest:v001 .\\n\\nerror checking context: \\'can\\'t stat \\'/home/andres/projects/data_engineering/ny_taxi_postgres_data\\'\\'.```\\nI know that directory is there, any idea?\",1643422971.917109,1643422971.917109,U02910MJNBY\\nfe40d8b9-dea2-4736-a88c-99790e4c8652,U02910MJNBY,,,\"Solved... I didn\\'t have access, root was the owner\",1643422971.917109,1643424155.192789,U02910MJNBY\\n1D4654BA-B2D8-4E59-978D-C25387F89DDD,,1.0,,\"Hi,\\nWhat is the minimum score to pass this Zoomcamp?\",1643424814.218069,1643424814.218069,U02U6DR551B\\n34A28275-DCF5-4627-8B48-3A05522AC6A1,U02U6DR551B,,,You only need to complete the project at the end to pass the bootcamp. ,1643424814.218069,1643425590.675339,U02UX664K5E\\n05b850bc-eb74-4f2f-8658-899095bae209,U02UAFF1WU9,,,\"This is how I have mine setup&gt; The credentials file is located in: */week_2_data_ingestion/google_credentials*/ where my airflow files are located in */week_2_data_ingestion/airflow*/\\n\\n```GOOGLE_APPLICATION_CREDENTIALS: /.google_credentials/google_credentials.json\\n\\nAIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT: \\'google-cloud-platform://?extra__google_cloud_platform__key_path=./week_2_data_ingestion/google_credentials/google_credentials.js\\'\\n\\nvolumes:\\n - ./week_2_data_ingestion/google_credentials/:/.week_2_data_ingestion/google_credentials:ro```\\nI hope you see the pattern.\",1643411641.340209,1643426107.982179,U02UX664K5E\\na881e2a0-36e7-4df8-91d1-69e348fbe7eb,U02TMP4GJEM,,,\"No, the terraform is only to create the services for week 2 where we ingest data to BigQuery with Airflow.\",1643409060.846469,1643426530.860479,U02UX664K5E\\n616588dc-918a-4e0c-8324-f4a615bd9c8f,U02T9GHG20J,,,\"So that command is to access Postgres, I remember facing a similar issue, are you able to access Postgres with pgAdmin? Did you map your port as 5431:5432 in your Dockerfile?\",1643411270.076219,1643426895.010339,U02UX664K5E\\n5ee03456-b64b-49f1-8dfb-832ebe4c50fd,U02T9GHG20J,,,I am able to connect via pgAmin like the picture below. I have the ports defined as `5432:5432` in this <https://github.com/mharty3/data_engineering_zoomcamp_2022/blob/main/week01/docker-sql/docker-compose.yml|docker-compose file>. I also tried `pgcli -h pg-database -U root -d ny_taxi` . And it also didn\\'t work,1643411270.076219,1643427846.272399,U02T9GHG20J\\na5910a68-a61b-4cbb-82dc-00099efbcf58,U02UM74ESE5,,,\"Hi Alvaro, I did try `mkdir .foldername` but doesn\\'t work for MAC OS Catalina\",1643341570.337969,1643428886.685179,U02UM74ESE5\\nd2034501-3789-4746-a859-bb408e58245c,U02VBG59VQ9,,,\"Even I was getting the same error , docker-compose has to run again, You had earlier ran it with wrong project number and storae name\",1643402199.685139,1643429399.834759,U02UQNXD9QR\\n23ec2010-5149-4334-be9f-cee89f9c6388,U02Q9P0A0NA,,,\"Yes <@U02UXRH2SV6> You are right thanks for that, I had to rerun docker compose up for the changes to reflect.\",1643299377.100400,1643429949.424289,U02UQNXD9QR\\n640745f0-9580-46a1-bd37-c1b62758765c,U02SPLJUR42,,,Did you create .env file?,1643407707.452239,1643430561.254189,U0290EYCA7Q\\n8d61397c-1a89-4034-8199-22335ef9e412,U02V90BSU1Y,,,\"I was also prompted to provide a Government issued Id card and a picture of the Card.\\nCurrently waiting for the verification to complete\",1643229926.418000,1643431804.797489,U02V90BSU1Y\\na0df4b7f-aef3-496e-915a-6c5e39a238d7,,,,\"Hi guys,\\n\\nYou can find my notes on week 2 here: <https://kargarisaac.github.io/blog/data%20engineering/jupyter/2022/01/25/data-engineering-w2.html>\\nI\\'m updating the notes as I go forward, but this should cover until the end of airflow videos :slightly_smiling_face:\",,1643435805.745989,U030FNZC26L\\n5146aa89-38b9-4f73-a5ee-668f8e02d09b,U0290EYCA7Q,,,<@U01DFQ82AK1> Any idea?,1643365258.216129,1643436668.735809,U0290EYCA7Q\\n75730695-ec08-477d-b8b5-d897dc5ac585,,2.0,,\"Hi guys,\\n\\nI ran the airflow DAGs without error but the taxi rides data is not in the BigQuery.\\nWhat I am doing wrong?\",1643440801.890239,1643440801.890239,U02TZ7KC7U7\\n1d4c4373-d2b9-453e-bdd7-d01b70007075,U02TZ7KC7U7,,,\"external_table is your ingested data\\ntry make query from that\",1643440801.890239,1643440956.066859,U02RA8F3LQY\\ne63c1c36-0180-477e-a626-247760618a7f,U02TZ7KC7U7,,,,1643440801.890239,1643441168.719439,U0290EYCA7Q\\n58015fdf-2a87-4d80-bb5f-23cf6392571c,,11.0,,\"I\\'m a bit confused by environment variables.\\n1- when we set a variable like `AIRFLOW_UID` in the `.env` file, is it automatically exported and can be used in docker-compose and Dockerfile?\\n2- what does this command mean: `\"\"${AIRFLOW_UID:-1000}:0\"\"` . Does it mean use the `AIRFLOW_UID`  variable if exists and use 1000 otherwise? I know 0 is for group id.\\n3- in docker-compose file, `x-airflow-common: &amp;airflow-common` is use. Is `&amp;airflow-common` a kind of alias that we use in other places of the file, for example airflow worker and scheduler, like `&lt;&lt;: *airflow-common`?\",1643442862.769809,1643442862.769809,U030FNZC26L\\n8a029737-637b-439a-a9f4-5862e1fb832c,U02V90BSU1Y,,,I was also prompted for. Its been two days and I am still waiting for the verification to complete,1643229926.418000,1643442930.530539,U02R0EE3420\\n34962b2f-bc1b-4f9b-8550-5bdc3dac3900,U02Q7JMT9P1,,,\"I have tried hardcoding the parquet file name, but still got the \\'Invalid arguments error\\'\\n\\n```Invalid arguments were:\\n**kwargs: {\\'python_callable\\': &lt;function upload_to_gcs at 0x7f1c01b51680&gt;, \\'op_kwargs\\': {\\'bucket\\': \\'nyc_yellow_taxi_de_is\\', \\'object_name\\': \\'raw/yellow_tripdata_2021-01.parquet\\', \\'local_file\\': \\'/opt/airflowoutput_tripdata_2021-01.parquet\\'}}```\\n\",1643671443.618389,1643702511.319259,U02Q7JMT9P1\\n10ea1857-71cc-4194-983c-a62c00e48d58,U02T8ANTJGM,,,I think it\\'s pretty secure unless you accidentally share your private ssh key somewhere =),1643692723.273259,1643702661.578369,U01AXE0P5M3\\na3bf9ed1-421c-4a8b-b02a-b12486a4bb8a,U02U6DR551B,,,Yesterday,1643702306.563189,1643702756.393659,U01AXE0P5M3\\n3c7fd848-a204-4482-836a-bf7a4dd5e370,U01AXE0P5M3,,,\"Thanx , did some searching and ii have created a completely new network, using this help and seems are fine now\\n<https://tjtelan.com/blog/how-to-link-multiple-docker-compose-via-network/>\",1642534337.023000,1643703191.292709,U02RTJPV6TZ\\nfb747ec3-02b8-48cb-8b55-7e856e2c1096,,,thread_broadcast,\"services:\\n\\xa0 pgdatabase:\\n\\xa0 \\xa0 image: postgres:13\\n\\xa0 \\xa0 environment:\\n\\xa0 \\xa0 \\xa0 - POSTGRES_USER=root\\n\\xa0 \\xa0 \\xa0 - POSTGRES_PASSWORD=root\\n\\xa0 \\xa0 \\xa0 - POSTGRES_DB=ny_taxi\\n\\xa0 \\xa0 volumes:\\n\\xa0 \\xa0 \\xa0 - \"\"./taxi_ingest/ny_taxi_postgres_data:/var/lib/postgresql/data:rw\"\"\\n\\xa0 \\xa0 ports:\\n\\xa0 \\xa0 \\xa0 - \"\"5432:5432\"\"\\n\\xa0 \\xa0 #networks:\\n\\xa0 \\xa0 # \\xa0- airflow\\n\\xa0 #pgadmin:\\n\\xa0 # \\xa0image: dpage/pgadmin4\\n\\xa0 # \\xa0environment:\\n\\xa0 # \\xa0 \\xa0- PGADMIN_DEFAULT_EMAIL=<mailto:admin@admin.com|admin@admin.com>\\n\\xa0 # \\xa0 \\xa0- PGADMIN_DEFAULT_PASSWORD=root\\n\\xa0 # \\xa0ports:\\n\\xa0 # \\xa0 \\xa0- \"\"8080:80\"\"\\n\\n\\xa0 \\xa0 \\xa0 # linking this docker-compose to docker-compose/week2/alex\\nnetworks:\\n\\xa0 default:\\n\\xa0 \\xa0 external:\\n\\xa0 \\xa0 \\xa0 name: sharedNetwork\",1642534337.023000,1643703245.728939,U02RTJPV6TZ\\n02eed24d-9d4a-46de-90bd-069e439cae5d,U02UKLHDWMQ,,,You dont need to set the group id explicitly  since inside container group and user id is by default set to 0. But setting user id also to 0 will lead to files created under shared volumes(e.g. logs) with root permissions which you cannot delete without using sudo. So setting AIRFLOW_UID is enough,1643670186.472959,1643705708.332779,U02S1QHR48P\\n8395507a-1393-43c9-95ab-18a8cf99ac4b,U030MHYAUNS,,,\"What exactly is not working, <@U030MHYAUNS>?\",1643646908.736339,1643705922.235799,U02CGKRHC9E\\n0434c719-2c78-4811-ac15-6a3b8ac4ba71,U02U6DR551B,,,\"\"\"Yesterday.... :notes: All my problems seems so far away\"\"\",1643702306.563189,1643707212.650049,U02CD7E30T0\\nddd66266-d86c-4c6a-8b5d-c7b76683957a,U02Q7JMT9P1,,,Can you share your code as well in a gist or something,1643671443.618389,1643707253.853109,U02TATJKLHG\\n1fd5f2a3-3c42-4557-ada8-3eb30c4ffd80,,,,\"In this homework, we\\'ll prepare data for the next week. We\\'ll need to put the NY Taxi data from 2019 and 2020 to our data lake.\",,1643709002.254059,U02U6DR551B\\n3c734934-6c68-4b5f-8a1d-fbeea00534e4,,,,do we need to put the data in monthly fashion for homework2?,,1643709019.008659,U02U6DR551B\\nbc0a1601-d9a6-49d7-8589-77fef1e6351a,U02U6DR551B,,,\"It means the start date of the schedule.\\n\\nIt is the baseline that airflow uses to calculate when to execute the DAG run (`execution_date`).\\nAirflow uses start_date together with schedule_interval to calculate the execution date.\\n\\n```start_date=days_ago(1)   # yesterday\\nschedule_interval=@daily.\\n```\\nSo from \"\"yesterday\"\", Airflow will execute the DAG run daily (after every 24hrs).\\nThis means that the first execution of the run will be \"\"today\"\" (24hrs after \"\"yesterday\"\", the start date).\\nThe second will be tomorrow, and so on.\",1643702306.563189,1643710255.798349,U02SBTRTFRA\\nfe10ccc6-bcc7-4aa5-8369-a4d7af1dbccf,,3.0,,\"Hello, am new here. I need help on how to install Terraform on windows\",1643710724.022469,1643710724.022469,U02UPU69BMF\\nc61809c6-4901-4ee3-ba64-4748e0852b42,U030MHYAUNS,,,i solve issue. Thank you!,1643646908.736339,1643711461.285849,U030MHYAUNS\\n3a1e7b07-1a29-4ad7-b7ae-8e73edf1e4e2,,3.0,,\"For homework i created the second dag file: dags/fhv_ingestion_dag.py but airflow doest see it, on web browser i can only see the dag from the first file, which i created during the lesson. Even if i rename the first file, the dag is not visible anymore. How can i fix it?\",1643712235.950979,1643712235.950979,U021RS6DVUZ\\n58591c79-21e5-4804-a49a-5b01dbf6be66,U021RS6DVUZ,,,When i restarted docker-compose all works correctly. But now i need to add another dag to download Zones and i will face the same problem…,1643712235.950979,1643712438.567119,U021RS6DVUZ\\na7344aae-e30e-4f68-bf5d-2abb0f4aa090,U01AXE0P5M3,,,\"Hi, I still get two empty folders _ny_taxi_postgress_data_\\xa0and _ny_taxi_postgress_data;_C. The storage does not persist and if I restart the container I need to ingest the data again. I am a bit behind schedule (still on week 1) and haven’t done the docker-compose part; I was hoping the problem might disappear there. What do you think?\",1642534337.023000,1643712954.941539,U02UCV3RAG4\\n8c7c7fb2-3b38-4565-b3dd-c282459dbb6e,U021RS6DVUZ,,,\"most likely you would also need to change the `dag_id`  at the moment you define the DAG (that is what gives the actual name to the DAG in the airflow environment, not the .py file name)\\n```with DAG(\\n    dag_id=\"\"fhv_ingestion_dag\"\", =&gt; change this\\n    schedule_interval=&lt;your schedule&gt;,\\n    default_args=default_args,\\n    catchup=True,\\n    max_active_runs=1,\\n    tags=[\\'dtc-de\\'],\\n) as dag:```\\nI hope this solves the problem\",1643712235.950979,1643713251.308269,U02UA0EEHA8\\nbf33c675-389c-4784-b976-59d096222e3f,U02UPU69BMF,,,<https://youtu.be/Cn6xYf0QJME|https://youtu.be/Cn6xYf0QJME>,1643710724.022469,1643713593.416209,U02SBTRTFRA\\n3ffc6b54-4cde-4459-82c3-4848431ff59d,U02UPU69BMF,,,Thanks,1643710724.022469,1643713902.116029,U02UPU69BMF\\n68DE02D9-6B25-4790-A1ED-D0AECA0834DB,,5.0,,\"For week 2 homework, question 1 &amp; 2, only concerned with adding data to our data lake right? So should we just leave the bigquery code as it is?\",1643714413.929649,1643714413.929649,U02U34YJ8C8\\n18b2bbe3-76e8-45d6-a5b5-6825f3a6b16b,U02U34YJ8C8,,,Yes! I think you should remove that step from the DAG. We\\'ll handle the Big Query part this week.,1643714413.929649,1643714587.765449,U02GVGA5F9Q\\na5e207ef-8a72-42ae-b179-9d620a5347e5,U021RS6DVUZ,,,\"the ID is new. If i restart airflow it works correctly, 2 dags run. But if i add new file, airflow doesn’t see it.\",1643712235.950979,1643714965.475969,U021RS6DVUZ\\n94082bfc-68f9-4467-b5e8-59e090456090,,8.0,,\"my airflow \"\"ingest\"\" logs, there is this failure don\\'t know how i shoud handle it, any ideas\\n\\n```*** Reading local file: /opt/airflow/logs/LocalIngestionDag/ingest/2022-02-01T11:15:01.966364+00:00/3.log\\n[2022-02-01, 11:24:36 UTC] {taskinstance.py:1032} INFO - Dependencies all met for &lt;TaskInstance: LocalIngestionDag.ingest manual__2022-02-01T11:15:01.966364+00:00 [queued]&gt;\\n[2022-02-01, 11:24:36 UTC] {taskinstance.py:1032} INFO - Dependencies all met for &lt;TaskInstance: LocalIngestionDag.ingest manual__2022-02-01T11:15:01.966364+00:00 [queued]&gt;\\n[2022-02-01, 11:24:36 UTC] {taskinstance.py:1238} INFO - \\n--------------------------------------------------------------------------------\\n[2022-02-01, 11:24:36 UTC] {taskinstance.py:1239} INFO - Starting attempt 3 of 3\\n[2022-02-01, 11:24:36 UTC] {taskinstance.py:1240} INFO - \\n--------------------------------------------------------------------------------\\n[2022-02-01, 11:24:36 UTC] {taskinstance.py:1259} INFO - Executing &lt;Task(PythonOperator): ingest&gt; on 2022-02-01 11:15:01.966364+00:00\\n[2022-02-01, 11:24:36 UTC] {standard_task_runner.py:52} INFO - Started process 3820 to run task\\n[2022-02-01, 11:24:36 UTC] {standard_task_runner.py:76} INFO - Running: [\\'***\\', \\'tasks\\', \\'run\\', \\'LocalIngestionDag\\', \\'ingest\\', \\'manual__2022-02-01T11:15:01.966364+00:00\\', \\'--job-id\\', \\'109\\', \\'--raw\\', \\'--subdir\\', \\'DAGS_FOLDER/data_ingestion_local.py\\', \\'--cfg-path\\', \\'/tmp/tmpy92ctv9m\\', \\'--error-file\\', \\'/tmp/tmpj98t0u52\\']\\n[2022-02-01, 11:24:36 UTC] {standard_task_runner.py:77} INFO - Job 109: Subtask ingest\\n[2022-02-01, 11:24:36 UTC] {logging_mixin.py:109} INFO - Running &lt;TaskInstance: LocalIngestionDag.ingest manual__2022-02-01T11:15:01.966364+00:00 [running]&gt; on host 6708b7b202a5\\n[2022-02-01, 11:24:37 UTC] {warnings.py:110} WARNING - /home/***/.local/lib/python3.7/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing \\'execution_date\\' from the template is deprecated and will be removed in a future version. Please use \\'data_interval_start\\' or \\'logical_date\\' instead.\\n  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))\\n\\n[2022-02-01, 11:24:37 UTC] {taskinstance.py:1426} INFO - Exporting the following env vars:\\nAIRFLOW_CTX_DAG_OWNER=***\\nAIRFLOW_CTX_DAG_ID=LocalIngestionDag\\nAIRFLOW_CTX_TASK_ID=ingest\\nAIRFLOW_CTX_EXECUTION_DATE=2022-02-01T11:15:01.966364+00:00\\nAIRFLOW_CTX_DAG_RUN_ID=manual__2022-02-01T11:15:01.966364+00:00\\n[2022-02-01, 11:24:37 UTC] {logging_mixin.py:109} INFO - yellow_taxi_2022_02 /opt/***/output_2022-02.csv 2022-02-01T11:15:01.966364+00:00\\n[2022-02-01, 11:24:37 UTC] {logging_mixin.py:109} INFO - connection established successfully, instering data...\\n[2022-02-01, 11:24:37 UTC] {taskinstance.py:1700} ERROR - Task failed with exception\\nTraceback (most recent call last):\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1329, in _run_raw_task\\n    self._execute_task_with_callbacks(context)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1455, in _execute_task_with_callbacks\\n    result = self._execute_task(context, self.task)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1511, in _execute_task\\n    result = execute_callable(context=context)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py\"\", line 174, in execute\\n    return_value = self.execute_callable()\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py\"\", line 185, in execute_callable\\n    return self.python_callable(*self.op_args, **self.op_kwargs)\\n  File \"\"/opt/airflow/dags/ingest_script.py\"\", line 22, in ingest_callable\\n    df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/pandas/core/generic.py\"\", line 5487, in __getattr__\\n    return object.__getattribute__(self, name)\\nAttributeError: \\'DataFrame\\' object has no attribute \\'tpep_pickup_datetime\\'\\n[2022-02-01, 11:24:37 UTC] {taskinstance.py:1277} INFO - Marking task as FAILED. dag_id=LocalIngestionDag, task_id=ingest, execution_date=20220201T111501, start_date=20220201T112436, end_date=20220201T112437\\n[2022-02-01, 11:24:38 UTC] {standard_task_runner.py:92} ERROR - Failed to execute job 109 for task ingest\\nTraceback (most recent call last):\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/task/task_runner/standard_task_runner.py\"\", line 85, in _start_by_fork\\n    args.func(args, dag=self.dag)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/cli_parser.py\"\", line 48, in command\\n    return func(*args, **kwargs)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/cli.py\"\", line 92, in wrapper\\n    return f(*args, **kwargs)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py\"\", line 298, in task_run\\n    _run_task_by_selected_method(args, dag, ti)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py\"\", line 107, in _run_task_by_selected_method\\n    _run_raw_task(args, ti)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py\"\", line 184, in _run_raw_task\\n    error_file=args.error_file,\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py\"\", line 70, in wrapper\\n    return func(*args, session=session, **kwargs)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1329, in _run_raw_task\\n    self._execute_task_with_callbacks(context)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1455, in _execute_task_with_callbacks\\n    result = self._execute_task(context, self.task)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1511, in _execute_task\\n    result = execute_callable(context=context)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py\"\", line 174, in execute\\n    return_value = self.execute_callable()\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py\"\", line 185, in execute_callable\\n    return self.python_callable(*self.op_args, **self.op_kwargs)\\n  File \"\"/opt/airflow/dags/ingest_script.py\"\", line 22, in ingest_callable\\n    df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/pandas/core/generic.py\"\", line 5487, in __getattr__\\n    return object.__getattribute__(self, name)\\nAttributeError: \\'DataFrame\\' object has no attribute \\'tpep_pickup_datetime\\'\\n[2022-02-01, 11:24:38 UTC] {local_task_job.py:154} INFO - Task exited with return code 1\\n[2022-02-01, 11:24:38 UTC] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedul```\\n\",1643715278.606679,1643715278.606679,U02RTJPV6TZ\\n9ce3894c-7db3-4d9a-984c-8658824c32d1,U02RTJPV6TZ,,,This should be the hint: `AttributeError: \\'DataFrame\\' object has no attribute \\'tpep_pickup_datetime\\'`,1643715278.606679,1643715429.018079,U02Q7JMT9P1\\na86a92a8-be29-486b-8bcc-2e22ed2726e3,U02RTJPV6TZ,,,it loaded fine from jan/feb......but onw failed at august,1643715278.606679,1643715567.203589,U02RTJPV6TZ\\n5a5fcc3b-cb63-4e08-94c3-e6a1b3d99b38,U02RTJPV6TZ,,,\"because there is no data after July 2021, if you check also on the website <https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page>\",1643715278.606679,1643715642.906429,U02UA0EEHA8\\n8808dff4-1900-4234-bc46-967e0f690b45,U02RTJPV6TZ,,,\"ooh thanx alot, atleast that is done, i feel relief <@U02UA0EEHA8>, this home work is surely tough to a guy like me:smile: wish there was a way to discuss it\",1643715278.606679,1643716102.935229,U02RTJPV6TZ\\n76572ae9-514b-4979-8be4-25b4b3b58e52,U02RTJPV6TZ,,,\"np, I was just wondering the same when I saw that sea of red :slightly_smiling_face:\",1643715278.606679,1643716392.776249,U02UA0EEHA8\\n0fff7784-c463-44f1-a3d5-a4dbfb95e93a,,6.0,,\"I have problems converting the *.csv files into *.parquet - the task exits with return code `Negsignal.SIGKILL` in 90% of tries, only a few of them were successful so far. Googling showed that it is related to limited resources, but I couldn\\'t find a workaround. I am using Ubuntu 20.04 on a laptop with 8 GiB RAM - Docker is free to use all the available memory in this case, so I don\\'t have a possibility to increase RAM designated for it. I have noticed however when DAG gets to the conversion task, my laptop starts to swap the hard drive a lot, and even sometimes the display freezes.\\n\\nSightly related topic - is there a way to store the downloaded raw files permanently? I have noticed they are deleted when docker-compose is restarted and the download task has to be repeated if downstream tasks were not successful.\",1643716410.018639,1643716410.018639,U02Q7JMT9P1\\n829fd669-a427-4fd1-9958-3464b760b3c2,U02Q7JMT9P1,,,\"For the second question- you can map the airflow_home directory (i think it\\'s /home/airflow) to your local file system and this way keep the csv files\\n\\nFor the first one, maybe you can try processing the files in chunks? E.g 500k rows at a time\",1643716410.018639,1643716619.299089,U01AXE0P5M3\\n698fe832-fb17-4d86-99db-43c68165b620,U02Q7JMT9P1,,,\"<@U02TATJKLHG> I was able to overcome this error by taking out the `op_kwargs` parameter completely. I also exclude this way the custom `upload_to_gcs` function. Btw, the way `LocalFilesystemToGCSOperator` task does not work for me as it is written in the example pipeline -I am getting an error that mandatory arguments `[src, bucket, dst]` are missing.\",1643671443.618389,1643716830.554119,U02Q7JMT9P1\\nc5832e23-58ac-4139-9ef0-7a0116dffc1b,U02RTJPV6TZ,,,\"Nothing is stopping you from discussing the homework; a few guys created their own channel to discuss it at the beginning of the course, but I think in their case was that they all lived in the US Pacific timezone so it was convenient for them. The main channel is too big for openly discussing the homework beyond questions because not everyone is going at the same pace and it could lead to people sharing solutions and other people not even bothering to trying to solve the homework.\",1643715278.606679,1643717093.468529,U02BVP1QTQF\\n644b8f1f-72d4-4fec-bff2-593480ec7eb5,U01AXE0P5M3,,,Maybe try //c/path - with double slash. It worked for some people,1642534337.023000,1643717186.563079,U01AXE0P5M3\\n088b4c31-4431-4c1f-a43a-0d9e4442fea3,U02U34YJ8C8,,,\"I left the big query code intact for the yellow taxi trips but then removed it for the other datasets. Since the example DAG already filled parts of the dataset, I’m guessing that we will create new ones for the next lessons and we will be supposed to delete any previous tables we’ve got.\",1643714413.929649,1643717203.885739,U02BVP1QTQF\\nc1fa967c-802d-4af5-84cc-9e05f55d7075,U01AXE0P5M3,,,\"And remove all the folders first, just in case\",1642534337.023000,1643717228.007899,U01AXE0P5M3\\n2f52aed2-5362-49f3-ac68-47b4d29c7223,U02UPU69BMF,,,\"<https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/windows.md|https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/windows.md>\\n\\nCould be helpful as well\",1643710724.022469,1643717322.469619,U01AXE0P5M3\\na6c6cf92-fee0-4955-b212-b87a8d3ddb51,U02RTJPV6TZ,,,ok thanx <@U02BVP1QTQF>,1643715278.606679,1643717368.476979,U02RTJPV6TZ\\nb8f6c3f0-2502-4898-94f0-b267cac6ba47,U02Q7JMT9P1,,,So were you using an operator from google cloud to upload the files? I guess we just did it directly via a python operator. I am not entirely sure but I think you passed `PythonOperator` parameters to `LocalFilesystemToGCSOperator` and hence it was not able to identify the correct parameters.,1643671443.618389,1643718259.163929,U02TATJKLHG\\neae33d80-d5e9-4f78-b4a6-3e8f88544cd6,U02BVP1QTQF,,,\"Thanks <@U02BVP1QTQF> , I was running it in local and all I could experience was lag. In VM everything is smooth.\",1643661114.567919,1643718521.118649,U02RW07CVTJ\\n196c2900-e574-4b5c-9e65-a4759f661699,U02U6DR551B,,,This thing breaks my brain every time I try to understand it,1643702306.563189,1643718865.556779,U01AXE0P5M3\\n1bacd97a-8281-4274-8047-269f27502b5b,,3.0,,\"For week 2 homework, where could we submit it?\",1643719063.030739,1643719063.030739,U02TA7FL78A\\n450623a8-1d6e-44c5-bd41-84cd0ce90200,U02TA7FL78A,,,You\\'re faster with the homework than we are with form preparation =) we\\'ll publish it today,1643719063.030739,1643719108.755239,U01AXE0P5M3\\n9d041472-8569-4365-86fe-9455f3618d5f,U02RTJPV6TZ,,,\"The channel purpose is to be able to discuss and help each other in topics around the course, like homework so feel free to do it, always welcomed :slightly_smiling_face:\\nJust small suggestion, please don\\'t post long messages in the channel directly but in the thread instead so it makes the channel and the message easier to read.\",1643715278.606679,1643719349.521849,U01B6TH1LRL\\n020b5f93-e5bd-4958-8317-fc5f46474a34,U02U34YJ8C8,,,\"Thanks guys. I ended up commenting it out from the DAG file, try speed things up a bit.\",1643714413.929649,1643719378.607539,U02U34YJ8C8\\n5aa99892-1e56-4ac5-a8e3-d79d38404d37,U02U34YJ8C8,,,\"I\\'m curious about the new datasets we\\'ll be creating. Big Query can read parquet file directly and I haven\\'t seen any word in the homework instructions related to the need to ingest the data in Big Query\\'s external dataset. Am I seeing this wrong? The `If you don\\'t have access to GCP, you can do that locally and ingest data to Postgres instead. If you have access to GCP, you don\\'t need to do it for local Postgres - only if you want.` part is a bit confusing, though...\",1643714413.929649,1643719389.064859,U02GVGA5F9Q\\n1200922e-e35d-4945-817b-313179dbc07c,U02Q7JMT9P1,,,Ooooh that was my mistake. First I have tried to build it on my own with `LocalFilesystemToGCSOperator` but gave up and copied the example solution after several tries. I have forgotten to change the operator though... Thanks for pointing it out!,1643671443.618389,1643719428.597409,U02Q7JMT9P1\\na68ec116-7fba-4d52-8f0a-54af01af419e,U02TA7FL78A,,,I miss the week_1 though and straight to week_2 and thanks for teaching us using VM instance the internet is so fast so no wait haha,1643719063.030739,1643719505.965019,U02TA7FL78A\\n2b6e743b-facc-4061-b227-74671d630ebc,U02U34YJ8C8,,,I commented it too. We can always run only that step if necessary later. I strongly believe we won\\'t need...,1643714413.929649,1643719547.082029,U02GVGA5F9Q\\n8b6780e1-bc76-443e-bb39-07c05d4d5e05,U02UKLHDWMQ,,,\"I Have\\nmv /home/simeon/Downloads/google_credentials.json ~/.google/credentials/google_credentials.json\",1643686137.792409,1643720463.729999,U02UKLHDWMQ\\nf408cfb1-e267-490c-9529-f149397e94ec,U02TA7FL78A,,,I just created the form - check <#C02V1Q9CL8K|announcements-course-data-engineering>,1643719063.030739,1643720476.120949,U01AXE0P5M3\\n9349c587-c8d4-458d-a7fd-5684b269e3f1,U02Q7JMT9P1,,,\"Thanks Alexey! The second hint worked just fine. The first looks like a lot of try-and-fail coding for an inexperienced person like me. Still doable though - we did it in week 1, just need to add exception handling...\",1643716410.018639,1643720490.646489,U02Q7JMT9P1\\n1b9303d6-b621-4d84-b2aa-4860cc0c9d52,U02UKLHDWMQ,,,<@U02TATJKLHG>,1643686137.792409,1643720556.664669,U02UKLHDWMQ\\nad1894de-9b2f-4998-8b9b-ca8e25198b97,U02Q7JMT9P1,,,but I\\'m sure you\\'ll remember it much better now :slightly_smiling_face:,1643716410.018639,1643720571.090519,U01AXE0P5M3\\n304192ac-7c79-47b6-b865-4ba9152f8688,,3.0,,\"Hey there, I have a question for the team. Apologies if this was covered in one of the live sessions I missed. I\\'m wondering how the project is going to work? Are we going to be given a framework for the project and conduct it using our own data, or given a fresh dataset and told what to do specifically? I just don\\'t have time for another ~2 weeks to really get in depth into the topics but once my own work calms down I\\'ll be able to dig in more.\",1643720791.868419,1643720791.868419,U02TVGE99QU\\n7db8f4cd-4460-4906-b2d5-263d5083dce1,U02T8ANTJGM,,,Thank you for the answers,1643692723.273259,1643720794.882929,U02T8ANTJGM\\n7199d069-e5fe-48ef-a72b-f100d901ecd0,U02Q7JMT9P1,,,Hope things are working fine for you now :smile:,1643671443.618389,1643720930.852319,U02TATJKLHG\\n40efca8d-bb4c-45c1-a6c2-ebea93e5591e,U02T8ANTJGM,,,\"I\\'ve seen gcp have a service called Secret Manager, maybe we can use it\",1643692723.273259,1643720989.522929,U02T8ANTJGM\\ned305c5f-21de-4b25-bb5d-ad0bea06db4b,U02TVGE99QU,,,\"The idea is to do the same thing we did in the course, but for a different dataset. You can use the same technologies as we presented in the course of similar (e.g. using Snowflake as a data warehouse and prefect for orchestration).\\n\\nTomorrow we\\'ll have a meeting and discuss projects and have more info about it hopefully\",1643720791.868419,1643721026.041059,U01AXE0P5M3\\n046ff0d0-f492-4e55-a283-5d744ed4c99f,U02UKLHDWMQ,,,And how are you passing this path in your docker-compose.yaml?,1643686137.792409,1643721499.558399,U02TATJKLHG\\n1acb4499-c102-4cad-a4c4-24eab8259fa2,,9.0,,Hello... Good day everyone... I just started the lecture and am currently on the 3rd lecture in week 1.. I am currently experiencing some problem in using PostgreSQL,1643721562.673419,1643721562.673419,U02SV2UV5GB\\n2528d96c-753f-4266-8e10-55d63e8203e8,U02UKLHDWMQ,,,\"this way\\nGOOGLE_APPLICATION_CREDENTIALS: /.google/credentials/google_credentials.json\\n    AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT: \\'google-cloud-platform://?extra__google_cloud_platform__key_path=/.google/credentials/google_credentials.json\\'\",1643686137.792409,1643721563.034729,U02UKLHDWMQ\\nd1baf0aa-6640-44a9-8e5c-68db69101047,U02TVGE99QU,,,\"ok cool, thank you Alexey. Just to confirm, homework completion isn\\'t necessary for the certificate if we complete the project, right?\",1643720791.868419,1643721669.560989,U02TVGE99QU\\n4b4b5a79-2723-4c7c-bdd0-dd8316e55821,U02SV2UV5GB,,,\"It keeps telling me docker: error response from daemon: pull access denied for root, repository does not exist or may require docker Login: denied: requested access to the resources denied\",1643721562.673419,1643721735.096179,U02SV2UV5GB\\n182cce15-fb22-456c-95d3-1602a5133a59,U02TVGE99QU,,,Yes that\\'s right,1643720791.868419,1643721851.324729,U01AXE0P5M3\\neb866436-29d6-4c19-9289-81ca09e02996,,2.0,,Guys I am stuck guys and it is frustrating but i don\\'t want to give up,1643723333.008119,1643723333.008119,U02UKLHDWMQ\\n46488364-4597-4631-9b2c-9aa4ab878217,,8.0,,\"Hello :wave: I’m having some issues loading multiple files from `Cloud Storage` into a *single table* in big query.\\n\\nFrom the airflow ingestion scripts I’m able to create all the monthly files in `Cloud Storage` separated by yellow taxi and fhv. But it’s the next part I’m having issues with understanding. If I want to create 2 tables (one for yellow taxi and one fhv) using all the monthly datasets, is the code provided in week 2 (shown below) sufficient? My understanding here is everytime I process a monthly dataset in Airflow, the `BigQueryCreateExternalTableOperator` will overwrite the current external table with a new external table, of the next month. What I would like to do is keep the data in the external table and append / update the table with the new months data.\\n\\nI know this is possible, but is it outside the work of the course? Or have I missed something in one of the videos that would give me a hint on where to go to do this?\\n\\nThanks!\\n```    bigquery_external_table_task = BigQueryCreateExternalTableOperator(\\n        task_id=\"\"bigquery_external_table_task\"\",\\n        table_resource={\\n            \"\"tableReference\"\": {\\n                \"\"projectId\"\": PROJECT_ID,\\n                \"\"datasetId\"\": BIGQUERY_DATASET,\\n                \"\"tableId\"\": \"\"name_of_table\"\",\\n            },\\n            \"\"externalDataConfiguration\"\": {\\n                \"\"sourceFormat\"\": \"\"PARQUET\"\",\\n                \"\"sourceUris\"\": [f\"\"gs://{BUCKET}/raw/{parquet_file}\"\"],\\n            },\\n        },\\n    )```\",1643723713.558069,1643723713.558069,U02TEERF0DA\\n6eb66785-3f06-435b-b4be-eb670000cc82,U02TEERF0DA,,,\"I asked a similar question, and Alexey replied that we would do it in the 3rd week of the course.\",1643723713.558069,1643724084.043749,U02R2PU9NLD\\naba3383a-54ae-4508-942a-39bf0417d3d2,U02UKLHDWMQ,,,\"I am not able to see anything wrong in what you have provided, maybe there is something beyond this that has an error\",1643686137.792409,1643724188.739469,U02TATJKLHG\\ne3e69eb6-a7cb-4ebc-b3b5-693cc64a4619,U02TEERF0DA,,,\"In week 3 videos, there\\'s a SQL code to create bigquery table combining the same dataset from google bucket\",1643723713.558069,1643724211.616909,U02TA7FL78A\\n5a9cd4c5-7916-4ea3-826a-8ab6caff5d91,U02UKLHDWMQ,,,Have you checked out your code to git? Is it possible to take a look?,1643686137.792409,1643724212.078169,U02TATJKLHG\\n633df9e1-0218-43af-b694-2d5da2d0dd1f,U02UKLHDWMQ,,,what\\'s up?,1643723333.008119,1643724343.662959,U01AXE0P5M3\\nd8efa4c0-0824-439a-82da-4c9a947b2b4b,U02SV2UV5GB,,,which OS are you on?,1643721562.673419,1643724359.406079,U01AXE0P5M3\\nfc5cd377-2244-490a-8018-387523d9ec8a,U02UKLHDWMQ,,,doing that now,1643686137.792409,1643724401.396839,U02UKLHDWMQ\\n6802977c-feb0-4c2f-9ea8-73f1488dffc5,U02SV2UV5GB,,,Windows 10,1643721562.673419,1643724471.175589,U02SV2UV5GB\\n69fbd522-bcd4-4dfb-a27f-c307774cd3a7,U02SV2UV5GB,,,Have you tried googling this error?,1643721562.673419,1643724516.354799,U01AXE0P5M3\\ne5259365-e550-46bc-ab88-abe5ab80194b,U02TEERF0DA,,,<@U02TA7FL78A> Has that video been released? I’m just looking over the playlist and I don’t see a description that would match that. Is it 3.1.1 ?,1643723713.558069,1643724678.590719,U02TEERF0DA\\nf9f3498a-a8db-4967-b4af-8dcb91652319,U02SV2UV5GB,,,Yes... I have but i was unable to get the correct answer,1643721562.673419,1643724683.558489,U02SV2UV5GB\\n4b964391-e290-4037-a83d-5dd6bd13d95c,U02TEERF0DA,,,Thanks <@U02R2PU9NLD>! Looks like I need to wait a bit :slightly_smiling_face:,1643723713.558069,1643724718.940299,U02TEERF0DA\\n83c9ecea-45f7-4cfb-8103-b77e1e226b57,U02UKLHDWMQ,,,\"COPY google_credentials.json $AIRFLOW_HOME/google_credentials.json\\nENV GOOGLE_APPLICATION_CREDENTIALS=\"\"${AIRFLOW_HOME}/google_credentials.json\"\"\\nENV AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT=\"\"google-cloud-platform://?extra__google_cloud_platform__key_path=${AIRFLOW_HOME}/google_credentials.json\"\"\\nThis is the only path i added has she did in the video\",1643686137.792409,1643724753.533469,U02UKLHDWMQ\\n835c3a08-9781-4d54-bbb5-b2f6ebc79544,U02UKLHDWMQ,,,<@U02TATJKLHG>,1643686137.792409,1643724769.375199,U02UKLHDWMQ\\n06b093d7-ee52-4fcc-b898-a318480af71a,U02SV2UV5GB,,,can you show what you type?,1643721562.673419,1643724882.661899,U01AXE0P5M3\\n9ebcb22f-f614-447b-8d84-2f5e06c8b7d9,U02UKLHDWMQ,,,I think i might just delete the .google folder in the homes directory and do it again,1643686137.792409,1643725153.570939,U02UKLHDWMQ\\nff1c5ddd-02f3-4b21-ae3e-38cb900e171e,U02TEERF0DA,,,<@U02TEERF0DA>  yes it\\'s 3.1.1,1643723713.558069,1643725280.875719,U02TA7FL78A\\nc53d1ba0-d2fb-4732-8176-3e7d8006eefe,U02SV2UV5GB,,,\"docker run -it  \\\\\\n      -e POSTGRES_USERNAME =\"\"root\"\" \\\\\\n      -e POSTGRES_PASSWORD = root \\\\\\n      -e POSTGRES_DB = \"\"ny_taxi\"\"  \\\\\\n      -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data  \\\\\\n      -p 5432:5432\\npostgres:13\",1643721562.673419,1643725433.548449,U02SV2UV5GB\\n7c98c43a-5f16-4bc9-b820-6953d1412db7,U02UKLHDWMQ,,,\"base) simeon@simeon-HP-ProBook-4530s:~$ cd .google/\\n(base) simeon@simeon-HP-ProBook-4530s:~/.google$ rm r .google/\\nrm: cannot remove \\'r\\': No such file or directory\\nrm: cannot remove \\'.google/\\': No such file or directory\\n(base) simeon@simeon-HP-ProBook-4530s:~/.google$\",1643686137.792409,1643725689.504789,U02UKLHDWMQ\\n563557d9-66f3-4f39-bddf-ec17ae6b5a49,U02UKLHDWMQ,,,says no such file or directory,1643686137.792409,1643725716.292669,U02UKLHDWMQ\\n31c3c593-3deb-4ac9-b86e-cd0168af0d23,U02UKLHDWMQ,,,deleted it,1643686137.792409,1643725882.850649,U02UKLHDWMQ\\n7d7f2e79-adb1-4085-9a00-2e4c778b306a,U02TEERF0DA,,,Great :thumbsup: I’ll take a look there. Thanks <@U02TA7FL78A>,1643723713.558069,1643726232.046129,U02TEERF0DA\\n18d32345-2a74-4cd6-a101-7e8de1fc415d,U02UKLHDWMQ,,,\"You did a cd into .google folder and then tried to delete it, so it is looking for .google inside .google\",1643686137.792409,1643726397.878989,U02TATJKLHG\\n28AE6FBB-6E12-4057-A3E0-9C4B1B16553C,,8.0,,\"In Docker Desktop I\\'ve set to memory to 8GB\\n\\nHowever checking the sizes of my running containers, my airflow worker container is using 13GB. \\n\\nI thought setting it to 8GB would restrict this?\",1643726441.258549,1643726441.258549,U02U34YJ8C8\\n09b84b0b-dfa4-4a65-8db0-f88ad8abe18f,U02UKLHDWMQ,,,\"You can check here Alexey\\n\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1643686137792409>\",1643723333.008119,1643726578.424019,U02TATJKLHG\\ndb9b59db-6880-4a72-bb95-07e68e9aae88,U02U34YJ8C8,,,Is it all in the memory though? I guess some part of it would be in the disk. Not entirely sure but consuming 13GB of memory would be crazy.,1643726441.258549,1643726726.278129,U02TATJKLHG\\nbd632bc0-2d59-43e1-ad70-91d987b91651,U02U34YJ8C8,,,\"I think so. When I go to Activity Monitor, I have a Docker process running using up nearly 15 GB. When I run:\\n\\n`docker ps --size`\\n\\nI can see `13.8GB (virtual 15.8GB)` next to airflow worker.\\n\\nBut then I don’t know a lot about this stuff.\",1643726441.258549,1643726886.014949,U02U34YJ8C8\\n5f9bd1f5-2c84-4977-a5a8-f51d75e7b250,U02UKLHDWMQ,,,can you do `ls` in the folder where you do docker build?,1643686137.792409,1643726911.090109,U01AXE0P5M3\\naba1452b-0f18-4e76-86b9-2a082c3b0ea9,U02UKLHDWMQ,,,what changes did you do to the compose file?,1643686137.792409,1643726955.939309,U01AXE0P5M3\\n324076c4-452e-4ff4-9b14-c81a7189e435,U02U34YJ8C8,,,\"Wow that\\'s a lot. I\\'ll have to spin this up in my local and and check the consumption. Also, has it been running for a long time?\",1643726441.258549,1643727116.765079,U02TATJKLHG\\nc9c3dab2-3153-4729-94a6-164629ce55f5,U02U34YJ8C8,,,\"Yeah it’s been running a while. It’s actually finished running the tasks. But I’ve not run `docker compose down` or anything like that yet, so it’s still active.\",1643726441.258549,1643727184.861739,U02U34YJ8C8\\n65e87706-b434-4e63-9a61-05d665485484,U02SV2UV5GB,,,You need to remove the spaces for the -e params,1643721562.673419,1643727919.048239,U01AXE0P5M3\\n207065e6-c596-45fb-94c7-79645bd3bc06,U02SV2UV5GB,,,\"If it solves your problem, can you update the FAQ (bookmarked link in the channel) with this?\",1643721562.673419,1643727952.129119,U01AXE0P5M3\\n15775050-d15a-44e9-a002-630318ce858f,U02U34YJ8C8,,,\"Although I notice when  I run `docker stats`  I can see a section for memory usage which looks right. Whereas the `size` (which is around 13GB) is shown under `block i/o`\\n\\nI think the block i/p is what’s written to disk. And the memory usage is what’s on memory.\\n\\nAlthough still not sure why there’s a docker process in my activity monitoring showing around 15GB of memory usage. seems a lot.\",1643726441.258549,1643728223.169439,U02U34YJ8C8\\na71a7de4-f91e-45b8-b54f-5befb55fd437,U02RTJPV6TZ,,,\"hellos, some help with this error\\n|   File \"\"/home/airflow/.local/bin/airflow\"\", line 5, in &lt;module&gt;\\ndtc-de-webserver-1  |     from airflow.__main__ import main\\ndtc-de-webserver-1  | ModuleNotFoundError: No module named \\'airflow\\'\\ndtc-de-webserver-1  | Traceback (most recent call last):\\ndtc-de-webserver-1  |   File \"\"/home/airflow/.local/bin/airflow\"\", line 5, in &lt;module&gt;\\ndtc-de-webserver-1  |     from airflow.__main__ import main\\ndtc-de-webserver-1  | ModuleNotFoundError: No module named \\'airflow\\'\\ndtc-de-webserver-1  | Traceback (most recent call last):\\ndtc-de-webserver-1  |   File \"\"/home/airflow/.local/bin/airflow\"\", line 5, in &lt;module&gt;\\ndtc-de-webserver-1  |     from airflow.__main__ import main\\ndtc-de-webserver-1  | ModuleNotFoundError: No module named \\'airflow\\'\\ndtc-de-webserver-1 exited with code 1\\ndtc-de-webserver-1  | Traceback (most recent call last):\\ndtc-de-webserver-1  |   File \"\"/home/airflow/.local/bin/airflow\"\", line 5, in &lt;module&gt;\\ndtc-de-webserver-1  |     from airflow.__main__ import main\\ndtc-de-webserver-1  | ModuleNotFoundError: No module named \\'airflow\\'\\ndtc-de-webserver-1  | Traceback (most recent call last):\\ndtc-de-webserver-1  |   File \"\"/home/airflow/.local/bin/airflow\"\", line 5, in &lt;module&gt;\\ndtc-de-webserver-1  |     from airflow.__main__ import main\\ndtc-de-webserver-1  | ModuleNotFoundError: No module named \\'airflow\\'\\ndtc-de-webserver-1  | Traceback (most recent call last):\\ndtc-de-webserver-1  |   File \"\"/home/airflow/.local/bin/airflow\"\", line 5, in &lt;module&gt;\\ndtc-de-webserver-1  |     from airflow.__main__ import main\\ndtc-de-webserver-1  | ModuleNotFoundError: No module named \\'airflow\\'\\ndtc-de-webserver-1 exited with code 1\",1641565510.010100,1643729078.856909,U02RTJPV6TZ\\nc8ddde0e-4706-4048-8b99-60aa7a7d1aec,U02RTJPV6TZ,,,\"Don\\'t use this nofrills, use the full one and check the video from <#C02V1Q9CL8K|announcements-course-data-engineering> that I shared yesterday\",1641565510.010100,1643729185.933739,U01AXE0P5M3\\na6c77d18-868f-426d-9544-a7c65459283a,U02RTJPV6TZ,,,ooh okay thanx,1641565510.010100,1643729228.234919,U02RTJPV6TZ\\nc02079a8-fa94-4616-9c2b-d0be718064e0,U030FNZC26L,,,\"I wonder why don\\'t we have worker container in the docker-compose file. This means that the tasks will be run on the local machine, right? Or I\\'m missing sth?\\nI also don\\'t know why it csnnot find the airflow package when we run `docker-compose up` . Not sure if it looks for it on my local machine or not. Maybe this is the reason.\",1643646196.837139,1643730136.031649,U030FNZC26L\\n7e96f649-7037-44f3-badc-661d339f2a05,U02UKLHDWMQ,,,\"these are the changes i did\\n\\nGOOGLE_APPLICATION_CREDENTIALS: /.google/credentials/google_credentials.json\\n    AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT: \\'google-cloud-platform://?extra__google_cloud_platform__key_path=/.google/credentials/google_credentials.json\\'\\n    GCP_PROJECT_ID: \\'bright-task-339417\\'\\n    GCP_GCS_BUCKET: \"\"dtc_data_lake_bright-task-339417\"\"\",1643686137.792409,1643730805.659369,U02UKLHDWMQ\\na6a87b64-9ec6-454b-84d6-2e681e920ca6,U02U34YJ8C8,,,\"Interesting. I like how you are just drilling into docker haha. Also possible that the docker process that you are seeing in the activity monitor is cumulative of all the services running inside it?\\n\\nWhen I did a `docker image ls` last time I was running it, the image sizes were like 2 GB each for all image.\\n\\n```REPOSITORY                  TAG       IMAGE ID       CREATED        SIZE\\nairflow_airflow-webserver   latest    076076a49d6e   5 hours ago    1.98GB\\nairflow_airflow-worker      latest    076076a49d6e   5 hours ago    1.98GB\\nairflow_flower              latest    076076a49d6e   5 hours ago    1.98GB\\nairflow_airflow-init        latest    076076a49d6e   5 hours ago    1.98GB\\nairflow_airflow-scheduler   latest    076076a49d6e   5 hours ago    1.98GB\\nairflow_airflow-triggerer   latest    076076a49d6e   5 hours ago    1.98GB\\nredis                       latest    f1b6973564e9   2 days ago     113MB\\napache/airflow              2.2.3     4a92e92f137e   11 days ago    981MB\\ndpage/pgadmin4              latest    e52ca07eba62   2 weeks ago    272MB\\nubuntu                      latest    d13c942271d6   3 weeks ago    72.8MB\\npostgres                    13        0896a8e0282d   3 weeks ago    371MB\\nhello-world                 latest    feb5d9fea6a5   4 months ago   13.3kB```\",1643726441.258549,1643730886.149399,U02TATJKLHG\\n2f6b37ca-3b75-419e-83a4-6b4b4afdd3cc,U02UKLHDWMQ,,,\"<@U01AXE0P5M3> result of ls\\n\\n(base) simeon@simeon-HP-ProBook-4530s:~/course-data-engineering/week_2_data_ingestion/airflow$ ls\\ndags  docker-compose.yaml  Dockerfile  logs  plugins  requirements.txt\",1643686137.792409,1643730887.155609,U02UKLHDWMQ\\n01721df9-c56c-4a45-86a7-919ef2673a99,U02UKLHDWMQ,,,did you change dockerfile? i\\'m not sure why it needs to find google_credentials.json in your current folder,1643686137.792409,1643730972.468339,U01AXE0P5M3\\nf7d53f01-cbcc-4986-9ba7-6ef8256e7388,U030FNZC26L,,,my understanding is that the scheduler will execute it,1643646196.837139,1643731027.412749,U01AXE0P5M3\\nb8f0a027-fd96-4cc8-ae11-126e5724fd4a,U02UKLHDWMQ,,,\"Changes in my docker file\\n\\nCOPY google_credentials.json $AIRFLOW_HOME/google_credentials.json\\nENV GOOGLE_APPLICATION_CREDENTIALS=\"\"${AIRFLOW_HOME}/google_credentials.json\"\"\\nENV AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT=\"\"google-cloud-platform://?extra__google_cloud_platform__key_path=${AIRFLOW_HOME}/google_credentials.json\"\"\",1643686137.792409,1643731032.377549,U02UKLHDWMQ\\n66893857-af1e-4374-b4a7-e040376b8ae6,U030FNZC26L,,,\"since it\\'s running inside the docker container, it shouldn\\'t care whether you have it on your local machine or not (I don\\'t)\",1643646196.837139,1643731125.328499,U01AXE0P5M3\\n3ef7a2f2-475c-45b2-b2f9-44196213f152,U02UKLHDWMQ,,,why did you need to do it?,1643686137.792409,1643731148.505179,U01AXE0P5M3\\ncf38db99-eb89-452b-8f3e-5a8667872896,U02UKLHDWMQ,,,I\\'d suggest reverting the dockerfile and follow the video I posted in <#C02V1Q9CL8K|announcements-course-data-engineering>,1643686137.792409,1643731170.228799,U01AXE0P5M3\\ne59329a5-5679-4e7c-badb-cd8680cbe062,U02UKLHDWMQ,,,what I don\\'t show in the video - the credentials are placed in ~/.google/credentials/google_credentials.json,1643686137.792409,1643731212.706729,U01AXE0P5M3\\n9edaea65-0b2d-4206-ae46-60b30e60b030,U02UKLHDWMQ,,,the video showed sejalv added those.,1643686137.792409,1643731263.980529,U02UKLHDWMQ\\neac89e4d-0fb6-4a83-a4b7-6a024419608a,U02UKLHDWMQ,,,yeah that is where i place mine at the home directory,1643686137.792409,1643731291.274299,U02UKLHDWMQ\\ncf72651f-269b-4ded-9743-199dbd3c1555,U02U34YJ8C8,,,\"Here is my `docker stats` output. I can see around 6-7 GB being consumed by docker right now in total in my task manager\\n\\n```CONTAINER ID   NAME                          CPU %     MEM USAGE / LIMIT     MEM %     NET I/O           BLOCK I/O   PIDS\\n81bc9a273a54   airflow-airflow-webserver-1   0.14%     1.138GiB / 7.695GiB   14.78%    1.94MB / 1.35MB   0B / 0B     56\\n849e1653e9cf   airflow-flower-1              0.01%     166.9MiB / 7.695GiB   2.12%     67.8kB / 62.7kB   0B / 0B     41\\n6a2c1a8dc227   airflow-airflow-worker-1      1.14%     1.603GiB / 7.695GiB   20.83%    39.5MB / 932kB    0B / 0B     31\\nfc53384f65a9   airflow-airflow-triggerer-1   17.88%    129.2MiB / 7.695GiB   1.64%     327kB / 485kB     0B / 0B     3\\n5ba41b37413a   airflow-airflow-scheduler-1   32.33%    233MiB / 7.695GiB     2.96%     3.79MB / 3.66MB   0B / 0B     3\\n5d9facde0921   airflow-redis-1               2.88%     17.29MiB / 7.695GiB   0.22%     187kB / 169kB     0B / 0B     5\\n4945aafb57a0   airflow-postgres-1            3.48%     65.07MiB / 7.695GiB   0.83%     6.51MB / 7.34MB   0B / 0B     18```\",1643726441.258549,1643731558.105749,U02TATJKLHG\\nfa20b4a2-9b38-49a5-a346-803bada812bb,U02UKLHDWMQ,,,I removed that part from my docker file,1643686137.792409,1643731652.206859,U02UKLHDWMQ\\n740acebd-a13f-416f-868f-6e2662e7312d,U02UKLHDWMQ,,,I probably missed that part from the video :see_no_evil: but I don\\'t do that and just use the dockerfile from the repo,1643686137.792409,1643731719.240919,U01AXE0P5M3\\n243ff3b3-9e8c-498b-8d5c-8af87ec54b8a,,9.0,,Is the form for homework of week 3 ready?,1643731739.532899,1643731739.532899,U02QNCUUHEY\\n82728831-3b8f-4f32-877c-c08e93e9a1cc,U02QNCUUHEY,,,Yes! - <https://forms.gle/ViWS8pDf2tZD4zSu5>,1643731739.532899,1643731803.749189,U02TATJKLHG\\nf6778d85-2fd2-4d52-b486-98403d3c0e3d,U02UKLHDWMQ,,,i removed the changes from the docker file it worked,1643686137.792409,1643731832.104959,U02UKLHDWMQ\\n534642b1-adcf-4f4b-ba2b-160e65baab70,U02QNCUUHEY,,,That’s the link for week 2. I haven’t even started looking at week 3 videos :exploding_head:,1643731739.532899,1643731873.562949,U02BVP1QTQF\\n703f0adc-35e8-4c49-a453-7b2e4018e3b4,U02UKLHDWMQ,,,thanks guys,1643686137.792409,1643732048.821219,U02UKLHDWMQ\\n8984dc51-7ac3-47fb-b9b2-f3aa3608f0eb,U02QNCUUHEY,,,\"I see the link for week 3, but it seems the form is not ready.\",1643731739.532899,1643732155.368559,U02QNCUUHEY\\n47267e5b-5590-4d8f-b728-8144ac98fb46,U02QNCUUHEY,,,\"Ohh my bad!! Didn\\'t see week 3 there :see_no_evil:.\\n\\nWeek 3 homework form??!? Are you done with the homework already? * _starts doubting myself with intensity_ *\",1643731739.532899,1643732156.975209,U02TATJKLHG\\n2a6ff4a0-8631-494c-adef-013256a0b6b3,,5.0,,\"Do we need to do `docker-compose up airflow-init` every time we start the airflow servers? I\\'ve been doing it just to avoid any issues later on, but I am not sure if I fully understand the reason to do it.\",1643732481.473959,1643732481.473959,U02TATJKLHG\\nc4146729-6c57-4dc4-a58d-3c1ce43b4c49,U02QNCUUHEY,,,:face_with_rolling_eyes:,1643731739.532899,1643732500.343619,U02RW07CVTJ\\nfe9a12c6-aa56-4cae-aac8-069bbf247722,U02TATJKLHG,,,\"I’m on the same boat. I think maybe it’s necessary after you build a custom image on the first time but not anymore after the first time you run it, because the yaml file already calls for the init service. But maybe I’m wrong\",1643732481.473959,1643732607.847059,U02BVP1QTQF\\nfaa52011-20b0-4b40-9400-47a6d94ef9d6,,6.0,,\"WARNING!!!: Not enough memory available for Docker.\\nairflow-airflow-init-1  | At least 4GB of memory required. You have 3.8G\",1643732613.179849,1643732613.179849,U02UKLHDWMQ\\n2ac9f472-59e7-4516-9798-6fc467dd744b,U01AXE0P5M3,,,\"Yes! That worked. Thanks a lot, Alexey!\",1642534337.023000,1643732625.605649,U02UCV3RAG4\\ne6f0e8dd-d9c3-41d0-8117-0f78c3c79a5f,U02UKLHDWMQ,,,also gave me a warning that am running docker containing has root user,1643732613.179849,1643732670.984919,U02UKLHDWMQ\\n11074b8f-827b-42c0-b0e8-7e702dbde63a,U02UKLHDWMQ,,,\"If you’ve got access to Google Cloud, I strongly suggest you run everything from within a VM. Check the video on the playlist to see how to create one. I also did a post yesterday with a link to a quick checklist for the VM\",1643732613.179849,1643732678.136739,U02BVP1QTQF\\n90f99d16-8366-4385-820b-c3be01785908,U02QNCUUHEY,,,so it seems weeks in DE zoomcamp might not necessarily map to real-world weeks - we\\'re still on week 2 (at least materials-wise and homework-wise) :smiley:,1643731739.532899,1643732859.901179,U01AXE0P5M3\\nda767035-9536-41aa-a873-b85060a60b3d,U02TATJKLHG,,,\"I think you need to run it only once. But since it\\'s declared in docker-compose, it runs it every time you do `up`\",1643732481.473959,1643732927.163529,U01AXE0P5M3\\n79ef88e8-d768-4df8-b53e-4a0463a0766c,U02QNCUUHEY,,,\"So deadline for week 3 is not the 7th of february, as in form?\",1643731739.532899,1643733032.676969,U02QNCUUHEY\\nd5596259-035a-4415-9ea6-04a5f1ca370e,U01AXE0P5M3,,,\"I was going to ask you to document it in FAQ, but it seems it\\'s already mentioned there - <https://docs.google.com/document/d/19bnYs80DwuUimHM65UV3sylsCn2j1vziPOwzBwQrebw/edit#>\\n\\nbut if you come across something that\\'s not there, please add it :slightly_smiling_face:\",1642534337.023000,1643733034.947539,U01AXE0P5M3\\n93afc7b0-788a-41ad-a7e4-3f8df5c0435e,U02QNCUUHEY,,,it\\'ll be 14th,1643731739.532899,1643733052.082959,U01AXE0P5M3\\n143bf9ea-7028-4088-8e2d-08423d71f4c6,U02QNCUUHEY,,,<https://github.com/DataTalksClub/data-engineering-zoomcamp/commit/41de474fa70ace5d872f7b76c16b0296f193bc36>,1643731739.532899,1643733083.351169,U01AXE0P5M3\\n5ad2681d-9a39-4f7d-9cb7-79b463483a1c,U02UKLHDWMQ,,,\"great, happy to hear it!\",1643686137.792409,1643733124.706019,U01AXE0P5M3\\n98dc2e61-5048-455b-89b0-218a19d38c85,U01AXE0P5M3,,,sure!,1642534337.023000,1643733126.962389,U02UCV3RAG4\\nec67f707-b13e-474c-aaca-aaf4c396c199,,12.0,,\"finally managed to do the week 1 :tada:\\nhad so much troubles with the postgres, python script on a mac with postgres installled and no wget … this was a pain but yeah :partying_face:\",1643733210.051579,1643733210.051579,U02UNQNMH7B\\n9add5fd2-d83f-4443-8a92-16563d420e24,U02UNQNMH7B,,,now let\\'s flow some air :smile:,1643733210.051579,1643733254.419619,U01AXE0P5M3\\na4c964a0-e378-45b8-b13c-63496408ce41,U02UKLHDWMQ,,,\"Also a funny thing Alvaro, I had the same warning come up over VM xD. I had to increase the disk size to 40 GB to accommodate. Around 21GB was consumed out of ~28 available.\\n\\nIt was a combination of downloading the datafiles and not deleting it, plus airflow running for a long time. I got the space back after a reboot though.\\n\\nBut yes Simon, work on a VM please, the file download speed on local will make you cry.\",1643732613.179849,1643733272.873299,U02TATJKLHG\\ndc1d5623-652c-4781-ad42-e2fdc58ba1ea,U02UNQNMH7B,,,:airflow-spin:,1643733210.051579,1643733280.218239,U02UNQNMH7B\\nbd4d1413-bde6-4064-8488-9c5387c532e6,U02TATJKLHG,,,\"Makes sense. Do all production grade airflow setups use this kind of a setup? Redis, flower, triggerer etc. ?\",1643732481.473959,1643733472.431789,U02TATJKLHG\\n1aa64301-cd04-4098-b37f-188ebf315b4f,U02TATJKLHG,,,\"to be completely honest with you I have no idea how exactly our data engineers set it up, it\\'s \"\"just working\"\" :sweat_smile:\\n\\nI assume it users celery and a bunch of workers, yes\",1643732481.473959,1643733547.752559,U01AXE0P5M3\\naa959354-51c4-43ab-adf0-6576c8316a81,U02UKLHDWMQ,,,\"Oh wow, I included the file deletion task from the get go because I think that the first CSV for the VFH dataset was like 1GB or so when I downloaded it manually and I didn’t want to deal with that kind of filesizes, so I never bothered to see what would happen if I didn’t delete them.\",1643732613.179849,1643733645.905529,U02BVP1QTQF\\nc22f0873-e408-4972-9b64-2e052e7e5083,U02TATJKLHG,,,Haha fair enough. Might help to know what role each service plays so that there is an understanding on how important are they to the setup.,1643732481.473959,1643733736.973909,U02TATJKLHG\\nba381b03-e309-4567-b40e-8a2ba8c78e26,U02UKLHDWMQ,,,\"That\\'s probably the best way to go about it. I just very naively triggered the DAG without caping the   `max_active_runs`, so 24 runs catching up all at once was not a good news for the worker. The VM just hung. I had to kill it from the UI. And then this warning showed up when I tried to boot up the airflow servers.\",1643732613.179849,1643734062.877849,U02TATJKLHG\\n32d0609e-1ee9-4c0f-875e-f9c798c00eab,,,,Hello.... what could be the issues here please?,,1643734374.213309,U01MFQW46BE\\n37d40c18-8f5e-4df6-b12b-52d6ad997fd5,,8.0,,\"$ winpty docker run -it \\\\\\n&gt;     -e POSTGRES_USER=\"\"root\"\" \\\\\\n&gt;     -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n&gt;     -e POSTGRES_DB=\"\"Ny-taxi\"\" \\\\\\n&gt;     -v c:/Users/wavy/desktop/D.E/ny-taxi-postgress:/var/lib/postgresql/data \\\\\\n&gt;     -p 5432:5432 \\\\\\n&gt;     postgres:13\\ndocker: Error response from daemon: invalid mode: \\\\Program Files\\\\Git\\\\var\\\\lib\\\\postgresql\\\\data.\",1643734396.114789,1643734396.114789,U01MFQW46BE\\n86118f1d-42eb-48c8-ac96-01f679b92d1f,U01MFQW46BE,,,I believe the issue could be,1643734396.114789,1643734532.212549,U02UQLZ071B\\nd419e29b-b687-4dba-b932-3ec5e030d5c0,U01MFQW46BE,,,-v c:/Users/wavy/desktop/D.E/ny-taxi-postgress:/var/lib/postgresql/data \\\\,1643734396.114789,1643734580.613359,U02UQLZ071B\\nb5384be4-4a01-4b33-8119-c400a4cd9026,U01MFQW46BE,,,Use this instead: -v //c/Users/wavy/desktop/D.E/ny-taxi-postgress:/var/lib/postgresql/data \\\\,1643734396.114789,1643734602.581369,U02UQLZ071B\\ncca1170a-00d5-40e8-855b-0cf4371334f3,U01MFQW46BE,,,\"Have tried this and it worked, but the postgres files that ought to be generated as was showed in the video wasn\\'t generated.\",1643734396.114789,1643734997.640069,U01MFQW46BE\\n2c24d117-0800-418a-85d7-fcfa07454278,U01MFQW46BE,,,can you try `-v /$(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data` if you are using git bash,1643734396.114789,1643735460.974559,U02TATJKLHG\\n9c9055e1-4a6b-4251-8ad6-6d8c5a27ac7e,U01MFQW46BE,,,Alright...thanks,1643734396.114789,1643735489.160409,U01MFQW46BE\\n308355c8-91de-4a99-b83d-6cc2fc4dbeb1,U01MFQW46BE,,,Are you using windows?,1643734396.114789,1643736960.524999,U02HFP7UTFB\\nE4BF2237-5383-4507-BFBB-DE5644CEE98B,,,,Hi for hw2,,1643737180.242039,U02U6DR551B\\n946B2611-DB59-42D8-8340-834E60CAF15C,,1.0,,Should we load the data in monthly fashion?,1643737192.672489,1643737192.672489,U02U6DR551B\\n72C80136-1AFF-4527-9E40-2AE8C00B358D,,,,I am confused ,,1643737197.595379,U02U6DR551B\\nb8d8db74-aa40-42ef-84f5-60849af54204,,3.0,,\"Hello everyone! In Airflow, is there a way of using f-strings and Jinja template variables together?\\n\\nIn below example, using f-string does not render the jinja-templated variable properly.\\n\\n```\"\"externalDataConfiguration\"\": {\\n                \"\"sourceFormat\"\": \"\"PARQUET\"\",\\n                \"\"sourceUris\"\": [f\"\"gs://{BUCKET}/raw/yellow_tripdata_{{ macros.ds_format(ds,\\'%Y-%m-%d\\',\\'%Y-%m\\') }}.parquet\"\"],\\n            }```\",1643737212.120739,1643737212.120739,U02ULP8PA0Z\\nAC8613EE-7E81-49C2-81A3-AD73A2482C82,,4.0,,I posted this question before and I don\\'t know why it got deleted,1643737213.557979,1643737213.557979,U02U6DR551B\\n18d16fc7-8fe2-47fd-9328-2c1a20eb4463,U02U6DR551B,,,maybe you posted it to a different channel like <#C02V1Q9CL8K|announcements-course-data-engineering>? I\\'m removing all questions from there,1643737213.557979,1643737240.916389,U01AXE0P5M3\\ncbc46e00-89c5-4a92-b6d0-5558f1ae5a4b,U02U6DR551B,,,I remember deleting something today from that channel,1643737213.557979,1643737252.295229,U01AXE0P5M3\\n4d8c7823-2519-4f16-bbf1-e780af799635,U02U6DR551B,,,\"one of the questions in the homework is about answering this - so I can\\'t answer your question without giving away the answer to the homework\\n\\nbut you can see how it\\'s done in the course videos to get some ideas\",1643737192.672489,1643737309.421769,U01AXE0P5M3\\nef8bd2a4-6cbf-47a3-bc40-4df9dc455d73,U02U6DR551B,,,also - can you please next time group all the messages in one post and hit the send button only once?,1643737213.557979,1643737374.001059,U01AXE0P5M3\\na461e187-99ef-416e-b60d-c2a74af1eff7,U02ULP8PA0Z,,,\"yes, they don\\'t play well together, so you should use either one or another, not both in the same string declaration\",1643737212.120739,1643737445.454689,U01AXE0P5M3\\ne4ee634c-75c5-4524-b6d0-31c73bd67e12,U01MFQW46BE,,,\"Yea, the command has worked but I don\\'t know why it\\'s taking time for the database to start after supplying the password.\",1643734396.114789,1643737513.363579,U01MFQW46BE\\n7c67a34b-ff69-48c7-bfc4-0cdcb82f3c41,U02ULP8PA0Z,,,\"Wouldn’t using four braces ( `{{{{`  ) for jinja segment work, assuming f-string is evaluated first?\\n\\n```test = \\'bar\\'\\n\\nf\\'foo{test} {{{{ Jinja }}}}\\'```\\ngives `\\'foobar {{ Jinja }}\\'` once the f-string is evaluated\",1643737212.120739,1643738806.851879,U020UP3TJ8Y\\n59149686-a1be-47bf-9325-a8022dd7d991,U02ULP8PA0Z,,,nice - didn\\'t know about it!,1643737212.120739,1643739053.564839,U01AXE0P5M3\\nE597BDE2-ACA3-42B1-8172-003D25CB814D,U02U6DR551B,,,Sure,1643737213.557979,1643740964.583939,U02U6DR551B\\n42ef5706-aa5a-4777-8925-52e857609bb1,,5.0,,\"Hi!\\nWe need to collect data from 2019 to 2022 or to 2020 for hw2? Because question 3 says:\\n\\n&gt; _[...] after finishing everything? [...]_\\nI think it is a bit confusing, because the note says something regarding to 2020, It sounds like we have to collect all up to date, but in the beginning:\\n\\n&gt;  [....] _for-hire vehicles, for 2019 only_\\n\",1643743935.708459,1643743935.708459,U02CNNC070C\\n8312e938-f228-489c-861a-d42275133766,U02CNNC070C,,,\"I have the same question :zipper_mouth_face: I believe that is 2019-01 and 2020-01, but I\\'m don\\'t sure.\",1643743935.708459,1643744154.194839,U02TJ69RKT5\\n21fa3230-80da-4625-a1ae-69c5d6032c06,U02CNNC070C,,,\"For yellow taxis we need data from 2019 to 2020 for week 3 lessons. You can throw in some 2021 as well, because why not.\\n\\nFor fhv, we\\'ll only need 2019 for week 3 homework\",1643743935.708459,1643744263.951339,U01AXE0P5M3\\n1e988a4b-de8b-4058-a08a-bc89ecbf3755,U02CNNC070C,,,\"And also note that there\\'s no data after August 2021, so we can\\'t really get anything for 2022\",1643743935.708459,1643744306.637199,U01AXE0P5M3\\n5484d20c-7110-4d20-a8a6-1924c90380b3,U02CNNC070C,,,Thanks :raised_hands:,1643743935.708459,1643744479.697269,U02TJ69RKT5\\n8c6f94ef-85cb-4574-9a9a-aaee2c72953e,,5.0,,\"Hi all, I\\'m working on my taxi data DAG. I\\'m wondering if I make a change to data_ingestion_gcs_dag.py, do I have to delete the DAG in Airflow and restart Docker Compose for it to take effect? That\\'s what I\\'ve been doing, but I wasn\\'t sure if there\\'s a faster way.\",1643745618.751489,1643745618.751489,U02U3E6HVNC\\n70626b84-3038-4beb-b0e8-f2d6bcaeb9c0,U02U3E6HVNC,,,\"Delete - yes. Restart - no, it should appear automatically. But you\\'ll need to change the name of the dag\",1643745618.751489,1643746049.958599,U01AXE0P5M3\\n565e9432-c1aa-4756-b11f-4fab72f0dbee,U02U3E6HVNC,,,\"Oh I renamed the .py file locally, but didn\\'t rename the DAG in the Airflow GUI. That must be why I didn\\'t notice that it changed?\",1643745618.751489,1643746177.298669,U02U3E6HVNC\\n2a79fd16-b317-4b01-930f-d810476ec17f,U02U3E6HVNC,,,Hmm I don\\'t see a way to change the name in Airflow but I\\'m pretty sure the changes took effect. Thanks for the reply!,1643745618.751489,1643746710.959259,U02U3E6HVNC\\nde406858-5032-417a-9b73-1becf0ed9c8e,U02U3E6HVNC,,,\"by \"\"name\"\" I mean the dag_id parameter - you can change it in the code to e.g. \"\"data_ingestion_gcs_dag_v2\"\"\",1643745618.751489,1643746811.233389,U01AXE0P5M3\\nf5683d6c-3fc0-4539-97d7-44a52cd47618,U02U3E6HVNC,,,Ohhh doy. Thank you for the clarification,1643745618.751489,1643747276.279239,U02U3E6HVNC\\nc59cbacf-3df4-4bc0-89f8-9b976eb0e693,,4.0,,\"Hey guys. I have shared above that I was struggling with csv-&gt;parquet conversion, which, thanks to Alexey\\'s valuable hints I managed to overcome by chopping data into smaller portions and saving them into separate parquet files. Unfortunately I wasn\\'t able to figure out how to append data to parquet files (apparently one has to build a separate ETL pipeline for that :sweat_smile:) so I have to upload a complete folder of parquet files for each month. No big deal - I make a list of files and upload them one by one. The problem I cannot overcome, the files are not getting uploaded. The task finishes without any errors, target folder in GCS bucket gets created, the file that is being uploaded is printed in a debug message - but nothing appears in the cloud.\\n\\nI am asking if someone could make me a favor to go through my code to figure out what is wrong. Thanks in advance\\n<https://gist.github.com/IliaSinev/0caeba16fbe122442ea7fbc0514731d0>\\n\\nOh, and since I am showing my code, any ideas why the `end_date` parameter of my DAG is not accepted - I am getting all months uploaded between `starting_date` and current date?\",1643747416.601209,1643747416.601209,U02Q7JMT9P1\\n810a5504-9ae5-4760-a0b3-5c3e8b53c05f,U02Q7JMT9P1,,,what happens if you run this code without airflow? can you successfully upload the files and see them in the bucket?,1643747416.601209,1643747840.382829,U01AXE0P5M3\\nd45e0bca-3440-49c5-a987-4e5e30835fd6,U02Q7JMT9P1,,,\"also, not sure - but maybe you need to create this blob object for every file\\n\\n<https://gist.github.com/IliaSinev/0caeba16fbe122442ea7fbc0514731d0#file-data_ingestion_gcs-py-L55>\",1643747416.601209,1643748150.015969,U01AXE0P5M3\\nbe039536-da91-471d-8b56-3f24623f6644,,,,\"Hello, does anybody know is there anything like execution plan in Bigquery? Where I can clearly see that partitioning or clustering worked for the query, but not just comparing actual data scan.\",,1643748296.214489,U02T65GT78W\\nbbb0cf13-30c3-4de3-a3cb-37753d29ec72,,4.0,,*Data Talk Club: Data Engineering Course - Week 2 Summary:*,1643748315.670039,1643748315.670039,U02S4A9TA5Q\\n2a7da7cf-dd41-4dcc-a07b-9698a084f862,U02Q7JMT9P1,,,\"the way I\\'d approach it:\\n\\n• get the csv file\\n• open jupyter and start a notebook there\\n• experiment with it and come up with the code that uploads all files one by one\\n• move this code to the dag \",1643747416.601209,1643748316.077619,U01AXE0P5M3\\n82f4a59c-a4d2-4f94-ab66-433f7b25dfdc,U02CNNC070C,,,I\\'ve updated the description - I hope it makes it clearer,1643743935.708459,1643748670.194769,U01AXE0P5M3\\ne758b124-4402-4f65-b789-df9b20a3d245,U02Q7JMT9P1,,,Thanks for your hints Alexey! I am going to try running it locally first,1643747416.601209,1643748761.297669,U02Q7JMT9P1\\na936f120-1238-44cc-8881-3c8e4573ae1e,U02S4A9TA5Q,,,Thanks for the summary! I would add that ELT can definitely leverage both solutions where the transformation (data modelling) done on the Data Lake is then stored in a Data Warehouse which analysts can use to query the data.,1643748315.670039,1643748855.165559,U02UX664K5E\\n6d0a690c-9326-44cc-b87c-0fbcd68b6039,U02S4A9TA5Q,,,Yes I agree..,1643748315.670039,1643748906.524719,U02S4A9TA5Q\\n46284666-c419-4305-9abd-f5bc076d950b,U02S4A9TA5Q,,,<https://github.com/vinoths-tech/vs-dtc-de/blob/main/README.md>,1643748315.670039,1643748966.071669,U02S4A9TA5Q\\n4fdab7e7-2e3a-4612-aaca-2f00b01854e5,U02S4A9TA5Q,,,Please feel free to use the summary from above Github link if anyone interested. Thanks.,1643748315.670039,1643748996.907299,U02S4A9TA5Q\\nb22d79d8-c221-462b-8285-68e34565aa8f,,8.0,,\"Hey did anyone encounter this issue? local_to_gcs_task error after exporting env variables. I\\'m blocked by this issue so any help would be appreciated! I think it may have to do with my environment variables? but i copied the template from github and filled in my credentials accordingly, so i\\'m not sure what the issue is.\",1643752002.367069,1643752002.367069,U02T9550LTU\\n82ea76c5-cf74-4192-ba6f-a7679642d8ee,U02T9550LTU,,,,1643752002.367069,1643752022.252439,U02T9550LTU\\nb9332698-2046-4438-944c-69dca811fae7,U02U07906Q0,,,\"The repo has been updated, maybe you should pull the repo, since the other files has been changed as well.\",1643635837.178319,1643753194.234449,U02UB59DKDL\\n1fa473c3-f77a-424c-a828-076dc7974eb7,U02UKLHDWMQ,,,<@U02BVP1QTQF> link to the post please my system has been hanging over 3 hours.,1643732613.179849,1643754645.051989,U02UKLHDWMQ\\n04b9ae86-7160-41dc-be6f-8683c67765d4,,2.0,,\"My system is hanging o, what do I do for over 3 hours now after I ran the airflow docker compose\",1643754663.743169,1643754663.743169,U02UKLHDWMQ\\n457F13FE-830B-4349-9636-916B9D9F5F99,U02UKLHDWMQ,,,Try running it on GCP Vm instead ,1643754663.743169,1643757453.848719,U02TBCXNZ60\\n085bba10-3c0d-41d9-93e6-b12f84a1a742,U02UKLHDWMQ,,,Okay,1643754663.743169,1643758540.222519,U02UKLHDWMQ\\n962cb093-2467-4b65-b531-cbf2578d3bce,,1.0,,\"In Terraform `<http://main.tf|main.tf>` file we are defining a Data Lake (Google Cloud Storage) where the `condition` is set to 30 days. Does this mean that it would be destroyed after 30 days from the creation?\\n```    condition {\\n      age = 30  // days\\n    }```\",1643761460.567589,1643761460.567589,U02UX664K5E\\n2a0b337c-57f4-4dba-8dc0-569d02980b46,,2.0,,\"Apologies if this has been asked and answered however I am running into trouble when trying to mount the NY taxi data (in week 1 video 1.2.2)\\n\\nWhen I attempt the command docker run -it (not typing the rest out) it is telling me that I have an invalid reference format\\n\\ndocker: invalid reference format.\\nSee \\'docker run --help\\'.\\n\\nhas anyone had this issue and if you have how did you fix it?\",1643761778.793289,1643761778.793289,U02SXQ9L0FJ\\nba49d4ea-19df-4d16-b033-bd89d44f8596,,2.0,,Please can I use an old ssh keya new VM?,1643762358.242459,1643762358.242459,U02UKLHDWMQ\\n32877100-28e8-412c-be19-6784ef71c3fa,,4.0,,\"Question about naming docker containers.\\n\\nI’m looking at the containers spun up in week 2, and they all start with `dtc-de` , and I was looking in the `docker-compose.yaml` to see if that was intentional but I could not find any of the services prefixed with `dtc-de`\\n\\nJust wondering how this naming was applied to the containers? Thanks!\\n```❯ docker-compose ps\\nNAME                         COMMAND                  SERVICE             STATUS              PORTS\\ndtc-de-airflow-init-1        \"\"/bin/bash -c \\'funct…\"\"   airflow-init        exited (0)\\ndtc-de-airflow-scheduler-1   \"\"/usr/bin/dumb-init …\"\"   airflow-scheduler   running (healthy)   8080/tcp\\ndtc-de-airflow-triggerer-1   \"\"/usr/bin/dumb-init …\"\"   airflow-triggerer   running (healthy)   8080/tcp\\ndtc-de-airflow-webserver-1   \"\"/usr/bin/dumb-init …\"\"   airflow-webserver   running (healthy)   0.0.0.0:8080-&gt;8080/tcp\\ndtc-de-airflow-worker-1      \"\"/usr/bin/dumb-init …\"\"   airflow-worker      running (healthy)   8080/tcp\\ndtc-de-flower-1              \"\"/usr/bin/dumb-init …\"\"   flower              running (healthy)   0.0.0.0:5555-&gt;5555/tcp\\ndtc-de-postgres-1            \"\"docker-entrypoint.s…\"\"   postgres            running (healthy)   5432/tcp\\ndtc-de-redis-1               \"\"docker-entrypoint.s…\"\"   redis               running (healthy)   6379/tcp```\",1643766679.185449,1643766679.185449,U02TEERF0DA\\n1a2c86ea-d25e-4e29-9330-21469ae6b079,,,,Happy feelings,,1643772351.813129,U02TC704A3F\\n4d6e01c4-0a4a-49c5-b794-e415a4b29cfe,U02T9550LTU,,,What is the error log? After the last line in the screenshot.,1643752002.367069,1643776247.324589,U02TATJKLHG\\n32fb3545-d832-48a0-8cd0-7e963d1fe707,U02UX664K5E,,,\"This is what I can see on the UI, so seems like 30 days after there is no update to any object.\",1643761460.567589,1643776381.555009,U02TATJKLHG\\ne1a2b347-17ab-49aa-bc05-02c338cbc8da,U02SXQ9L0FJ,,,Can you paste the entire command here?,1643761778.793289,1643776409.473079,U02TATJKLHG\\n9c11cf7d-ed4d-4fb4-8c17-940cc890e2e6,U02T9550LTU,,,\"```*** Reading local file: /opt/airflow/logs/data_ingestion_gcs_dag/local_to_gcs_task/2022-02-01T22:42:00.619917+00:00/2.log\\n[2022-02-01, 22:47:29 UTC] {taskinstance.py:1032} INFO - Dependencies all met for &lt;TaskInstance: data_ingestion_gcs_dag.local_to_gcs_task manual__2022-02-01T22:42:00.619917+00:00 [queued]&gt;\\n[2022-02-01, 22:47:29 UTC] {taskinstance.py:1032} INFO - Dependencies all met for &lt;TaskInstance: data_ingestion_gcs_dag.local_to_gcs_task manual__2022-02-01T22:42:00.619917+00:00 [queued]&gt;\\n[2022-02-01, 22:47:29 UTC] {taskinstance.py:1238} INFO - \\n--------------------------------------------------------------------------------\\n[2022-02-01, 22:47:29 UTC] {taskinstance.py:1239} INFO - Starting attempt 2 of 2\\n[2022-02-01, 22:47:29 UTC] {taskinstance.py:1240} INFO - \\n--------------------------------------------------------------------------------\\n[2022-02-01, 22:47:29 UTC] {taskinstance.py:1259} INFO - Executing &lt;Task(PythonOperator): local_to_gcs_task&gt; on 2022-02-01 22:42:00.619917+00:00\\n[2022-02-01, 22:47:29 UTC] {standard_task_runner.py:52} INFO - Started process 472 to run task\\n[2022-02-01, 22:47:29 UTC] {standard_task_runner.py:76} INFO - Running: [\\'***\\', \\'tasks\\', \\'run\\', \\'data_ingestion_gcs_dag\\', \\'local_to_gcs_task\\', \\'manual__2022-02-01T22:42:00.619917+00:00\\', \\'--job-id\\', \\'32\\', \\'--raw\\', \\'--subdir\\', \\'DAGS_FOLDER/data_ingestion_gcs_dag.py\\', \\'--cfg-path\\', \\'/tmp/tmpnsc87xxv\\', \\'--error-file\\', \\'/tmp/tmpcmniew2j\\']\\n[2022-02-01, 22:47:29 UTC] {standard_task_runner.py:77} INFO - Job 32: Subtask local_to_gcs_task\\n[2022-02-01, 22:47:29 UTC] {logging_mixin.py:109} INFO - Running &lt;TaskInstance: data_ingestion_gcs_dag.local_to_gcs_task manual__2022-02-01T22:42:00.619917+00:00 [running]&gt; on host 1398432f6646\\n[2022-02-01, 22:47:30 UTC] {taskinstance.py:1426} INFO - Exporting the following env vars:\\nAIRFLOW_CTX_DAG_OWNER=***\\nAIRFLOW_CTX_DAG_ID=data_ingestion_gcs_dag\\nAIRFLOW_CTX_TASK_ID=local_to_gcs_task\\nAIRFLOW_CTX_EXECUTION_DATE=2022-02-01T22:42:00.619917+00:00\\nAIRFLOW_CTX_DAG_RUN_ID=manual__2022-02-01T22:42:00.619917+00:00\\n[2022-02-01, 22:47:31 UTC] {taskinstance.py:1700} ERROR - Task failed with exception\\nTraceback (most recent call last):\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py\"\", line 2594, in upload_from_file\\n    retry=retry,\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py\"\", line 2412, in _do_upload\\n    retry=retry,\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py\"\", line 2237, in _do_resumable_upload\\n    retry=retry,\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py\"\", line 2112, in _initiate_resumable_upload\\n    timeout=timeout,\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/requests/upload.py\"\", line 421, in initiate\\n    retriable_request, self._get_status_code, self._retry_strategy\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/requests/_request_helpers.py\"\", line 147, in wait_and_retry\\n    response = func()\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/requests/upload.py\"\", line 416, in retriable_request\\n    self._process_initiate_response(result)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/_upload.py\"\", line 503, in _process_initiate_response\\n    callback=self._make_invalid,\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/_helpers.py\"\", line 104, in require_status_code\\n    *status_codes\\ngoogle.resumable_media.common.InvalidResponse: (\\'Request failed with status code\\', 403, \\'Expected one of\\', &lt;HTTPStatus.OK: 200&gt;, &lt;HTTPStatus.CREATED: 201&gt;)\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1329, in _run_raw_task\\n    self._execute_task_with_callbacks(context)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1455, in _execute_task_with_callbacks\\n    result = self._execute_task(context, self.task)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1511, in _execute_task\\n    result = execute_callable(context=context)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py\"\", line 174, in execute\\n    return_value = self.execute_callable()\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py\"\", line 185, in execute_callable\\n    return self.python_callable(*self.op_args, **self.op_kwargs)\\n  File \"\"/opt/airflow/dags/data_ingestion_gcs_dag.py\"\", line 51, in upload_to_gcs\\n    blob.upload_from_filename(local_file)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py\"\", line 2735, in upload_from_filename\\n    retry=retry,\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py\"\", line 2598, in upload_from_file\\n    _raise_from_invalid_response(exc)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py\"\", line 4466, in _raise_from_invalid_response\\n    raise exceptions.from_http_status(response.status_code, message, response=response)\\ngoogle.api_core.exceptions.Forbidden: 403 POST <https://storage.googleapis.com/upload/storage/v1/b/dtc_data_lake/o?uploadType=resumable>: {\\n  \"\"error\"\": {\\n    \"\"code\"\": 403,\\n    \"\"message\"\": \"\"<mailto:dtc-de-user@dtc-de-338723.iam.gserviceaccount.com|dtc-de-user@dtc-de-338723.iam.gserviceaccount.com> does not have storage.objects.create access to the Google Cloud Storage object.\"\",\\n    \"\"errors\"\": [\\n      {\\n        \"\"message\"\": \"\"<mailto:dtc-de-user@dtc-de-338723.iam.gserviceaccount.com|dtc-de-user@dtc-de-338723.iam.gserviceaccount.com> does not have storage.objects.create access to the Google Cloud Storage object.\"\",\\n        \"\"domain\"\": \"\"global\"\",\\n        \"\"reason\"\": \"\"forbidden\"\"\\n      }\\n    ]\\n  }\\n}\\n: (\\'Request failed with status code\\', 403, \\'Expected one of\\', &lt;HTTPStatus.OK: 200&gt;, &lt;HTTPStatus.CREATED: 201&gt;)\\n[2022-02-01, 22:47:31 UTC] {taskinstance.py:1277} INFO - Marking task as FAILED. dag_id=data_ingestion_gcs_dag, task_id=local_to_gcs_task, execution_date=20220201T224200, start_date=20220201T224729, end_date=20220201T224731\\n[2022-02-01, 22:47:31 UTC] {standard_task_runner.py:92} ERROR - Failed to execute job 32 for task local_to_gcs_task\\nTraceback (most recent call last):\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py\"\", line 2594, in upload_from_file\\n    retry=retry,\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py\"\", line 2412, in _do_upload\\n    retry=retry,\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py\"\", line 2237, in _do_resumable_upload\\n    retry=retry,\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py\"\", line 2112, in _initiate_resumable_upload\\n    timeout=timeout,\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/requests/upload.py\"\", line 421, in initiate\\n    retriable_request, self._get_status_code, self._retry_strategy\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/requests/_request_helpers.py\"\", line 147, in wait_and_retry\\n    response = func()\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/requests/upload.py\"\", line 416, in retriable_request\\n    self._process_initiate_response(result)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/_upload.py\"\", line 503, in _process_initiate_response\\n    callback=self._make_invalid,\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/_helpers.py\"\", line 104, in require_status_code\\n    *status_codes\\ngoogle.resumable_media.common.InvalidResponse: (\\'Request failed with status code\\', 403, \\'Expected one of\\', &lt;HTTPStatus.OK: 200&gt;, &lt;HTTPStatus.CREATED: 201&gt;)\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/task/task_runner/standard_task_runner.py\"\", line 85, in _start_by_fork\\n    args.func(args, dag=self.dag)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/cli_parser.py\"\", line 48, in command\\n    return func(*args, **kwargs)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/cli.py\"\", line 92, in wrapper\\n    return f(*args, **kwargs)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py\"\", line 298, in task_run\\n    _run_task_by_selected_method(args, dag, ti)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py\"\", line 107, in _run_task_by_selected_method\\n    _run_raw_task(args, ti)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py\"\", line 184, in _run_raw_task\\n    error_file=args.error_file,\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py\"\", line 70, in wrapper\\n    return func(*args, session=session, **kwargs)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1329, in _run_raw_task\\n    self._execute_task_with_callbacks(context)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1455, in _execute_task_with_callbacks\\n    result = self._execute_task(context, self.task)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1511, in _execute_task\\n    result = execute_callable(context=context)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py\"\", line 174, in execute\\n    return_value = self.execute_callable()\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py\"\", line 185, in execute_callable\\n    return self.python_callable(*self.op_args, **self.op_kwargs)\\n  File \"\"/opt/airflow/dags/data_ingestion_gcs_dag.py\"\", line 51, in upload_to_gcs\\n    blob.upload_from_filename(local_file)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py\"\", line 2735, in upload_from_filename\\n    retry=retry,\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py\"\", line 2598, in upload_from_file\\n    _raise_from_invalid_response(exc)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py\"\", line 4466, in _raise_from_invalid_response\\n    raise exceptions.from_http_status(response.status_code, message, response=response)\\ngoogle.api_core.exceptions.Forbidden: 403 POST <https://storage.googleapis.com/upload/storage/v1/b/dtc_data_lake/o?uploadType=resumable>: {\\n  \"\"error\"\": {\\n    \"\"code\"\": 403,\\n    \"\"message\"\": \"\"<mailto:dtc-de-user@dtc-de-338723.iam.gserviceaccount.com|dtc-de-user@dtc-de-338723.iam.gserviceaccount.com> does not have storage.objects.create access to the Google Cloud Storage object.\"\",\\n    \"\"errors\"\": [\\n      {\\n        \"\"message\"\": \"\"<mailto:dtc-de-user@dtc-de-338723.iam.gserviceaccount.com|dtc-de-user@dtc-de-338723.iam.gserviceaccount.com> does not have storage.objects.create access to the Google Cloud Storage object.\"\",\\n        \"\"domain\"\": \"\"global\"\",\\n        \"\"reason\"\": \"\"forbidden\"\"\\n      }\\n    ]\\n  }\\n}\\n: (\\'Request failed with status code\\', 403, \\'Expected one of\\', &lt;HTTPStatus.OK: 200&gt;, &lt;HTTPStatus.CREATED: 201&gt;)\\n[2022-02-01, 22:47:31 UTC] {local_task_job.py:154} INFO - Task exited with return code 1\\n[2022-02-01, 22:47:31 UTC] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check```\\n<@U02TATJKLHG> here is the full output.... its not much help..\",1643752002.367069,1643776483.125809,U02T9550LTU\\n00912c97-5e47-4704-9cd7-5ff8069aea83,U02T9550LTU,,,\"``` \"\"message\"\": \"\"<mailto:dtc-de-user@dtc-de-338723.iam.gserviceaccount.com|dtc-de-user@dtc-de-338723.iam.gserviceaccount.com> does not have storage.objects.create access to the Google Cloud Storage object.\"\",\\n    \"\"errors\"\": [\\n      {\\n        \"\"message\"\": \"\"<mailto:dtc-de-user@dtc-de-338723.iam.gserviceaccount.com|dtc-de-user@dtc-de-338723.iam.gserviceaccount.com> does not have storage.objects.create access to the Google Cloud Storage object.\"\",\\n        \"\"domain\"\": \"\"global\"\",\\n        \"\"reason\"\": \"\"forbidden\"\"```\\noh hmmm any idea to debug this?\",1643752002.367069,1643776535.061589,U02T9550LTU\\n5ef9475e-bc1b-4cbe-ac03-b67f5b84ae7b,U02T9550LTU,,,\"Can you check if your account has the right permissions? Basically the ones that allow you to create objects in the storage. Ideally you should if you followed what was shown in the lessons.\\n\\nSecond, have you enabled the APIs?\\n\\nIf yes and nothing works, can you remove the storage roles and add them again?\",1643752002.367069,1643776826.850709,U02TATJKLHG\\n498767da-04ab-4166-9c2f-7c16ea2ab93c,U02T9550LTU,,,You can also google the above error and you will find something that works for you,1643752002.367069,1643776851.723569,U02TATJKLHG\\n195047b2-9286-43f0-b48a-54155bf30027,,3.0,,\"For HW #2, could anyone please clarify Q2 and Q4 on the frequency of the DAGs?  I could be overthinking it, but in the .MD file, Q2/4 asks, “How often do we need to run this DAG?”  Is that asking for the scheduling frequency, as in we need to schedule the DAG to run every year, to pull data for the year, or every day, to pull data for every day, or is it asking about the frequency with which we need to trigger the DAG, by pressing the play button, in the upper right hand corner of the screen?\",1643777143.877529,1643777143.877529,U02U8CB58G3\\n58c0beb9-5ef8-4801-be5f-4e41d1c98506,U02U8CB58G3,,,How often is your DAG running with the help of the schedule and not what you do manually while pressing the play button. Basically what schedule_interval have you set,1643777143.877529,1643777384.597729,U02TATJKLHG\\n83aa0461-65ec-483a-97a0-8eda7a53d418,U02T9550LTU,,,\"<@U02TATJKLHG> hey thanks bro, having a second look at the error made me realize i didn\\'t specify the right bucket and project id name. this issue was in an earlier thread . I rebuilt the image and it worked. thanks :raised_hands::raised_hands:\",1643752002.367069,1643777533.283289,U02T9550LTU\\n55707265-fc6a-4d86-a90b-1d3a76931932,,5.0,,\"Hi, everyone :slightly_smiling_face: I have used the `@montly` setting in the dag. It seems to work exactly as we want, however, <@U01AXE0P5M3> used another interval for it - `0 6 2 * *`, and looks like it is also monthly? What is correct way to set up? `@monthly` or `0 6 2 * *` ? Kindly assist :pray:\",1643777598.302299,1643777598.302299,U02U5L97S6T\\nba7f1ee2-98f9-4942-887a-fa04c0910338,U02T9550LTU,,,Haha alright :smile:,1643752002.367069,1643777619.787589,U02TATJKLHG\\n93bd6957-613e-4317-94a1-3167b85ad1dd,U02U5L97S6T,,,\"I guess both can work as long as you are sure that the data for let\\'s say March 2022 will be uploaded before 1st March 12:00 AM. That is in the month of Feb and not after that. So 2nd of the month is IMO a buffer for the data to arrive so that our DAG doesn\\'t fail.\\n\\nAlso currently we were fetching data that is already present, but if we build a DAG to run for the future, we might have to pull March\\'s data in April since data for March can arrive only after March is complete.\",1643777598.302299,1643778033.274549,U02TATJKLHG\\n7d9cf001-bc5e-46c1-9f89-1d6cff254c27,U02U5L97S6T,,,\"Also to add, both intervals are monthly, your will run at 12:00 AM UTC on 1st of every month. The one that Alexey set will run at 6:00 AM UTC on 2nd of every month.\",1643777598.302299,1643778273.488239,U02TATJKLHG\\nb2895b62-f58a-45a1-8634-4091ca33ab6c,U02U5L97S6T,,,Makes so much sense!,1643777598.302299,1643778420.976439,U02U5L97S6T\\n95554054-efa0-4338-a169-2c4dafe0545b,U02U8CB58G3,,,\"That’s what made the most sense to me, but just wanted to clarify.  Thanks!\",1643777143.877529,1643779415.977739,U02U8CB58G3\\n206498a4-31eb-4c1a-aba5-dfe712cc473a,,1.0,,\"Hi guys i am now using the VM to run airflow but i can build this is the error\\n\\ndocker-compose build\\nGot permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get \"\"<http://%2Fvar%2Frun%2Fdocker.sock/_ping>\"\": dial unix /var/run/docker.sock: connect: permission denied\\n\\ni used sudo and it says\\n\\nsudo: docker-compose: command not found\",1643779495.315739,1643779495.315739,U02UKLHDWMQ\\nae546045-2566-43d1-b664-753c0b9c05c7,U02U8CB58G3,,,No problem :smile:,1643777143.877529,1643779953.917879,U02TATJKLHG\\nbb86256c-afe3-4313-9b8f-267e92ab4473,,3.0,,\"To draw parallels between something like Spark and Big Query, it seems to me after watching the \\'Internals Of Big Query\\' video that both share similar ideologies when it comes to processing the data.\\n\\nCompute is separate from storage. Just like how Spark does processing in memory, but the underlying storage could be anything like HDFS, S3, Delta tables.\\nDremel would just like Spark, divide the entire query into tasks and process individual partitions on executors and then collate the final answer.\\n\\nDoes this seem correct?\",1643781517.371389,1643781517.371389,U02TATJKLHG\\n208b6f2c-da65-4f98-b335-fd204624d03a,U02UKLHDWMQ,,,Solved,1643779495.315739,1643781572.723529,U02UKLHDWMQ\\n88989f3c-3444-4850-a62b-7e4f18ca2ac1,U02UKLHDWMQ,,,I think so,1643762358.242459,1643782272.668379,U01AXE0P5M3\\n5bea692e-43ed-430a-acd9-d1f0a7e87b55,U02TEERF0DA,,,I think it\\'s coming from .env,1643766679.185449,1643782298.200329,U01AXE0P5M3\\n89f4affe-882a-46bd-ae4c-06ec16642aca,U01AXE0P5M3,,,\"Hello <@U01AXE0P5M3> I successfully installed pgcli and created the database instance as well, but while trying to connect to the instance, after entering password, I get this error\\n\\n`root@localhost:ny_taxi&gt; Exception in thread completion_refresh:`\\n`Traceback (most recent call last):`\\n\\xa0`File \"\"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py\"\", line 973, in _bootstrap_inner`\\n\\xa0\\xa0`self.run()`\\n\\xa0`File \"\"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py\"\", line 910, in run` \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0`self._target(*self._args, **self._kwargs)`\\n\\xa0`File \"\"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pgcli/completion_refresher.py\"\", line 68, in _bg_refresh`\\n\\xa0\\xa0`refresher(completer, executor)`\\n\\xa0`File \"\"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pgcli/completion_refresher.py\"\", line 110, in refresh_tables`\\n\\xa0\\xa0`completer.extend_columns(executor.table_columns(), kind=\\'tables\\')`\\n\\xa0`File \"\"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pgcli/pgcompleter.py\"\", line 204, in extend_columns`\\n\\xa0\\xa0`for schema, relname, colname, datatype, has_default, default in column_data:`\\n\\xa0`File \"\"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pgcli/pgexecute.py\"\", line 483, in table_columns`\\n\\xa0\\xa0`for row in self._columns(kinds=[\\'r\\']):`\\n\\xa0`File \"\"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pgcli/pgexecute.py\"\", line 478, in _columns`\\n\\xa0\\xa0`cur.execute(sql)`\\n`psycopg2.errors.UndefinedColumn: column def.adsrc does not exist`\\n`LINE 7: \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0def.adsrc as default`\",1642511128.453300,1643784912.058979,U02UR9XTSMV\\n832ff5e3-ef1c-43a4-bba5-0272fb1801e4,U01AXE0P5M3,,,\"On the Terminal where I created the database, I get this error\\n\\n`2022-02-02 06:38:37.350 UTC [38] ERROR:\\xa0type \"\"hstore\"\" does not exist at character 8`\\n`2022-02-02 06:38:37.350 UTC [38] STATEMENT:\\xa0SELECT \\'hstore\\'::regtype::oid`\\n`2022-02-02 06:38:37.398 UTC [39] ERROR:\\xa0type \"\"hstore\"\" does not exist at character 8`\\n`2022-02-02 06:38:37.398 UTC [39] STATEMENT:\\xa0SELECT \\'hstore\\'::regtype::oid`\\n`2022-02-02 06:38:37.430 UTC [39] ERROR:\\xa0column def.adsrc does not exist at character 289`\\n`2022-02-02 06:38:37.430 UTC [39] STATEMENT:` \\xa0\\n\\t\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0`SELECT\\xa0nsp.nspname schema_name,`\\n\\t\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0`cls.relname table_name,`\\n\\t\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0`att.attname column_name,`\\n\\t\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0`att.atttypid::regtype::text type_name,`\\n\\t\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0`att.atthasdef AS has_default,`\\n\\t\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0`def.adsrc as default`\\n\\t\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0`FROM\\xa0\\xa0<http://pg_catalog.pg|pg_catalog.pg>_attribute att`\\n\\t\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0`INNER JOIN <http://pg_catalog.pg|pg_catalog.pg>_class cls`\\n\\t\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0`ON att.attrelid = cls.oid`\\n\\t\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0`INNER JOIN <http://pg_catalog.pg|pg_catalog.pg>_namespace nsp`\\n\\t\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0`ON cls.relnamespace = nsp.oid`\\n\\t\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0`LEFT OUTER JOIN pg_attrdef def`\\n\\t\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0`ON def.adrelid = att.attrelid`\\n\\t\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0`AND def.adnum = att.attnum`\\n\\t\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0`WHERE \\xa0cls.relkind = ANY(ARRAY[\\'r\\'])`\\n\\t\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0`AND NOT att.attisdropped`\\n\\t\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0`AND att.attnum\\xa0&gt; 0`\\n\\t\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0`ORDER BY 1, 2, att.attnum`\",1642511128.453300,1643784996.518049,U02UR9XTSMV\\n67ee7355-9d88-49dd-81a7-b9b1467cd8d1,U01AXE0P5M3,,,It seems you installed an older version of pgcli. Check the readme file for how to install a newer version with conda,1642511128.453300,1643786432.041079,U01AXE0P5M3\\n9e310b77-1a7b-4eae-a643-18e293613b8e,U01AXE0P5M3,,,Oh I didn\\'t realize you replied in this thread with the instructions =) does the conda way of installing not work for you?,1642511128.453300,1643786581.944939,U01AXE0P5M3\\n4ea3be0a-46a1-443c-bb4a-353cc512f226,U01AXE0P5M3,,,\"Also if pgcli doesn\\'t work for you, move on - it caused a lot of problems for many students and it\\'s not essential for the course. In the next video I show how to connect to PG with pandas, you can use that instead\",1642511128.453300,1643786672.246889,U01AXE0P5M3\\n18302af9-8192-4f05-815e-f55ece987597,,2.0,,\"Hi All, for the week 2 homework, I have few questions want to ask.\\n\\n1. For our coding, do we need upload the *coding* about DAGS (ny_taxi, fhv, lookup) &amp; ingest_script.py?\\n2. Besides, do we need upload the screenshot about DAGS activity like attachment?\\nHappy Lunar New Year for those celebrate Chinese New Year! :congratulations::partyparrot::catshake:\",1643786895.379309,1643786895.379309,U02T697HNUD\\n72022b95-92eb-4305-9d79-4cd1ff4ad869,U02U5L97S6T,,,\"at <@U02U5L97S6T> how did u use the monthly setting, where do i find it....sorry for asking\",1643777598.302299,1643788713.646759,U02RTJPV6TZ\\n1d2178a6-54b6-4b80-9c12-4ef7e4c430f1,U030FNZC26L,,,\"2. <https://unix.stackexchange.com/questions/30470/what-does-mean-in-a-shell-script>\\n```${parameter:-word}\\nUse Default Values. If parameter is unset or null, the expansion of word is substituted. Otherwise, the value of parameter is substituted.```\\n3. Yes - It\\'s _YAML alias and anchor feature_, to reuse properties and env variables.\",1643442862.769809,1643443324.822939,U0290EYCA7Q\\n8e778315-efc5-4fec-a05f-b1c49740f0c4,U030FNZC26L,,,\"For 1, I confirmed it created a default user with UID 10000, which i had it in the .env file.\\n\\n```[2022-01-27, 18:34:25 UTC] {subprocess.py:89} INFO - default:x:10000:0:default user:/home/***:/sbin/nologin\\n[2022-01-27, 18:34:25 UTC] {subprocess.py:89} INFO - default\\n[2022-01-27, 18:34:25 UTC] {subprocess.py:89} INFO - uid=10000(default) gid=0(root) groups=0(root)```\\nStill don\\'t know where it create an id named airflow. Maybe this, but don\\'t know how and where it named airflow. `user: \"\"${AIRFLOW_UID:-50000}:0\"\"`\\n```[2022-01-27, 18:34:25 UTC] {subprocess.py:89} INFO - ***:x:50000:0:First Last,RoomNumber,WorkPhone,HomePhone:/home/***:/bin/bash```\",1643442862.769809,1643443793.760719,U0290EYCA7Q\\nd3e8f951-fdfc-4e14-b190-4f3b1f8f91c5,U0290EYCA7Q,,,\"<@U0290EYCA7Q> Like Sejal mentioned, you either download it manually to some location and set the path to the environment variable. Or you do it from the command line with gcloud auth. Dont do both.\",1643274368.027300,1643443929.824529,U030HKR0WK0\\na79c3f23-d9b3-4e7d-9e78-68100ea5c211,U030FNZC26L,,,\"Thank you for the answers :slightly_smiling_face: So everything will be automatic after running `echo -e \"\"AIRFLOW_UID=$(id -u)\"\" &gt; .env` .\",1643442862.769809,1643444130.544069,U030FNZC26L\\n3dd3d8c6-1e3b-4389-8724-dd6b43fe1d8b,U030FNZC26L,,,\"That\\'s what I understood. Should play with different UIDs, and check what happens if we change the name of the user, while building the image.\",1643442862.769809,1643444287.759419,U0290EYCA7Q\\n1a4a2c58-cc7e-4ea8-b0c0-bcc160f24bbc,,5.0,,\"Hello, I am confused by the behaviour of `gcloud init` which asks to UNSET the environment variable `GOOGLE_APPLICATION_CREDENTIALS`  (which points to the file I downloaded from the service account)\\n```$ gcloud auth application-default login\\n\\nThe environment variable [GOOGLE_APPLICATION_CREDENTIALS] is set to:\\n  [/mnt/c/Users/mb/Projects/credentials/fast-pagoda-339723-11ccba6aec9d.json]\\nCredentials will still be generated to the default location:\\n  [/home/mb/.config/gcloud/application_default_credentials.json]\\nTo use these credentials, unset this environment variable before\\nrunning your application.\\n\\nDo you want to continue (Y/n)?  Y ```\",1643444940.433659,1643444940.433659,U030HKR0WK0\\ne6a57394-58fb-4323-81f1-2c601dc7f6c2,U030FNZC26L,,,\"yes! for linux it\\'s especially important - if you don\\'t specify AIRFLOW_UID this way, airflow won\\'t be able to read the files you put to `dag`  and it also won\\'t be able to read the credentials\",1643442862.769809,1643446361.008249,U01AXE0P5M3\\nf2cc3729-dd7d-4282-a3fa-25e2b3b77389,U030HKR0WK0,,,\"the way I understood it, when you do login, you log in with your google account using OAuth and it overwrites the login info from the credentials file with a new file\\n\\nif you want to use the service account credentials, you need to use this:\\n\\n```gcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS```\",1643444940.433659,1643446520.512389,U01AXE0P5M3\\n050f67e6-6624-4ae8-9cf3-2832e3505fc2,,14.0,,\"I think downloading all the yellow_taxi_trips files for 2 years (2020 &amp; 2019) took a toll on the Airflow Worker. Since all the catchup dags ran at once, the worker was not able to keep up with the ask. My VM just got stuck and I had to kill it from the UI. Now when I login back and do `airflow-init`  it warns with not enough space. More details in the thread.\",1643447997.850919,1643447997.850919,U02TATJKLHG\\n4c2d3a9c-8c33-4789-9a07-fff632e97784,U02TATJKLHG,,,\"`airflow-airflow-init-1  |`\\n`airflow-airflow-init-1  | WARNING!!!: Not enough Disk space available for Docker.`\\n`airflow-airflow-init-1  | At least 10 GBs recommended. You have 8.0G`\\n`airflow-airflow-init-1  |`\\n`airflow-airflow-init-1  |`\\n`airflow-airflow-init-1  | WARNING!!!: You have not enough resources to run Airflow (see above)!`\\n`airflow-airflow-init-1  | Please follow the instructions to increase amount of resources available:`\\n`airflow-airflow-init-1  |    <https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html#before-you-begin>`\",1643447997.850919,1643448023.350319,U02TATJKLHG\\n8c05d606-5b1b-4651-8ffe-e2d312ae4e7c,U02TATJKLHG,,,\"I did `df -h` to check the disk space available and got this -\\n\\n`(base) ankur@de-zoomcamp:/$ df -h`\\n`Filesystem      Size  Used Avail Use% Mounted on`\\n`/dev/root        29G   21G  8.0G  73% /`\",1643447997.850919,1643448069.686369,U02TATJKLHG\\n9bc7c43a-fe7b-4aa0-b3ae-4231adbba7be,U02UKLHDWMQ,,,\"The VM is an alternative if you were not able to do the setup with docker locally, that\\'s why it covers things that were shown in docker before\",1643407243.219729,1643448126.523599,U01B6TH1LRL\\nd5468e49-e26f-4ab7-aa69-0f9f9360820d,U02V90BSU1Y,,,Verification has been confirmed but haven\\'t received any free credits :cry:,1643229926.418000,1643448186.617669,U02R0EE3420\\ne7a76237-f32e-4d83-8463-cab442dbca7b,U02TATJKLHG,,,\"I am not sure what folders to delete. Were the files that were downloaded saved anywhere on the disk? Or were they just on the Airflow worker? I am not sure what is consuming all the disk space and what changed. I can just reset the VM and setup from scratch but would want to see why this is happening and what can be done to stop this.\\n\\nAlso will adding a mount and downloading the files there be better for the worker? I am thinking of doing something like this in the next run.\\n\\n`- ~/yellow_taxi_trips:/opt/airflow/yello_taxi_trips`\",1643447997.850919,1643448252.218349,U02TATJKLHG\\n446b3661-cc7d-4998-bb05-240ce21fed53,U02UKLHDWMQ,,,\"indeed - and when it comes to terraform, I just wanted to show you how to install it and run it in case you had problems with running it previously\",1643407243.219729,1643448360.064419,U01AXE0P5M3\\n46ba6ec3-96f7-4ce6-ad5b-778cce7d4741,U01T2HV3HNJ,,,\"terraform is indeed cloud-agnostic alternative to cloudformation, but inside you still use AWS or google cloud provider, so you\\'re still kind of tied to a particular cloud vendor\\n\\nadvantages - it\\'s the defacto standard for infrastructure-as-code approach and most companies use it\\n\\ndisadvantages - syntax :see_no_evil:\",1643408827.911599,1643448603.637909,U01AXE0P5M3\\n426f0060-2ecf-4e50-ab96-b79d1d50b3ea,,2.0,,\"One question regarding the start time and the interval.\\nWhen we set the starting date to January 2021 and set the interval to one month, it actually starts the tasks and just uses a simulated time, right? I mean, it doesn\\'t just start now and run the tasks from now on, but executes them from the beginning and with those timestamps?\",1643448620.120559,1643448620.120559,U030FNZC26L\\nbc525686-3d72-4be8-8047-a53f5aefa269,U02T9GHG20J,,,\"I\\'m not 100% sure, but I don\\'t think you need to do port forwarding for WSL. Usually WSL does that automatically - at least for me\",1643411270.076219,1643448674.534869,U01AXE0P5M3\\n70f233f6-f4e7-4435-aaf2-54d7402b54e9,,4.0,,\"how to make airflow read my script instantly when I have some changes to my script? because every time i make some changes to my script, i need to shut down the docker-compose and it takes time to do it\",1643448737.346929,1643448737.346929,U02RA8F3LQY\\n3ae18c51-13d4-4b47-8c6c-1f72269e7b67,U02UAFF1WU9,,,\"maybe a dot before \"\"google\"\" is missing?\\n\\nwhere are your pictures taken? which part of the world is this place?\",1643411641.340209,1643448771.515369,U01AXE0P5M3\\ndaeabfdf-4397-4a34-a55a-f6c022fc7b97,U02RA8F3LQY,,,\"Think You can refresh the DAG on UI. Or Go to graph view -&gt; select the first task -&gt; clear the task.\\nI created a new task, and it automatically picked up.\",1643448737.346929,1643448864.223619,U0290EYCA7Q\\na9cf2ec5-2e1e-4545-b4c4-316e78c700d6,U030FNZC26L,,,\"To quote from the documentation -\\n\\n&gt; _The date specified in this context is called the logical date (also called execution date for historical reasons), which simulates the scheduler running your task or DAG for a specific date and time, even though it physically will run now (or as soon as its dependencies are met)._\\n&gt; \\n&gt; _We said the scheduler runs your task for a specific date and time, not at. This is because each run of a DAG conceptually represents not a specific date and time, but an interval between two times, called a data interval. A DAG run’s logical date is the start of its data interval._\",1643448620.120559,1643448886.328649,U02TATJKLHG\\n3caf82df-9fe6-4e1a-8c3b-f07e6b2c7905,U02910MJNBY,,,\"yes. when recording the video I was on windows and didn\\'t have this problem, but later on linux I noticed it\\n\\n<https://www.youtube.com/watch?v=tOr4hTsHOzU&amp;list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&amp;index=13>\\n\\nyou can check this one, in the middle I show how to solve it with .dockerignore\",1643422971.917109,1643448889.924709,U01AXE0P5M3\\n7abdb0f1-2c06-4e82-918a-af76ba1895a0,U030FNZC26L,,,thanks,1643448620.120559,1643449230.670449,U030FNZC26L\\n2d5fb7d7-774a-4aef-abb8-be1cbfd67963,,2.0,,which timezone did the airflow use and how to change and make it default based on my timezone?,1643449628.141719,1643449628.141719,U02RA8F3LQY\\ne85e253a-8095-448c-99ec-939ba9e479b8,U02RA8F3LQY,,,\"Same for me, it picks the changes automatically\",1643448737.346929,1643449839.490019,U01AXE0P5M3\\n9ce97bab-948e-47d3-b52d-2a7527601873,U02RA8F3LQY,,,Which OS do you use?,1643448737.346929,1643449865.953619,U01AXE0P5M3\\n3847200e-cbca-4aca-ad8d-3455978b40ee,U02RA8F3LQY,,,I think it\\'s utc,1643449628.141719,1643449875.080639,U01AXE0P5M3\\n95fe455c-292e-42b8-bcb5-50fa3cdc08c5,U02TATJKLHG,,,\"Monting it to local file system sounds like a good idea\\n\\nCan you run docker image ls? Probably there\\'s an image which you can delete, then compose will create a new one for the next run\",1643447997.850919,1643450203.366249,U01AXE0P5M3\\na368ff56-cf39-4f97-ab86-37b0b9a1e262,U02TATJKLHG,,,Also maybe it\\'s a good idea to run not more than 3 jobs in parallel =) I set max_active_runs to 3,1643447997.850919,1643450298.586589,U01AXE0P5M3\\n08af09bd-569e-4c97-b0df-588b72652bf6,,5.0,,\"I have a doubt regarding docker, In the video 1.2.2, we are running a docker container for PostgreSQL and in another tab of terminal we are installing pgclient and jupyter. So I want to know how do these pgclient and jupyter notebook communicate with the postgres. And what will happen when we close the tabs? Can someone please explain?\",1643451068.954329,1643451068.954329,U0254S545D5\\na1d4ce88-298f-4bfd-b9a8-796f3485836a,U0254S545D5,,,\"If you close the tab with the docker container (and it\\'s not running in a detached mode),  the container will stop and postgres will stop working as well\",1643451068.954329,1643451299.757899,U01AXE0P5M3\\n2a772c02-e253-4e22-8361-9d2166f1a85d,U0254S545D5,,,\"So should I assume, as long as docker container is running, all the additional libraries that we are installing eg: pgclient and jupyter will work?\\n\\nAnd the moment we again start docker we will have to install these libraries again?\",1643451068.954329,1643451461.224699,U0254S545D5\\nf4eea18d-bd22-4a6e-a3ec-d5702f44c041,U0254S545D5,,,\"Jupyter will work regardless of postgres, but you won\\'t be able to connect to it from Jupyter\",1643451068.954329,1643451566.642479,U01AXE0P5M3\\n65faebf5-01da-41a5-8538-69dfaf2e22fd,U0254S545D5,,,And you won\\'t need to install them again if you install them on your host - just like you don\\'t need to install chrome every time you turn on your computer,1643451068.954329,1643451623.286909,U01AXE0P5M3\\n037a6ac6-a7f9-4a8c-bcc4-e522ab1dca9a,U0254S545D5,,,\"Got it, I guess we are not installing these libraries inside the container instead, we are installing them on the host machine. I got confused because in video 1.2.1 we were installing pandas inside the container and it was adding layers to the existing image.\",1643451068.954329,1643451845.346389,U0254S545D5\\n630aaf59-5249-4473-be38-a7f22ca2e7ad,U02RA8F3LQY,,,\"From the\\xa0<https://airflow.apache.org/timezone.html|documentation>:\\n&gt; Support for time zones is enabled by default. Airflow stores datetime information in UTC internally and in the database. It allows you to run your DAGs with time zone dependent schedules. At the moment Airflow does not convert them to the end user’s time zone in the user interface. There it will always be displayed in UTC. Also templates used in Operators are not converted.\\n```import pendulum\\n\\nlocal_tz = pendulum.timezone(\"\"Europe/Amsterdam\"\")\\n\\ndefault_args = dict(start_date=datetime(2016, 1, 1, tzinfo=local_tz), owner=\"\"airflow\"\")\\n\\ndag = DAG(\"\"my_tz_dag\"\", default_args=default_args)\\nop = DummyOperator(task_id=\"\"dummy\"\", dag=dag)\\nprint(dag.timezone)  # &lt;Timezone [Europe/Amsterdam]&gt;```\\n\",1643449628.141719,1643452215.976689,U0290EYCA7Q\\n26a09451-8f30-4c02-b43f-dc4f10a6d02d,U02UAFF1WU9,,,I\\'d check the volumes section in the `docker-compose.yaml` too,1643411641.340209,1643452900.528419,U02GVGA5F9Q\\n3d686582-386a-47e0-873f-61146410a1ae,U02TATJKLHG,,,\"Thanks Alexey!\\n\\n`max_active_runs` should help with going easy on the worker.\\n\\nWith `docker image ls` I don\\'t see a lot of images which I can delete. Maybe pgadmin, postgres. But man the airflow_* images are dense!\\n\\n```REPOSITORY                  TAG       IMAGE ID       CREATED        SIZE\\nairflow_airflow-webserver   latest    076076a49d6e   5 hours ago    1.98GB\\nairflow_airflow-worker      latest    076076a49d6e   5 hours ago    1.98GB\\nairflow_flower              latest    076076a49d6e   5 hours ago    1.98GB\\nairflow_airflow-init        latest    076076a49d6e   5 hours ago    1.98GB\\nairflow_airflow-scheduler   latest    076076a49d6e   5 hours ago    1.98GB\\nairflow_airflow-triggerer   latest    076076a49d6e   5 hours ago    1.98GB\\nredis                       latest    f1b6973564e9   2 days ago     113MB\\napache/airflow              2.2.3     4a92e92f137e   11 days ago    981MB\\ndpage/pgadmin4              latest    e52ca07eba62   2 weeks ago    272MB\\nubuntu                      latest    d13c942271d6   3 weeks ago    72.8MB\\npostgres                    13        0896a8e0282d   3 weeks ago    371MB\\nhello-world                 latest    feb5d9fea6a5   4 months ago   13.3kB```\\nI temporarily increased 10 disk space so that I can progress.\\n\\nMounting the volume seems to have worked fine. I just had one hiccup where it threw permission denied while writing files. Which I assume that the airflow user did not have access to write files to the disk folders that I provided. So I had to `chmod -R 777 yellow_taxi_trips/`  in order to stop the permission denied error.\\n\\nHowever, how was airflow able to write to the logs, dags and plugins folders then? Shouldn\\'t we ideally have to do the same for these folders as well? What am I missing.\\n\\nAlso sorry for verbose replies, haha!\",1643447997.850919,1643452925.360139,U02TATJKLHG\\n7e8b0430-faf7-42a5-88e9-731276a4c297,U02SPLJUR42,,,yes i created .env file,1643407707.452239,1643453432.698399,U02SPLJUR42\\n2220761c-5226-4afc-9df6-8e757bca97db,U02TATJKLHG,,,\"Hi Ankur, apart from what Alexey said, we agree the official Docker setup provided by Airflow isn’t working optimally for many of us, and we’re currently working on a more light-weight version of it that doesn’t have so many services or high memory-consumption/disk-space\",1643447997.850919,1643453465.009929,U01DHB2HS3X\\nad9d41af-f94a-4025-9998-c7e92f638550,U02TATJKLHG,,,Airflow was able to write to dags because we set airflow uid,1643447997.850919,1643453465.282699,U01AXE0P5M3\\n401DC63C-0A7D-4E25-A2E4-F5DEF438902E,U02UM74ESE5,,,\"That\\'s odd, I can do it just fine from the terminal. Try showing invisible files in Finder, you should be able to find how to do so with a quick search on Google. Finder supposedly should allow you to create folders starting with a period then.\",1643341570.337969,1643453708.508969,U02BVP1QTQF\\nac62f2c9-b487-4409-a9f3-093b3fa7ab8a,,3.0,,\"Hi all, did someone face an issue while ingesting ny_taxi data into the table? I am getting this error.\",1643454038.673009,1643454038.673009,U0254S545D5\\n5fc526a6-2b8e-4841-b92d-36c77cea7b2c,U0254S545D5,,,,1643454038.673009,1643454088.827309,U0254S545D5\\n7dcd1344-d936-44f4-b031-309d3a18a4b7,U0254S545D5,,,\"That\\'s because nothing there to iterate. You can either catch the exception, or leave it for now.\\n\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1643057976474600?thread_ts=1643057832.473700&amp;cid=C01FABYF2RG>\",1643454038.673009,1643454212.534329,U0290EYCA7Q\\nd35c3a07-8475-45cc-a3d6-9ac0322404e4,U0254S545D5,,,Thanks <@U0290EYCA7Q>,1643454038.673009,1643454349.717559,U0254S545D5\\n6e1c10fd-a3dc-43b7-a09c-ad4c8d5baba7,U02TATJKLHG,,,\"Hi Sejal, Yes I did see folks working on lightweight versions. I didn\\'t face any issue with this one so went ahead with it until now. Might check out the lighter one now I guess.\\nAlso, the file sizes I see are huge, 600mb for Jan 2019 for instance. So I guess that is also causing the overall size to increase. A word of caution for everyone would be to add a file deletion task after it has been pushed to GCS. Atleast, that\\'s what I am doing since I am mounting disk volume.\",1643447997.850919,1643454831.210689,U02TATJKLHG\\n0d543eed-7f48-4a37-ad3a-09cd6b4e6e76,U02TATJKLHG,,,\"Hi Alexey, setting UID allows airflow to write to the dags, logs folder but not to the additional volume? I am still a little confused.\",1643447997.850919,1643454961.581159,U02TATJKLHG\\n350d1c0e-df6c-4444-8d56-db5782b13397,U02TATJKLHG,,,\"Did you create the folder yourself you let compose do it? If yourself, I\\'m not sure why you needed to set the permissions. If compose, it probably does it from root\",1643447997.850919,1643456149.450459,U01AXE0P5M3\\n120d6404-2b39-4cc4-8dc3-507faa902c10,U02TATJKLHG,,,\"As for the dag folder and others, we created them ourselves\",1643447997.850919,1643456175.966219,U01AXE0P5M3\\n3b86152b-4811-49b8-9142-140bc55de7b1,,1.0,,\"Seems like there is no Homework for week 2, or am i not seeing it?\",1643456200.744309,1643456200.744309,U02RSAE2M4P\\n5105a427-d007-4879-acd6-ec11405789da,U02RSAE2M4P,,,<https://datatalks-club.slack.com/archives/C02V1Q9CL8K/p1643404664539529?thread_ts=1643404664.539529&amp;cid=C02V1Q9CL8K|https://datatalks-club.slack.com/archives/C02V1Q9CL8K/p1643404664539529?thread_ts=1643404664.539529&amp;cid=C02V1Q9CL8K>,1643456200.744309,1643456228.392299,U01AXE0P5M3\\n796d2201-66c1-440c-8172-23e1db6fa36f,U02TATJKLHG,,,I did create the folder myself. Must have missed something. Will redo it with some other volume and check. Thanks!,1643447997.850919,1643456524.336699,U02TATJKLHG\\nd8420ab8-ff00-4b38-a313-e9b4e3ff1103,,3.0,,why was my airflow not triggered even the schedule was passed?,1643456773.078689,1643456773.078689,U02RA8F3LQY\\n0f178f39-7cac-4457-9b49-99f5acf73edf,U02RA8F3LQY,,,What schedule did you set?,1643456773.078689,1643457151.938989,U02TATJKLHG\\n4676a7d1-8f95-4e7a-aed7-029bae6d1729,U030HKR0WK0,,,\"Thanks so much Alexey.\\n\\nI have another unrelated question. I joined the course just yesterday. But I am actively working, documenting the learning journey as I go and I am quite motivated to do all the homeworks.\\n\\nBut I see I am unable to submit Homework 1 because the deadline has passed. If possible, please can you make an exception. I can try to submit it tonight\",1643444940.433659,1643457512.205989,U030HKR0WK0\\n50a40764-f1f9-4acb-adc7-77e0d5379c75,U01T2HV3HNJ,,,\"thank you! we use cloudformation directly. It seems a bit more straight forward to me, but maybe just because I am used to it.\",1643408827.911599,1643457790.441799,U01T2HV3HNJ\\n814d9c42-1f13-41d1-b930-24b1f5097a66,U01T2HV3HNJ,,,\"The syntax in terraform is quite confusing (at least to me) - maybe that\\'s why cloudformation looks more straightforward\\n\\nI did use CF in the past and I agree with what you say\",1643408827.911599,1643457913.818689,U01AXE0P5M3\\n97d2ca83-400e-4a20-9ca5-0b843f0040f1,U030HKR0WK0,,,\"The homework solution is already published, so I\\'m afraid it won\\'t make much sense now\",1643444940.433659,1643457963.364339,U01AXE0P5M3\\ne55179c6-e1cd-4bab-bd81-a30b47248d12,U02RA8F3LQY,,,\"`local_workflow = DAG(`\\n\\xa0 \\xa0 `dag_id = \"\"DataIngestionLocal\"\",`\\n\\xa0 \\xa0 `schedule_interval = \"\"41 11 * * *\"\",`\\n\\xa0 \\xa0 `start_date = datetime(2022, 1, 29)`\\n`)`\",1643456773.078689,1643458122.540699,U02RA8F3LQY\\n56b94f81-f1b4-41ea-be23-fe310fe39235,U02RA8F3LQY,,,Did you adjust it based on your time zone? The default time zone for airflow I guess is UTC. So you might have to consider that,1643456773.078689,1643458829.792459,U02TATJKLHG\\n57f8b1b7-0553-4b14-a6da-1eccb7aa3fe0,U02TATJKLHG,,,\"Thanks for reporting us your observations, Ankur! Helpful for us as well as others, to prepare further guidelines\",1643447997.850919,1643459029.041049,U01DHB2HS3X\\n48517fb2-a4a3-411d-ad10-ed1adb84f82a,,5.0,,\"What does a common Airflow production environment look like?\\n\\n1. Would Airflow be deployment somewhere on GCP/AWS, perhaps a managed Kubernetes service?\\n    ◦ From there we can expose Airflow, login to the GUI, run/rerun/clear/etc... our DAGs.\\n2. How would we update our DAGs? This seems like a relevant operation.\\n    ◦ Bind mounting as we do locally with docker-compose (using `volumes`) is probably not possible/desired?\",1643459103.627659,1643459103.627659,U02TGS5B4R1\\n87c2ec30-64bd-4bbc-8bdd-def09265dab1,,1.0,,\"friend: do you know of any magician, i miss magic staff\\nme: mmmh yes i do, a code magician, he makes magic out of code\\nfriend : who is that\\nme: <@U01AXE0P5M3>\\n\\n\"\":smile: I rili admire your troubleshooting skills and yes making magic\"\"\",1643459109.033459,1643459109.033459,U02RTJPV6TZ\\n3d7adc9b-d3fd-4dd7-9db0-88239ff6fbd8,U02TGS5B4R1,,,\"Here\\'s a setup (simplified) we have at work:\\n\\n• Airlfow is deployed to kubernetes\\n• Dags are pushed to git and then airflow picks them up  \",1643459103.627659,1643459583.320559,U01AXE0P5M3\\n19db9254-2f5f-4a8b-ad22-ad5dc4ec4bf9,U02TGS5B4R1,,,\"To be honest I\\'m not 100% how exactly it works, but I did a quick search in google and found this:\\n\\n<https://airflow.apache.org/docs/helm-chart/stable/manage-dags-files.html>\",1643459103.627659,1643459654.166499,U01AXE0P5M3\\n6c8eee69-7e29-4b36-8dc2-6b73087821e5,,2.0,,\"I am using BigQuery, so is the \"\"Ingesting data to Local Postgres with Airflow\"\" video optional?\",1643459671.590519,1643459671.590519,U0303JLGS23\\n9bc737b1-01e3-40d2-b90c-75d6b81c244a,U02TGS5B4R1,,,\"At my current job we have the same setup right now. Airlfow is deployed to Kubernetes + GitLab repo with DAGs is synced with an Airflow\\'s dags folder. This is our first steps using it and we have only about 10-15 constantly working DAGs, but this setup works well for us.\",1643459103.627659,1643460737.631479,U0315CM33UZ\\n02dc653b-037b-451a-92eb-e9273502f5cf,U0303JLGS23,,,\"Yes, but you would learn something out of it.\",1643459671.590519,1643462912.962999,U0290EYCA7Q\\n40656dea-272a-44c9-a2bf-7b5ad2ce88ea,,6.0,,\"If we have the repo forked and want to update it, is there an easy way to update it from the DataTalksClub main repo while keeping our edits (Homework answers, notes etc.) ?\",1643463262.432319,1643463262.432319,U02TNEJLC84\\n49d45947-284d-4d4a-b77b-4efd31b8b644,U02TNEJLC84,,,Tried *Fetch Upstream -&gt; Fetch and Merge* on github?,1643463262.432319,1643463534.215719,U0290EYCA7Q\\nd6363309-26a1-400c-854b-9f47bb020dde,U02TNEJLC84,,,\"The only option I have when I choose Fetch Upstream is Open Pull Request. (this branch is 110 commits behind...) I know that the main repo is constantly being updated/fixed. Was hoping there was a series of \"\"magical\"\" commands I could use which would update everything except the homework. i.e. Update everything, but keep any changes I\\'ve made. Gotta step up my Github game.\",1643463262.432319,1643463777.109299,U02TNEJLC84\\n705134f6-61d1-4c0e-a24c-c626bf870949,U02TNEJLC84,,,Or might just destroy the fork and pull a fresh copy and make a separate repo for my notes/homework. Would be curious to know how other students are going about this.,1643463262.432319,1643463849.571759,U02TNEJLC84\\n4d6ba8fb-b0bd-4ce7-a791-0216b9e5da51,U02TNEJLC84,,,\"if you google \"\"git keeping fork up to date\"\", you\\'ll find a lot of articles about it\\n\\nactually I\\'m not sure if you actually should keep a fork - maybe having your own repo with your files will make more sense?\",1643463262.432319,1643463912.832859,U01AXE0P5M3\\n80b14378-e29b-46ef-925c-65ec82b701e3,U02TNEJLC84,,,<https://2buntu.com/articles/1459/keeping-your-forked-repo-synced-with-the-upstream-source/>,1643463262.432319,1643463996.661609,U0290EYCA7Q\\n101c7a2f-9da2-46b7-98e8-26f4539f61a1,,17.0,,\"I am facing some issues with airflow and I am not sure how to solve them. Does anyone has an idea what is going on? My tasks do not seem to start and I also don\\'t get any logs. However, my docker-compose.yaml is identical to the one in the datatalks repo.\",1643464059.787079,1643464059.787079,U01T2HV3HNJ\\n26de47e4-7bb7-405d-80d0-3eefd48b2e27,U01T2HV3HNJ,,,\"Maybe you\\'d find something in the URLs below. From what I read in the git issue, it seems like it\\'s a long running bug in airflow. Also, just to check, have you created the logs folder and mounted it?\\n\\n<https://stackoverflow.com/questions/59591008/airflow-giving-log-file-does-not-exist-error-while-running-on-docker>\\n\\n<https://github.com/apache/airflow/pull/6722#issuecomment-821573997>\",1643464059.787079,1643464814.033279,U02TATJKLHG\\na1798236-c018-4a90-88cd-399a7cc1f792,U02TGS5B4R1,,,\"Alright cool.\\nNot sure I fully understand the docs but I think they are suggesting to use a PVC which is kept up to date by a sidecar GitSync container (running in the same pod as Airflow I assume) and I guess we can set up GitSync to monitor a specific (prod) branch.\\nI think I have a better idea how people do this now, thanks :)\",1643459103.627659,1643465084.107389,U02TGS5B4R1\\nb7e9fe19-bfa5-491d-b2ab-b6798bea048b,U01T2HV3HNJ,,,I believe I did. Maybe a permission problem?,1643464059.787079,1643465293.175149,U01T2HV3HNJ\\n36606ac5-e290-4eb8-a0e7-b45205d23b53,U01T2HV3HNJ,,,maybe this helps?,1643464059.787079,1643465368.305549,U01T2HV3HNJ\\n346293d2-8b0e-49c3-8038-7e93f684dbab,U01AXE0P5M3,,,\"Ye... Mine too. I think I am going to build a GCP VM instance and work from it to the rest of the course. \\nI think it is possible to connect the VM instance to the remote desktop program (I still have difficulty in bash commands)\",1643404097.008539,1643465410.171229,U02CD7E30T0\\n6ac8e08d-0c29-4524-9035-bd9e1735048a,U02TNEJLC84,,,\"Thank you both! Think the strategy going forward is going to be to create a separate repo for note/homework and just keep the class repo separate and up to date. For anyone stumbling upon this I synced the local repo with the main one from DataTalksClub by running\\n```git remote add upstream <https://github.com/DataTalksClub/data-engineering-zoomcamp.git>\\ngit reset --hard\\ngit rebase upstream/main```\",1643463262.432319,1643465718.155429,U02TNEJLC84\\n39802b06-26cf-43ee-a3a8-f7a81228ab86,U01T2HV3HNJ,,,\"btw, I am using Ubuntu\",1643464059.787079,1643466835.653419,U01T2HV3HNJ\\n65962189-8850-4a90-a8eb-86a9c3e1e123,,2.0,,\"I\\'ve done my material on ingesting data to local, did anyone have the same problem as me (the dag failed on ingesting task but all the data were successfully ingested to posgres)?\",1643467304.542559,1643467304.542559,U02RA8F3LQY\\nefbb6c4c-9875-40b8-9b67-f46a89f23b1a,U01T2HV3HNJ,,,<https://stackoverflow.com/questions/51775370/airflowexception-celery-command-failed-the-recorded-hostname-does-not-match-t>,1643464059.787079,1643468282.125629,U02TATJKLHG\\nf3bb9360-60d4-4281-9a8c-a97fba89a70d,U01T2HV3HNJ,,,Can you see if some the answers here work for you?,1643464059.787079,1643468295.259189,U02TATJKLHG\\nfe4fe79b-f377-43c2-8bf0-dda8d430d64e,U01T2HV3HNJ,,,\"If you have a mac, you can try this suggested answer on stack overflow -\\n\\n&gt; I had a similar problem on my Mac. It fixed it setting\\xa0`hostname_callable = socket:gethostname`\\xa0in\\xa0`airflow.cfg`.\",1643464059.787079,1643468329.127399,U02TATJKLHG\\n6230A928-6601-40B5-AF06-790A2FD8D858,U030FNZC26L,,,\"I’m having issues with the AIRFLOW_UID too. I have some more ideas to debug. But can you tell me where the .env file should be? If it\\'s in the same directory as the docker-compose.yaml, it will just know to look in that file for the variable? \\n\\nI\\'m not very familiar with how environment variables work. \",1643442862.769809,1643468355.703599,U02T9GHG20J\\n35c52986-9e26-4a99-8ec8-9340aa6a46df,U01T2HV3HNJ,,,But I\\'d suggest you google errors in your stack trace and try suggested solutions. Seems like a tricky one. Maybe Alexey has some idea.,1643464059.787079,1643468377.587749,U02TATJKLHG\\n7d784ef2-257c-494f-a067-8f891ead41fa,U01T2HV3HNJ,,,\"yes, I saw that one, but the error message seems to be different, mhhh\",1643464059.787079,1643468470.871939,U01T2HV3HNJ\\n32E4019D-DC9F-46D8-8B16-0BBC32DF23B9,U02UAFF1WU9,,,\"The pics are from Southeast Utah near the town of Moab in USA. If I recognized Steve’s picture, it\\'s Delicate Arch, the most famous arch in Arches National Park. Im at the Fisher Towers. In my biased (both sampling bias and from my background as a geologist) opinion this is one of the most amazing areas on Earth!\",1643411641.340209,1643468616.159839,U02T9GHG20J\\n2477520D-A013-49CC-BA85-149C5DDF1B34,U02T9GHG20J,,,\"Sorry I don\\'t think I was clear. I am using WSL on my laptop, but I\\'m following your tutorial to set up the GCP environment. WSL may or may not be related here. \\n\\nI\\'m using the remote extension in VS code in Windows to forward the ports from GCP. I can access port 8080 in my browser from Windows, but I can\\'t access port 5432 in the WSL terminal. \\n\\nThis isn\\'t a big deal, since it\\'s not holding me up in the course so let\\'s not spend too much of our time debugging it. I\\'m curious though if anyone ran into a similar issue and has a quick fix\",1643411270.076219,1643468976.761619,U02T9GHG20J\\nc20f74c6-b35a-40a2-a623-f1011a841b80,U030FNZC26L,,,\"Yeah, for me it\\'s in the airflow folder.\",1643442862.769809,1643469656.521639,U030FNZC26L\\nda723ef4-52e4-44f5-a1bb-043b2cbd5381,U02T9GHG20J,,,\"Okay, got it. If you have a VM, I wouldn\\'t connect to it from WSL, but from \"\"main\"\" windows - to have fewer hops between your and the VM\",1643411270.076219,1643469916.155239,U01AXE0P5M3\\n326AF31E-A4D9-42D6-9368-9E16AE42F69D,U02UAFF1WU9,,,<@U02UAFF1WU9> have u tried using curl .? It worked as an alternative for wget on my Mac ,1643400810.237809,1643470429.477379,U02AGF1S0TY\\n0a071c5b-7036-4117-95e3-87e722e0b73c,U030FNZC26L,,,That\\'s because we ran that command inside airflow folder. :smirk:,1643442862.769809,1643470634.292639,U0290EYCA7Q\\nad08ce41-3df5-4fc7-85a8-05b0b65cc0cc,,3.0,,\"Hi, I tried to reproduce all the steps to ingest data to local Postgres with Airflow (video 2.3.3), but when I try to run the DAG, it fails with this error:\\n```sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name \"\"pgdatabase\"\" to address: Temporary failure in name resolution```\\nDoes anyone know how to solve this?\",1643471419.178939,1643471419.178939,U030F0YHDAM\\n383AD5BE-1EB5-4CB9-8593-FF29CF5B58AE,U030FNZC26L,,,Thanks!,1643442862.769809,1643472037.163709,U02T9GHG20J\\nd674f5cd-a751-41d5-9d0b-71b4d03170ae,U02TBTX45LK,,,\"Thanks for reporting this. The no-frills version wasn’t fully-tested, which is why I kept it low-key so far. I’m just fixing/cleaning it up so that it can run independently, and will release it officially by tomorrow.\\n\\nThank you again for supporting the community and bringing these points, in order to help us improve and cover all aspects! :tada:\",1643229595.416000,1643472300.298789,U01DHB2HS3X\\n82f7c136-4480-49f8-81f7-05396ecb4cfc,,3.0,,\"Hi, I have a conceptual question concerning DAG scheduling with Airflow.\\nAs part of the homework, i wanted to define a DAG with a start_date in 2019 and an end_date in 2021 …\\nMy naive approach was that i can define that period with a scheduling interval of a month.\\nUnfortunately this does not work, how can i schedule tasks for a period in the past.\",1643472529.504569,1643472529.504569,U02SZMRHT7Z\\n51d126d3-675e-4f95-bd01-fd9514be8e9f,,4.0,,\"Hi all, I am just joining the channel, do I still have a chance of catching up?, it seems I have missed a whole lot already\",1643472546.953679,1643472546.953679,U02TJ9VV6A0\\n902C4C0A-714B-479A-99F3-78847DE4D783,U02TJ9VV6A0,,,\"You can definitely still catch up if you put the time this weekend to got through week 1, for me with 0 background it took me 10 hours. If you have some background, it should be straight forward. \",1643472546.953679,1643473062.900729,U02UX664K5E\\nc9a463dc-7575-4052-ac5e-177aa80b4583,U01T2HV3HNJ,,,\"Meanwhile someone has an answer to your problem, may I suggest that you move your setup to a VM instance. You might have the free 300$ on you. I am using the VM instance as well. It\\'ll also be helpful when you want to do the homework for week 2 because the file download is extremely slow on local.\",1643464059.787079,1643473185.848249,U02TATJKLHG\\n61f6de66-d992-4ba6-a609-bde1e79fcd5c,U02SZMRHT7Z,,,I think you are right conceptually. Have you set `catchup=True` ?,1643472529.504569,1643473375.982249,U02TATJKLHG\\nc4eaebfd-93e8-4442-8141-504ecb9e3371,U02SZMRHT7Z,,,\"This might help -\\n\\n<https://airflow.apache.org/docs/apache-airflow/stable/dag-run.html#catchup>\",1643472529.504569,1643473402.725929,U02TATJKLHG\\n6E851681-4286-4201-9459-8C41FA084FCC,U02TJ9VV6A0,,,\"Also note that the homework does have deadlines (the first has passed), but submitting homework is not required to complete the course and get the certificate \",1643472546.953679,1643474009.619499,U02T9GHG20J\\nC4331279-CB9A-4EB5-88B3-B5DCBA0CCAD9,U02TGS5B4R1,,,\"<https://youtu.be/QgzkB1hcq5s|https://youtu.be/QgzkB1hcq5s>\\n\\nI thought this was a good overview example of Airflow in production. From my understanding, they have a CI job triggered on push to the repo containing the DAG code that runs an airflow workflow to ingest the DAGs\",1643459103.627659,1643474192.217029,U02T9GHG20J\\n0ae7e9b4-8c35-4dce-ab18-9f684f5a5399,U01T2HV3HNJ,,,it\\'s working on the VM,1643464059.787079,1643474890.416679,U01T2HV3HNJ\\nd08bee16-4b51-40c2-b1ee-15df7f2faecf,U01T2HV3HNJ,,,\"I tried a couple of more things locally, but couldn\\'t make it work so far.... a bit strange\",1643464059.787079,1643475064.067489,U01T2HV3HNJ\\n6CA8DA40-0033-43F4-9595-20AA4114B3CF,U030FNZC26L,,,\"Here\\'s some more relevant documentation \\n\\n<https://docs.docker.com/compose/environment-variables/|https://docs.docker.com/compose/environment-variables/>\",1643442862.769809,1643476046.997759,U02T9GHG20J\\nc79df2fd-442e-44ae-bc64-ac95aba62258,,4.0,,\"Hi All, I am getting this error while connecting pgadmin to postgres. Can someone please help\",1643476442.870869,1643476442.870869,U0254S545D5\\n27c1a9ce-e83c-474d-9c22-86c0d2903345,U0254S545D5,,,Attached are the details for postgres docker run and pgAdmin connection details,1643476442.870869,1643476572.586089,U0254S545D5\\nc2c0deb1-65e4-40d9-833c-4d8656605bb9,U0254S545D5,,,,1643476442.870869,1643476631.539949,U0254S545D5\\n3c5c248d-6d80-4c19-b4a4-6d889b2e50e0,U01T2HV3HNJ,,,\"can you try running this\\n\\n```docker-compose down -v --rmi all --remove-orphans```\\nand then start again?\\n\\nit should remove all volumes and images that were created by compose\\n\\n(taken from <https://vsupalov.com/cleaning-up-after-docker/|here>)\",1643464059.787079,1643476656.825229,U01AXE0P5M3\\n9200ff8e-50d4-4a25-b4a7-9dc1cc514f72,U01T2HV3HNJ,,,oh an also clean the logs directory,1643464059.787079,1643476683.506549,U01AXE0P5M3\\n01bc4b79-bcc6-4148-b602-e7a692009430,U0254S545D5,,,\"You need to use 5432 here. You are connecting to pg-database3 directly here, and they are in the same network.\",1643476442.870869,1643476729.316239,U0290EYCA7Q\\n42734bef-14ad-463f-9373-35b46b98df12,U01T2HV3HNJ,,,I saw a similar error - I think it was trying to find a log that didn\\'t exist anymore and the whole thing got stuck,1643464059.787079,1643476732.992749,U01AXE0P5M3\\n4ed88f26-6943-40b0-bd99-5b0f0d48c8b8,U0254S545D5,,,ohk..Thank you!,1643476442.870869,1643476820.565009,U0254S545D5\\nf7f4ef89-6b00-435f-b7cf-27ce31569f93,U02RA8F3LQY,,,\"Did you catch the stop iterator thing? I forgot to handle it in the video, but added a note in the repo\",1643467304.542559,1643477430.300359,U01AXE0P5M3\\na109faca-8fa6-443b-a5c3-da3071027938,U030F0YHDAM,,,\"Is the database running? Maybe something happened and it stopped \\n\\nYou can check that by running docker ps\",1643471419.178939,1643477513.589249,U01AXE0P5M3\\n9c7a3606-3033-408d-83ef-b6cde41738e5,,4.0,,\"Hi all, it\\'s taking me close to 2 hrs for the official docker compose airflow to build. Is it the same for anyone else? The tutorial said it\\'ll only take about 20 mins.\",1643479805.379069,1643479805.379069,U02SBTRTFRA\\na01947aa-88fd-4589-a76b-4f79dce68552,U02RTJPV6TZ,,,I admire the way he troubleshoot,1643459109.033459,1643480029.341869,U02T0CYNNP2\\n0c56fe43-2d43-49aa-98e7-df3466eec4e4,U02RA8F3LQY,,,\"One related but conceptual query (might be a silly question) - we have created a new Docker image with our customized libraries related to GCP etc. and now we are running this docker image which means the image is static now. Then , how this image is able to detect the script which we have on our local machine and is not part of Dockerfile. My understanding was - once i build a docker image and if i want to add a new file e.g. DAG - i need to rebuild the image by instructing docker to build the new version of container with a DAG specific py file , similar to the way we did in our week1 exercise for our ingest-data.py... am i missing something here conceptually .....\",1643448737.346929,1643480659.859089,U02U9MNQG7Q\\n0C4FA5F1-0021-4CE3-AB37-7795DCD3BF31,U02V90BSU1Y,,,<@U02R0EE3420> it should have worked. I have 2 <http://gmail.com|gmail.com>  email accounts that are linked together. Last year I had already claimed the 300$ using my main email account. 2 weeks ago I claimed the 300$ free GCP voucher using my other account and it worked ,1643229926.418000,1643481271.512839,U02TB5NK4FL\\n4ab0e155-e2d2-4126-9329-f67ce3031489,U030FNZC26L,,,\"Thanks for sharing the link.\\n```\\nYou can set default values for environment variables using a .env file, which Compose automatically looks for in project directory (parent folder of your Compose file). Values set in the shell environment override those set in the .env file.\\n\\nThe .env file feature only works when you use the docker-compose up command and does not work with docker stack deploy.```\\n\",1643442862.769809,1643482029.537699,U030FNZC26L\\nDD9E7991-4A76-4009-80E0-002647A752C4,U02SBTRTFRA,,,\"I guess it all depends on your connectivity, it took less than 5 minutes to complete for me. 2 hours sounds like an eternity. \",1643479805.379069,1643484106.205289,U02UX664K5E\\n047802d2-cadd-4b67-b63d-8f6e3bdb7740,U02QZN0LSBT,,,<@U02QZN0LSBT> <@U01AXE0P5M3> Don’t see the notes in the repo (maybe am not looking in the right place)?  Would be a great reference!  Thanks!!,1642548032.081900,1643484354.845669,U02U5DPET47\\n179ebd70-fb67-4bef-b148-5415ed77433c,U02QZN0LSBT,,,\"<https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup#community-notes|https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup#community-notes>\\n\\nSame for week 2\",1642548032.081900,1643484547.493959,U01AXE0P5M3\\nbe7ffe9c-229d-4f2b-a2c8-b548ccbf9473,U02TJ9VV6A0,,,\"Actually, you did not miss much. You can start with Week 2.\\nDon\\'t worry about catching up on messages. Most of them are apply to poster only. You will see ton of messages going forward and will have chance to respond. :laughing:\",1643472546.953679,1643485148.872489,U02S83KSX3L\\n643e718b-e9fa-4d4a-a67b-05892558af65,U02SBTRTFRA,,,Definitely sounds like too long. Unless you\\'re on a very slow connection I\\'d guess it\\'s gotten stuck. I would probably cancel (ctrl-c) and try again.,1643479805.379069,1643485326.860849,U02UXRH2SV6\\n645bec18-9fa2-49c7-93e1-d00aa2a04787,U02CUH15ACE,,,\"(base) macbookpro@MacBooks-MacBook-Pro HW1_1 % terraform plan -var=\"\"project=&lt;braided-hangout-339419&gt;\"\"\\n\\n\\n\\n\\n\\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the\\nfollowing symbols:\\n  + create\\n\\nTerraform will perform the following actions:\\n\\n  # google_bigquery_dataset.dataset will be created\\n  + resource \"\"google_bigquery_dataset\"\" \"\"dataset\"\" {\\n      + creation_time              = (known after apply)\\n      + dataset_id                 = \"\"trips_data_all\"\"\\n      + delete_contents_on_destroy = false\\n      + etag                       = (known after apply)\\n      + id                         = (known after apply)\\n      + last_modified_time         = (known after apply)\\n      + location                   = \"\"europe-west6\"\"\\n      + project                    = \"\"&lt;braided-hangout-339419&gt;\"\"\\n      + self_link                  = (known after apply)\\n\\n      + access {\\n          + domain         = (known after apply)\\n          + group_by_email = (known after apply)\\n          + role           = (known after apply)\\n          + special_group  = (known after apply)\\n          + user_by_email  = (known after apply)\\n\\n          + view {\\n              + dataset_id = (known after apply)\\n              + project_id = (known after apply)\\n              + table_id   = (known after apply)\\n            }\\n        }\\n    }\\n\\n  # google_storage_bucket.data-lake-bucket will be created\\n  + resource \"\"google_storage_bucket\"\" \"\"data-lake-bucket\"\" {\\n      + force_destroy               = true\\n      + id                          = (known after apply)\\n      + location                    = \"\"EUROPE-WEST6\"\"\\n      + name                        = \"\"dtc_data_lake_&lt;braided-hangout-339419&gt;\"\"\\n      + project                     = (known after apply)\\n      + self_link                   = (known after apply)\\n      + storage_class               = \"\"STANDARD\"\"\\n      + uniform_bucket_level_access = true\\n      + url                         = (known after apply)\\n\\n      + lifecycle_rule {\\n          + action {\\n              + type = \"\"Delete\"\"\\n            }\\n\\n          + condition {\\n              + age                   = 30\\n              + matches_storage_class = []\\n              + with_state            = (known after apply)\\n            }\\n        }\\n\\n      + versioning {\\n          + enabled = true\\n        }\\n    }\\n\\nPlan: 2 to add, 0 to change, 0 to destroy.\\n\\n──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\\n\\nNote: You didn\\'t use the -out option to save this plan, so Terraform can\\'t guarantee to take exactly these actions if you\\nrun \"\"terraform apply\"\" now.\\n(base) macbookpro@MacBooks-MacBook-Pro HW1_1 % terraform apply -var=\"\"project=&lt;braided-hangout-339419&gt;\"\"\\n\\n\\n\\n\\n\\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the\\nfollowing symbols:\\n  + create\\n\\nTerraform will perform the following actions:\\n\\n  # google_bigquery_dataset.dataset will be created\\n  + resource \"\"google_bigquery_dataset\"\" \"\"dataset\"\" {\\n      + creation_time              = (known after apply)\\n      + dataset_id                 = \"\"trips_data_all\"\"\\n      + delete_contents_on_destroy = false\\n      + etag                       = (known after apply)\\n      + id                         = (known after apply)\\n      + last_modified_time         = (known after apply)\\n      + location                   = \"\"europe-west6\"\"\\n      + project                    = \"\"&lt;braided-hangout-339419&gt;\"\"\\n      + self_link                  = (known after apply)\\n\\n      + access {\\n          + domain         = (known after apply)\\n          + group_by_email = (known after apply)\\n          + role           = (known after apply)\\n          + special_group  = (known after apply)\\n          + user_by_email  = (known after apply)\\n\\n          + view {\\n              + dataset_id = (known after apply)\\n              + project_id = (known after apply)\\n              + table_id   = (known after apply)\\n            }\\n        }\\n    }\\n\\n  # google_storage_bucket.data-lake-bucket will be created\\n  + resource \"\"google_storage_bucket\"\" \"\"data-lake-bucket\"\" {\\n      + force_destroy               = true\\n      + id                          = (known after apply)\\n      + location                    = \"\"EUROPE-WEST6\"\"\\n      + name                        = \"\"dtc_data_lake_&lt;braided-hangout-339419&gt;\"\"\\n      + project                     = (known after apply)\\n      + self_link                   = (known after apply)\\n      + storage_class               = \"\"STANDARD\"\"\\n      + uniform_bucket_level_access = true\\n      + url                         = (known after apply)\\n\\n      + lifecycle_rule {\\n          + action {\\n              + type = \"\"Delete\"\"\\n            }\\n\\n          + condition {\\n              + age                   = 30\\n              + matches_storage_class = []\\n              + with_state            = (known after apply)\\n            }\\n        }\\n\\n      + versioning {\\n          + enabled = true\\n        }\\n    }\\n\\nPlan: 2 to add, 0 to change, 0 to destroy.\\n\\nDo you want to perform these actions?\\n  Terraform will perform the actions described above.\\n  Only \\'yes\\' will be accepted to approve.\\n\\n  Enter a value: yes\\n\\ngoogle_bigquery_dataset.dataset: Creating...\\ngoogle_storage_bucket.data-lake-bucket: Creating...\\n╷\\n│ Error: googleapi: Error 400: Invalid bucket name: \\'dtc_data_lake_&lt;braided-hangout-339419&gt;\\', invalid\\n│\\n│   with google_storage_bucket.data-lake-bucket,\\n│   on <http://main.tf|main.tf> line 19, in resource \"\"google_storage_bucket\"\" \"\"data-lake-bucket\"\":\\n│   19: resource \"\"google_storage_bucket\"\" \"\"data-lake-bucket\"\" {\\n│\\n╵\\n╷\\n│ Error: Error creating Dataset: googleapi: Error 400: Invalid resource name projects/&lt;braided-hangout-339419&gt;; Project id: &lt;braided-hangout-339419&gt;, badRequest\\n│\\n│   with google_bigquery_dataset.dataset,\\n│   on <http://main.tf|main.tf> line 45, in resource \"\"google_bigquery_dataset\"\" \"\"dataset\"\":\\n│   45: resource \"\"google_bigquery_dataset\"\" \"\"dataset\"\" {\",1643317231.156000,1643486061.400019,U02CUH15ACE\\ne823ca0a-e39f-4887-bef9-7d857e56845a,,6.0,,\"Hi, My third DAG task `local_to_gcs_task` is failing again and again.\\nThe `exception` i\\'m getting from the `logs`  of `local_to_gcs_task`   is\\n`google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials` .\\nWhile I have stated the `GCP_PROJECT_ID: \\'xyz\\'`  `GCP_GCS_BUCKET: \\'dxyz\\'`  in the  docker-compose.yaml and also my  `google_credentials.json` is in my home directory .\",1643486360.180649,1643486360.180649,U0297ANJF6F\\n2649e354-f679-48dd-8506-8565e01df5bc,U030F0YHDAM,,,\"yes, it is. I can connect to the DB from PGAdmin.\",1643471419.178939,1643487198.662129,U030F0YHDAM\\nee81bece-0683-4708-bdf2-7a832caa1dab,,3.0,,Please where can I find the training materials used in the training videos? I am specifically looking for the nyc-tl data used in the second video (that is Introduction to Google Cloud Platform),1643487287.302899,1643487287.302899,U02VDTF1ENT\\n9e491b87-24a4-4166-b7cf-2e785bc3f9d7,U030HKR0WK0,,,Oh that is fine then. I would like to continue the course. Please can you share me the link to where the solution is published?,1643444940.433659,1643487860.144179,U030HKR0WK0\\n7b3295e2-1abf-48c5-ac28-3161f5f7a442,U02SBTRTFRA,,,This is too long. On which step does it get stuck?,1643479805.379069,1643487871.949139,U01AXE0P5M3\\n6fb19772-2b18-46bc-9a43-9323aef61cd4,U030F0YHDAM,,,Fixed it! After deleting and creating  Postgres yaml file again,1643471419.178939,1643488132.353209,U030F0YHDAM\\n328c67b2-738d-4f47-8f54-69814ea50149,U02VDTF1ENT,,,\"Have a look here in week 1:\\n<https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup/2_docker_sql>\\n(scroll down to heading \"\"NY Trips Dataset\"\" in the readme section)\",1643487287.302899,1643488579.104929,U02UP3KN3SQ\\n3bef362d-2603-45db-aed2-e7d85c6dec1b,U030HKR0WK0,,,I just added it here - <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/homework.md>,1643444940.433659,1643488634.763629,U01AXE0P5M3\\nc250239b-c670-4a56-9163-2b0e4164ffcf,U02VDTF1ENT,,,<https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page>,1643487287.302899,1643488814.177139,U02GVGA5F9Q\\n7788cb7a-a30f-4117-b4ee-dd6b420b5ef5,U02VBG59VQ9,,,can you do try to get into the docker container and check (by running `env`) if the bucket variable is set correctly?,1643399968.823989,1643489549.473739,U01AXE0P5M3\\n75d7fcb4-911d-4192-8ae6-bd568688e889,U02BVP1QTQF,,,Can you show it?,1643397723.580519,1643489576.266179,U01AXE0P5M3\\n093d2eba-095c-4fe0-a7f0-ef403eb8f870,U02UKLHDWMQ,,,Have you seen the terraform videos on week 1? Sejal shows how to get this file,1643398799.967989,1643489609.667369,U01AXE0P5M3\\nc1cf1f76-8b45-49f3-b265-7728a3fe952f,U0297ANJF6F,,,can you try to get into the running container to make sure that the file with credentials is actually there?,1643486360.180649,1643489714.259189,U01AXE0P5M3\\nc4614d04-dc87-4100-91aa-3c7026080425,U02TJ9VV6A0,,,\"ok, thanks\",1643472546.953679,1643491383.808239,U02TJ9VV6A0\\n4e2e7e0d-c1c0-4190-88ca-9e7d93ddaa8e,U02VDTF1ENT,,,\"Thanks <@U02UP3KN3SQ>, <@U02GVGA5F9Q>. Will check them out\",1643487287.302899,1643491605.671059,U02VDTF1ENT\\nfc231084-a60f-4502-ac7f-43967b76dcf8,,9.0,,\"Hi, My airflow web UI is not coming up on localhost:8080. I\\'ve killed docker containers and gone through the docker compose steps in video 2.3.1. Would it come up on another port ?\",1643492178.106049,1643492178.106049,U02TMP4GJEM\\n0eafe745-b829-4b3f-b901-4d9f949add16,U02SZMRHT7Z,,,\"Actually I quite don\\'t understand the question. Since we are extracting and load historical data we only need to 1 time. \\nBut then... We can keep the dag to run monthly to run each new month available. \\nI am little confused...\",1643472529.504569,1643493724.898649,U02CD7E30T0\\nd17195a5-4af6-4030-bfc0-2262c1adf32a,,7.0,,\"I am a little confuse with some questions in the HW. \\nWe want to build one DAG to extract and load all the history data or we want to build a DAG that will extract and load the historical but also keeps extracting in the future?\\nBecause one DAG only for the historical is one conceptual approach with parallel tasks. And a DAG to keep extracting in the future will be an approach of multiple dag runs.\",1643493897.468629,1643493897.468629,U02CD7E30T0\\n78165aec-e996-46a9-b9d9-11e4f7086b48,U01T2HV3HNJ,,,\"thank you, I will give it a try tomorrow, I also found this <https://vsupalov.com/cleaning-up-after-docker/> but it looks slightly different\",1643464059.787079,1643495083.849799,U01T2HV3HNJ\\nce7f4eda-4d5a-4007-8033-ae8ca7087380,U02CD7E30T0,,,\"there\\'s no data after july 2021, but we can pretend it\\'s there and build it in such a way that we\\'ll be able to process the future data as well\",1643493897.468629,1643495634.137409,U01AXE0P5M3\\n4ac75f85-7fc6-4b4d-9eeb-a4a463a8af6f,U02CD7E30T0,,,\"so, kind of both, but it\\'ll fail for august 2021 onwards\",1643493897.468629,1643495654.581769,U01AXE0P5M3\\n5d2b4a26-9691-43ad-954b-868ff8f45a44,U02CD7E30T0,,,\"also - just curious - does it matter? practically both ways will result in the same DAG, or not?\",1643493897.468629,1643495727.367159,U01AXE0P5M3\\nbd74ba74-5c73-4868-bd37-6cd672a0bb56,U02TMP4GJEM,,,I don\\'t think so. Maybe restart the whole thing?,1643492178.106049,1643495813.816639,U01AXE0P5M3\\n42b7f714-4f80-4d42-8f37-9ce11334b50c,U02TMP4GJEM,,,I\\'ve done that 6 times already,1643492178.106049,1643495856.117379,U02TMP4GJEM\\n175a9964-e4e7-4286-a856-39470a10537c,U02TMP4GJEM,,,\"I\\'m using the docker compose file in week2. Any advice ? I\\'ve moved my google credentials file into my airflow folder. When I look at docker ps, soe of the containers have a status of unhealthy\",1643492178.106049,1643495897.823689,U02TMP4GJEM\\n6c81e793-ec7f-4616-8f40-6fcf5e556191,U02TMP4GJEM,,,\"Which OS are you on? If mac/ windows, have you given docker enough ram?\",1643492178.106049,1643495932.845269,U01AXE0P5M3\\ne9cdace6-a0be-4aed-804d-e6f8795e8bb7,U02TMP4GJEM,,,What do you see in the logs?,1643492178.106049,1643495946.088489,U01AXE0P5M3\\nb6202881-b70e-4d55-a8c2-5d520a6f34f0,U02TMP4GJEM,,,I\\'m on a mac. MacOS Monterey,1643492178.106049,1643496124.084219,U02TMP4GJEM\\n6a1d15d1-2d4f-49d2-bdb2-c8820b7fefac,,6.0,,\"Alexey, I\\'m almost finishing the local ingestion \"\"long\"\" video, but I\\'m stuck (at least for now) with a strange problem: my docker postgres (from week1) is running on port 5433, and the ingest task in our local DAG fails with this error:\\n``` File \"\"/home/airflow/.local/lib/python3.7/site-packages/psycopg2/__init__.py\"\", line 122, in connect\\n    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)\\nsqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at \"\"pgdatabase\"\" (172.23.0.9), port 5432 failed: FATAL:  password authentication failed for user \"\"root\"\"```\\nI tried to change the .env file, but to no avail... I wonder where is it reading that port, and why does it fail with the password... It seems it isn\\'t connecting to my local docker postgres.\",1643496742.748149,1643496742.748149,U02GVGA5F9Q\\n350fe6ec-f0bf-4021-b769-51178b5d629a,U02RA8F3LQY,,,\"Ohh i see, i didn\\'t check the repo yesterday. Thanks\",1643467304.542559,1643496920.491529,U02RA8F3LQY\\n0c984246-ea03-4280-b053-0c3d808cc7a4,U02CD7E30T0,,,\"In my oppinion no. \\nIf I only want to extract the historical and we don\\'t care about having a functional DAG for future data I can do a for loop for the months and run only one time.\\n\\nBut I can build a DAG that runs every month to extract only the previous month (in the Feb run it will extract Jan). I think it is possible to set a DAG to run \"\"in the past\"\" i.e. to run 12 times in 2019 and 12 times in 2021.  So it would be possible to extract the past. \\n\\nSo it will be a parallelization of tasks running once Vs straight run running many times. \\n\\nPerhaps I am not KISS :)\",1643493897.468629,1643497045.122399,U02CD7E30T0\\n0586d1c5-7deb-44e6-9f10-04c383006e30,,2.0,,\"Has anyone experienced this ? Starting airflow, watching the logs and I can see the airflow logo. It says listening at <http://0.0.0.0:8080> or <localhost://8080> however when i go to this address in my chrome browser, I see nothing. I\\'m on a Mac running Monterey OS\",1643499686.008109,1643499686.008109,U02TMP4GJEM\\n533a741d-d81a-45a2-89f6-cb46e8b9f1dc,U01AXE0P5M3,,,\"Is it possible to run in the same instance created in week 1? In this case, something in the docker-compose file change?\",1643404097.008539,1643501807.235539,U02TJ69RKT5\\nb1e9d4b6-b17f-45a4-9bf0-a4e9c338e445,U02SBTRTFRA,,,It\\'s done now. Thank you.,1643479805.379069,1643501973.936139,U02SBTRTFRA\\n00e9f0ca-af33-4238-94e6-65c34ba31cc4,,4.0,,Can someone remind me how I keep my cloned workspace up to date with the class repo on github. pull? What happens to my changes?,1643502041.313719,1643502041.313719,U02UBQJBYHZ\\n98a01198-711e-47c9-9148-fec3a74d881e,,3.0,,Hi All another noob question my airflow server is running however in the front end I am unable to see anything as compared to the DE Zoomcamp 2.3.2 video where their is an existing dag. Is this normal? .. if it is do I then need to create a DAG from scratch ? PC:Win11,1643502870.034139,1643502870.034139,U02UY1QTGHW\\nc83734c6-619b-479b-988e-ee6f716e44b5,U02UAFF1WU9,,,Maybe you didn\\'t add the folder to the docker-compose. In volumes add the folder: - ~/.google/credentials/:/.google/credentials:ro,1643411641.340209,1643503540.382589,U02TJ69RKT5\\nda234410-dfec-47f0-a56a-cabcbcd82e8a,U02UY1QTGHW,,,\"Mine also took a while to show. Try refreshing the page or restarting docker compose.\\n\\nAlso, make sure the `data_ingestion_gcs_dag.py`  is the `dags` directory.\",1643502870.034139,1643503913.574159,U02SBTRTFRA\\n69CD992B-6FA9-4F15-8919-79B5366D97F3,U02UBQJBYHZ,,,You can commit your changes and just pull ,1643502041.313719,1643503914.794519,U02TBCXNZ60\\n8760918a-66e8-4f49-8fd3-1aa5a726d655,U02UY1QTGHW,,,make sure you already create your dag script inside folder dags,1643502870.034139,1643504255.671119,U02RA8F3LQY\\n871f81fb-2c15-4005-8e6a-8c9c4cd782da,,2.0,,Can someone explain me why we do this `docker-compose up airflow-init` from home directory? (DE Zoomcamp 2.3.1 - Setup Airflow Environment with Docker-Compose),1643506738.619509,1643506738.619509,U02E30U011U\\n91cd25e0-081f-4dc3-ba0d-38a75199018c,U02UY1QTGHW,,,\"Thanks both I missed the step, now I can see the dag\",1643502870.034139,1643508998.555769,U02UY1QTGHW\\n6c4fa125-043e-4668-a573-8c761fa87709,U02E30U011U,,,I ran it in the airflow directory.,1643506738.619509,1643510679.206089,U02UBQJBYHZ\\n28bc010a-0efe-412f-a69f-cbe8b88896d4,,6.0,,\"Another question, I am getting this error when running the dag captured from the log file. From my understanding this looks like the account is not getting access to storage..... The GCS setup looks fine tho .....\\n`\"\"message\"\": \"\"<mailto:dctdecourse@dtcdecourse-xxxxxx.iam.gserviceaccount.com|dctdecourse@dtcdecourse-xxxxxx.iam.gserviceaccount.com> does not have storage.objects.create access to the Google Cloud Storage object.\"\",`\\n        `\"\"domain\"\": \"\"global\"\",`\\n        `\"\"reason\"\": \"\"forbidden\"\"`\",1643511546.181979,1643511546.181979,U02UY1QTGHW\\n5e3d6f8b-dc37-4402-b885-79660d6846cf,U02UAFF1WU9,,,\"<@U02T9GHG20J> I spent a couple of days in Kanab last year and hiked at Grand Canyon North Rim, Bryce Canyon and Red Canyon. Totally amazing.\",1643411641.340209,1643512578.142659,U02UBQJBYHZ\\n152797d4-18fe-4417-97e9-969308a1808e,U02UAFF1WU9,,,\"Anyway my dag is failing too. I thought my credentials were supposed to go in my home directory, not at the same level as airflow.\",1643411641.340209,1643512618.885059,U02UBQJBYHZ\\nc8f86605-c27f-4bd6-be1e-3c4e9c47f566,,15.0,,\"Just ran my Airflow DAG and it failed at uploading the Parquet to GCP telling me that the GCP key was not found. My json file is stored in `/week_2_data_ingestion/google_credentials/` and I even tried to put it in `/week_2_data_ingestion/airflow/google_credentails/` but it failed again which leads me tothink there is an issue with the way my path is formatted?\\n```google.auth.exceptions.DefaultCredentialsError: File /.google_credentials/google_credentials.json was not found.```\",1643512731.472799,1643512731.472799,U02UX664K5E\\nc6f3484b-8de3-414b-abea-affb1763918b,U02UY1QTGHW,,,\"Further to the above it looks like the below are not picking up the correct info as per the error below..... even tho I have placed the .json file in the right dir\\n\\n`PROJECT_ID = os.environ.get(\"\"GCP_PROJECT_ID\"\")`\\n`BUCKET = os.environ.get(\"\"GCP_GCS_BUCKET\"\")`\\n\\n\\n`google.api_core.exceptions.Forbidden: 403 POST *<https://storage.googleapis.com/upload/storage/v1/b/dtc_data_lake_pivotal-surfer-336713/o?uploadType=resumable>: {*`\\n  `\"\"error\"\": {`\\n    `\"\"code\"\": 403,`\\n    `\"\"message\"\": \"\"<mailto:dct-de-course@dtc-de-course-338623.iam.gserviceaccount.com|dct-de-course@dtc-de-course-338623.iam.gserviceaccount.com> does not have storage.objects.create access to the Google Cloud Storage object.\"\",`\\n    `\"\"errors\"\": [`\\n      `{`\\n        `\"\"message\"\": \"\"<mailto:dct-de-course@dtc-de-course-338623.iam.gserviceaccount.com|dct-de-course@dtc-de-course-338623.iam.gserviceaccount.com> does not have storage.objects.create access to the Google Cloud Storage object.\"\",`\\n        `\"\"domain\"\": \"\"global\"\",`\\n        `\"\"reason\"\": \"\"forbidden\"\"`\\n      `}`\\n    `]`\\n  `}`\",1643511546.181979,1643512933.817569,U02UY1QTGHW\\nd303ac97-c4a5-49b9-84fb-ac3da49ddce2,U02UX664K5E,,,i think the .json should be here,1643512731.472799,1643513961.919109,U02UY1QTGHW\\ne4081282-6bbc-4ccf-80e8-f5eba3525efd,U02UX664K5E,,,I get a similar error: `google.auth.exceptions.DefaultCredentialsError: File ~/.google/credentials/google_credentials.json was not found.`,1643512731.472799,1643514591.723449,U02UBQJBYHZ\\na5be7876-9b5e-4138-93dc-bf54fbbf4969,U02UX664K5E,,,However that file exists in that directory. So I don\\'t know what I\\'m doing wrong. The instructions were to put the credentials in home/.google/credentials.,1643512731.472799,1643514657.559209,U02UBQJBYHZ\\nc278641d-435a-4c11-90e5-8a170192c0a7,U02UX664K5E,,,\"I think I defined the variable wrong in the docker-compose.yaml file. But is it the case that every time I fix something in that file I have to wait half an hour to rebuild, init and run?\",1643512731.472799,1643514791.716339,U02UBQJBYHZ\\ncb65b1ab-9056-4ff1-b235-1b959d48271d,U02TMP4GJEM,,,I\\'m having the same issue. In MacOS. When I look in Docker Desktop everything seem to be running except airflow-init,1643492178.106049,1643514998.224419,U02CPH3FR33\\n37e4e873-e96d-48c5-8177-83e4932ae760,U02TMP4GJEM,,,,1643492178.106049,1643515018.460089,U02CPH3FR33\\n93f5a6b5-ee7f-494e-a2d4-12cd8d33eabf,U02TMP4GJEM,,,\"In the logs it has the following warning:\\n`2022-01-30\\xa003:50:46,460]\\xa0{db.py:921}\\xa0INFO\\xa0-\\xa0Creating\\xa0tables`\\n`Upgrades\\xa0done`\\n`[2022-01-30\\xa003:51:09,180]\\xa0{manager.py:512}\\xa0WARNING\\xa0-\\xa0Refused\\xa0to\\xa0delete\\xa0permission\\xa0view,\\xa0assoc\\xa0with\\xa0role\\xa0exists\\xa0DAG\\xa0Runs.can_create\\xa0User`\\n`airflow\\xa0already\\xa0exist\\xa0in\\xa0the\\xa0db`\\n`2.2.3`\",1643492178.106049,1643515063.007209,U02CPH3FR33\\n58aa5b67-5037-4406-9353-d0cb7c2476a8,U02UX664K5E,,,OK I got it to run. You have to set your variables the way she does it in the latest file on the github for week2.,1643512731.472799,1643515277.767789,U02UBQJBYHZ\\nbd6988a0-8842-4af0-a8f6-2dcc459a469b,U02UX664K5E,,,\"The latest docker-compose.yaml file in the airflow directory. But put your credentials file where she says, in your home directory/.google/credentials.\",1643512731.472799,1643515381.362039,U02UBQJBYHZ\\na70c9c77-1216-404e-a487-16787f8f1683,U0303JLGS23,,,\"Got it, thanks\",1643459671.590519,1643516274.735369,U0303JLGS23\\nF0DCA56E-3F04-475B-A74C-EF19EE908473,U0290EYCA7Q,,,\"Hi, how did you authenticate?\",1643365258.216129,1643516665.510969,U01DFQ82AK1\\n81D40A10-B0E2-40A5-8D2C-CF1E96506EF0,U0290EYCA7Q,,,\"Check the README, you need to be assigned OWNER role. That will make it easy\",1643365258.216129,1643516692.987879,U01DFQ82AK1\\n8238c114-7c0f-41fd-9ae0-2b22460dda28,U02U9G1P76X,,,still could not,1642928489.050200,1643517392.375489,U02U9G1P76X\\nd35c3974-6254-4712-b4df-5a1929a3ab48,,1.0,,ssh: connect to host 35.240.90.204 port 22: Connection timed out,1643517609.293989,1643517609.293989,U02U9G1P76X\\n10be2881-16a6-44ef-ba3e-5bfcdda4347d,,,,i have an issue making connection back to my ssh de-zoomcamp environment. what do i do?,,1643517664.470709,U02U9G1P76X\\n75145f54-0f14-420f-b156-6f39a0b3ced2,U02U9G1P76X,,,Did u change the IP in config file?,1643517609.293989,1643518146.673689,U0290EYCA7Q\\n40352040-a916-490c-9657-67c179038684,,1.0,,Getting is error when trying to do docker-compose build. Any one know why this is happening ?,1643518843.789109,1643518843.789109,U02R9P66EQY\\n1aee9487-f056-470a-8161-29f904b9c835,U02UX664K5E,,,Thanks <@U02UY1QTGHW> and <@U02UBQJBYHZ> I did not have the key in home and my docker-compose.yaml wasn\\'t pointing to the right path. I got Airflow completing the DAG successfully! :partyparrot:,1643512731.472799,1643519236.501609,U02UX664K5E\\n49ae0a63-dc57-4a0c-a4b3-88d779764c4c,,5.0,,\"Hey folks.. just trying to unravel the Homework for <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_2_data_ingestion/homework.md|Week 2> Are these the correct interpretations\\n1. put the NY Yellow taxi dataset from 2019 and 2020 to our data lake.\\n2. Create a new dag for transferring the FHV data and run for all the months of 2019 and 2020 to upload to GCS\\n3. Create a new dag for transferring the Taxi Zone lookup Table to upload to GCS\\nQuestions:\\n1. Should we load them the FHV and 2019-2020 data to local postgres as well?\\n2. What should be the table names for BigQuery. the video named the table external_table in BigQuery. But for these data points, should we append to the same table or create multiple tables.. \\nSorry if these were answered elsewere.. but my search on slack wasnt too fruitful\",1643522160.532659,1643522160.532659,U02SMBGHBUN\\nbb5203b2-406d-4ca9-909f-a3d1f517f081,U02SMBGHBUN,,,\"q1- in the homework, they mention that if you don\\'t have access to gcp, you can upload data to postgres, otherwise no.\",1643522160.532659,1643523122.662239,U030FNZC26L\\n7a168a5a-286c-4a8d-a939-92d203947f71,,4.0,,\"1- What should be the debugging procedure for airflow? when we do `docker-compose up`  and then realize some bugs in the code and change the dag file, it automatically catches the changes. The question is that it doesn\\'t start from starting date (2019-01) and when I use the `play button &gt; trigger dag`,  it seems it runs the job at current date (2022).\\n2- I set the interval to monthly (tested both cron expression and `@monthly`) but I see that it runs daily. Or maybe I misunderstood it:\",1643524027.252679,1643524027.252679,U030FNZC26L\\n839B8CC0-AA3D-427A-A597-81A606571780,U030FNZC26L,,,\"To start from the beginning , you can clear the tasks on UI. Can you please try Graph view -&gt; Click on the first task -&gt; clear?\",1643524027.252679,1643524491.136249,U0290EYCA7Q\\nff9ab379-7256-4ccc-8745-8c6c42fb9a95,U02CD7E30T0,,,\"yes, you\\'re right - if you only need to do it once, then a for loop in bash would be the best way to go\\n\\nbut for the purposes of this exercise and learning airflow let\\'s pretend we must have a funtioning workflow :)\",1643493897.468629,1643527747.360269,U01AXE0P5M3\\nb5dfe69d-9fdf-4d00-82b3-8c82b89d6772,U02GVGA5F9Q,,,\"if you bash to the airflow worker, does it connect? do you see the env variables when you run `env`?\",1643496742.748149,1643527879.078309,U01AXE0P5M3\\n9e91621f-caa5-4918-84e2-7b02a6d38df2,U02GVGA5F9Q,,,well... probably you do because it knows it needs to connect to `pgdatabase`,1643496742.748149,1643527902.955529,U01AXE0P5M3\\nd71b10c8-ba71-4135-baf2-9bdac0c24878,U02TMP4GJEM,,,make sure it\\'s localhost:8080 not <localhost://8080>,1643499686.008109,1643527954.678509,U01AXE0P5M3\\n7ee74993-5305-407b-8a56-9b2855c779b5,U02E30U011U,,,yes you should run it form the airflow directory (the one with docker-compose.yaml) - else docker compose won\\'t see the yaml file and will not know what airflow-init is,1643506738.619509,1643528051.975259,U01AXE0P5M3\\n8d4bd9e2-e70c-4794-b5c3-6cbc91779b98,U01AXE0P5M3,,,\"yes, it\\'s possible to use the same instance. you just go to a different directory\",1643404097.008539,1643528178.778079,U01AXE0P5M3\\ne66805a9-da3f-4640-b83a-c6d4bc398ae6,U02UY1QTGHW,,,try putting the absolute path in your docker-compose file,1643511546.181979,1643528230.136259,U01AXE0P5M3\\n5dfae3d8-61f6-413e-9aa0-681f89d57faf,U02CUH15ACE,,,thanks i solved the problem my self .. the problem that the gcp was using an old project so after i changed it works.,1643317231.156000,1643528781.567779,U02CUH15ACE\\n5f8f578a-f5a4-4faf-b5a4-ca0b611ef7ff,U02UY1QTGHW,,,\"hm actually it seems it has picked the credentials file, but can\\'t connect... weird. maybe you can try creating a new one?\",1643511546.181979,1643529013.184799,U01AXE0P5M3\\n1c29543a-8a41-40c2-a17c-5bdf89dc4907,,5.0,,I\\'m running across some errors while trying to deploy the google services with Terraform from a GCP VM. I think I have covered all the steps done in the videos. Anyone has faced the same issues? (Error in the thread),1643529191.191009,1643529191.191009,U025S978QRG\\na143c8f5-ccf7-463b-bd84-d8149ad6cfcd,U025S978QRG,,,\"While running `terraform apply` I come across these errors\\n```│ Error: googleapi: Error 403: The project to be billed is associated with an absent billing account., accountDisabled\\n│ \\n│   with google_storage_bucket.data-lake-bucket,\\n│   on <http://main.tf|main.tf> line 19, in resource \"\"google_storage_bucket\"\" \"\"data-lake-bucket\"\":\\n│   19: resource \"\"google_storage_bucket\"\" \"\"data-lake-bucket\"\" {\\n│ \\n╵\\n╷\\n│ Error: Error creating Dataset: googleapi: Error 403: Access Denied: Project de-zoomcamp: User does not have bigquery.datasets.create permission in project de-zoomcamp., accessDenied\\n│ \\n│   with google_bigquery_dataset.dataset,\\n│   on <http://main.tf|main.tf> line 45, in resource \"\"google_bigquery_dataset\"\" \"\"dataset\"\":\\n│   45: resource \"\"google_bigquery_dataset\"\" \"\"dataset\"\" {```\",1643529191.191009,1643529248.773869,U025S978QRG\\n9f2ec189-f2b2-4b75-8d1e-44d45be56994,,2.0,,\"Hi All, I have followed the video, *DE Zoomcamp 2.3.2 - Ingesting Data to GCP with Airflow.* And I\\'m able ran the dag, _*data_ingestion_gcs_dag*_ in airflow.\\n\\nHowever when I go check the big query dataset, I found no data in my external_table.:sweat_smile:\\nAnyone can guide through me about this matter?\",1643529548.231549,1643529548.231549,U02T697HNUD\\naa8e85e0-83aa-4c70-b9ba-c502425e806d,U02UX664K5E,,,\"Seems like a common problem\\n\\nJust in case, I added this:\\n\\n<https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_2_data_ingestion/airflow/1_setup.md#file-googlecredentialsgoogle_credentialsjson-was-not-found|https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_2_data_ingestion/airflow/1_setup.md#file-googlecreden[…]json-was-not-found>\",1643512731.472799,1643529580.914639,U01AXE0P5M3\\n4631f298-2aa4-4344-a514-a255e8ba6f78,U02R9P66EQY,,,\"did you change the file? try resetting it with\\n\\n```git checkout path/to/docker-compose.yaml```\",1643518843.789109,1643529644.476569,U01AXE0P5M3\\n7c69ec41-dd86-4f8f-84b2-454f2c61121d,U02SMBGHBUN,,,\"yes exactly\\n\\nfor big query we\\'ll create a table in the next week, so for this homework you don\\'t need to create a table, only uploading files to GCS is enough\",1643522160.532659,1643529797.409859,U01AXE0P5M3\\n07da475d-fca7-456f-b012-f99ccdda743c,U030FNZC26L,,,you also might need to delete your dag from the UI and create a new one (by appending `_v02` for example),1643524027.252679,1643530395.446449,U01AXE0P5M3\\ne3f7e462-c7d6-44a8-820c-af8cfde20f56,U025S978QRG,,,did you give it permissions?,1643529191.191009,1643530426.358909,U01AXE0P5M3\\n910fdf47-5fc9-4d12-91e2-fb3ba75fdf93,U02T697HNUD,,,\"add a star there:\\n\\n```SELECT * FROM ... LIMIT 10;```\",1643529548.231549,1643530466.612299,U01AXE0P5M3\\n39173083-fd20-4c7d-9803-a13d890e3786,U02T697HNUD,,,Sorry for silly question and thanks for help to solve Alexey:grin:,1643529548.231549,1643530814.297059,U02T697HNUD\\n6f4f3c17-fdd5-4056-b40b-41a95d5575b1,U025S978QRG,,,I think I have given it all the permissions:,1643529191.191009,1643531925.755149,U025S978QRG\\n13d4bae1-9391-4b11-aa8d-771a67ad12f8,U025S978QRG,,,I have searched the errors on internet but I haven\\'t found any relevant information. I don\\'t know what I have messed up... so I have finally created the services directly on the Google Cloud Console,1643529191.191009,1643532105.977889,U025S978QRG\\n0b244a2f-ebbf-49c1-ba2e-30e4511e024c,U025S978QRG,,,Can you try creating another service account and test it?,1643529191.191009,1643532526.672019,U01AXE0P5M3\\n59af52c8-48a9-4a84-a8dd-382699150b0a,,3.0,,\"I\\'m having problems running Airflow on my Mac. I\\'ve set up a new airflow in completely new directory, increased the Docker memory resources to the maximum, used the airflow image of 2.2.0 instead of 2.2.3. I still get the same issue. When i do a docker compose up, and then do a docker ps after a while, it seems all the docker airflow containers all become suddenly unhealthy. I can see from the logs the airflow logo however when I go to localhost:8080 or even 0.0.0.0:8080, I don\\'t see the airflow webserver UI. There is nothing. Any suggestions ? <@U01AXE0P5M3> If I can\\'t get it running on my computer, would I be able to continue with the course content doing a local install of airflow (without Docker ?)\",1643533588.868979,1643533588.868979,U02TMP4GJEM\\nb4028e5c-942e-4d62-ad11-c0cdc72e8add,,,,\"For those want to do homework, if you need to create another DAG, you can edit the DAG_ID to another one.\\nThis is make sure the webserver UI can detect another DAG.\",,1643534290.003809,U02T697HNUD\\n927e86b6-4bd8-4ac4-9cff-5db9c8841d03,U02TMP4GJEM,,,\"I think so, docker was supposed to make it easier for students to run it... :sweat_smile: \\n\\nBut if you can run it without docker, it\\'s okay. \\n\\nAlso you can try getting a VM from GCP\",1643533588.868979,1643534409.622789,U01AXE0P5M3\\n9f3f9b3d-66d2-4c3d-a5a8-93d6158ed642,U02TMP4GJEM,,,that\\'s an idea. so installing docker-airflow on my GCP VM ? will do that. Thanks,1643533588.868979,1643534906.780989,U02TMP4GJEM\\n4c4b9ca1-22d4-4672-8c70-bb908f16ecdd,U01T2HV3HNJ,,,\":tada::tada::tada: It\\'s working know :slightly_smiling_face: I am not sure what it was exactly. I guess, it was a combination of different things and purging everything helped. I believe it also had to do with the permissions of the folder I was working in. Thanks a lot for your support <@U01AXE0P5M3> and <@U02TATJKLHG>\",1643464059.787079,1643535452.280269,U01T2HV3HNJ\\ne7e43121-2f05-4a0c-a048-27062661a640,,6.0,,\"Hi All,\\nI am watching video 1.3.2, while executing \"\"terraform plan\"\", I am getting this error. Can someone please help.\",1643537168.562499,1643537168.562499,U0254S545D5\\ndc02d502-4088-4239-9ac7-30b6ddce8a91,U0254S545D5,,,,1643537168.562499,1643537176.593079,U0254S545D5\\n603f06f7-8ba9-4a80-a1ec-26ada3c8dd1b,U030FNZC26L,,,\"It seems that the trick is to set `catchup=True` and change the `dag_id` , then it will start to run from the beginning.\",1643524027.252679,1643537644.591199,U030FNZC26L\\n6e22906e-fe11-4df5-85d8-146355b3b034,,3.0,,hey guys did the transfer service videos only work for anyone who use cloud service outside gcp? i mean if i dont have aws or any other cloud service can i skipped it?,1643538118.843609,1643538118.843609,U02RA8F3LQY\\nbbfc5ce2-63d5-4b3f-bff2-649b0c35422b,U02UY1QTGHW,,,\"Try the suggestions here.\\n\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1643299377100400?thread_ts=1643299377.100400&amp;cid=C01FABYF2RG|https://datatalks-club.slack.com/archives/C01FABYF2RG/p1643299377100400?thread_ts=1643299377.100400&amp;cid=C01FABYF2RG>\",1643511546.181979,1643538432.024849,U02SBTRTFRA\\n583ef975-1eb9-4ea0-9b68-93a229a1dcb7,U0254S545D5,,,which version of terraform do you use? have you changed anything in the files?,1643537168.562499,1643538465.500119,U01AXE0P5M3\\n772b1163-868e-413c-b2ef-5d0260903822,U0254S545D5,,,looks like one of the terraform files is not picked up,1643537168.562499,1643538499.666299,U01AXE0P5M3\\n437bf2be-e7ae-4f05-bb1c-65e2af3774d1,U02RA8F3LQY,,,\"it\\'s optional, yes\",1643538118.843609,1643538823.868539,U01AXE0P5M3\\n06f5578f-ad17-4ed1-9d95-4cb7c17271ef,U0254S545D5,,,these are the files I am having. And I have not changed anything in the files. I am using version Terraform v1.1.4,1643537168.562499,1643538830.346459,U0254S545D5\\nfe8f755b-f3b6-4a8a-93fa-96b0587fd643,U02RA8F3LQY,,,\"it\\'s quite common that you have files in AWS and you want to use big query for analytics\\n\\ntransfer service makes it easy to keep the buckets in sync\",1643538118.843609,1643538887.114099,U01AXE0P5M3\\n0f1fe4b3-792d-4f88-b7d3-236fbeeeaf2c,U0254S545D5,,,\"can you try it with terraform 1.1.3? (not sure if it matters, but this is the version I have)\",1643537168.562499,1643539001.710669,U01AXE0P5M3\\nc3d65550-e467-435d-8285-8d90bd3fc861,U0254S545D5,,,sure,1643537168.562499,1643539073.212449,U0254S545D5\\nfecebca8-dbe1-45b8-a884-e26d6a169944,U02UBQJBYHZ,,,You\\'ll have probably merge conflicts that you\\'ll have to solve.,1643502041.313719,1643539792.714449,U02GVGA5F9Q\\n606645f8-ec82-4778-9894-f59c50466277,U02RA8F3LQY,,,Okay then thanks,1643538118.843609,1643540114.858829,U02RA8F3LQY\\n47b8fe28-bbc3-4f35-94ca-0ccfdb454409,,2.0,,\"Hello everyone, I am just starting the course and I will have to do it at my own pace because of long hours at work. That being said, I am just seeing that you can get a certificate by completing it. Is it possible to get the certificate if I am not able to follow week to week? I will do the course regardless, but I would also be interested in the certificate\",1643541024.769969,1643541024.769969,U031HNNSW3A\\nf0ccd6fb-0fcc-44e4-90b7-ec75a5da5cfd,,4.0,,\"Hello! Not sure if it is a homework spoiler or not, but when I run the FHV ingest DAG for 2020-01 I get this error (I also debugged locally, at first I thought it was a docker problem). Are we suppose to write the specific task in a way that it solves this issue or leave it as is?\",1643542063.897669,1643542063.897669,U02UA0EEHA8\\na2bc2978-9957-4a28-9351-233bc13ad2a3,U02UA0EEHA8,,,\"```[2022-01-30, 11:22:00 UTC] {taskinstance.py:1426} INFO - Exporting the following env vars:\\nAIRFLOW_CTX_DAG_OWNER=***\\nAIRFLOW_CTX_DAG_ID=data_ingestion_gcs_dag_fhv\\nAIRFLOW_CTX_TASK_ID=format_to_parquet_task\\nAIRFLOW_CTX_EXECUTION_DATE=2020-01-01T00:00:00+00:00\\nAIRFLOW_CTX_DAG_RUN_ID=scheduled__2020-01-01T00:00:00+00:00\\n[2022-01-30, 11:22:00 UTC] {taskinstance.py:1700} ERROR - Task failed with exception\\nTraceback (most recent call last):\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1329, in _run_raw_task\\n    self._execute_task_with_callbacks(context)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1455, in _execute_task_with_callbacks\\n    result = self._execute_task(context, self.task)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1511, in _execute_task\\n    result = execute_callable(context=context)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py\"\", line 174, in execute\\n    return_value = self.execute_callable()\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py\"\", line 185, in execute_callable\\n    return self.python_callable(*self.op_args, **self.op_kwargs)\\n  File \"\"/opt/airflow/dags/data_ingestion_gcs_dag_fhv.py\"\", line 31, in format_to_parquet\\n    table = pv.read_csv(src_file)\\n  File \"\"pyarrow/_csv.pyx\"\", line 867, in pyarrow._csv.read_csv\\n  File \"\"pyarrow/_csv.pyx\"\", line 876, in pyarrow._csv.read_csv\\n  File \"\"pyarrow/error.pxi\"\", line 143, in pyarrow.lib.pyarrow_internal_check_status\\n  File \"\"pyarrow/error.pxi\"\", line 99, in pyarrow.lib.check_status\\npyarrow.lib.ArrowInvalid: CSV parse error: Expected 7 columns, got 1: B02765```\",1643542063.897669,1643542069.317819,U02UA0EEHA8\\nc5ff5dc6-1511-4a5e-90a2-f255ce0b0196,U02TMP4GJEM,,,\"<@U01AXE0P5M3> just to let you know, it worked! Running Docker Airflow on a GCP VM\",1643533588.868979,1643542390.063789,U02TMP4GJEM\\n97d09f68-891e-488a-8d81-805bba809422,U031HNNSW3A,,,\"For a certificate you\\'ll need to do a project. If you can do it at the same time as everyone and pass it, you\\'ll get a certificate\",1643541024.769969,1643542861.912229,U01AXE0P5M3\\n209788f7-6947-4afd-bd42-a50eeb73abfb,U02UA0EEHA8,,,\"If the file is corrupted, it\\'s okay, you can just leave the task red\",1643542063.897669,1643542946.023509,U01AXE0P5M3\\n2f704981-fe2d-4ef1-8fce-0798ebb149bf,,3.0,,\"Hi on homework week 2\\n\\nny_taxi data only available until june 2021\\nso if I put the `retries = 0` for DAG can\\'t try retry every single time for download the data if not available or return 404 it\\'s ok ?\\n\\nor on airflow task we can put execption ?\\n\\n```get_task = BashOperator(\\n        task_id = \"\"get_data\"\",\\n        retries = 0,\\n        bash_command = f\"\"curl -sSLf {FILE_TEMPLATE} &gt; {OUTPUT_FILE_TEMPLATE}\"\"\\n    )```\",1643543108.607059,1643543108.607059,U02SQQYTR7U\\nc3dfddf4-df0d-4104-a6da-2f839df48c2a,U02UA0EEHA8,,,\"another side note:\\n• if in the download task we write the BashOperator with `curl -sSLf` , data from 2020-08 FHV will not be downloaded \\n```curl -sSLf <https://s3.amazonaws.com/nyc-tlc/trip+data/fhv_tripdata_2021-08.csv> &gt; test_2020_08.csv\\ncurl: (22) The requested URL returned error: 404```\\n• if we run it with `curl -sS`  this will be downloaded, but all the missing months after July 2021 will also run successfully, but generate empty parquet files because the .csv do not exist\\nHow do we deal with 2020-08 in light of the homework?\",1643542063.897669,1643543912.250469,U02UA0EEHA8\\n9d6d604e-6088-4d0f-878a-f64342508f73,U02UA0EEHA8,,,\"if you run it without f, it won\\'t be downloaded either because the file doesn\\'t exist - if you look at the content, it says something along the lines of \"\"key doesn\\'t exist\"\"\\n\\nthat\\'s why I think having the -f flag is the best way, it\\'ll fail the job, and when the files appear there, we can re-execute it\",1643542063.897669,1643544689.250359,U01AXE0P5M3\\na867c0ce-18d9-49d1-bc9b-9e8dfed5d6c3,,3.0,,Hi. Do we need to upload all *FHV* files from 2019 and 2020? Or we can upload only one file for example,1643546984.714899,1643546984.714899,U02QL1EG0LV\\n2a196930-a160-418c-a406-811e1f04562a,U02QL1EG0LV,,,\"For week 3 homework, you\\'ll need at least 2019\\n\\n<https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_3_data_warehouse/homework.md>\",1643546984.714899,1643547350.255969,U01AXE0P5M3\\nafe800c6-660e-4ece-a8bc-7a5555ea6ca3,U02QL1EG0LV,,,so it seems that you won\\'t need FHV data for 2020,1643546984.714899,1643547407.576179,U01AXE0P5M3\\n1951bce6-2aab-4b3a-bc35-7666a71510ea,U02SQQYTR7U,,,\"retries=0 means that the task will be attempted once, but if it fails, it won\\'t be executed again, right? (I always get confused)\\n\\nif yes, then 0 is totally fine\",1643543108.607059,1643547924.994599,U01AXE0P5M3\\na56435b9-8a1c-444b-b6b0-1772cc24864b,U02SQQYTR7U,,,and for other dates you can re-execute things manually if they fail for some other reason,1643543108.607059,1643547957.045309,U01AXE0P5M3\\n84766067-624b-4275-84b8-2db19ecdbfac,U02SQQYTR7U,,,so if download data is failed because file is not found (404) it\\'s ok,1643543108.607059,1643548674.452139,U02SQQYTR7U\\ndb638dda-9a53-4eaa-b523-6b2da0a6b27e,U02UBQJBYHZ,,,\"Hi <@U02UBQJBYHZ>,\\nI had the same problem. Solve by (I am using wsl2 and installed pgcli using sudo) removing the pgcli and reinstalling it via conda.\",1642965033.177400,1643554855.723919,U02J42ZFLRL\\na8551628-5db7-40ee-8830-70ab51d42d17,U0297ANJF6F,,,\"<@U01AXE0P5M3> There are several containers  running(which one to get into) , I don\\'t know how to get in to container using the bash!\",1643486360.180649,1643555348.793219,U0297ANJF6F\\n029C43BE-012E-4D3A-BE70-347B997F7BA4,U02QL1EG0LV,,,\"Wow, I only executed for just 2021 \",1643546984.714899,1643555744.439989,U02TBCXNZ60\\nef8bc26b-368d-45bf-b185-4f56ecef6c29,,7.0,,\"Hello <@U01AXE0P5M3> do you intend to teach how to deploy Airflow on Container Orchestration Tools such as Kubernetes??\\n\\nAlso another question is about week 1( while do we need to specify pgdatabase as host and not the localhost of the two docker container Network??)\",1643556765.611519,1643556765.611519,U02SPLJUR42\\na8366ad5-8ae8-4b4b-90fb-3efc8063e7bc,,3.0,,\"Hi Alexey! I am watching the *DE Zoomcamp Week #2 Office Hours* video, and I didn\\'t understand what you said about submitting docker containers in spark. Do you know any post or link where I can deep in more in the subject?\",1643556926.292499,1643556926.292499,U025S978QRG\\n5cce40b5-5d1b-48c0-aacf-f34a24cc4ea5,U0297ANJF6F,,,You need to select the worker,1643486360.180649,1643558279.332029,U01AXE0P5M3\\n64bf3daa-8beb-47eb-be55-829a55245a5a,U02SPLJUR42,,,\"Good suggestion! But maybe not for this course, because there are other topics we want to cover. But hopefully we give you a good introduction so you now have enough background to figure it out on your own\",1643556765.611519,1643558517.203209,U01AXE0P5M3\\na6d0372e-e931-4f78-9c80-4fed1867d760,U02SPLJUR42,,,\"If you need an intro to Kubernetes, check ML zoomcamp\",1643556765.611519,1643558568.758689,U01AXE0P5M3\\n0168df62-2d31-4f6e-ae30-7318a49478db,U02SPLJUR42,,,\"I don\\'t understand your second question, can you please rephrase it?\",1643556765.611519,1643558630.046559,U01AXE0P5M3\\nf7695232-c068-4369-9b9c-e737f9939d2a,U025S978QRG,,,\"Yes\\n<https://datatalks-club.slack.com/archives/C01B0JGRBMY/p1606317328081100|https://datatalks-club.slack.com/archives/C01B0JGRBMY/p1606317328081100>\",1643556926.292499,1643558814.284059,U01AXE0P5M3\\n67193526-df87-456e-9f0c-d17f313bff5e,U025S978QRG,,,\"It\\'s for Spark on AWS, but hopefully it gives you an idea how to package the dependencies with docker in spark\",1643556926.292499,1643559064.915209,U01AXE0P5M3\\n4f9a6eb2-7cfd-4447-84f5-3f8f608dcffb,U0297ANJF6F,,,\"Hi, I am currently facing the same difficulty. I have checked that my google_credentials.json is copied correctly into the container\",1643486360.180649,1643559478.045489,U02UEJ730SG\\n7c3f7a22-5d16-45c9-9257-b27b5f8c8a56,U02SPLJUR42,,,\"Okay, I have experience with Kubernetes but not with running dags on Kubernetes \\n\\n\\nThe second question, is about the host that was specified while connecting pgadmin with postgresDB\\n\\nThe host was pgdatabase which is different from localhost ( why did we use pgdatabase as against localhost given that both containers are on same network)\",1643556765.611519,1643559577.385389,U02SPLJUR42\\nf862b94c-32d4-44cd-8b6d-aba3db8bb9c6,U0297ANJF6F,,,\"ah sorry, I actually faced a different error message when opening the airflow log:\\n```google.resumable_media.common.InvalidResponse: (\\'Request failed with status code\\', 403, \\'Expected one of\\', &lt;HTTPStatus.OK: 200&gt;, &lt;HTTPStatus.CREATED: 201&gt;)```\\n, which seems my credentials are not allowed to create Google Cloud Storage project... but I am pretty sure I have set the correct IAM access\",1643486360.180649,1643559600.649929,U02UEJ730SG\\ne7cada91-4c18-4af9-8b90-2e1b235f9fa1,U02SPLJUR42,,,You can use a Kubernetes operator which will create a job on Kubernetes,1643556765.611519,1643559707.827819,U01AXE0P5M3\\na5a9ec62-3178-46c0-a972-6bd44e23f11e,U02SPLJUR42,,,\"If you say localhost, for the worker container it\\'ll mean itself and not postgress\",1643556765.611519,1643559822.971649,U01AXE0P5M3\\n18751b6e-fcc2-40c5-8ae5-817a142447bc,U02UBQJBYHZ,,,\"fetch and merge works for me, you might want to try that\",1643502041.313719,1643559828.185219,U02HFP7UTFB\\n09545241-df5d-4e35-8fb0-d2b1afb473b4,U02GVGA5F9Q,,,\"I found that recreating the container images and restarting everything again it reads the .env file. Changing the port to 5433 I found that it was connecting to the internal postgresql and not the one it should...\\nThis are the relevant env variables of the worker: `PG_PASSWORD={PG_}\\nPG_DATABASE={PG_}\\nPG_USER=root`\",1643496742.748149,1643560328.072259,U02GVGA5F9Q\\n8576c18d-2ae2-4589-b1eb-796dc55d369a,,10.0,,\"I\\'m finding week2 homework questions very confusing.\\nShould we import the fhv data only for 2019-2020 exactly. Are question 1-2 related no yellow taxi or fhv data?\\nThe fhv csv for 2020-01 is not able to be converted to parquet, probably because a delimiter. Should we consider changing the function to parse the csv to parquet so this task succeed?\\nOr should we let it fail and on the question 3, the amount of green runs to count 1 less?\",1643560330.528159,1643560330.528159,U02U0U8LGHM\\ndc82a888-920e-4ada-84cc-e496510c39a1,U02U0U8LGHM,,,\"Q1 and q2 are about yellow taxis\\n\\nfor 2020-01 - I was able to reproduce it. I think it\\'s up to you. I checked the homework for week 3 and we\\'ll need only data for 2019, so I\\'ll rephrase the question to count only runs for 2019\",1643560330.528159,1643561561.880869,U01AXE0P5M3\\nc7f4bfe9-5ad7-46cf-b4d2-b526919436e2,U02U0U8LGHM,,,\"by the way, do you know how to fix it? pyarrow doesn\\'t seem to be as flexible as pandas when it comes to handling malformed rows in csv\",1643560330.528159,1643561611.882429,U01AXE0P5M3\\n2d8d8c23-c8f7-45fe-a3ab-1acb17dd0abf,U030FNZC26L,,,<@U030FNZC26L> Airflow does the backfill (aka catchup) by default thus `catchup=True` is a default value,1643524027.252679,1643561627.868909,U02UKBMGJCR\\n942a8792-9710-418f-b5fd-158be462e173,U02U0U8LGHM,,,\"I\\'ve updated the description, I hope it\\'s less confusing now. Please let us know if some things there are still confusing\",1643560330.528159,1643562021.824849,U01AXE0P5M3\\n7a6c3e9d-3541-4df9-99bb-3cc3563dcbf0,,5.0,,When I run `docker-compose up airflow-init` I get this: `no such service: airflow-init` I think everything is built succesfully.,1643562133.938679,1643562133.938679,U02S565BCKX\\n66c3e893-2b08-429a-8535-a91481fd66ac,U02GVGA5F9Q,,,\"It would be nice to have a working version of the local ingest files, but creating them is nevertheless a good challenge I\\'m willing to tackle. I just don\\'t have time to do it now, as this has already consumed too much. Moving now to the homework stuff and hopefully save some time to recover part of the delayed stuff of this week. Thanks!\",1643496742.748149,1643562560.650239,U02GVGA5F9Q\\ncf07898f-c4d4-4e8f-9706-f2bd1063d57f,U02UBQJBYHZ,,,\"Thanks! I used pull which is fetch and merge together, and the merge was automatic. So I have updated repo and my edits weren\\'t lost.\",1643502041.313719,1643562572.008539,U02UBQJBYHZ\\n6100975d-2f91-4fac-87c7-0badaffff271,U02GVGA5F9Q,,,yea sorry I have no clue why it doesn\\'t work for you :see_no_evil:,1643496742.748149,1643562612.707009,U01AXE0P5M3\\n5e395349-688c-4a53-9091-86cbac675719,U02GVGA5F9Q,,,\"No prob. I think you should have started with clean files, I guess. I pruned lots of containers, images and networks.\",1643496742.748149,1643562691.095399,U02GVGA5F9Q\\n69c1cc06-b296-4f63-a5bb-38e12f05e180,U02S565BCKX,,,Did you run the official docker-compose file from airflow?,1643562133.938679,1643563091.000969,U02HFP7UTFB\\nabb45a58-f9fc-4184-a9b7-1d2eb63083b5,U02SPLJUR42,,,Oh okay thanks,1643556765.611519,1643563637.367789,U02SPLJUR42\\n676be6ab-1386-4b6c-afee-a8665d532de2,U0297ANJF6F,,,\"<@U01AXE0P5M3>\\nI solved the problem , I was going through each steps again and I was missing the ENV variable in the docker-compose.yaml file `GOOGLE_APPLICATION_CREDENTIALS:/.google/credentials/google_credentials.json`\",1643486360.180649,1643564178.598449,U0297ANJF6F\\n31649b24-7207-4ac7-b79b-24de8c68fae3,U02S565BCKX,,,No. I tried with the -nofrills . I can run the airflow-init with the other yaml.  Thanks! But now I have bunch of other errors :confused:,1643562133.938679,1643564564.571369,U02S565BCKX\\n8197e439-cabf-4e2f-8e65-866bfd489d22,U02U0U8LGHM,,,\"Thank you for clarifying. The error is this:\\npyarrow.lib.ArrowInvalid: CSV parse error: Expected 7 columns, got 1: B02765\\n\\nI downloaded the data twice, with wget and curl, still the same error.\\nI\\'m using WSL.\",1643560330.528159,1643564836.470259,U02U0U8LGHM\\nf3aa261b-a708-4d3b-9672-e2c25c57ddf1,U02U0U8LGHM,,,\"However, I\\'m able to read through pandas, then parse the timestamps, and convert to parquet.\",1643560330.528159,1643564865.613479,U02U0U8LGHM\\nbc39ac4e-aacf-4bac-b303-3f7e8ecbe0eb,,2.0,,\"Work around used.\\n\\n~I have a very weird issue and no amount of googling is helping. The bash command to download the file is successful, but the parquet fails saying \"\"Empty CSV\"\", even for the 2019-01.  When i check in the container, it is a zero size file. However, when i docker exec -it and execute the same command it downloads the file fully. Anyone faced any similar issues?~\",1643565033.053839,1643565033.053839,U02SMBGHBUN\\n5cdc8d2e-bebf-458f-ae9f-24b7e357e46c,,9.0,,\"Trying to access airflow at `0.0.0.0:8080/` keep getting `The connection was reset` The terminal shows a lot of info. This I think is the most obvious error\\n```        | $ airflow db check\\n        | Unable to load the config, contains a configuration error.```\",1643565128.991799,1643565128.991799,U02S565BCKX\\n282b5e4f-626c-434a-9c68-925d6eccc1ae,U031HNNSW3A,,,I understand. I will try and jump into the project when it starts. Thanks!,1643541024.769969,1643565411.629569,U031HNNSW3A\\n3c180801-7f51-4439-93db-6bc33b95f838,,,thread_broadcast,\"Hi, for the -nofrills version, did you update your `.env` file? Also, please clear all the previous setup (volumes, history &amp; cache), using this command:\\n`docker-compose down --volumes --rmi all`\\nand restart\",1643562133.938679,1643566083.541059,U01DHB2HS3X\\n953541a7-32d4-4664-8ad1-b2493d1f3075,U02BVP1QTQF,,,This. Sorry hor the delay on the reply,1643397723.580519,1643567310.832569,U02BVP1QTQF\\nc2d72653-251e-451e-a653-247e976f7dd4,U02S565BCKX,,,\"What kind of errors do you have? Something like\\n\\n```ModuleNotFoundError: No module named \\'airflow\\'```\\n?\",1643562133.938679,1643568966.077199,U01AXE0P5M3\\n793bcc94-df5b-4213-adae-1370d446a562,,3.0,,\"Hi everyone! I hope the course is going well for all.\\nI was moving countries in the last two weeks so I could not keep up properly with week 1 and 2. Is it too late to get a certificate, or is the final project submission the only \"\"madatory\"\" deadline?\\nThanks in advance!\",1643569059.017099,1643569059.017099,U02RY943Z0Q\\n5ff57bee-4b0d-4b5a-a05f-1d9fc05413fe,U02RY943Z0Q,,,the project is the only mandatory deadline - but you can also take the course at your own pace,1643569059.017099,1643569098.651329,U01AXE0P5M3\\nbc32468c-29cf-4d72-b74c-746cd9d3640c,U02RY943Z0Q,,,Thank you Alexey! I will see if I can catch up,1643569059.017099,1643569135.579069,U02RY943Z0Q\\n6138b822-da52-45b1-b1cf-b0a552670d98,U02BVP1QTQF,,,it\\'s for showing the history - how many runs you want to see,1643397723.580519,1643569151.456149,U01AXE0P5M3\\nf6cc033e-1f75-40fe-a7a6-5964c1b0e34d,U02CD7E30T0,,,\"Still regarding the DAG, can I assume that the previous month data is available immediately in the next run? (We know that it\\'s not the reality :) )\\nEach run would be on 01/MM/YYYY and  will extract MM-1/YYYY data.\",1643493897.468629,1643569367.022159,U02CD7E30T0\\n42dcd7e9-fc5f-46cc-abe7-c6e5d548b00a,U02BVP1QTQF,,,\"Got it, thanks!\",1643397723.580519,1643569643.056169,U02BVP1QTQF\\ne7882b73-d1f0-47e1-8baa-521c543fd45a,,5.0,,\"```File \"\"/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/bucket.py\"\", line 1098, in path\\n    raise ValueError(\"\"Cannot determine path without bucket name.\"\")\\nValueError: Cannot determine path without bucket name.```\\nHi, Even though I\\'m providing correct bucket name I\\'m facing this issue. Can u guys suggest fix for this!\",1643570058.391529,1643570058.391529,U02V4412XFA\\n69aacf74-1fac-4791-a672-74c177f3a809,U02SMBGHBUN,,,\"If anyone is facing the same issue as mine, simply ditch the BashOperator and use PythonOperator instead. You get a better handle using urllib (or use requests as I did) and this gives ability to log, introspect error scenarios etc.,\",1643565033.053839,1643570421.122749,U02SMBGHBUN\\nc5c09b30-97b0-481f-a34b-2bd9f75f8437,U02VBG59VQ9,,,<@U02VBG59VQ9> Were u able to fix it?,1643399968.823989,1643570755.375879,U02V4412XFA\\n32fcab90-f92e-4b8f-8463-4e7f18877d2f,,2.0,,\"From my understanding, ingesting data to Postgres with Airflow part is a bonus? Is it necessary to complete this part to move into week 3? I would rather use my time to do a recap of week 1 :sweat_smile:\",1643571114.740279,1643571114.740279,U02UX664K5E\\ne5b99c43-9443-4a97-8ba6-5f9c21c59d02,U02UX664K5E,,,\"It\\'s not necessary. If you figured out how airflow works, you can skip it\",1643571114.740279,1643571269.966259,U01AXE0P5M3\\n55738b3a-8ecf-4d80-a8c6-e3c3276aaf1e,U02S565BCKX,,,<@U01AXE0P5M3> I wrote in another thread here: <https://datatalks-club.slack.com/archives/C01FABYF2RG/p1643565128991799>,1643562133.938679,1643571344.154439,U02S565BCKX\\nc872666d-88bf-44b4-a483-0f747417c81b,U02CD7E30T0,,,I guess so. But we don\\'t explicitly need to use the data from the past,1643493897.468629,1643571359.531439,U01AXE0P5M3\\nbe565f7c-f3fc-42e5-92c2-662e04c042b2,U02S565BCKX,,,Can you try localhost instead?,1643565128.991799,1643571391.424699,U01AXE0P5M3\\n07d58b56-c69b-455a-997c-392fd1252d79,U02S565BCKX,,,I get the same with localhost.  Also trying in firefox and chrome.,1643565128.991799,1643571470.770789,U02S565BCKX\\ncdb92b7f-6161-49b4-8626-ffc27c28fc01,U02V4412XFA,,,<@U01DHB2HS3X> Can u please look into this!,1643570058.391529,1643571569.664619,U02V4412XFA\\ne0b1bbcc-d908-4bb6-8b65-f77fba28b943,U02S565BCKX,,,What does it show in the terminal?,1643565128.991799,1643572008.687949,U01AXE0P5M3\\n81eb0dd6-d554-4e2a-80dc-7a134f497b8a,,3.0,,\"Hey guys, I ran\\xa0`docker build -t taxi_ingest:v001 .` \\xa0 succesfully but when i try\\n```docker run -it \\\\\\n    --network=pg-network \\\\\\n    taxi_ingest:v001 \\\\\\n    --user=root \\\\\\n    --password=root \\\\\\n    --host=pg-database \\\\\\n    --port=5432 \\\\\\n    --db=ny_taxi \\\\\\n    --table_name=us_500 \\\\\\n    --url=${URL}```\\nit throws an error, any ideas?\",1643572099.139669,1643572099.139669,U02UA8WGF44\\n0312ff91-cb3f-443b-895e-2006ad1b9fdb,U02V4412XFA,,,Can you check if the bucket variable is set inside the container?,1643570058.391529,1643572295.858669,U01AXE0P5M3\\n1d163459-cdb6-4ae6-8153-4c0519495892,U02S565BCKX,,,I am not sure which part to paste. The log is too long.,1643565128.991799,1643572731.050349,U02S565BCKX\\n0b971f13-30fe-4d54-a71f-6cffec5fa73f,U02UA8WGF44,,,\"Had similar access issues that were solved by using Bash/Powershell in admin mode. Are you using Windows?\\n\\nLooks like permission problems that generaly happens with folders/directories, how is your project structure right now?\",1643572099.139669,1643573034.306659,U02TC704A3F\\n2d090463-cdbb-43f2-a17e-a8c4d03332cc,U02TBKWL7DJ,,,\"<@U02BVP1QTQF> Correct me if I’m wrong but `docker-compose` comes into the picture when you want to use multiple containers and communicate with each other.\\n\\nBtw I went down a rabbit hole for docker and realized that\\n1. If you want to run docker through command line, you need to install `docker-machine` and `virtualbox` \\n2. Or if you’re using Docker app then it uses Hyperkit which is lightweight MacOS virtualization solution.\\nDoes it mean that if you have to run `postgresql` instance  - you need to run a container for `virtualbox`, `postgresql`, and something more?\",1642507885.440700,1643573107.257669,U01HNUYV81L\\nbfe2eef9-018f-4375-93d3-d93069280c03,U02S565BCKX,,,I had this one as well. Try pruning all your volumes and images (there are commands in the instructions) and start again,1643565128.991799,1643573125.845809,U01AXE0P5M3\\n04ecbd1d-3e6c-4e56-8182-acaf58ae6214,,9.0,,\"Hi everyone. I just started the course and basically completed the first video. However, I am running into a problem at the very end, in the video, while ingesting the dataset to postgres, he doesn\\'t run it all the way. But when I am doing it, it runs most of the way through and it throws an error saying the following:\\n```\"\"/tmp/ipykernel_4274/698783813.py:6: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\\ndf = next(df_iter)```\\nIt seems to be caused because there is a mismatch in datatype in some of the columns. Anyone else getting this? it ingested 1269765 out of 1369766 rows\",1643573149.826459,1643573149.826459,U031HNNSW3A\\n882f529f-0669-4cf9-abfb-147b88b96079,,1.0,,\"Finally got my first data_ingestion DAG running. Just cost me two days to realize that my problem was a variable name typo when exporting from .env\\n\\nIf you are having problems with your DAGs, the tab \"\"Rendered template\"\" can help to visualize what kwargs your function is trying to execute.\",1643573241.511969,1643573241.511969,U02TC704A3F\\n3ccf2a9a-af07-44a1-98d9-7908ccece553,U02TBKWL7DJ,,,\"You do use `docker-compose` to run multiple containers and connect them, that is correct. But on a Mac the only thing you need to do is to install Docker Desktop; there is no need to install additional dependencies because Docker Desktop installs everything you need to run `docker` and `docker-compose` from a command line. And as far as I know Virtualbox isn’t used at all for Docker in MacOS\",1642507885.440700,1643573310.749179,U02BVP1QTQF\\n23e8b1c8-7685-47a9-89a7-a4d80578bbeb,U031HNNSW3A,,,I had this as well (maybe even in the videos),1643573149.826459,1643573632.382869,U01AXE0P5M3\\n4b9249dc-557d-4d38-9795-d9d3adf3e564,U031HNNSW3A,,,it\\'s not a problem - it\\'s a warning and the thing keeps working,1643573149.826459,1643573679.143809,U01AXE0P5M3\\n4324d7a1-8ef8-4b3a-a6a4-1e43a5b2e206,U031HNNSW3A,,,\"My code outputs the same warning but ends ingesting all rows. It\\'s a standard Pandas warning because this column (6) has multiple data types. You can silence this error by adding the low_memory parameter when reading the csv file with Pandas.\\n```import pandas as pd\\npd.read_csv(\\'file.csv\\', low_memory=False)```\\n\",1643573149.826459,1643573695.776419,U02TC704A3F\\n2f9f390a-50ea-49d9-9dee-7d5f9913a00b,U031HNNSW3A,,,\"The solution would be use converters or specify a datatype for the column.\\n\\nYou can read more about this in <https://www.roelpeters.be/solved-dtypewarning-columns-have-mixed-types-specify-dtype-option-on-import-or-set-low-memory-in-pandas/|this link.>\",1643573149.826459,1643573761.453019,U02TC704A3F\\nf3a309ca-1959-4a99-82a7-8d60816a1cef,U02TBKWL7DJ,,,Thanks Alvaro! :slightly_smiling_face:,1642507885.440700,1643573797.102169,U01HNUYV81L\\n33d913b3-5edf-49f8-9f6d-5d5adb08e260,U031HNNSW3A,,,\"I see, I thought it was weird I was getting that problem. But I don\\'t understand why it seemed to continue but then it actually stopped the iterations. here is the full trace, in case it clarifies the problem\\n```inserted another chunk..., it took 6.934\\ninserted another chunk..., it took 6.347\\ninserted another chunk..., it took 6.266\\ninserted another chunk..., it took 6.190\\ninserted another chunk..., it took 6.260\\ninserted another chunk..., it took 6.194\\ninserted another chunk..., it took 6.380\\ninserted another chunk..., it took 6.464\\ninserted another chunk..., it took 6.288\\ninserted another chunk..., it took 6.394\\ninserted another chunk..., it took 6.411\\n\\n/tmp/ipykernel_4274/698783813.py:6: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\\n  df = next(df_iter)\\n\\ninserted another chunk..., it took 6.668\\ninserted another chunk..., it took 4.150\\n\\n---------------------------------------------------------------------------\\nStopIteration                             Traceback (most recent call last)\\nInput In [16], in &lt;module&gt;\\n      4 t_start = time()\\n      5 # get next chunk of dataframe \\n----&gt; 6 df = next(df_iter) \\n      7 # convert date columns into datetime\\n      8 df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\\n\\nFile ~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1187, in TextFileReader.__next__(self)\\n   1185 def __next__(self):\\n   1186     try:\\n-&gt; 1187         return self.get_chunk()\\n   1188     except StopIteration:\\n   1189         self.close()\\n\\nFile ~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1280, in TextFileReader.get_chunk(self, size)\\n   1278         raise StopIteration\\n   1279     size = min(size, self.nrows - self._currow)\\n-&gt; 1280 return self.read(nrows=size)\\n\\nFile ~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1250, in TextFileReader.read(self, nrows)\\n   1248 nrows = validate_integer(\"\"nrows\"\", nrows)\\n   1249 try:\\n-&gt; 1250     index, columns, col_dict = self._engine.read(nrows)\\n   1251 except Exception:\\n   1252     self.close()\\n\\nFile ~/.local/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py:225, in CParserWrapper.read(self, nrows)\\n    223 try:\\n    224     if self.low_memory:\\n--&gt; 225         chunks = self._reader.read_low_memory(nrows)\\n    226         # destructive to chunks\\n    227         data = _concatenate_chunks(chunks)\\n\\nFile ~/.local/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:830, in pandas._libs.parsers.TextReader.read_low_memory()\\n\\nStopIteration: ```\",1643573149.826459,1643573967.174159,U031HNNSW3A\\nf1125bae-621d-4b7b-9360-2f6668196b8a,U031HNNSW3A,,,\"it mentions something about low memory, but I don\\'t understand the actual cause\",1643573149.826459,1643574019.769459,U031HNNSW3A\\nbfe9cef5-29f7-4d9c-8fb5-c93614b1175a,U02S565BCKX,,,\"Right. The reason was that I had a postgres image from before, I guess. Now it works,\",1643565128.991799,1643574120.729659,U02S565BCKX\\ne08932b9-bdf6-4137-a90d-c00566daf47f,U031HNNSW3A,,,\"hmm I guess it might be because the iterator is finished? if that\\'s the case, wouldn\\'t I get all rows?\",1643573149.826459,1643574500.288789,U031HNNSW3A\\n4f0e8c6a-ce03-4dc1-be40-45a86eae154c,U031HNNSW3A,,,\"it finishes, yes. you don\\'t get all the rows? are you sure you use the same file?\",1643573149.826459,1643574781.515259,U01AXE0P5M3\\n8a4fc985-9c37-4016-9dcc-1a2b501a2e9c,U02S565BCKX,,,the nofrills version? which OS are you on?,1643565128.991799,1643574854.000359,U01AXE0P5M3\\n85dbd27b-f9ba-4c4b-8a74-f5af3d14d2f0,U031HNNSW3A,,,\"I found my problem! I thought it might have been cut short because of the error message from the stop iterator. But I see now it is the expected outcome. My problem was that I had initiated the iterator before the loop for my testing, so I was going into the loop without the starting chunk. Thanks for the help!\",1643573149.826459,1643575439.288139,U031HNNSW3A\\nd6232844-db85-464e-a34a-0dfbb4b1d2d9,U02V4412XFA,,,I need more context to where this error is appearing and at what step. Please provide a longer trace,1643570058.391529,1643575978.211329,U01DHB2HS3X\\n36bf7bf7-9366-4d3c-812a-daffb0421340,U02S565BCKX,,,\"I am no linux. I was using the official one, since that one was suggested previously. But I may have to redo everything again as my computer is really struggling with memory :slightly_smiling_face:\",1643565128.991799,1643576354.948079,U02S565BCKX\\n986f8328-a237-404a-97cc-7bf8a8b30ed6,U02S565BCKX,,,got it!,1643565128.991799,1643576438.426309,U01AXE0P5M3\\n62f1cf56-c7fb-4c66-a828-114549c0779b,U02UX664K5E,,,\"I will advise you watch the video, it is really useful\",1643571114.740279,1643577919.377659,U02SPLJUR42\\n51fd46c5-db9e-486c-95ad-1026d488578d,,10.0,,\"My task `local_to_gcs_task` is failing all time. Logs: \\xa0`WARNING - No project ID could be determined`, `OSError: Project was not passed and could not be determined from the environment.` I have already run `gcloud config set project &lt;my-project-id&gt;` and checked GCP_PROJECT_ID, GCP_GCS_BUCKET in the docker-compose.yaml.\",1643578051.958769,1643578051.958769,U02E30U011U\\nbd8e4880-abfa-4095-859e-aed4d4c9834b,U02E30U011U,,,maybe try restarting docker compose thing?,1643578051.958769,1643578128.991889,U01AXE0P5M3\\nd3722f8b-4d37-43ec-91b3-6dc56958146f,U02U0U8LGHM,,,\"I also caught on this csv, it has some random line breaks, but as it uses \\\\r\\\\n for legitimate line breaks so we can easily fix it with something like\\n```perl -i -pe \\'s/(?&lt;!\\\\r)\\\\n/\\\\1/g\\' fhv_tripdata_2020-01.csv```\\nit delete all `\\\\n` not preceded by `\\\\r`\",1643560330.528159,1643580221.481819,U02T95PEBJP\\n12e4b65f-cbfa-40d5-88d4-583f8f0855c6,U02SMBGHBUN,,,Thanks for asking the Q and the clarification...I was banging my head for last 30 mins to figure out what to do to get DAG append all the files from datalake to external table,1643522160.532659,1643583053.329129,U02UM74ESE5\\n864cc23f-1b3a-48bb-b19b-e02543782d93,U02SMBGHBUN,,,\"<@U01AXE0P5M3> But just for sake of fun, how would I go about it? I checked BigQueryCreateExternalTableOperator but don\\'t see any options to append data in it (may be I am missing something?!).  Or do I need to create a new task after BigQueryCreateExternalTableOperator one?\",1643522160.532659,1643583294.394719,U02UM74ESE5\\n942c7b8a-b58c-4e3d-8c19-fba5f88e5bbd,,8.0,,Homework Week 2 - Due Date?,1643583593.701639,1643583593.701639,U02U5SW982W\\nf72d486e-37f3-409c-86d2-67c778e57699,U02U5SW982W,,,\"Deadline: February 4, 22:00 CET\\n\\nSource: <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_2_data_ingestion/homework.md>\",1643583593.701639,1643583703.483969,U02TC704A3F\\nca13c650-b8ac-4af4-95e5-ca34fbabbab8,U02U5SW982W,,,:sweat_smile:... Thanks <@U02TC704A3F> Given I\\'ve not even finished the content yet my chances of getting the homework in by COB Monday was not looking good. Hope everything is going well for you and everyone else :),1643583593.701639,1643583807.490469,U02U5SW982W\\n808495e4-a680-4824-b395-e68ea312fa83,U02E30U011U,,,<@U01AXE0P5M3>.. I\\'m beginning to think you are a vampire. You are always on here answering questions. Do you sleep? Or did you give that up 3000 years ago? ;),1643578051.958769,1643583910.720569,U02U5SW982W\\nf780d08f-39ed-4ce4-bf15-d7ca42006dbf,U02U0U8LGHM,,,\"<@U02T95PEBJP> Thank you, this solved the issue.\",1643560330.528159,1643584206.905959,U02U0U8LGHM\\nc2e00d16-33a1-4384-b4f2-7cc252870a60,U02U5SW982W,,,\"No problem Sandy.\\n\\nI\\'m far from finishing Week 2 but things are better now, Airflow and Postgre worked, GCP is all functioning.\\n\\nHope everything is going well for you too :smiley:\",1643583593.701639,1643584714.734699,U02TC704A3F\\nb3f931bb-763f-4024-8de9-7b4cfb148a03,U02U5SW982W,,,This week was way off schedule... But the extended deadline will suffice!,1643583593.701639,1643584726.478969,U02GVGA5F9Q\\n9ad3a112-9aee-4f62-81f9-00dae089de57,U02U5SW982W,,,\"This homework is quite interesting, and will benefit from this time extension!\",1643583593.701639,1643584780.762239,U02GVGA5F9Q\\nba065c13-945a-46fd-8353-875a04f83953,,3.0,,In week 1 we are using `SQLAlchemy`  library to connect and insert data to `Postgres` although I see that we are also loading `psycopg2` library which is a `Postgres` adapter for Python. Can anyone enlighten me as to how `psycopg2` is involved in the process of ingesting data to `Postgres` ?,1643586080.345839,1643586080.345839,U02UX664K5E\\n394fe0e8-bbb8-4d5b-883b-20fbc9e25868,U02U5SW982W,,,Absolutely guys. I feel like I\\'m kind of fumbling around in the dark but I am making some progress too <@U02TC704A3F> - albeit very slowly. If you want to feel like an intellectual giant and like you\\'ve got it all together just have a look at my <https://learningdataengineering540969211.wordpress.com/|blog> for how dark things can truly get . Have only glanced at the homework <@U02GVGA5F9Q> so I\\'m really not sure how \\'interesting\\' it is. I\\'m hoping \\'interesting\\' isn\\'t a synonym for \\'How about you start tearing your hair out now?\\' :rolling_on_the_floor_laughing:,1643583593.701639,1643587518.356549,U02U5SW982W\\nf6a06a5c-080d-4418-a71c-3fc428ee0835,U02UX664K5E,,,Hi <@U02UX664K5E> does <https://stackoverflow.com/questions/8588126/sqlalchemy-or-psycopg2|this> help at all? When we used SQLAlchemy I can\\'t remember if we used psycopg2 as the driver? Not sure where this code is now so can\\'t check it out. I assume you have it in front of you and can check?,1643586080.345839,1643587731.286199,U02U5SW982W\\nb87d5eaa-3f9b-4e2b-b46b-9ba1c29d83c8,U02TC704A3F,,,:slightly_smiling_face: welcome to my world <@U02TC704A3F> - tracking down typos... good tip though. Thanks for sharing.,1643573241.511969,1643587856.681039,U02U5SW982W\\n1e5ce3c6-d8cf-4a0a-8418-ef0ddfa8f178,U02UX664K5E,,,\"Hey <@U02U5SW982W> The `Dockerfile` has both listed as dependencies `RUN pip install pandas sqlalchemy psycopg2` . Thanks for the link, in summary `SQLAlchemy` generates the SQL statements while`psycopg2` sends them to the database. `SQLAlchemy` depends on `psycopg2` so this is why both are imported as libraries in the `Dockerfile`.\",1643586080.345839,1643588115.524119,U02UX664K5E\\nd9400b9e-4926-4b1f-b8e5-0af75ad61463,U02UX664K5E,,,Awesome - that\\'s it in a nutshell <@U02UX664K5E> :clap:,1643586080.345839,1643588750.249749,U02U5SW982W\\nc73e2763-4831-4e8b-940c-6b152f103aa2,,6.0,,How do you clear the DAG on Airflow? I keep seeing it done on the video and I can\\'t find the menu that Alexey always uses to start over.,1643590610.763829,1643590610.763829,U02UBQJBYHZ\\n8baaca68-af91-43a6-ad09-ed4724b2e030,U02UBQJBYHZ,,,Click on the circle on the top row to clear all tasks for a run,1643590610.763829,1643591169.701509,U02T9GHG20J\\n47593786-5f48-442f-84c5-d5eefd530cfa,U02UBQJBYHZ,,,That will open this menu,1643590610.763829,1643591209.776419,U02T9GHG20J\\nb7ce941c-e120-4be3-b78c-b21a22b677b7,U02UX664K5E,,,\"Hi <@U02UBQJBYHZ> you\\'ve probably answered this question yourself already but just in case you haven\\'t. No you don\\'t have to wait half an hour to build, init and run. Simply perform your changes (make sure you\\'ve got the file where it should be), go to your Airflow on localhost8080 and hit the play button (Trigger DAG).\",1643512731.472799,1643591455.887889,U02U5SW982W\\n59556cc6-dcc0-4d5b-a96a-dfb95616c814,U02UBQJBYHZ,,,I figured that out but it only clears one task at a time. The one I want is this:,1643590610.763829,1643591719.631779,U02UBQJBYHZ\\nc6564831-45cb-4efd-9ce5-b2c89beb39d2,U02UY1QTGHW,,,looks like for some weird reason it trying to push the data to Sejal\\'s bucket `google.api_core.exceptions.Forbidden: 403 POST *<https://storage.googleapis.com/upload/storage/v1/b/dtc_data_lake_pivotal-surfer-336713/o?uploadType=resumable>:*`,1643511546.181979,1643591823.832319,U02UY1QTGHW\\n076e8457-d9d0-4189-a4fb-de8d1764c74a,U02UBQJBYHZ,,,\"For me, when I click clear on the menu  I showed, it clears all the tasks for that particular run. To see the menu you want, click on one of the squares. But clicking clear there will only clear that single task I think.\",1643590610.763829,1643592298.120529,U02T9GHG20J\\n84789e8c-d31b-476d-b2eb-3eec7d2fd589,U02UBQJBYHZ,,,Right now I\\'m using delete.,1643590610.763829,1643592734.777059,U02UBQJBYHZ\\n7d6025d2-7617-4d72-9ccb-003c9cdb8377,U02U0U8LGHM,,,interesting. <@U02T95PEBJP> did you somehow incorporate that into one of the operators for the DAG? Or apply it on a one off for this particular month? I\\'m not sure how to take your advice and apply it :sweat_smile:,1643560330.528159,1643594114.802079,U02T9GHG20J\\n3c738357-8d57-4dd5-89a9-2ad7ee1d1ff9,,3.0,,\"Please help on how he got to this stage in the video, `Ingesting data to local Postgres with airflow`\",1643594880.204659,1643594880.204659,U02TZ7KC7U7\\n4890ee54-915d-4eff-996a-7f36301426f2,,5.0,,\"Hi All, when I run the docker-compose command for airflow_no_frills version. I have faced this error.\\n\\nit indicated that my airflow_progres doesnt have the database, airflow.\\n\\nAnyone can guide me through on this? Thanks\",1643595374.252149,1643595374.252149,U02T697HNUD\\nc0b7decd-9176-4ec3-ae72-550791cbe57d,,3.0,,\"So I have setup `Postgres` and `pgAdmin` with `docker-compose up`. I want to run now my ingestion script and it is returning me error `sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name \"\"pg-database\"\" to address: Temporary failure in name resolution`\",1643595743.667459,1643595743.667459,U02UX664K5E\\n1bed7127-e2fe-4506-ab62-af1b2ab74f60,U02UX664K5E,,,\"This is what I\\'m executing in my terminal to start the ingestion and `my docker-compose.yaml` is the same as the one built in the tutorial only that I\\'m mapping the ports as `5431:5432`\\n```URL=\"\"<https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>\"\"\\ndocker run -it \\\\\\n  --network=pg-network \\\\\\n  data_ingestion:v1 \\\\\\n  --user=root \\\\\\n  --password=root \\\\\\n  --host=pg-database \\\\\\n  --port=5431 \\\\\\n  --db=ny_taxi \\\\\\n  --table_name=yellow_taxi_trips \\\\\\n  --url=${URL}```\",1643595743.667459,1643595823.803209,U02UX664K5E\\nf2efde04-901f-4cec-89e4-f7631a762f1a,,1.0,,\"Looking back week 1, I just need reminded of some things. Regarding the folder `my_taxi_posgres_data` - I believe this contains our postgres files (copied from a folder on our container).\\n\\nSo does this folder contain our database, table data, and so on? And also, is it possible to open up and query our tables, even when not running our postgresql docker container (using this folder)?\\n\\nI’m getting myself a bit muddled trying to understand networks, ports etc alongside containers.\\n\\nAlso - in a production environment, would the container for airflow be left running continuously on a server or VM somewhere?\",1643596143.886909,1643596143.886909,U02U34YJ8C8\\nff2e5c89-4144-41b8-9435-e898aa26b142,U02U5SW982W,,,\"<@U02U5SW982W> woo ur learning journey/ Notes is so long and detailed! I believe having a table of content would help to navigate around, great work!\",1643583593.701639,1643596409.479499,U02AX5NC5B6\\n7FDAF76F-9C59-4113-9BE0-A32F1714B0B4,U02UX664K5E,,,\"You probably want to change the network. \\n\\nType docker network ls to see the list of networks. There should be a default network created when you ran docker compose that both pgadmin and Postgres will be part of. Use that one in place of pg-network. \\n\\nAlso probably need to replace pg-database with pgdatabase (assuming this is the service name you gave Postgres in the Yaml file)\",1643595743.667459,1643596626.264389,U02U34YJ8C8\\na04c0781-77c5-4886-904d-f7477c42d628,,13.0,,\"For the week 2 homework,  I have few questions want to ask.\\n\\n1. question 1 &amp; 2 all the data (2019-2020) we need to insert into 1 GBQ table? or separate by month\\n2. question 1 &amp; 2 we only can use 1 dags job to run all the ingestion into GBQ? or can have the multiple dags job\",1643596998.639849,1643596998.639849,U02T697HNUD\\n16bb4ab4-aaff-4665-8538-13e3f3d8466b,U02TZ7KC7U7,,,This is the Dockerfile inside the airflow folder in Github,1643594880.204659,1643597617.425269,U02T697HNUD\\na9321050-68e5-42d3-94c1-0dc64dd93a2f,,6.0,,\"For me, the local data ingestion (video 2.3.3) is taking 10 minutes to load each chunk. airflow has downloaded all the files, apparently, but the process of putting them in the database is taking forever. I can see the tables in the database, but some of them stopped filling and the ingest task instance failed. Is this expected? Is it just too much data to manage locally?\",1643598603.077489,1643598603.077489,U02UBQJBYHZ\\n454a779f-8793-4ce4-9038-c6560c416351,U02UX664K5E,,,Thanks <@U02U34YJ8C8> for pointing me to the right direction. The network was wrong as was the host. I can go now sleep in peace!,1643595743.667459,1643599099.708249,U02UX664K5E\\n7cc3821e-4b08-4db2-8d58-df10d91f62c3,U02UBQJBYHZ,,,Hi <@U02UBQJBYHZ> do you have enough resources available to perform these tasks? Are you getting any useful error messages?,1643598603.077489,1643600977.195859,U02U5SW982W\\n5daefd9c-e634-40a5-a445-bd65aada5e5f,U02T697HNUD,,,Hi <@U02T697HNUD> my understanding is that:,1643596998.639849,1643601984.311189,U02U5SW982W\\n33e2f532-5be7-483b-b548-b5f44852eb99,U02T697HNUD,,,1. we need to insert into 1 table,1643596998.639849,1643602001.851409,U02U5SW982W\\nb9e7c3a8-87e7-4b83-83ad-f2860a265bdb,U02T697HNUD,,,2. we only need 1 dag job. This is the job we wrote before but we will modify this same job,1643596998.639849,1643602033.649819,U02U5SW982W\\n84f61562-7d38-4b30-b205-7aaa709f7339,U02TZ7KC7U7,,,\"In the video, he was running the dockerfile with VM but editing its location files in VS code, house did he make the connection?\",1643594880.204659,1643603280.032869,U02TZ7KC7U7\\n381f64fb-ca04-48f9-ab28-a1bdfc1b3bc9,U02U5SW982W,,,Thanks <@U02AX5NC5B6> ! I will try and put in a TOC to help with that - once I figure out how to do it of course :slightly_smiling_face: Thanks for the suggestion.,1643583593.701639,1643606272.087049,U02U5SW982W\\nac6903c7-a83c-44c6-8922-fc96834d42b4,U02SFFC9FPC,,,Sorry <@U02SFFC9FPC> I only just saw this. I think others have a similar problem depending on where they are in the world. I\\'m in Australia and I didn\\'t have a problem with this. Just wondering if this might be your problem? Have you had any luck at all in the last few days figuring this out?,1643237452.453700,1643606520.186519,U02U5SW982W\\n212da495-c9a3-4f87-9bb2-220ebb4a55fc,U02UE7NTLUU,,,Hi <@U02V5EP18PQ> sorry I missed your message but glad you figured out your problem :raised_hands: [I\\'m running Ubuntu 20.04 btw],1642465146.363400,1643606681.737469,U02U5SW982W\\n72174a1c-a351-49cd-abca-5f7f9b6ce1e9,,3.0,,Hello please i need help. i cant get my parameterised dag to pick the 2021 data. it defaults to todays date and year 2022 .,1643606875.920119,1643606875.920119,U02U5G0EKEH\\n3bc74f0f-a014-4cf8-afde-aea195c8d0a4,,2.0,,cc <@U01AXE0P5M3>,1643606923.092969,1643606923.092969,U02U5G0EKEH\\n36d5f64c-df1a-4c30-981e-a4ec255708de,U02U34YJ8C8,,,\"Hi <@U02U34YJ8C8> sorry I only saw this now. I\\'m trying to get to grips with how everything works including Slack which I haven\\'t used before. I\\'ve taken your advice and fired up a Blog where I take notes (essentially transcripts from the videos), as well as record my own walkthroughs and thought processes. I feel like I\\'ve got to write this down for myself and record it somewhere because I\\'ve got the memory of your average chimp. Also, because it\\'s pretty \\'dense\\' as <@U02BVP1QTQF> says. So to all thanks for sharing and also the advice. And yes to <@U02GVGA5F9Q> as <@U02BVP1QTQF> says I believe your skills with Linux and Open Source tools will only serve to help you.\",1642810460.254700,1643607159.905519,U02U5SW982W\\nd72b159f-76a8-493c-b565-5914c677d943,U02V4412XFA,,,<@U01AXE0P5M3> I am passing it through .env file,1643570058.391529,1643609021.116619,U02V4412XFA\\n84032880-5f27-484a-ab28-c143f3834e5b,U02T697HNUD,,,\"<@U02T697HNUD> <@U02U5SW982W> As I understood, we need separate DAGs for each of the objects. 1 for yellow, 1 for fhv, 1 for zones.\\n\\nThere is no need to insert in GCP case. I created an external tables. Each of external tables looks at its own folder in GCS (all the data loaded to separate folders)\",1643596998.639849,1643612027.785969,U02T65GT78W\\n8c634a97-df3a-45b4-ac9e-75a846e48d07,,1.0,,\"what does \"\"setup-nofrills mean?, should i ignore it and use \"\"setup-official\"\"?\",1643612744.884929,1643612744.884929,U02RTJPV6TZ\\n1f2fef25-1922-4b5b-8167-645831880e87,U02UBQJBYHZ,,,\"So if you click on a circle, it\\'ll clear the entire DAG run. If on a square- only a task and all dependencies\",1643590610.763829,1643612786.098579,U01AXE0P5M3\\nf260262e-1252-445d-bcb6-3cbb754aa68d,U02U5G0EKEH,,,can you set `catchup=True` in DAG arguments and try?,1643606875.920119,1643612899.320649,U02TATJKLHG\\nb6bac340-7e3e-48c1-893a-0f1aa4f0dc64,,2.0,,\"Hi all, was following alex’s 2.3.3 ingesting data to local PG, however when i try to run docker compose up on week1\\'s yml, i’m getting `PANIC:  could not locate a valid checkpoint record` error. Full error in thread. I don’t think this is same issue as someone who raised it here on 28th Jan.\\n\\nto add on, I was able to successfully ingest few months of data. Then i went to drop all of them on pgAdmin as i wanted to re-try whole process again. That’s when issue starts to arise. Any one could guide please? thanks!\",1643612900.649649,1643612900.649649,U02ULMHKBQT\\n856fff0b-b0ba-45cb-a4f2-b2fcf6b2ffa6,U02TZ7KC7U7,,,I use ssh remote extension: <https://code.visualstudio.com/docs/remote/ssh|https://code.visualstudio.com/docs/remote/ssh>,1643594880.204659,1643612906.496719,U01AXE0P5M3\\nccf6106b-3443-4e81-bc62-4f824c5ea939,U02ULMHKBQT,,,\"docker-compose up                                                                                                                                         \\ue73c data-eng 03:05:12 pm\\nCreating week_1_pgdatabase_1 ... done\\nAttaching to week_1_pgdatabase_1\\npgdatabase_1  |\\npgdatabase_1  | PostgreSQL Database directory appears to contain a database; Skipping initialization\\npgdatabase_1  |\\npgdatabase_1  | 2022-01-31 07:05:16.943 UTC [1] LOG:  starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\\npgdatabase_1  | 2022-01-31 07:05:16.943 UTC [1] LOG:  listening on IPv4 address “0.0.0.0”, port 5432\\npgdatabase_1  | 2022-01-31 07:05:16.943 UTC [1] LOG:  listening on IPv6 address “::“, port 5432\\npgdatabase_1  | 2022-01-31 07:05:16.947 UTC [1] LOG:  listening on Unix socket “/var/run/postgresql/.s.PGSQL.5432”\\npgdatabase_1  | 2022-01-31 07:05:16.969 UTC [27] LOG:  database system was interrupted; last known up at 2022-01-28 08:26:18 UTC\\npgdatabase_1  | 2022-01-31 07:05:19.391 UTC [27] LOG:  invalid primary checkpoint record\\npgdatabase_1  | 2022-01-31 07:05:19.391 UTC [27] PANIC:  could not locate a valid checkpoint record\\npgdatabase_1  | 2022-01-31 07:05:19.392 UTC [1] LOG:  startup process (PID 27) was terminated by signal 6: Aborted\\npgdatabase_1  | 2022-01-31 07:05:19.392 UTC [1] LOG:  aborting startup due to startup process failure\\npgdatabase_1  | 2022-01-31 07:05:19.396 UTC [1] LOG:  database system is shut down\\nweek_1_pgdatabase_1 exited with code 1\",1643612900.649649,1643612933.830809,U02ULMHKBQT\\n57defe41-3f74-4b76-88a4-2b390e5dbb8d,U02T697HNUD,,,I have the same error when running it on windows (and a bunch of other errors). I don\\'t know how to solve it. So for windows I\\'m still using the other compose,1643595374.252149,1643613007.407549,U01AXE0P5M3\\ne8e9213b-4b6d-47a1-8ff5-b502d950005c,U02T697HNUD,,,Hi <@U02T65GT78W> and <@U02T697HNUD> but I think the question  <@U02T697HNUD> asked was related only to Question 1 and 2 of the homework. To complete this part of the homework the intention was to only have one single external table right? And after reading the advice just on these two questions (just underneath them) we should only have one DAG file? I\\'m really not too sure - and to be clear I haven\\'t yet braced myself for the 1hr 20min video by Alexey for this week so some hints might also be in there. Possibly?,1643596998.639849,1643613197.743669,U02U5SW982W\\n2e4380ac-2238-44c2-83f9-f821520a31d2,U02U34YJ8C8,,,\"Yes this folder contains files that postgres created, but you can only read the content with postgres. So you need postgres running. (I hope I understand the question correctly)\\n\\nYes, airflow needs to be running all the time on some server. Quite often it\\'s running on Kubernetes, but could be anything\",1643596143.886909,1643613300.186169,U01AXE0P5M3\\n7863eb67-8ce2-485a-b737-394747954cd1,U02T697HNUD,,,\"In that video I show how to pass params to tasks, so yes, in a way there are hints there. And in my solution I use quite a lot of code from that long video - I use it to modify the DAG that Sejal created \\n\\nYou indeed don\\'t need to create a table, we\\'ll create a table in week 3 for all these files at the same time \\n\\nAnd finally like Eugene said, you need a separate dag for each type of taxi\",1643596998.639849,1643613521.002359,U01AXE0P5M3\\nde8dab07-2eb3-4012-b62c-8114476e4018,U02UBQJBYHZ,,,How many tasks are you running in parallel?,1643598603.077489,1643613613.969839,U01AXE0P5M3\\n052524ac-87f5-43dc-9420-bb9aaf9a6fe6,U02UBQJBYHZ,,,\"Postgres shouldn\\'t fail because of volume, it\\'s quite good at dealing with large datasets. But maybe the insertion strategy isn\\'t optimal\",1643598603.077489,1643613728.822669,U01AXE0P5M3\\n1a60f735-b499-47b7-a654-9bba59ae6895,U02RTJPV6TZ,,,\"If it works for you, then it\\'s better because it\\'s more lightweight than the official\",1643612744.884929,1643614244.047579,U01AXE0P5M3\\n528f01a2-dd6e-41bf-9d3e-8523ab1f2193,,1.0,,\"Hi all,\\nDo you have any solution to run docker-compose when a VPN is connected? It seems it cannot access the network.\",1643614481.149069,1643614481.149069,U030FNZC26L\\n92efbc31-ca99-4339-81c9-b49c14bb829c,U02UY1QTGHW,,,have you changed the env variables in the docker compose file?,1643511546.181979,1643614797.786829,U01AXE0P5M3\\ndb816a9d-d0ba-450e-a072-6197744ab95e,U02SMBGHBUN,,,honestly I don\\'t know how to do it :sweat_smile:,1643522160.532659,1643614868.978309,U01AXE0P5M3\\n4ce4a68f-b9ae-487c-b6aa-8bba3327b8a7,U030FNZC26L,,,\"Found the solution here:\\n<https://stackoverflow.com/questions/45692255/how-make-openvpn-work-with-docker>\",1643614481.149069,1643615389.537149,U030FNZC26L\\nf483efb4-3ab6-45eb-a6b2-165178605f62,,5.0,,\"Unfortunately, I am stuck with a permission issue when trying to run postgres:13 container. I am running docker from wsl on windows10 machine. I understood this is because of the way the postgres docker image is constructed. However, I am not sure what is the correct way to handle this issue. Can anyone who faced this issue help me? (Another suggestion I have is to document most commonly occuring problems in an issues page on the repo itself.)\\n```$ docker run -it\\\\\\n  -e POSTGRES_USER=\"\"root\"\" \\\\\\n  -e POSTGRES_PASSWORD=\"\"admin\"\" \\\\\\n  -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n  -v \"\"/mnt/c/Users/mb/Projects/data-engineering-zoomcamp/learning_artifacts/week1/dockerized_postgres/ny_taxi_postgres_data\"\":\"\"/var/lib/postgresql/data\"\" \\\\\\n  -p 5431:5432 \\\\\\n  postgres:13\\n\\n# Output\\nThe files belonging to this database system will be owned by user \"\"postgres\"\".\\nThis user must also own the server process.\\n\\nThe database cluster will be initialized with locale \"\"en_US.utf8\"\".\\nThe default database encoding has accordingly been set to \"\"UTF8\"\".\\nThe default text search configuration will be set to \"\"english\"\".\\n\\nData page checksums are disabled.\\n\\nfixing permissions on existing directory /var/lib/postgresql/data ... initdb: error: could not change permissions of directory \"\"/var/lib/postgresql/data\"\": Operation not permitted```\",1643615516.282529,1643615516.282529,U030HKR0WK0\\nfbf993de-33c0-4d15-a921-336b301bc4df,U030HKR0WK0,,,\"try deleting this folder and let docker create it\\n\\nby the way, there\\'s a document with FAQ - <https://docs.google.com/document/d/19bnYs80DwuUimHM65UV3sylsCn2j1vziPOwzBwQrebw/edit?usp=sharing>\\n\\nif your issue is not there, we\\'ll appreciate if you document the solution when you figure it out\",1643615516.282529,1643615793.186709,U01AXE0P5M3\\nbe591eb3-c834-4321-8971-2b544cae04f2,,2.0,,\"Hello Everyone,  My name is Temilade\\nI\\'m new here, where do I start from?\\nI\\'ll appreciate a prompt response, Thanks.\",1643615860.647929,1643615860.647929,U0316CVHMPT\\n15d51f7c-c0d8-47af-91d6-9de435ebd9ef,U0316CVHMPT,,,\"Hi Temilade! There\\'s a link to the course repo on the top, you can start with it - find week 1 and start there\",1643615860.647929,1643615933.483409,U01AXE0P5M3\\naa1148bc-4ed0-4b40-b555-b25108e73f22,,5.0,,\"```.<http://iam.gserviceaccount.com|iam.gserviceaccount.com> does not have storage.objects.create access to the Google Cloud Storage object.\"\",```\\ndid anyone come across this error?\",1643875265.795319,1643875265.795319,U02U6DR551B\\nc00ab4dd-8a35-423c-a8f1-05d97ea6034e,U02U6DR551B,,,\"you need to assign admin access to your service account to the GCS service you are directing the data to, it was explained in one of the weekly videos\",1643875265.795319,1643875770.251429,U02UA0EEHA8\\nf550fc76-ebdc-4700-9764-3f648df2016c,U02U6DR551B,,,<https://stackoverflow.com/questions/57268540/what-is-the-difference-between-backfill-and-catchup-in-airflow>,1643873524.132099,1643875821.090039,U02UA0EEHA8\\n13d69bbd-887b-4919-acf3-a3d4cfc873ad,U02RTJPV6TZ,,,\"```[2022-02-03, 08:13:48 UTC] {logging_mixin.py:109} INFO - Running &lt;TaskInstance: data_ingestion_gcs_dag.bigquery_external_table_task scheduled__2022-01-01T12:00:00+00:00 [running]&gt; on host 0bd4e02e214b\\n[2022-02-03, 08:13:48 UTC] {warnings.py:110} WARNING - /home/***/.local/lib/python3.7/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing \\'execution_date\\' from the template is deprecated and will be removed in a future version. Please use \\'data_interval_start\\' or \\'logical_date\\' instead.\\n  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))\\n\\n[2022-02-03, 08:13:48 UTC] {taskinstance.py:1426} INFO - Exporting the following env vars:\\nAIRFLOW_CTX_DAG_OWNER=***\\nAIRFLOW_CTX_DAG_ID=data_ingestion_gcs_dag\\nAIRFLOW_CTX_TASK_ID=bigquery_external_table_task\\nAIRFLOW_CTX_EXECUTION_DATE=2022-01-01T12:00:00+00:00\\nAIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-01-01T12:00:00+00:00\\n[2022-02-03, 08:13:48 UTC] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.\\n[2022-02-03, 08:13:49 UTC] {taskinstance.py:1700} ERROR - Task failed with exception\\n raise exceptions.from_http_response(response)\\ngoogle.api_core.exceptions.BadRequest: 400 POST <https://bigquery.googleapis.com/bigquery/v2/projects/data-zoomcamp-338514/datasets/trips_data_all/tables?prettyPrint=false>: Error while reading table: external_table, error message: Failed to expand table external_table with file pattern <gs://dtc_data_lake_data-zoomcamp-338514/raw//opt/airflow/src_file_2022-01.parquet>: matched no files.\\n[2022-02-03, 08:13:49 UTC] {taskinstance.py:1277} INFO - Marking task as UP_FOR_RETRY. dag_id=data_ingestion_gcs_dag, task_id=bigquery_external_table_task, execution_date=20220101T120000, start_date=20220203T081348, end_date=20220203T081349\\n[2022-02-03, 08:13:49 UTC] {standard_task_runner.py:92} ERROR - Failed to execute job 26 for task bigquery_external_table_task\\nTraceback (most recent call last):```\\n*<@U01AXE0P5M3>* *<@U01DHB2HS3X> so sorry for disturbing but my last task of bigquery still failing kindly help me make sense out this*:pray:\",1643790128.790069,1643876422.503399,U02RTJPV6TZ\\nfe2a1ee9-6bf2-4f55-9f57-a9264ecac4e2,U02U6DR551B,,,i did. but still getting this error,1643875265.795319,1643876893.790489,U02U6DR551B\\nc59e52a5-ff9a-473b-bb57-7fe7f4dec3c3,U02U6DR551B,,,Maybe try to download the credentials file again - just to make sure you have the correct one,1643875265.795319,1643877498.113179,U01AXE0P5M3\\n79b74bfd-e452-4d51-adb2-88d198e2139a,U030HKR0WK0,,,Perhaps I should make another video explaining this error and saying that there\\'s nothing to worry about,1643868570.240969,1643877561.340319,U01AXE0P5M3\\n3ebccd2f-3a0a-448a-a204-96f90d48055c,U02U6DR551B,,,sure,1643875265.795319,1643878381.450279,U02U6DR551B\\n44177469-730a-43c7-ab1e-776bdd6a9c11,,10.0,,\"how the external table is created in bigquery? I am getting an error on it\\n```Error while reading table: external_table, error message: Failed to expand table external_table with file pattern <gs://dtc_data_lake_zoomcamp-de-339322/raw//output_yellowtaxi_2022-01.parquet>: matched no files```\\nAlthough i do have a file in my data lake\",1643878397.499529,1643878397.499529,U02U6DR551B\\nd72c11e1-f0be-4547-b1bd-e1bc2e008f01,U030F0YHDAM,,,There is nothing. Logs empty...,1643848524.574759,1643880330.334319,U030F0YHDAM\\n44c20d12-f432-4d03-bb3c-3c0f8c3e021d,U030HKR0WK0,,,My question is not that there is an error. I logged into pgadmin and checked this. I am wondering if there is a best practice to ensure or verify if all the data rows have been correctly inserted into the table as intended?,1643868570.240969,1643880876.307319,U030HKR0WK0\\n30fafbba-d774-44fa-8769-5db0f63bee5b,U030HKR0WK0,,,Looking at the counts is probably the way to go,1643868570.240969,1643881499.707009,U01AXE0P5M3\\n1D84ACE9-428B-4D15-A2B6-142EC35EF7DF,U02U34YJ8C8,,,\"It was stored on the VM running Docker, whereas I\\'d been looking for it on my local file system \",1643840028.483199,1643881638.935999,U02U34YJ8C8\\nabfcb5ec-0acd-458a-8452-a9942e9f69cc,U02U6DR551B,,,I saw someone had passed the wrong project/bucket name and had a similar error. Make sure you\\'ve changed that. From the ones in the git repo to your own project and bucket,1643875265.795319,1643881920.918509,U02TATJKLHG\\n740ec70a-a6c0-49af-9e4a-4c634e840160,U02U6DR551B,,,How did you get the path? Click on the three dots on the right side of the files list and copy the gsutil uri,1643878397.499529,1643882005.857049,U02TATJKLHG\\nd9bb0767-df47-4c80-b252-692712be1e89,U02RTJPV6TZ,,,There\\'s an extra `/` between the `raw` directory and the AIRFLOW_HOME directory (`/opt/airflow`),1643790128.790069,1643882577.352919,U01DHB2HS3X\\n5e658cac-1c5c-41a5-9e8e-ab623d7d23c9,U02RTJPV6TZ,,,\"You have two slashes in the path, maybe that\\'s the reason\",1643790128.790069,1643882580.106929,U01AXE0P5M3\\n1e67de88-99eb-4a2c-81d5-0e89dbb4875a,U02RTJPV6TZ,,,Hehe Sejal was first  :stuck_out_tongue_winking_eye:,1643790128.790069,1643882596.554509,U01AXE0P5M3\\n45790dba-e674-4537-acba-297f70443187,U02RTJPV6TZ,,,\"Also you probably don\\'t need you airflow home directory part in the path, only the filename\",1643790128.790069,1643882664.625549,U01AXE0P5M3\\nc2aa1efb-dddb-4f2a-927f-b079fe9a7458,U02U6DR551B,,,i did,1643878397.499529,1643882927.259539,U02U6DR551B\\n12cb1324-e3c2-487f-8193-537a179be382,U02U6DR551B,,,\"So from what I understood after reading the documentation is let\\'s say you pass your start date as `2019-01-01` and you set `catchup=True` then the DAG will start it\\'s interval from `2019-01-01`.\\n\\nBut what if after all the runs you realized you wanted to start it from `2018-01-01`, or you wanted data just for January 2018, backfill comes to your rescue. You can trigger your DAG with an interval that contains historical dates like this\\n\\n```airflow dags backfill \\\\\\n    --start-date 2018-01-01 \\\\\\n    --end-date 2018-01-31 \\\\\\n    dag_id```\\nSo in addition to 2019 and onwards, you also now have the data for January 2018. Which I think is not really possible with catchup\",1643873524.132099,1643882928.714419,U02TATJKLHG\\ncf7341da-62fc-4737-907a-8ce23328bf49,U02U6DR551B,,,it was the same path,1643878397.499529,1643882934.168939,U02U6DR551B\\n1075bea9-fd79-48f1-b6ee-b6bd2c08876b,U02U6DR551B,,,is the external table in big query created automatically,1643878397.499529,1643883041.649739,U02U6DR551B\\n558b88a6-25f9-4f4a-af95-8df274270e1a,U02U6DR551B,,,?,1643878397.499529,1643883042.375469,U02U6DR551B\\ncd298c41-8463-483a-8fd4-1775fe52b377,U02U6DR551B,,,You have to create the external table with the `CREATE OR REPLACE EXTERNAL TABLE` command. Can you paste your code here?,1643878397.499529,1643883167.282879,U02TATJKLHG\\n5c99a0fa-2990-4426-88dc-139ec34f5d52,,7.0,,\"Hello, please in week 2, I keep getting the error of google credentials no found and I have changed the path severally and I dont why it cant find it.\\n\\nThe error:\\n```google.auth.exceptions.DefaultCredentialsError: File /d:/Documents/Learning/ZoomCamp/.google/credentials/google_credentials.json was not found.```\\n\",1643883268.799159,1643883268.799159,U02QS4BD1NF\\nb4219a98-6122-473b-9e51-f93fb8eef513,U02QS4BD1NF,,,Put the absolute path in the compose file,1643883268.799159,1643883306.038879,U01AXE0P5M3\\naa356309-2d18-441e-850a-f3df02561ad7,U02QS4BD1NF,,,Also why is it d: in the container? Please don\\'t change the part after the : in the config,1643883268.799159,1643883343.331879,U01AXE0P5M3\\nb2f8631a-ba6b-43ab-b71f-c3869c6dade1,U02QS4BD1NF,,,\"I put the absolute path, I only changed the slash from back to forward\",1643883268.799159,1643883478.091009,U02QS4BD1NF\\n18fcdd9d-9494-4b89-8ba9-07fd4a3fb27f,U02QS4BD1NF,,,\"`D:\\\\Documents\\\\Learning\\\\ZoomCamp\\\\.google\\\\credentials\\\\google_credentials.json`\\n\\nThis is the absolute path and it still gave the same error of not being found\",1643883268.799159,1643884058.235809,U02QS4BD1NF\\n3b5a33cf-0836-48b7-bf79-13c8daa3b254,U02QS4BD1NF,,,\"The way it is in the docker-compose:\\n`GOOGLE_APPLICATION_CREDENTIALS: D:\\\\Documents\\\\Learning\\\\ZoomCamp\\\\.google\\\\credentials\\\\google_credentials.json`\\n\\n\\xa0 \\xa0 `AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT: \\'google-cloud-platform://?extra__google_cloud_platform__key_path=D:\\\\Documents\\\\Learning\\\\ZoomCamp\\\\.google\\\\credentials\\\\google_credentials.json\\'`\",1643883268.799159,1643884216.102659,U02QS4BD1NF\\ne273ec64-b78f-4152-b503-ff252170c4ce,U02U6DR551B,,,i am not sure about this step. How do i create external table?,1643878397.499529,1643884252.383469,U02U6DR551B\\n447ae4d7-4df3-4a92-b946-1b59e3c983ad,U02QS4BD1NF,,,\"I guess you are looking for the path to the credentials files in your local machine from docker. Docker doesn\\'t have a `D:\\\\Documents\\\\Learning\\\\ZoomCamp\\\\` path. It just has the `/.google/credentials/`  since you mounted that in the volume.\\n\\nAll you have to do is this -\\n\\n```GOOGLE_APPLICATION_CREDENTIALS: /.google/credentials/google_credentials.json\\nAIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT: \\'google-cloud-platform://?extra__google_cloud_platform__key_path=/.google/credentials/google_credentials.json\\'```\\nSince you would want to pass the path to the creds folder in the docker container and not your local path here\",1643883268.799159,1643884546.509019,U02TATJKLHG\\nfdaff1f2-bda1-4055-898d-870de196328e,U02U6DR551B,,,\"You can check the video by Ankush, he creates the external table there. Or the syntax would look something like this\\n\\n```CREATE OR REPLACE EXTERNAL TABLE `table_name`\\nOPTIONS (\\n    format=\\'parquet\\',\\n    uris=[\\'add/your/gsutil/path/here/fhv_tripdata_2019-*.parquet\\']\\n);```\",1643878397.499529,1643884732.757549,U02TATJKLHG\\n65e59252-6ce5-440f-8f10-d012c7d76b5a,,2.0,,\"if I\\'m running airflow on a GCP VM instance, do I still need to have my gcloud credentials available on the instance to connect to gcloud buckets ?\",1643885749.441099,1643885749.441099,U02TMP4GJEM\\n1709d446-bc94-4bdf-8105-a10fac2c17ad,U02RTJPV6TZ,,,\"```    raise exceptions.from_http_response(response)\\ngoogle.api_core.exceptions.BadRequest: 400 POST <https://bigquery.googleapis.com/bigquery/v2/projects/data-zoomcamp-338514/datasets/trips_data_all/tables?prettyPrint=false>: Error while reading table: external_table, error message: Failed to expand table external_table with file pattern <gs://dtc_data_lake_data-zoomcamp-338514/raaw//opt/airflow/src_file_2022-01.parquet>: matched no files.\\n[2022-02-03, 11:03:38 UTC] {local_task_job.py:154} INFO - Task exited with return code 1```\\nA folder i specified as  *\"\"raaw\"\"* gets created in my bucket but there are no files , then i get the above error what could issue which gcp seeting do i need to change, *i read about error 400*, but not sure how to handle, anyone with ideas <#C01FABYF2RG|course-data-engineering>\",1643790128.790069,1643886617.495679,U02RTJPV6TZ\\n94e3f2e7-602e-47e5-a8f0-667e838b3890,U02RTJPV6TZ,,,You still have // in the path,1643790128.790069,1643887306.968459,U01AXE0P5M3\\nf37b69fd-9cd2-4937-8f80-cedb2b2bbfe6,U02QS4BD1NF,,,\"Thank you, it worked.\\nI forgot we bound the path on the local machine to docker and still put the local path in the docker-compose file\",1643883268.799159,1643887328.155499,U02QS4BD1NF\\n016b759e-75b8-49a3-a82c-917147ced333,,7.0,,\"Hello everyone, I am trying to reduce code duplication as I am using the same functions with different parameters over tree dag files. But I getting a ModuleNotFound error in Airflow UI, is someone experienced sth similar. I will send my directory structure as a comment to the thread.\",1643887552.482999,1643887552.482999,U02DD97G6D6\\ndb66315b-56f8-4822-ab82-99b756f945cd,U02DD97G6D6,,,\"By directory structure is like that:\\n\\nweek1_basic_setup/\\nweek2_data_ingestion/\\n    ◦ airflow/\\n        ▪︎ __init__.py\\n        ▪︎ dags/\\n            • __init__.py\\n            • dag1.py\\n            • dag2.py\\n        ▪︎ logs/\\n        ▪︎ plugins/\\n        ▪︎ workflow_utils/\\n            • __init__.py\\n            • utils.py\\xa0\\n    ◦ transfer_service/\\n    ◦ __init__.py\",1643887552.482999,1643887861.890339,U02DD97G6D6\\n0d6ca066-d156-43cc-8e06-edc0a387b989,U02DD97G6D6,,,I have also seen that <https://airflow.apache.org/docs/apache-airflow/stable/modules_management.html> but not sure what I am missing.,1643887552.482999,1643887976.202249,U02DD97G6D6\\n6fa84a3b-d845-4303-8688-7a49b1cc9a31,U02TMP4GJEM,,,\"I guess you do. The credentials are a way for the bucket to -\\n1. Authenticate the user\\n2. Check if the user has the right permissions\\nSo they might be necessary for the cloud storage to verify and allow the program to consume data from it.\\nI don\\'t think the underlying system can act as an authenticator. But I am just speculating. Someone can correct if that\\'s not true.\",1643885749.441099,1643888332.996309,U02TATJKLHG\\n67206f1f-b404-42c0-98fb-df5469fa3efd,,1.0,,\"Hello <@U01DHB2HS3X>\\nI realised that the parquet file is significantly smaller than the csv file. Does the parquet format do some sort of compression on the csv?\",1643888499.675599,1643888499.675599,U02QS4BD1NF\\na9b1ac79-29b0-4eee-bb2b-ffe2173c26c8,U02QS4BD1NF,,,\"I\\'ll be sharing a Best Practices doc with that info soon as Extras. But yes, that\\'s the main highlight of using Parquet format, and saving storage space and costs on Cloud\",1643888499.675599,1643888690.667219,U01DHB2HS3X\\n41427ed2-d402-41a8-a896-0a090914cf71,,2.0,,\"Hello! Please advise me. Do I understand correctly that if my laptop is &lt; 8GB of RAM, then starting from the second week it is better to use the de-zoomcamp vm (which was created by DE Zoomcamp 1.4.1) and take the rest of the course on it?\",1643888814.967529,1643888814.967529,U02RC9GPNG0\\n20ed8f59-8686-48ff-8c67-5cf8216d2776,U02U6DR551B,,,\"Instead of trigger the DAG, you can enable the DAG on top left pane. Because if you trigger the DAG, the next run must be the current datetime\",1643873524.132099,1643888845.886529,U02T697HNUD\\nd20389fb-e894-43d1-93b1-74b76f945d60,U02DD97G6D6,,,\"And seems like moving workflow_utils into plugins/ folder worked well with\\n```from workflow_utils.utils import prepare_filename, format_to_parquet, upload_to_gcs```\\nBut is it the best way to solve it, I found it a bit strange.\",1643887552.482999,1643889218.252399,U02DD97G6D6\\n743381b7-713c-42c7-a1fa-a89479dbc94b,U02RC9GPNG0,,,I think it\\'s better to run on VM even if your RAM size is more that 8GB. You\\'ll be downloading ~30 files adding up to GBs of data. The file download from local is extremely slow and add to that the size of the files. On VM it\\'ll be done in seconds/minutes.,1643888814.967529,1643890317.631729,U02TATJKLHG\\n89c769cb-36d6-443e-bb41-7c701b28fc98,U02RC9GPNG0,,,\"Thanks, Ankur!\",1643888814.967529,1643890583.340369,U02RC9GPNG0\\n07cad3d4-ab84-41a1-82ec-481355fc9039,U030F0YHDAM,,,\"Might be because you’ve set `start_date` twice.\\n\\nTry removing `\"\"start_date\"\": days_ago(1)`  from your script.\",1643848524.574759,1643891001.943339,U02U34YJ8C8\\n9f43b700-36c0-48f8-89ca-e36a9b894dc8,,3.0,,\"Hey, good day everyone, I ran into an with the week 1 class 1.2.2 video why trying to connect ny_taxi_postgres_data. \\n\\nRunning the red circled code in my Git bash terminal gives me the error in the next picture; rocker: Error response from daemon: invalid mode: \\\\Program Files\\\\Git\\\\var\\\\lib\\\\postgresql\\\\data.\\n\\nAny help would be appreciated.\",1643893753.858269,1643893753.858269,U02V2G4R796\\n02cf3344-c3b9-4291-b27e-8a9092d5fc4a,U02RTJPV6TZ,,,\"```   raise exceptions.from_http_response(response)\\ngoogle.api_core.exceptions.BadRequest: 400 POST <https://bigquery.googleapis.com/bigquery/v2/projects/data-zoomcamp-338514/datasets/trips_data_all/tables?prettyPrint=false>: Error while reading table: external_table, error message: Failed to expand table external_table with file pattern <gs://dtc_data_lake_data-zoomcamp-338514/raaw/opt/airflow/src_file_2022-01.parquet>: matched no files.```\\n An extra / was resolved but the  error persists\",1643790128.790069,1643893833.672799,U02RTJPV6TZ\\ne391fac9-6d12-4133-9feb-d510a49e9121,U02RTJPV6TZ,,,Do you see these files in your buckets?,1643790128.790069,1643893956.463589,U01AXE0P5M3\\nefc10fa5-639e-48c6-83dc-231c66400399,U02V2G4R796,,,\"If you check the FAQ, you\\'d find your answer :smile:\\n<https://docs.google.com/document/d/19bnYs80DwuUimHM65UV3sylsCn2j1vziPOwzBwQrebw/edit#heading=h.aqmhas6swn46>\",1643893753.858269,1643893997.999759,U02TATJKLHG\\nc654c9e5-b21d-46f3-a995-28b1ade4de0b,U02RTJPV6TZ,,,the folders are created but the files are not,1643790128.790069,1643894086.935669,U02RTJPV6TZ\\n2BD06B99-203C-46DF-87C9-3403CFBBF29F,U02TATJKLHG,,,\"Hi Ankur, without giving too much the answer should be part of the options. In case you don\\'t see any matching options please take a look at the data source \",1643795559.794799,1643894156.451239,U01DFQ82AK1\\na038308a-e55f-457a-ad17-751a5299e7bd,U02RTJPV6TZ,,,I guess that\\'s what you need to fix,1643790128.790069,1643894435.227029,U01AXE0P5M3\\nac24056d-6afb-4d02-ac45-31e26e79b48b,U02V2G4R796,,,Use //c instead of c:,1643893753.858269,1643895082.270239,U01MFQW46BE\\n3f462f21-6567-4cd0-b856-9e429ee9fb3c,U02TATJKLHG,,,\"Had to adjust the query logic to get the answer, will get more clarity when the solution is out. Thanks :smile:\",1643795559.794799,1643895236.598169,U02TATJKLHG\\n24d6358d-f279-4d4e-aabb-6c887275e649,U02Q7JMT9P1,,,\"hi llia, do you mind sharing how exactly did u resolve the `Negsignal.SIGKILL` issue, or point me to any resource that guide how we can process csv to parquet in chunks? thanks!\",1643716410.018639,1643896384.540879,U02ULMHKBQT\\nca136ba5-4ba5-4452-a0ae-4909cd113589,,2.0,,\"Hi all, regarding week2 homework qn 4 - could i just confirm that meaning of “Create the final DAG - for Zones” refers to ingesting the `Taxi Zone Lookup Table` csv file ? Asking as the description for the question in both markdown and google form isn’t too clear. thanks!\",1643898674.332069,1643898674.332069,U02ULMHKBQT\\n6972c1da-aeff-48fd-8f66-a930608dfc59,,3.0,,\"Hi, has anyone had any luck getting the no frills airflow to run ? When I look at the messages I see airflow-webserver: ValueError unable to configure processor. Any ideas on how to get this to run ? I\\'ve followed all the instructions and am trying to launch this no frills airflow on a GCP VM\",1643898839.031179,1643898839.031179,U02TMP4GJEM\\nD25BB90E-B2CE-4BCD-98CF-E285779D6886,U02TMP4GJEM,,,\"Have you gone through the walkthrough by Alexey ? \\n<https://www.youtube.com/watch?v=A1p5LQ0zzaQ&amp;list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb|https://www.youtube.com/watch?v=A1p5LQ0zzaQ&amp;list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb>\",1643898839.031179,1643899087.219419,U02AGF1S0TY\\n3490721D-8246-426A-90CD-957062482DAD,U02TMP4GJEM,,,This worked for me :point_up_2:,1643898839.031179,1643899134.198499,U02AGF1S0TY\\n3614e91a-dcc5-4455-8716-91769e63e57f,U02ULMHKBQT,,,\"You need to upload the zones file to Google storage, yes\",1643898674.332069,1643899194.554119,U01AXE0P5M3\\nf3743f11-86af-44cf-9e76-cf59fdd506e8,,2.0,,\"Hello everyone.\\nI have 2 questions regarding one issue of the HW. You wrote this:\\nSo we want to change and allow to fail or not?\\nAnd it says adding `-f` but the previous code was `curl -sS`   and now we are setting  `-sSlf` . So it is f or lf ?\",1643899370.663149,1643899370.663149,U02CD7E30T0\\n47e5a82d-9d9a-451d-9034-3ea0b9b95a3c,U02TMP4GJEM,,,Great thanks Hari!,1643898839.031179,1643899399.126019,U02TMP4GJEM\\ne2d5a1eb-c65c-4c1b-9701-d2a7a21e515d,U02V2G4R796,,,\"<https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/asking-questions.md|https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/asking-questions.md>\\n\\nAlso this\",1643893753.858269,1643899513.579539,U01AXE0P5M3\\n51934a85-8507-4b91-944c-6209fe2248e4,U02CD7E30T0,,,\"L optional, but f will be quite helpful if you want your jobs to fail on 404\",1643899370.663149,1643899650.016559,U01AXE0P5M3\\nf55e5a5d-0025-44d8-b5cc-a15afd6931b7,U02ULMHKBQT,,,thanks!,1643898674.332069,1643899692.290889,U02ULMHKBQT\\nba6ab814-e4cf-4a25-9348-b617f043cd92,U02CD7E30T0,,,Thank you Alexey,1643899370.663149,1643899725.613579,U02CD7E30T0\\n6f3ed4f6-7bc2-4b8a-8fdb-edfc5f3646c9,U02U5DPET47,,,\"… it’s my project ID that’s really basic….  But <@U01AXE0P5M3> my hope is that if it allowed me to use it,  then at the time it was unique, and it won’t be compromised….   Further reassurance would be appreciated!\",1643836670.890039,1643901087.311109,U02U5DPET47\\n22b0e2cb-2d58-475c-a1fe-673eba00f5a7,U02U5DPET47,,,…the documentation from google isn’t super-clear on this IMO…<https://cloud.google.com/resource-manager/docs/creating-managing-projects>,1643836670.890039,1643901101.643759,U02U5DPET47\\n203969eb-e39e-4793-902a-1589baa1fbef,U02U5DPET47,,,…realize this quandry is my own doing…. and appreciate <@U01AXE0P5M3> any further reassurance!,1643836670.890039,1643901203.715809,U02U5DPET47\\n0a4dbaf7-ca79-4474-b50c-23455b74a290,U02U5DPET47,,,\"You got it! If it let you use it, then don\\'t worry about anything\",1643836670.890039,1643901300.535989,U01AXE0P5M3\\na0e5d222-e1ff-457f-9934-29224e22d398,U030F0YHDAM,,,<@U030F0YHDAM> did it work now after Aaron\\'s suggestion?,1643848524.574759,1643902101.622879,U02T8HEJ1AS\\nc4f7c804-3282-4423-843f-0af5c3ee7840,U02U6DR551B,,,Where is the score sheet located?,1643842392.246149,1643902339.484359,U02T8HEJ1AS\\n79ccd7dd-1e24-4eaa-831b-f67c23557f8c,U02U6DR551B,,,<#C02V1Q9CL8K|announcements-course-data-engineering>,1643842392.246149,1643903205.509429,U01AXE0P5M3\\nd42b8bd2-4d97-4b9c-8902-8ff5c43b6f1f,U02U6DR551B,,,<@U01AXE0P5M3> Thank you,1643842392.246149,1643903581.644689,U02T8HEJ1AS\\n4d7f5185-232a-4487-a24f-0d3ec06a7841,U02TEERF0DA,,,\"<@U02TNEJLC84> it’s “hidden” (kinda not really :thinking_face:)  in the example SQL statements being executed in BQ. Take a look at the syntax where the `EXTERNAL` table is being built. You’ll use part of that syntax to then populate the code in the airflow task.\\n\\n```CREATE OR REPLACE EXTERNAL TABLE `name_of_table`\\nOPTIONS (\\n    format = \\'(CSV|PARQUET)\\',\\n    uris = [\\'path_to_files\\', \\'possibly_another_set_of_files\\']\\n    )```\\nHave a look at this and if you’re still having some issues, I’ll push you toward the right answer :slightly_smiling_face:\",1643723713.558069,1643903637.144109,U02TEERF0DA\\n7957e979-a97e-4243-ba61-efa21e4c233a,U030FNZC26L,,,\"wow, that was a long journey but I\\'ve figured out what\\'s going on and how to make it working as expected :dizzy_face:  <@U01DHB2HS3X> actually entrypoint  script was the last man standing and does not really required any modifications.\\nAre you willing to update the custom setup configs or let it as is?\",1643646196.837139,1643906144.702719,U02UKBMGJCR\\nc6f96a21-07d0-4ebb-8a0a-ff2de51d4983,U030FNZC26L,,,\"That\\'s great to hear, thanks <@U02UKBMGJCR>! Sure, if you have enough bandwidth, we would love to get your help on this!\",1643646196.837139,1643906251.393149,U01DHB2HS3X\\n93add0a2-1169-4d29-9cec-286f30f4dc4d,,6.0,,\"the very first DAG keeps failing on the local_to_gcs_task and the logs say \"\"No project ID can be determined\"\" and then suggested I run gcloud config set prject &lt;google project id&gt; which I\\'ve done. I then get the error that says I do not appear to have access to project id. I have checked my Dockerfile and my bucket name and project name is correctly referenced. I\\'ve checked the enviromental variables for the bucket name and google name on my GCP VM and they are correct too. Is there an extra step I\\'m missing ? By the way I\\'m running airflow on a GCP VM\",1643906892.975779,1643906892.975779,U02TMP4GJEM\\nE0A9ADB1-D55D-4368-B5C9-B27EA13D3D7B,U02Q7JMT9P1,,,\"<@U02ULMHKBQT> I have posted my code, that chops big CSV files into separate parquets, to gist <https://gist.github.com/IliaSinev/0caeba16fbe122442ea7fbc0514731d0|https://gist.github.com/IliaSinev/0caeba16fbe122442ea7fbc0514731d0>\\nIn brief, I copied the trick Alexey showed in the first week, loading a limited number of rows into pandas df. Then I used pyarrow to convert each of them into a separate parquet (I could find the way how to append to parquet file).\",1643716410.018639,1643907322.871779,U02Q7JMT9P1\\n2BD2F8A0-27C5-4463-B7E5-56E1378580D7,,4.0,,\"does anyone had problems with the dates on which dag runs?\\nmine runs once on 2019-01 then runs on 2022-02\",1643909488.503579,1643909488.503579,U02U6DR551B\\n635E03D8-128C-44E0-83C5-E2F563FB48A1,U02U6DR551B,,,\"I think it happens when we manually trigger a dag \\nLook at this as well\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1643873524132099|https://datatalks-club.slack.com/archives/C01FABYF2RG/p1643873524132099>\",1643909488.503579,1643909761.358009,U02AGF1S0TY\\n5817fef3-b1eb-494e-97b4-2bb4179a38bc,U02BVP1QTQF,,,Have you tried googling the error? From the looks of it it seems that your project is missing the necessary permissions,1643661114.567919,1643910739.676329,U02BVP1QTQF\\n08174b72-af2a-42e6-a6b7-98556ad265d5,U02TMP4GJEM,,,Try to log in to airflow and verify that the credentials are there,1643906892.975779,1643910957.214229,U01AXE0P5M3\\n6061aec1-617f-4d88-90fc-989a941bce0a,U02TMP4GJEM,,,Check the github repository for the instructions,1643906892.975779,1643910971.853489,U01AXE0P5M3\\n3b11258a-6891-43ce-b309-470034186c59,U02TMP4GJEM,,,i looked through the Slack channel and found the solution. Thanks Alexey,1643906892.975779,1643911389.989899,U02TMP4GJEM\\n485e4980-ad2f-4d44-a254-0948f082a1e4,U02TMP4GJEM,,,\"Thank you! Can you please describe your problem here and also the way you solved it?\\n\\n<https://docs.google.com/document/d/19bnYs80DwuUimHM65UV3sylsCn2j1vziPOwzBwQrebw/edit#heading=h.kefvw5280wff>\",1643906892.975779,1643911628.101749,U01AXE0P5M3\\n6d0a4289-f36a-4da2-8625-84593c11ef72,,4.0,,Hi all. I recently heard about the course. I wonder if I can still try if I can catch them up,1643912043.072869,1643912043.072869,U031XAU5UGZ\\nA69A4DD4-4F79-454B-B82B-054E3A224BF1,U031XAU5UGZ,,,\"I\\'d say it\\'s possible. Will be a lot of work, especially the first week! But doable if you\\'ve got a lot of free time to commit to it.\",1643912043.072869,1643912423.550559,U02U34YJ8C8\\nba11e4af-0c00-48e2-ae07-45d877bc53db,,7.0,,Does anyone have a good resource for concatenating variables to form a string for the BashOperator? I\\'ve been struggling with this since yesterday and can\\'t seem the bash_command with echo to print out the variables to the log.,1643912876.648679,1643912876.648679,U02TNEJLC84\\n1ddbd28c-351e-47eb-883e-617fe343fdc6,U02TNEJLC84,,,Aren\\'t f-strings working? I was able to do it like this if I remember correctly `f\\'echo {var1} {var2}\\'`,1643912876.648679,1643912991.322109,U02TATJKLHG\\n0a4355fa-f2c1-4cb2-9c37-7f9056865235,U02U6DR551B,,,\"Mine did that as well, but if I waited it would continue to run on the correct dates. I had to refresh to see that though\",1643909488.503579,1643914969.412849,U02U3E6HVNC\\n2c3479c0-a2bf-4571-9940-61bea4ff105d,U02UBQJBYHZ,,,\"May I ask, how we are seeing our grades for the homeworks? Should I check some specific place ?\",1643841939.618359,1643915255.305889,U02DD97G6D6\\n01dd3d94-9ea6-465e-a636-046a1700d7ce,U031XAU5UGZ,,,\"i would say follow along, most important is getting the knowledge. i didn\\'t even started yet\",1643912043.072869,1643915276.562599,U02RE7T06AY\\nf1fd89c0-27b7-4c8f-b64a-8d4a15e3b269,U02TNEJLC84,,,The way to concate string in bash shell is echo ${var1} ${var}. This could be placed inside quotes too.,1643912876.648679,1643915639.806899,U0290EYCA7Q\\n32956b9f-09e4-48aa-89b7-6317fd9bbe72,U02UBQJBYHZ,,,<https://datatalks-club.slack.com/archives/C02V1Q9CL8K/p1643836784703539>,1643841939.618359,1643915827.198759,U01B6TH1LRL\\n7cd74d43-2b80-42fb-8ad7-6ff86cf490c0,U02TNEJLC84,,,\"Was trying to do the $, but it wasn\\'t working for me. <@U02TATJKLHG> f-strings worked! I\\'m ashamed to say I\\'ve been working on getting it going since yesterday. Thank you friend! You rock!\",1643912876.648679,1643915970.182299,U02TNEJLC84\\n9a5f83a3-416c-4517-856e-25203c064ada,,8.0,,\"ok, so i\\'m really stuck and don\\'t how to proceed. The very first DAG data_ingestion_gcs_dag keeps failing on the local_to_gcs_task. When I look at the airflow logs it says that No project ID could be determined. I have ensured that the correct project ID is in the docker-compose file, have unset and reset the environmental variable $GOOGLE_CLOUD_PROJECT and made it was my project id, have run the command \\'gcloud config set project\\' and pointed it to the project id to make sure that my VM has access to the project, have turned on Cloud Resource Manager API to ensure that the project is accessible and it still fails on the local_to_gcs_task. Do I need to stop and then start again my VM <@U01DHB2HS3X> ? Any ideas ? For some reason Airflow is not picking up my project id\",1643916304.005979,1643916304.005979,U02TMP4GJEM\\n50a962f9-7693-475d-af5d-7bd18615dc2d,U02TMP4GJEM,,,I was a bit too happy there. I solved the missing google credentials issue but the project ID not found error still remains,1643906892.975779,1643916426.204369,U02TMP4GJEM\\ncc346e3d-6b7b-40d5-85e9-555fcf787d5b,U02TMP4GJEM,,,\"Have you brought airflow containers down, rebuild and then brought it back up again and tried to rerun?\",1643916304.005979,1643916739.002069,U02TNEJLC84\\n5e4ba593-39b6-4fe2-8cad-5eb1ec1d9757,U02TMP4GJEM,,,yep I\\'ve spent 3 hours on this without success,1643916304.005979,1643916777.279679,U02TMP4GJEM\\n88912439-fed4-479b-8e03-4b38cae78012,U02TMP4GJEM,,,And if you echo $GOOGLE_CLOUD_PROJECT it\\'s the correct project ID and you grabbed the bucket name from after you ran terraform apply?,1643916304.005979,1643916873.482749,U02TNEJLC84\\nca4204a1-2c88-4cc9-b010-27399535da54,U02TMP4GJEM,,,can you execute `env` on the container to make sure that the project id is what you set?,1643906892.975779,1643916975.538549,U01AXE0P5M3\\ne004b41c-0b5c-4c89-ac2b-afc66bfa8c0d,U02QK4ZV4UX,,,\"Or perhaps, what error you are getting and where?\",1643869666.937519,1643917030.588269,U02U3E6HVNC\\n07be49ec-a660-4ebf-b38c-1449b5c4e663,U02TMP4GJEM,,,yeah. Just checked it again after you asked and the project id is the env variable,1643916304.005979,1643917250.361989,U02TMP4GJEM\\n53ef1cfa-74b1-4163-adc0-fe9406938aae,U02TMP4GJEM,,,\"is there anywhere on the Airflow GUI that I can check the env variables, do you know ?\",1643916304.005979,1643917413.820669,U02TMP4GJEM\\n0bb0a9de-3095-432f-a436-be4309bd9713,U02UBQJBYHZ,,,Thank you :slightly_smiling_face:,1643841939.618359,1643917927.016629,U02DD97G6D6\\n858bbeca-9a80-4d6e-be7f-b95fc67237fc,,2.0,,\"Hello everyone, I like the focus of this course on the modern data stack. With all the stuff we have been doing with Docker/Airflow to ingest data, how that compares with a solution such as Fivetran which which promises to do all the extraction and ingestion with no-code?\",1643918217.868769,1643918217.868769,U02UX664K5E\\n256978a8-be5c-49e0-a930-d795823831b2,U02TMP4GJEM,,,\"Think this issue is beyond me. Googling around I see quite a bit about making sure the user has rights on the project. That would be the .json file creds I believe. Sorry friend, I think this one is beyond me. :disappointed:\",1643916304.005979,1643918242.685649,U02TNEJLC84\\n88eed455-290b-422e-babc-f224cc81d495,U031XAU5UGZ,,,\"<@U031XAU5UGZ> I would just go straight to Terraform in week 1, provision your GCP account, and then go straight to the transfer service in week 2 to load all the data. It will take you 1 hour to go through this and you\\'ll be all set to start and make progress with week 3.\",1643912043.072869,1643918481.440779,U02UX664K5E\\n5b80bc78-4c57-4588-93bf-5c50e12186a4,,5.0,,\"Hello everyone,\\n\\nI’m trying to ingest data with GCP remote instance, but it fails to find my gcloud credentials, even though I uploaded them to VM. Could you tell me please how can I configure GCP credentials on the VM so that airflow finds them?\",1643918769.602789,1643918769.602789,U02ULGHNT33\\n2b6fe6aa-2a11-42dc-a30a-1399ac25ce8a,U02TMP4GJEM,,,Thanks for your help anyway :slightly_smiling_face:,1643916304.005979,1643918901.408829,U02TMP4GJEM\\nb707fea0-34df-4f69-9ed2-7b4991679bb2,U02ULGHNT33,,,Where did you put them?,1643918769.602789,1643919609.044729,U01AXE0P5M3\\n86153919-3a51-409e-90a7-bfa66197a907,U02ULGHNT33,,,Is it in ~/.google/credentials?,1643918769.602789,1643919639.341709,U01AXE0P5M3\\ndad14475-c8e0-4036-a0fa-ba045a4e2397,U02U6DR551B,,,\"I had same problem. I fixed it by adding end_date argument.\\nAlso the the interval should be of same day(like start_date(2020-1-1) , end_date(2020,6,1) ). Also check the max_active_runs argument\",1643909488.503579,1643920040.467459,U0297ANJF6F\\nf762580b-48f6-4bf8-8e6f-49bddb2f95a0,U02ULGHNT33,,,no I got a permission error when I tried to put there. Should the `.google` folder be in home directory?,1643918769.602789,1643920175.489699,U02ULGHNT33\\n40d5b295-77c0-4467-b8c1-1af26344bad8,U030F0YHDAM,,,\"No, it didn\\'t work. I\\'ve just tried it\",1643848524.574759,1643920545.342749,U030F0YHDAM\\n9b6cda5f-f521-4ec7-9b03-002bf057ef9a,U02TEERF0DA,,,\"<@U01DHB2HS3X>  Just to confirm… It seems that the no-frills version was moved out of the extras directory and into just the airflow directory, airflow/docker-copose-nofrills.yml ??  … am starting late so am thinking of going straight to the nofrills version…\",1643208225.305900,1643921194.570739,U02U5DPET47\\n27f35209-9279-46b4-9e07-34d58d5d93f9,U02ULGHNT33,,,managed to put it to `~/.google/credentials`  going to try now,1643918769.602789,1643921219.661369,U02ULGHNT33\\n8d3d0149-c6d8-4328-9543-3c776bd54219,U030F0YHDAM,,,\"I\\'ve managed to make it work by following this StackOverflow suggestion:\\n<https://stackoverflow.com/a/67535016/14432026>\\n\\nDeleted all the previous runs, and deleted the original version of the DAG, from which I adapted the code.\\n\\n\\n\\nThanks for your attention <@U02TC704A3F> <@U02U34YJ8C8> <@U02T8HEJ1AS>\",1643848524.574759,1643921352.691809,U030F0YHDAM\\n75731254-592f-4f58-9755-b13d09ebf2ab,,2.0,,\"<@U01DHB2HS3X> <@U01AXE0P5M3> or others — would be great to have more context on the function of “id” in Linux?   in what other contexts is it commonly used?  seeing it for the first time here…\\n```echo -e \"\"AIRFLOW_UID=$(id -u)\"\" &gt;&gt; .env```\\n\",1643921755.795129,1643921755.795129,U02U5DPET47\\nd60129f1-e634-4a01-9fb7-36b991cf758f,U02U6DR551B,,,For some reason I thought it was capped at one! Thanks!,1643838239.194739,1643921773.988819,U02UBQJBYHZ\\n72c8fc08-d0c2-4756-a532-606f3298890a,U02U5DPET47,,,\"Found a few sites that describe, e.g. <https://linuxhint.com/linux-id-command/>, but would be helpful to get perspectives more relevant to your engineering experience (haven’t come across it before in my data science work…)\",1643921755.795129,1643921881.183529,U02U5DPET47\\n8b38a053-20e3-488f-b3ce-de736b90dd73,U02ULGHNT33,,,it worked:slightly_smiling_face: thanks Alexey!,1643918769.602789,1643922668.350719,U02ULGHNT33\\nfc5a8085-fea5-46e0-b64e-1bebf1bd2b3c,U02TEERF0DA,,,You can check the updated version in the repo,1643208225.305900,1643922907.943219,U01DHB2HS3X\\n1ce9fe91-4c8a-419e-9f93-f53ba88e8a46,U02TMP4GJEM,,,\"Can you enter the container and check what the ProjectID is being set (eg. try `echo`)? Also, check what is in your `.env` file\",1643916304.005979,1643923091.868879,U01DHB2HS3X\\na42d3b9d-2e21-4057-af5c-97c87bc0a053,U02U6DR551B,,,Have you set up an interval?,1643909488.503579,1643923711.423019,U02TEERF0DA\\n95914e28-4dda-46e6-af4d-5cc6f4ef909e,U02DD97G6D6,,,\"I think it is because of\\n~~~\\nvolumes: - ./dags:/opt/airflow/dags - ./logs:/opt/airflow/logs - ./plugins:/opt/airflow/plugins - ~/.google/credentials/:/.google/credentials:ro\\n~~~\\nextract from docker-compose.yaml\",1643887552.482999,1643924130.299819,U02QNCUUHEY\\n30a9fdc2-aa45-4ec0-9c0d-c30d1f3b5053,,4.0,,\"Trying to use f-string for downloading the files. I\\'m using\\n```f\"\"echo {URL_TEMPLATE} &gt; {AIRFLOW_HOME}/output_{{ execution_date.strftime(\\\\\\'%Y-%m\\\\\\') }}.csv\"\"```\\nbut what comes out in the logs is\\n```\\n[2022-02-03, 21:19:02 UTC] {subprocess.py:74} INFO - Running command: [\\'bash\\', \\'-c\\', \"\"echo <https://nyc-tlc.s3.amazonaws.com/trip+data/fhv_tripdata_2021-01.csv> &gt; /opt/***/output_{ execution_date.strftime(\\'%Y-%m\\') }.csv\"\"]```\\nAre we allowed to use jinga templating inside f-strings or should the output file name be defined outside of the dag and then put into the f-string using {whatever variable} :thinking_face:\",1643924451.068959,1643924451.068959,U02TNEJLC84\\naf131473-4f67-4412-9db1-70d414f46aed,U02TNEJLC84,,,\"```file_date = \"\"{{ execution_date.strftime(\\\\\\'%Y-%m\\\\\\') }}\"\"\\n\\ndataset_url = f\"\"<https://s3.amazonaws.com/nyc-tlc/trip+data/{dataset_file}>\"\"\\n....```\",1643924451.068959,1643924739.994779,U029JNJBPED\\n79dd694e-81cf-467e-a001-6a0f3285e51b,U02TNEJLC84,,,So Jinja Templating in an f-string is a no no?,1643924451.068959,1643924932.512089,U02TNEJLC84\\n7546c57d-09ce-43b6-b12c-325374749df5,,4.0,,\"Kind of a conceptional question, how atomic DAG tasks should be? For instance, I would clean up the local files once the load to cloud is done. It can be done either in the task for loading data using python or in the next task in bash - what would be the best practice?\",1643925402.979699,1643925402.979699,U02Q7JMT9P1\\n72febd45-42a9-453c-bf45-47751c24ae05,,,,This is better than Wordle :D,,1643925508.436979,U02UCHRV7FU\\nafcafc55-b1b7-4a82-b62a-68e8a41da921,U02DD97G6D6,,,\"Olga is right. You have to mount the folder, or use one of already mounted. The local folder structured is local. The one that is going to be running the dags is other, and only sees what\\'s mapped/mounted.\",1643887552.482999,1643927832.815839,U02GVGA5F9Q\\nd1eae7f4-6aca-47b5-9a31-e2c7f27cf930,U02Q7JMT9P1,,,\"Very interesting question. Reading the documentation <https://airflow-tutorial.readthedocs.io/en/latest/airflow-intro.html> <https://airflow.apache.org/docs/apache-airflow/1.10.10/concepts.html?highlight=atomic> , I have not a clear opinion. For me what makes sense is to delete the files by the task that consumes that file, after the task is executed successfully.\\n\\nMy doubt is that, if an operator may run on a different machine, it is theoretically possible that the csv file could not be found by the format_to_parket operator? Just in case, I mounted a volume with docker to store these temp files, as a way to have a centralized storage between containers\",1643925402.979699,1643927881.679389,U02LQMEAREX\\n8843accc-49bd-4623-97ee-870c22a63630,U02TNEJLC84,,,\"In python you can concatenate like this \"\"str1\"\" \"\"str2\"\"\",1643912876.648679,1643928039.942259,U02GVGA5F9Q\\nfec3b0a7-2e6b-4d48-bba6-b3675c461f0b,U02TNEJLC84,,,or use the + operator,1643912876.648679,1643928049.782259,U02GVGA5F9Q\\n411abfb8-29ab-4259-9b8c-4f1096630ace,U02TNEJLC84,,,\"f strings are useful to produce cleaner code, but not interesting to mix with Jinja, for example\",1643912876.648679,1643928112.317699,U02GVGA5F9Q\\nf4607036-57f8-4b97-9e2c-57fbdcd4395b,U02TNEJLC84,,,\"why don\\'t you use the \"\"+\"\" operator?\",1643924451.068959,1643928232.646939,U02GVGA5F9Q\\n9eb25b9b-7ed9-4bab-a10d-e4ecaca176de,U02TMP4GJEM,,,\"Yes, you’ll need to have the service account credentials available on the instance. Alexey goes over it in this video at 40:35: <https://www.youtube.com/watch?v=ae-CV2KfoN0&amp;t=2435s>\",1643885749.441099,1643928712.869849,U02SQ1X29GE\\nc95338cc-2e41-4660-ba9e-3dca9fd1d709,U02U34YJ8C8,,,Aaron you are a lifesaver thanks,1642871725.398200,1643929322.341799,U02A3AU35LL\\n2ef40230-008b-4b17-9099-3f3f14b9fdfe,U02BRPZKV6J,,,Added to the FAQ in Week 2. Thanks!,1643795839.822289,1643929431.395289,U02BRPZKV6J\\nf320fe84-7786-4f54-ad32-3b3a87435945,,9.0,,\"Hi everyone, I\\'m in DE Zoomcamp 3.1.1. trying to run the first CREATE SQL Statement and got this error below.\\nDoes anyone how to resolve this error? Thanks!\\n\\nI already added \"\"BigQuery Admin\"\" Role to the user.\\n```Access Denied: Dataset taxi-rides-ny:nytaxi: Permission bigquery.tables.create denied on dataset taxi-rides-ny:nytaxi (or it may not exist).```\",1643929919.420949,1643929919.420949,U02BRPZKV6J\\naf183033-e3be-4f91-b211-b7695ac716b7,U02BRPZKV6J,,,<@U02BRPZKV6J> give creator access to that account,1643929919.420949,1643930007.833969,U02VBG59VQ9\\n49732dff-86b3-44ea-a5d3-bea609d415fe,U02BRPZKV6J,,,\"Hi <@U02VBG59VQ9>, what do you mean by creator access?\",1643929919.420949,1643930276.855629,U02BRPZKV6J\\n1b42918c-ac15-43f9-95f9-5e8f85aacaf2,U02BRPZKV6J,,,bigquery creator like bigquery admin,1643929919.420949,1643930711.726669,U02VBG59VQ9\\nf75c4052-c10f-4113-8329-96b6d4c92864,U02SFFC9FPC,,,\"I solved this earlier in the week indeed, apologies, i concur with your perspective here. Quite peculiar considering the platform is supposed to provide a free credit. I merely created an nascent project and it solved itself?(quite odd after not having access to the platform for 8 days after registration.)\",1643237452.453700,1643931251.233289,U02SFFC9FPC\\n911ffc0e-1db7-47b8-86a4-d58ce145388d,,2.0,,\"Within the powershell when attempting to run any docker tasks i receive this error unless i comment the entire command on one line and remove the backslashes, and advice?(beyond modifying the comment to the above? On windows10.\\nFor instance this comment is accurate and executes w/o an error;\\n\\n``` docker run -it  -e PGADMIN_DEFAULT_EMAIL=\"\"<mailto:admin@admin.com|admin@admin.com>\"\"  -e PGADMIN_DEFAULT_PASSWORD=\"\"root\"\"  -p 8080:80  dpage/pgadmin4    ```\\nHowever this comment returns an error for all actions:\\n```docker run -it\\\\ \\n\\xa0 -e PGADMIN_DEFAULT_EMAIL=\"\"<mailto:admin@admin.com|admin@admin.com>\"\" \\\\\\n\\xa0 -e PGADMIN_DEFAULT_PASSWORD=\"\"root\"\" \\\\\\n\\xa0 -p 8080:80 \\\\\\n\\xa0 dpage/pgadmin4\\ndocker pull dpage/pgAdmin4```\\nThe term \\'dpage/pgadmin4\\'is not recognized as a name of a cmdlet, function, script file, or executable program.\\nCheck the spelling of the name, or if a path was included, verify that the path is correct and try again.\"\"\\nSolved for now merely inquiring.\",1643931419.667799,1643931419.667799,U02SFFC9FPC\\n1e027f1d-8057-43e4-b5db-eb2052e53622,,5.0,,\"Hi All. Does anyone know why my dag was not run for the date 2019-01-01, despite the start date being this date?\",1643931487.722259,1643931487.722259,U02V1JC8KR6\\n4dc5f48e-1c6f-4afa-afb0-0471d897e11b,U02BRPZKV6J,,,\"Having the same issue, `BigQuery Admin` role has already editor privileges, I don\\'t think it is related to IAM roles. It must be something else.\",1643929919.420949,1643931860.328959,U02UX664K5E\\n9d3208f3-eef6-4ff0-bb1f-9683dd9cc596,U02V1JC8KR6,,,<@U02V1JC8KR6> mine didn\\'t for the January FHV only it did for the rest,1643931487.722259,1643932362.286219,U02VBG59VQ9\\n63f75494-73e6-4bef-a1c5-d3bc919366bd,U02UX664K5E,,,<@U01AXE0P5M3> Can it be installed on Bash separately too? Tried a lot of searches. Can\\'t get anything,1642721639.474000,1643933103.065269,U02UAMG740J\\n68462fc8-9a02-42f7-ada7-5e958076ff53,U02BRPZKV6J,,,\"The screenshot below is taken from video 3.1.1 at 9:39. From your screenshots, it seems your project and data set names are the same as Ankush\\'s.\\n\\nHave you checked they actually are? Perhaps yours have different names\",1643929919.420949,1643934185.770569,U02S2TZRBL7\\nb56b5a7f-ff01-424f-8ebb-08ec18447e99,U02BRPZKV6J,,,\"Thanks <@U02S2TZRBL7>! You\\'re totally right, the project name and dataset name should match with what\\'s on your account and the CSV file should be also pointing to your bucket name like this:`uris\\xa0=\\xa0[\\'<gs://YOUR_GCS_BUCKET_NAME/trip>\\xa0data/yellow_tripdata_2019-*.csv\\',\\xa0\\'<gs://YOUR_GCS_BUCKET_NAME/trip>\\xa0data/yellow_tripdata_2020-*.csv\\']`\",1643929919.420949,1643936360.998069,U02UX664K5E\\n8a8d3353-75b4-4097-a59f-ba21c40f9287,,5.0,,\"<@U01AXE0P5M3> <@U01DHB2HS3X>  Can I pick up the class next week with BigQuery and dbt without this week’s material?   (This debugging is taking just too long, would hope I can just transfer files over/ figure a way around)….\",1643937023.081149,1643937023.081149,U02U5DPET47\\n4e7b8c8b-2cd4-406f-a64a-eed12a9d6dfc,,14.0,,\"Pls help:  My airflow gives the error. Totally confused because I think the postgress container is running.\\n```psycopg2.OperationalError: could not translate host name \"\"pgdatabase\"\" to address:```\\n\",1643937705.301049,1643937705.301049,U02RSAE2M4P\\n335aa219-e5f9-416c-8e04-a5aa7c9f8bdd,U02BRPZKV6J,,,I\\'m glad it helped!,1643929919.420949,1643938144.998659,U02S2TZRBL7\\nd94e769c-4b20-4d7c-a576-ad5f5e001a90,,1.0,,\"Good news everyone, time travel is real :joy: I think my DAGs ran fine, but I just queried the data and I have dates from 2001 all the way up to 2088. I\\'m curious if this is an issue with the data or if I did something wrong on my end...\",1643939551.508339,1643939551.508339,U02U3E6HVNC\\na05e5116-9e93-45f9-ace1-5fe20ba0ff17,,5.0,,\"Hey all,\\xa0 i\\'m having trouble understanding data_ingestion_local.py conceptually attached below. I understand that the start date in the video was set to datetime(2021, 1, 1) , but how does airflow know to iterate through each month to download the files from the yellow taxi data website? I don\\'t see like an iterator/loop that specifies to do that. Or does airflow just do that automatically when you trigger manually on a monthly schedule interval ?\\n\\nI\\'d appreciate any explanation for this!\",1643940333.121189,1643940333.121189,U02T9550LTU\\n5a7ead93-7bb3-43e4-8e06-8b5383f0b69a,U02T9550LTU,,,,1643940333.121189,1643940347.154759,U02T9550LTU\\nac31ac1d-abf7-42e2-9db9-b8a62eddaf77,,7.0,,\"Im kinda lost in BigQuery part.\\n\\nI\\'ve FHV data uploaded into my Cloud Storage but I can\\'t see how to query this data in BigQuery.\\n\\nIts weird because the last part of Airflow DAG is meant to create external tables in BigQuery, right? Because I don\\'t see any external tables in my BigQuery besides one that was already there.\\n\\nShould I add it using Data Transfer?\",1643940376.662509,1643940376.662509,U02TC704A3F\\nd3360f97-a833-44d8-ad5b-0b7396632c72,U02T9550LTU,,,\"&gt; Or does airflow just do that automatically when you trigger manually on a monthly schedule interval\\nYeah is just like that.\\n\\nThe name of the file is a variable set to change according to execution date.\\n\\nYou are setting a monthly schedule, where the execution date will be [2021-01, 2021-02, 2021-03,..., 2021-12]. One time for each month until the end_date.\\n\\nAirflow will run these jobs for the date range that you specified and there you have it, some sort of \"\"implicit iterator loop\"\".\",1643940333.121189,1643940845.699369,U02TC704A3F\\nF0C1FF3E-436A-4286-AE0A-076213CE33F7,U02V1JC8KR6,,,Please check on browse tab on top for status of all dag runs and check if the dag run for the particular month has a failed status ,1643931487.722259,1643941119.523479,U02AGF1S0TY\\n7f25ce26-948e-4381-9f0f-110e0d4d9461,U02TC704A3F,,,in weeks 2 you only need to upload your data into cloud storage and you dont need to create table from that,1643940376.662509,1643941347.855709,U02RA8F3LQY\\n7809EF4C-49B2-4716-AF47-3C0876E37151,U02TC704A3F,,,\"In the homework for week 2 , it was mentioned that task is to put the data in GCs , anyway in week 3 videos explain how to create a table in BQ from gcs \",1643940376.662509,1643941398.429689,U02AGF1S0TY\\n562074e9-5ef8-4a24-b6c6-f257e6aea3d2,U02Q7JMT9P1,,,\"thanks for getting back, will take a look!\",1643716410.018639,1643942371.025399,U02ULMHKBQT\\nefbb2b58-609f-4d54-a5ab-19ad696c71f8,U02QK4ZV4UX,,,\"<@U02U3E6HVNC> not getting any error in the dag as such. I m trying to access the env variables, but they are returning null values.\",1643869666.937519,1643943137.238529,U02QK4ZV4UX\\nc12885ce-b445-43a8-950f-e10eca853b4e,U02TC704A3F,,,\"<https://www.youtube.com/watch?v=9ksX9REfL8w&amp;list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&amp;index=19&amp;ab_channel=DataTalksClub|This video>, in 16:18, talks about the last function in DAG should create external tables for the data.\\n\\nLooks like should\\'ve external tables already created after running Airflow.\",1643940376.662509,1643943450.278239,U02TC704A3F\\n49277a6a-10b1-4102-ba0b-3c6f42fb7146,U02T9550LTU,,,<@U02TC704A3F> Thank you for the great explanation! :raised_hands:,1643940333.121189,1643945869.788899,U02T9550LTU\\n2999fef9-e768-46b7-b545-efbf113717e7,U02TC704A3F,,,\"adding on to what the others said above, I think specifically our homework is only to upload/ingest the data to storage. For the external table part, i believe alexey mentioned that in week 3 they will go over how to integrate all of the data into an external table so you\\'re not supposed to do that.\\n\\nHe says it here <https://datatalks-club.slack.com/archives/C01FABYF2RG/p1643613521002359?thread_ts=1643596998.639849&amp;cid=C01FABYF2RG>\",1643940376.662509,1643946180.031169,U02T9550LTU\\n29d6f179-190e-48fc-8d8c-8482b23a5b3d,U02RSAE2M4P,,,\"make sure you already install for psycopg2\\n\\n<https://pypi.org/project/psycopg2/>\",1643937705.301049,1643948567.684429,U02SQQYTR7U\\n2a75ef8a-04e4-486d-a5b3-78449e362bbd,U02TNEJLC84,,,<@U02TNEJLC84> Glad it worked for you!,1643912876.648679,1643949044.504159,U02TATJKLHG\\n94f9681e-7219-44bc-a9f3-c9cc4d379b79,,8.0,,I have just completed watching the Week 2 video of 2.3.3 Ingesting Data to local Postgres with Airflow. I cant seem to get the environment variables passed to the DAG properly. Code is in the thread.,1643949531.146559,1643949531.146559,U02QK4ZV4UX\\n33a9d156-5532-440f-8518-e1ceb08ed2d9,U02QK4ZV4UX,,,\"I have done build multiple times, but dont seem to get the env var working\",1643949531.146559,1643949609.636839,U02QK4ZV4UX\\nfd347989-2627-40e6-917b-1795be1a022c,U02QK4ZV4UX,,,Please use the thread to paste your code,1643949531.146559,1643949630.452329,U02TATJKLHG\\n1ab6ecfc-2b85-4d5d-b704-f7f7e1dcaec8,U02QK4ZV4UX,,,\".env file:\\n```AIRFLOW_UID=197609\\nPG_HOST=pgdatabase\\nPG_USER=root\\nPG_PASSWORD=root\\nPG_PORT=5432\\nPG_DATABASE=ny_taxi```\\ndocker-compose.yaml:\\n```environment:\\n    PG_HOST: \\'${PG_HOST}\\'\\n    PG_USER: \\'${PG_USER}\\'\\n    PG_PASSWORD: \\'${PG_PASSWORD}\\'\\n    PG_PORT: \\'${PG_PORT}\\'\\n    PG_DATABASE: \\'${PG_DATABASE}\\'```\\ndata_ingestion_local.py (dag file):\\n```PG_HOST = os.getenv(\\'PG_HOST\\')\\nPG_USER = os.getenv(\\'PG_USER\\')\\nPG_PASSWORD = os.getenv(\\'PG_PASSWORD\\')\\nPG_PORT = os.getenv(\\'PG_PORT\\')\\nPG_DATABASE = os.getenv(\\'PG_DATABASE\\')\\n\\ningest_task= PythonOperator(\\n        task_id= \\'ingest_task\\',\\n        python_callable= ingest_callable_task,\\n        op_kwargs= dict(user=PG_USER,\\n         password=PG_PASSWORD,\\n         host=PG_HOST,\\n         port=PG_PORT,\\n         db=PG_DATABASE,\\n         table_name=TABLE_NAME_TEMPLATE,\\n         csv_file=OUTPUT_FILE_TEMPLATE\\n         )\\n    )```\\ndata_ingestion_script.py:\\n```def ingest_callable_task(user, password, host, port, db, table_name, csv_file):\\n    print(f\\'Table Name: {table_name} CSV file name: {csv_file}\\')\\n    print(f\\'postgresql://{user}:{password}@{host}:{port}/{db}\\')```\\nlog output:\\n```[2022-02-04, 04:35:01 UTC] {logging_mixin.py:109} INFO - Table Name: yellow_taxi_trip_2021_01 CSV file name: /opt/***/output_2021-01.csv\\n[2022-02-04, 04:35:01 UTC] {logging_mixin.py:109} INFO - <postgresql://None:None@None>:None/None```\\n\",1643949531.146559,1643950400.870239,U02QK4ZV4UX\\n9bfa9212-bf21-49a7-ad44-08a9e6e6e866,U02BVP1QTQF,,,Yes and I have checked that I have added the three IAM roles and enabled the two APIs too for my service account,1643661114.567919,1643950574.112659,U02SEB4Q8TW\\n01e0eecb-2619-4084-88da-5ca29a473482,U02T9550LTU,,,\"Also to add to what Leonardo said, you can think of Airflow in terms of a cron job rather than an iterator. It is similar to adding your cron expression in the crontab file. Each time the schedule specified in the crontab is hit on the clock, the command get\\'s executed.\\n\\nSo in very simpler terms, Airflow is the fancier, good looking, more powerful brother of cron jobs :smile:\",1643940333.121189,1643951049.435039,U02TATJKLHG\\n387cb282-8311-42dd-bfa4-edbee946d41e,U02U5DPET47,,,\"You need to transfer the files to your Google Cloud storage somehow. If you do it with transfer service or find another way, then yes\",1643937023.081149,1643953345.901259,U01AXE0P5M3\\nc0bce60f-7ae6-4961-80ab-3bf61ed45f78,U02U5DPET47,,,\"id -u outputs the ID of the current user. It\\'s needed when you want some other processes to run on your behalf, just like airflow will run as you here.\\n\\nAnd the reason we need it to set it - for airflow to be able to access the folders and the files\",1643921755.795129,1643953734.330679,U01AXE0P5M3\\nffe71012-20ee-42fd-ac8a-37fcda34759a,U02TNEJLC84,,,\"You can also use the \"\"old\"\" way of formatting \\n\\n<https://pyformat.info/|https://pyformat.info/>\",1643924451.068959,1643953800.916499,U01AXE0P5M3\\n0eee0d98-5911-458b-8abd-5feb2672be81,U02Q7JMT9P1,,,\"In real life scenarios it\\'s possible that there are many many different workers, so you indeed may need to put downloading, parquetization and uploading to gcs in one step\",1643925402.979699,1643953945.622009,U01AXE0P5M3\\ndfbd86bd-e6ea-49cc-a8da-0b8f5d87d664,U02SFFC9FPC,,,How did you solve it?,1643931419.667799,1643954062.609809,U01AXE0P5M3\\n75f5a6e2-52a8-4de2-909c-1c42994bf1b0,U02RSAE2M4P,,,Maybe your container uses a different name?,1643937705.301049,1643954133.038629,U01AXE0P5M3\\n9cfab13e-19b3-4f45-8edb-6c60aa0e48cb,U02U3E6HVNC,,,I think it\\'s an issue with the data,1643939551.508339,1643954166.826519,U01AXE0P5M3\\n057a1292-307c-466e-8f54-9bb0f4cddc37,U02TC704A3F,,,In the first video Ankush shows how to create a table in BQ from csv files. You\\'ll need to replace csv with parquet and you\\'re good to go,1643940376.662509,1643954375.137919,U01AXE0P5M3\\n6ca36aa4-83b6-4ff9-babb-2944dc84ec1a,U02QK4ZV4UX,,,Can you execute `env` on the container and check the output?,1643949531.146559,1643954462.607599,U01AXE0P5M3\\n7fbdddca-0532-4eb7-9026-b4ced71e20d5,U02UX664K5E,,,\"On Ubuntu it\\'ll be apt-get install wget\\nOn windows you\\'ll need to download it\",1642721639.474000,1643954652.159249,U01AXE0P5M3\\nf9f1c3d5-3ab4-4a07-aee7-a40d49d0444e,U02U6DR551B,,,One post every day =),1643838239.194739,1643955002.544459,U01AXE0P5M3\\n6f4cf699-ed42-4377-b1aa-73d33ea31718,U02QK4ZV4UX,,,i tried doing that. to print out all the env vars. dint show up.,1643949531.146559,1643957117.539729,U02QK4ZV4UX\\n43a3182d-6334-4bb2-8fef-22d6039b5c5a,U02QK4ZV4UX,,,is your .env file in the same directory as docker-compose.yaml?,1643949531.146559,1643957172.503829,U01AXE0P5M3\\n53f26870-18a2-4319-a232-19ee83a9997a,U02QK4ZV4UX,,,But I used to docker down to properly destroy the artifacts and rebuilt and the env started showing up,1643949531.146559,1643957179.547109,U02QK4ZV4UX\\n208ac2b9-e45b-4f2b-8a92-c65e5c3b49de,U02QK4ZV4UX,,,dont know why earlier docker-compose buold dint pick up the env vars. But now they are updated,1643949531.146559,1643957263.411529,U02QK4ZV4UX\\nfba55b64-7a63-4349-ae11-cb631a76bdff,U02V1JC8KR6,,,\"~~~\\nA DAG run is usually scheduled\\xa0after\\xa0its associated data interval has ended, to ensure the run is able to collect all the data within the time period. In other words, a run covering the data period of 2020-01-01 generally does not start to run until 2020-01-01 has ended, i.e. after 2020-01-02 00:00:00.\\n~~~\\nfrom <https://airflow.apache.org/docs/apache-airflow/stable/dag-run.html|https://airflow.apache.org/docs/apache-airflow/stable/dag-run.html>\\n\\nSo try 2018-12-31 as start date.\",1643931487.722259,1643957735.189029,U02QNCUUHEY\\nd3563678-e982-4b0b-946d-34959fe6b795,U02BVP1QTQF,,,\"As an update, I had to add the roles `Compute Admin` and `Service Account User`.  There were some other differences for my setup so I forked and <https://gist.github.com/will-fong/6304faba8f42bbfff97f25b80f4d9e41|updated the gist with my experience here>.\\n\\nJust to clarify, after completing your setup guide, we are not finished and still have to resume setup in the 1.4.1 video at the ~27:00 mark?\",1643661114.567919,1643958704.619449,U02SEB4Q8TW\\n19A9FBE8-6C37-4C08-B9FA-368844143898,,2.0,,\"Hi <@U01AXE0P5M3> \\nI followed through your video on ingesting data to local Postgres with airflow. \\nWhen I ran pgcli and checked the data, unlike yours, mine contained the yellow taxi data. Is that fine please ?\",1643960371.831749,1643960371.831749,U02TZ1JCVEC\\nd4dc3b52-0c36-4658-b9c1-89a07dc40c9e,U02S6KXPH8W,,,what worked is deleting the local volume mapped and the cache file.,1644341363.870689,1644396038.595069,U02S6KXPH8W\\n9c9833c7-2895-459b-b815-2b99a880311b,U02S6KXPH8W,,,this will clear the tasks history and keep the dag,1644341363.870689,1644396065.854969,U02S6KXPH8W\\ncee08b64-aba7-4884-89fe-b403b1cba9fc,U02SM3LKD2B,,,i also have separate repo,1644362512.657099,1644396188.925899,U02S6KXPH8W\\n4cbe53f9-752a-4f38-98cd-c0aa05a784ae,U029JNJBPED,,,quick PR:  <https://github.com/DataTalksClub/data-engineering-zoomcamp/pull/76/files>,1644364947.174169,1644396279.919969,U029JNJBPED\\nb6bcd94f-f5cf-4ac2-a23b-bd836bf210f3,U02QS4BD1NF,,,\"<@U02UP3KN3SQ> since we are passing in the start date in the DAG, could we also not pass in the end date either as a default argument or in the DAG?\",1644286226.784259,1644396711.507519,U02SEB4Q8TW\\na8a08f4b-3582-43e8-941e-b5669cca840e,U0300EGP2EL,,,\"Did you run these commands?\\n\\ncd ~ &amp;&amp; mkdir -p ~/.google/credentials/\\n    mv &lt;path/to/your/\\n\\nservice-account-authkeys&gt;.json ~/.google/credentials/\",1644392599.340299,1644397791.266299,U02QL1EG0LV\\ne23cf97a-9aab-4c77-9347-ca11844366c5,U02QS4BD1NF,,,I have passed it as part of default_arguments <@U02SEB4Q8TW>,1644286226.784259,1644398801.476499,U02DD97G6D6\\nf7ad7284-aa0c-4cab-8db7-00e624261210,U0300EGP2EL,,,\"the `docker compose` is the new version where it is used as a docker plugin.\\nThey should do the same thing, but because it\\'s a major version, the .yaml file is slightly different.\\n\\nI also tried the plugin, it wasn\\'t compatible with our files so I use `docker-compose` to be safe\",1644389115.865409,1644399491.510729,U029JNJBPED\\n951d23c0-5478-4660-bae8-f13a8d56a4b9,U0300EGP2EL,,,Issue resolved after restarting the system. Thanks.,1644392599.340299,1644401468.301759,U0300EGP2EL\\n,USLACKBOT,14.0,tombstone,This message was deleted.,1644401511.198979,1644401511.198979,USLACKBOT\\nedbe00b2-1f5e-4db9-af2f-67c6f779ce6a,USLACKBOT,,,Can you please move the log to the thread?,1644401511.198979,1644402043.874509,U01AXE0P5M3\\nf7374736-3166-4908-bfb9-813bf408270e,USLACKBOT,,,\"``` ERROR [airflow_airflow-triggerer 4/7] RUN pip install --no-cache-dir -r requirements.txt          194.6s\\n------\\n &gt; [airflow_airflow-triggerer 4/7] RUN pip install --no-cache-dir -r requirements.txt:\\n#30 6.926 Collecting apache-airflow-providers-google\\n#30 11.43   Downloading apache_airflow_providers_google-6.3.0-py3-none-any.whl (767 kB)\\n#30 84.56 Collecting pyarrow\\n#30 84.67   Downloading pyarrow-7.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\\n#30 193.2 ERROR: Exception:\\n#30 193.2 Traceback (most recent call last):\\n#30 193.2   File \"\"/usr/local/lib/python3.7/site-packages/pip/_vendor/urllib3/response.py\"\", line 438, in _error_catcher\\n#30 193.2     yield\\n#30 193.2   File \"\"/usr/local/lib/python3.7/site-packages/pip/_vendor/urllib3/response.py\"\", line 519, in read\\n#30 193.2     data = self._fp.read(amt) if not fp_closed else b\"\"\"\"\\n#30 193.2   File \"\"/usr/local/lib/python3.7/http/client.py\"\", line 465, in read\\n#30 193.2     n = self.readinto(b)\\n#30 193.2   File \"\"/usr/local/lib/python3.7/http/client.py\"\", line 509, in readinto\\n#30 193.2     n = self.fp.readinto(b)\\n#30 193.2   File \"\"/usr/local/lib/python3.7/socket.py\"\", line 589, in readinto\\n#30 193.2     return self._sock.recv_into(b)\\n#30 193.2   File \"\"/usr/local/lib/python3.7/ssl.py\"\", line 1071, in recv_into\\n#30 193.2     return self.read(nbytes, buffer)\\n#30 193.2   File \"\"/usr/local/lib/python3.7/ssl.py\"\", line 929, in read\\n#30 193.2     return self._sslobj.read(len, buffer)\\n#30 193.2 socket.timeout: The read operation timed out\\n#30 193.2 \\n#30 193.2 During handling of the above exception, another exception occurred:\\n#30 193.2 \\n#30 193.2 Traceback (most recent call last):\\n#30 193.2   File \"\"/usr/local/lib/python3.7/site-packages/pip/_internal/cli/base_command.py\"\", line 173, in _main\\n#30 193.2     status = self.run(options, args)\\n#30 193.2   File \"\"/usr/local/lib/python3.7/site-packages/pip/_internal/cli/req_command.py\"\", line 203, in wrapper\\n#30 193.2     return func(self, options, args)\\n#30 193.2   File \"\"/usr/local/lib/python3.7/site-packages/pip/_internal/commands/install.py\"\", line 316, in run\\n#30 193.2     reqs, check_supported_wheels=not options.target_dir\\n#30 193.2   File \"\"/usr/local/lib/python3.7/site-packages/pip/_internal/resolution/resolvelib/resolver.py\"\", line 95, in resolve\\n#30 193.2     collected.requirements, max_rounds=try_to_avoid_resolution_too_deep\\n#30 193.2   File \"\"/usr/local/lib/python3.7/site-packages/pip/_vendor/resolvelib/resolvers.py\"\", line 472, in resolve\\n#30 193.2     state = resolution.resolve(requirements, max_rounds=max_rounds)\\n#30 193.2   File \"\"/usr/local/lib/python3.7/site-packages/pip/_vendor/resolvelib/resolvers.py\"\", line 341, in resolve\\n#30 193.2     self._add_to_criteria(self.state.criteria, r, parent=None)\\n#30 193.2   File \"\"/usr/local/lib/python3.7/site-packages/pip/_vendor/resolvelib/resolvers.py\"\", line 172, in _add_to_criteria\\n#30 193.2     if not criterion.candidates:\\n#30 193.2   File \"\"/usr/local/lib/python3.7/site-packages/pip/_vendor/resolvelib/structs.py\"\", line 151, in __bool__\\n#30 193.2     return bool(self._sequence)\\n#30 193.2   File \"\"/usr/local/lib/python3.7/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\"\", line 140, in __bool__\\n#30 193.2     return any(self)\\n#30 193.2   File \"\"/usr/local/lib/python3.7/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\"\", line 128, in &lt;genexpr&gt;\\n#30 193.2     return (c for c in iterator if id(c) not in self._incompatible_ids)\\n#30 193.2   File \"\"/usr/local/lib/python3.7/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\"\", line 32, in _iter_built\\n#30 193.2     candidate = func()\\n#30 193.2   File \"\"/usr/local/lib/python3.7/site-packages/pip/_internal/resolution/resolvelib/factory.py\"\", line 209, in _make_candidate_from_link\\n#30 193.2     version=version,\\n#30 193.2   File \"\"/usr/local/lib/python3.7/site-packages/pip/_internal/resolution/resolvelib/candidates.py\"\", line 301, in __init__\\n#30 193.2     version=version,\\n#30 193.2   File \"\"/usr/local/lib/python3.7/site-packages/pip/_internal/resolution/resolvelib/candidates.py\"\", line 156, in __init__\\n#30 193.2     self.dist = self._prepare()\\n#30 193.2   File \"\"/usr/local/lib/python3.7/site-packages/pip/_internal/resolution/resolvelib/candidates.py\"\", line 227, in _prepare\\n#30 193.2     dist = self._prepare_distribution()\\n#30 193.2   File \"\"/usr/local/lib/python3.7/site-packages/pip/_internal/resolution/resolvelib/candidates.py\"\", line 306, in _prepare_distribution\\n#30 193.2     self._ireq, parallel_builds=True\\n#30 193.2   File \"\"/usr/local/lib/python3.7/site-packages/pip/_internal/operations/prepare.py\"\", line 508, in prepare_linked_requirement\\n#30 193.2     return self._prepare_linked_requirement(req, parallel_builds)\\n#30 193.2   File \"\"/usr/local/lib/python3.7/site-packages/pip/_internal/operations/prepare.py\"\", line 552, in _prepare_linked_requirement\\n#30 193.2     self.download_dir, hashes\\n#30 193.2   File \"\"/usr/local/lib/python3.7/site-packages/pip/_internal/operations/prepare.py\"\", line 243, in unpack_url\\n#30 193.2     hashes=hashes,\\n#30 193.2   File \"\"/usr/local/lib/python3.7/site-packages/pip/_internal/operations/prepare.py\"\", line 102, in get_http_url\\n#30 193.2     from_path, content_type = download(link, temp_dir.path)\\n#30 193.2   File \"\"/usr/local/lib/python3.7/site-packages/pip/_internal/network/download.py\"\", line 145, in __call__\\n#30 193.2     for chunk in chunks:\\n#30 193.2   File \"\"/usr/local/lib/python3.7/site-packages/pip/_internal/cli/progress_bars.py\"\", line 144, in iter\\n#30 193.2     for x in it:\\n#30 193.2   File \"\"/usr/local/lib/python3.7/site-packages/pip/_internal/network/utils.py\"\", line 87, in response_chunks\\n#30 193.2     decode_content=False,\\n#30 193.2   File \"\"/usr/local/lib/python3.7/site-packages/pip/_vendor/urllib3/response.py\"\", line 576, in stream\\n#30 193.2     data = self.read(amt=amt, decode_content=decode_content)\\n#30 193.2   File \"\"/usr/local/lib/python3.7/site-packages/pip/_vendor/urllib3/response.py\"\", line 541, in read\\n#30 193.2     raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\\n#30 193.2   File \"\"/usr/local/lib/python3.7/contextlib.py\"\", line 130, in __exit__\\n#30 193.2     self.gen.throw(type, value, traceback)\\n#30 193.2   File \"\"/usr/local/lib/python3.7/site-packages/pip/_vendor/urllib3/response.py\"\", line 443, in _error_catcher\\n#30 193.2     raise ReadTimeoutError(self._pool, None, \"\"Read timed out.\"\")\\n#30 193.2 pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host=\\'<http://files.pythonhosted.org|files.pythonhosted.org>\\', port=443): Read timed out.\\n#30 194.5 WARNING: You are using pip version 21.2.4; however, version 22.0.3 is available.\\n#30 194.5 You should consider upgrading via the \\'/usr/local/bin/python -m pip install --upgrade pip\\' command.\\n------\\nfailed to solve: rpc error: code = Unknown desc = executor failed running [/bin/bash -o pipefail -o errexit -o nounset -o nolog -c pip install --no-cache-dir -r requirements.txt]: exit code: 2```\\n\",1644401511.198979,1644402631.157869,U0300EGP2EL\\nb2f02cd5-534f-4733-81f9-782266da13b4,USLACKBOT,,,\"seems like you have some internet problems, can you try one more time?\",1644401511.198979,1644405645.165679,U01AXE0P5M3\\n9cfde2f4-1c9d-4765-93ce-67e2837c525b,USLACKBOT,,,\"I got it. my issue resolved. Image got built and exited with 0 after\\n```docker compose build\\ndocker compose up airflow-init\\ndocker compose up```\\nbut after that I got series of errors as follows\\n```INFO - Starting the triggerer\\nairflow-airflow-triggerer-1  | [2022-02-09 11:33:33,427] {triggerer_job.py:345} ERROR - Triggerer\\'s async thread was blocked for 5.29 seconds, likely by a badly-written trigger. Set PYTHONASYNCIODEBUG=1 to get more information on overrunning coroutines.\\nairflow-airflow-triggerer-1  | [2022-02-09 11:33:36,159] {triggerer_job.py:345} ERROR - Triggerer\\'s async thread was blocked for 1.08 seconds, likely by a badly-written trigger. Set PYTHONASYNCIODEBUG=1 to get more information on overrunning coroutines.\\nairflow-airflow-triggerer-1  | [2022-02-09 11:33:37,052] {triggerer_job.py:345} ERROR - Triggerer\\'s async thread was blocked for 0.61 seconds, likely by a badly-written trigger. Set PYTHONASYNCIODEBUG=1 to get more information on overrunning coroutines.\\nairflow-airflow-triggerer-1  | [2022-02-09 11:33:37,997] {triggerer_job.py:345} ERROR - Triggerer\\'s async thread was blocked for 0.60 seconds, likely by a badly-written trigger. Set PYTHONASYNCIODEBUG=1 to get more information on overrunning coroutines.\\nairflow-airflow-triggerer-1  | [2022-02-09 11:33:49,453] {triggerer_job.py:345} ERROR - Triggerer\\'s async thread was blocked for 0.76 seconds, likely by a badly-written trigger. Set PYTHONASYNCIODEBUG=1 to get more information on overrunning coroutines.\\nairflow-airflow-triggerer-1  | [2022-02-09 11:33:50,103] {triggerer_job.py:345} ERROR - Triggerer\\'s async thread was blocked for 0.65 seconds, likely by a badly-written trigger. Set PYTHONASYNCIODEBUG=1 to get more information on overrunning coroutines.```\\nlocalhost:8080 is not working but localhost:5555 is working.\",1644401511.198979,1644406607.598859,U0300EGP2EL\\n338eb7b5-80cc-4898-9b28-0e897caeb31d,,8.0,,\"I have a question regarding the flow from week 2, to week 3 and now week 4. I don\\'t know how it happens but it feels to me there is more info added to the course during the weeks and slack does not help to keep up with it imho.\",1644407789.816199,1644407789.816199,U02TC8X43BN\\n041dfe42-a772-42bc-bba6-d25f827e6e55,U02TC8X43BN,,,\"In week 2, we did like you said. During the lectures, the instructors told how to download csv, convert to parquet, upload to gcs, and then create the table. However, for the assignment, we were told that creating the table was optional since it was not required for the HW.\\n\\nIn week 3, we picked up from we left after the assignment  - we have the data in parquet and we do the following\\n• create table (exactly as shown in the videos for week 2)\\n• partition it\\nThe query you have shown belongs to the second step, which answers your question \\'Would not it make more sense to...\\'; this is exactly what we do! Whether we create the table at the end of week 2 or in week 3 is a mere logistic question.\\n\\nWeek 3 additionally shows that we can run SQL queries using airflow, using which we partition the table (and for the HW, you may cluster it within airflow or in BigQuery). And of course, we learned partition/cluster in the lectures.\\n\\nLet me know if this clears your thoughts. Or ask again here (after tagging me :slightly_smiling_face: )\",1644407789.816199,1644408657.556889,U02HB9KTERJ\\n0f13a1ca-6f9f-4004-ba0d-1a037065f94d,U02SM3LKD2B,,,\"I use a separate repo for submissions, oversimplified probably. But I started with a cloned repo, so I ended commiting my changed but not pushing them to the remote - only pulling to update. If I had to issue PRs, I\\'d clone to a clean local repo again to do it.\",1644362512.657099,1644409137.247169,U02GVGA5F9Q\\nf4a401a2-a8ee-4a1a-b69f-8db838baf0f2,U02TC8X43BN,,,\"I moved the rest of my initial message here to not clutter the feed.\\n\\nSo in week 2 we upload every month csv as a bucket to our raw bucket. In week 3, the slides demonstrated the query to build an external table by appending all the months csv.\\nFor example:\\n```CREATE OR REPLACE EXTERNAL TABLE `taxi-rides-ny.nytaxi.external_yellow_tripdata`\\nOPTIONS (\\n  format = \\'CSV\\',\\n  uris = [\\'<gs://nyc-tl-data/trip> data/yellow_tripdata_2019-*.csv\\', \\'<gs://nyc-tl-data/trip> data/yellow_tripdata_2020-*.csv\\']\\n);```\\nHowever when I see the new dag that was inserted in week 3 (and of which I did not quite see where or when it was coming from in the week 3 videos/infos).\\n\\nI see the SQL query already assume the external table are made.\\n``` CREATE_BQ_TBL_QUERY = (\\n            f\"\"CREATE OR REPLACE TABLE {BIGQUERY_DATASET}.{colour}_{DATASET} \\\\\\n            PARTITION BY DATE({ds_col}) \\\\\\n            AS \\\\\\n            SELECT * FROM {BIGQUERY_DATASET}.{colour}_{DATASET}_external_table;\"\"\\n        )```\\nWould not it make more sense to first have a query to compile the external table and then do the partition.\\nOr were we supposed to have that in the Dag of week 2 already ?\\nbut the homework solution of Alexey stops at having the parquet on google storage\\n\\nIt feels like I am missing something or there is a gap between the two.\\n\\nIn the homework of week 2 it was only supposed to be yellow taxis and fhv, fhv for 2019 only, and that is what is still written on git.\\nIn the homework solution we added the green taxis, I guess no big deal as it only needs a dag edit (but also restarting the whole VM for me).   The starting dag file had a bigquery step as well and created a table for each months.\\nNow on victoria\\'s video there are only green and yellow tables, all the rest has been deleted.\\nAre we supposed to clean our bigquery folder in the same fashion.\\nIs it something we have to do manually from the interface or should it be part of the dag flow once the external table and then the partitioned tables are made.\\nHowever if we drop the external table, would not it be annoying to rebuild it in case we want to partition/cluster differently in the future.\\n\\n\\n\\n\\n\\n\\n\\n<@U02HB9KTERJ> Thank you for your answer.\\nIt is not so much that I am confused by what we are doing or how we are doing it.\\nWhat I am wondering is why or if  there is a gap in instructions between one week to the other or kind of.\\nI get that maybe the answer is that we just have to apply what we understood as you say, however it seems to me it will make more sense to either stipulate it clearly or have the solution/instruction file with all the steps.\\nMaybe it is obvious to everyone but me.\\n\\nAs of now though, I am not sure whether we are also supposed to get 2019 and 2020 for the green taxis. Is that written somewhere.\\n\\nEdit: ah ok nevermind, I see it was in the dag of week 3. I completely missed it in the file.\",1644407789.816199,1644409225.418469,U02TC8X43BN\\ne5253aa8-0cac-4307-83bb-9d7efff0d7e1,U0300EGP2EL,,,\"I use `docker compose` without issues. Moreover, my setup converts `docker-compose` so whatever command I run, it\\'s always the former.\",1644389115.865409,1644409294.279349,U02GVGA5F9Q\\n9a83d8f1-826d-4003-b469-24539ac8d273,U02TC8X43BN,,,\"Green taxis isn\\'t needed; this was stated in week 2 (Even on github).\\n\\nMy advice is to use the videos as a template; do not replicate them, e.g. I haven\\'t uploaded the data for green taxis etc. Unfortunately, there are steps we can\\'t retry (e.g. gcs_2_gcs  task)\",1644407789.816199,1644410679.010029,U02HB9KTERJ\\n7ea0875f-4c04-444b-b936-6a40cbdd2671,,9.0,,Hello there! Quick question I haven\\'t found a clear answer. How clustering in GCP is related to CLUSTERED INDEX in SQL server?,1644410774.156349,1644410774.156349,U02QDKU4GTY\\nd6c4f262-4db4-4137-8702-827f90fa3a5c,U02TC8X43BN,,,\"Well I am not 100% sure about that cause in the video of week 4 victoria states she is expecting us to have similar files online, meaning tables for both green and yellow taxis.\\n<https://youtu.be/iMxh6s_wL4Q?t=142>\",1644407789.816199,1644411055.219089,U02TC8X43BN\\na04aaec5-f3b2-4eb6-bad2-7f55316919de,U02TC8X43BN,,,I meant for week 3 :stuck_out_tongue:,1644407789.816199,1644411327.219009,U02HB9KTERJ\\nfe8fb8cd-58e7-48a8-a95e-0916f0fc71bf,U02QDKU4GTY,,,\"Overall the concept is the same, you define one or more fields that will define the way the order in which the records of those tables will be physically stored to improve performance.\\nThe implementation is a bit different because of the way you would define them and also in BQ you have more freedom to define certain parameters.\",1644410774.156349,1644411504.155529,U01B6TH1LRL\\nd4a8783e-801a-45f8-b52c-a1e0a453a185,U02TC8X43BN,,,\"<@U02TC8X43BN> I also added this as a note in the readme of week 4, if you don\\'t have ALL that data, as long as you have some data for green and yellow data you will be able to follow the videos and run the project anyways.\\nAnd fhv data will be used for the homework of week 4 (same as week 3).\\nRegardin more specifically on green taxi, you can re use the code you set up for yellow to upload the data, the URL is quite similar, you just change the name.\",1644407789.816199,1644411766.014879,U01B6TH1LRL\\nfece4b14-d669-4d30-abfa-839d27f16cb0,U02TC8X43BN,,,\"Thank you.\\nIf you don\\'t me asking, as it is probably now lost in my mess of messages:\\nBecause I see in your video it is all looking neat.\\n\\nAre we or should  we/can we, clear the bigquery folder of table that are not needed anymore.\\nShould/can it be done manually from the interface or should it be done otherwise. It seems a bit cumbersome to delete each table individually since now I have one for every month.\\n\\nAlso my bad for not seeing it in the note. I kind of go with the flow of the video in the playlist but I should read the note and launch the videos accordingly. Maybe that is the main source of my confusion with the instructions.\",1644407789.816199,1644412006.812539,U02TC8X43BN\\n420f8c39-527d-4583-ad60-7ebfa5798795,U02TC8X43BN,,,\"No worries on the note, I added just to have things clear! But except fhv, which is meant to be used for homework I also showed it in the video as you pointed out.\\nMy video is looking neat because I prepared it specifically for week 4 and didn\\'t want to show things that could confuse the students as other tables, etc. No need to clean your folder, unless you want to. Those 3 tables will be the ones you need, everything else you have won\\'t hurt.\\n\\nEither way, you can execute multiple drop table statements as one, you could even \"\"hack\"\" the query preparation using information_schema. something like this:\\n(is this useful?)\",1644407789.816199,1644413071.127879,U01B6TH1LRL\\nd14f3799-977a-404e-9151-a1258d2fa4a5,U02QDKU4GTY,,,\"Thanks Victoria for the explanation! Also to clarify further, can we perform clustering only once just like Clustered Indexes? I mean we can only have one clustered index but multiple non-clustered indexes. Does the same idea apply here?\",1644410774.156349,1644413835.430389,U02TATJKLHG\\n2045e4b0-fea6-4808-8872-c2a7b271e19b,U029JNJBPED,,,\"So what datatypes are not supported for partitioning?\\n\\nI found this in the <https://www.ibm.com/docs/en/db2/11.5?topic=types-database-partition-compatible-data|IBM Docs> -\\n\\n&gt; • Data types that are not supported as part of a distribution key are not applicable for database partition compatibility. Examples of such data types are:\\n&gt;     ◦ BLOB\\n&gt;     ◦ CLOB\\n&gt;     ◦ DBCLOB\\n&gt;     ◦ XML\\n&gt;     ◦ A distinct type based on BLOB, CLOB, DBCLOB, or XML\\n&gt;     ◦ A structured type\\nIs this the exhaustive list? Or there\\'s more to it?\\n\\nEdit: I see a table too in the docs for Partition compatibility but not sure how to read it\",1644364947.174169,1644414138.150879,U02TATJKLHG\\n1d600213-6968-4e05-af06-228ecffeb029,USLACKBOT,,,It takes some time for 8080 to start working,1644401511.198979,1644414397.474109,U01AXE0P5M3\\n3ac52182-5674-4417-83c6-a2aa3a0a9fdf,U02QDKU4GTY,,,\"In the case of the sql server non clustered indexes I don\\'t think there\\'s an equivalent in BQ yet. In fact I haven\\'t seen such concept outside of SQL server, and only recently I found out Snowflake is working in something along those lines.\",1644410774.156349,1644414619.782479,U01B6TH1LRL\\ne85329ee-6ebc-4b72-9ef6-5b93e04de142,U02QDKU4GTY,,,Gracias!,1644410774.156349,1644414700.650379,U02QDKU4GTY\\n9b1b1d0f-b4c9-42a9-80bd-bc62dd2fc4bc,USLACKBOT,,,\"sir, I have been waiting for 1 hr, still not loading. is .env file has any effect on that. I have changed AIRFLOW_UID=50000, while it was 1000 previously.\",1644401511.198979,1644414738.321469,U0300EGP2EL\\n0b83f3cc-b3cd-4dfb-b1ea-bae7f8f2d384,USLACKBOT,,,\"I don\\'t know, the information from the logs is not enough to see what could be the issue. maybe clean everything and start again?\",1644401511.198979,1644414903.334249,U01AXE0P5M3\\nb063a433-6d5a-4cb2-9f8a-4655467de849,USLACKBOT,,,ok will try and come back.,1644401511.198979,1644415017.888859,U0300EGP2EL\\n122a507c-7688-488e-a556-d9e7cd821d66,U02QDKU4GTY,,,Adding clustered index in SQL server DB changes the way the table is *physically* stored - is it also true for clustering in BQ?,1644410774.156349,1644415415.823859,U02Q7JMT9P1\\n5401a0fa-14bf-4517-aa3d-2057d6a243a5,U02SM3LKD2B,,,Thanks so much everyone!,1644362512.657099,1644415867.333649,U02SM3LKD2B\\n449e163e-03af-42b1-8009-edda8be792f0,,5.0,,\"Hey guys.\\nI had a late start and I\\'m on the first week trying to run Postgres on docker.\\nBut unable to get it right. Getting the error response from daemon.\\nHave tried all sorts of fixes and different file paths\\nCould anybody help out?\",1644416150.726059,1644416150.726059,U02JE8J1W8P\\n40B6EF27-5DE1-4F7A-B1F7-9127A69C3951,U02JE8J1W8P,,,Do you have docker daemon running? What os you on?,1644416150.726059,1644417065.783349,U02U34YJ8C8\\n24801960-e5b9-4748-abe2-38df6cf6c4d0,USLACKBOT,,,\"I have deleted airflow folder and again  followed the same procedure. Some where I am doing wrong, all the following are unhealthy\\n```CONTAINER ID   IMAGE                       COMMAND                  CREATED         STATUS                     PORTS                              NAMES\\n9d629c6a6827   airflow_flower              \"\"/usr/bin/dumb-init …\"\"   6 minutes ago   Up 5 minutes (unhealthy)   0.0.0.0:5555-&gt;5555/tcp, 8080/tcp   airflow-flower-1\\n3e95cb98c771   airflow_airflow-triggerer   \"\"/usr/bin/dumb-init …\"\"   6 minutes ago   Up 5 minutes (unhealthy)   8080/tcp                           airflow-airflow-triggerer-1\\ncae19b29263a   airflow_airflow-worker      \"\"/usr/bin/dumb-init …\"\"   6 minutes ago   Up 5 minutes (unhealthy)   8080/tcp                           airflow-airflow-worker-1\\nf9e395b2e303   airflow_airflow-webserver   \"\"/usr/bin/dumb-init …\"\"   6 minutes ago   Up 5 minutes (unhealthy)   0.0.0.0:8080-&gt;8080/tcp             airflow-airflow-webserver-1\\n789666596ce7   airflow_airflow-scheduler   \"\"/usr/bin/dumb-init …\"\"   6 minutes ago   Up 5 minutes (unhealthy)   8080/tcp                           airflow-airflow-scheduler-1\\n4c2219dc6d99   postgres:13                 \"\"docker-entrypoint.s…\"\"   7 minutes ago   Up 7 minutes (healthy)     5432/tcp                           airflow-postgres-1\\n25cfb0569fa6   redis:latest                \"\"docker-entrypoint.s…\"\"   7 minutes ago   Up 7 minutes (healthy)     6379/tcp                           airflow-redis-1```\",1644401511.198979,1644417236.445679,U0300EGP2EL\\n13692dc5-98cb-46ce-8fe7-70440027ca71,U02JE8J1W8P,,,Win 11,1644416150.726059,1644417386.739289,U02JE8J1W8P\\ne93233c7-f3df-4f98-bc2e-1e74c081d8f3,U02JE8J1W8P,,,Getting the following error,1644416150.726059,1644417511.914089,U02JE8J1W8P\\n064e0ad9-1d50-47a5-9759-ad5b27ece9af,USLACKBOT,,,You probably don\\'t have sufficient ram,1644401511.198979,1644417601.047319,U01AXE0P5M3\\nae531efc-895c-430b-b261-d60a8275bd61,USLACKBOT,,,\"in wsl .wslconfig file, I have allocated memory=6GB , my laptop RAM capacity is 8GB\",1644401511.198979,1644417710.994399,U0300EGP2EL\\n43550938-e884-4507-9980-0938d99b2a2c,U02JE8J1W8P,,,try `/e/...` instead of `e:/...`,1644416150.726059,1644417963.967499,U02TATJKLHG\\n3af62e7c-1a33-4994-9d38-304c65e3c120,,2.0,,\"Hi everyone,\\n\\nI am trying to rename multiple files with wildcard in Google Cloud Storage with the following command, but got a permission error as shown in the screenshot.\\n\\n`gsutil mv <gs://dtc_data_lake_dtc-de-338802/yellow/*.parquet>` `<gs://my_bucket/yellow/yellow_tripdata_*.parquet>`\\n\\nI have already given \"\"Storage Admin\"\" and \"\"Storage Object Admin\"\" in the GCS Permission to the service account that I am using for gsutil.\\n\\nIt would be great if someone can enlighten me regarding this issue. Thanks!\",1644418295.625149,1644418295.625149,U02BRPZKV6J\\n1cdfec1d-eda3-498a-b924-363fdb9c914f,,,,,,1644418534.709669,U02BRPZKV6J\\nb1f75d0b-fb92-4b8a-bb0f-1d855662ff01,U02JE8J1W8P,,,Thanks a lot man,1644416150.726059,1644419004.659829,U02JE8J1W8P\\n4A70104D-9E23-431F-954C-D8D2F470D466,,7.0,,\"Hello, im doing the second week and i have a problem with build the image with docker-compose build \",1644419991.966299,1644419991.966299,U030PFH5CRX\\n2776ee8e-946f-4457-9565-123007651b72,U030PFH5CRX,,,\"COPY failed: file not found in build context or excluded by .dockerignore: stat scripts: file does not exist\\nERROR: Service \\'airflow-init\\' failed to build : Build failed\",1644419991.966299,1644420145.797659,U030PFH5CRX\\n7db12db0-25ee-42c5-a390-9093cff652a1,U030PFH5CRX,,,Which line it complains about?,1644419991.966299,1644420289.030289,U01AXE0P5M3\\ndaa160e0-912d-4c2f-b75d-b89171cbbbab,U030PFH5CRX,,,throws me many errors,1644419991.966299,1644420434.389109,U030PFH5CRX\\n28083a70-59f3-4a53-8e6e-e5abea6eeda5,U030PFH5CRX,,,\"`WARNING: The scripts pyrsa-decrypt, pyrsa-encrypt, pyrsa-keygen, pyrsa-priv2pub, pyrsa-sign and pyrsa-verify are installed in \\'/root/.local/bin\\' which is not on PATH.`\\n  `Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.`\\n  `WARNING: The script normalizer is installed in \\'/root/.local/bin\\' which is not on PATH.`\\n  `Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.`\\n  `WARNING: The script pyjwt is installed in \\'/root/.local/bin\\' which is not on PATH.`\\n  `Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.`\\n  `WARNING: The scripts f2py, f2py3 and f2py3.7 are installed in \\'/root/.local/bin\\' which is not on PATH.`\\n  `Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.`\\n  `WARNING: The script jsonschema is installed in \\'/root/.local/bin\\' which is not on PATH.`\\n  `Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.`\\n  `WARNING: The script flask is installed in \\'/root/.local/bin\\' which is not on PATH.`\\n  `Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.`\\n  `WARNING: The script pybabel is installed in \\'/root/.local/bin\\' which is not on PATH.`\\n  `Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.`\\n  `WARNING: The script virtualenv is installed in \\'/root/.local/bin\\' which is not on PATH.`\\n  `Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.`\\n  `WARNING: The script slugify is installed in \\'/root/.local/bin\\' which is not on PATH.`\\n  `Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.`\\n  `WARNING: The script pygmentize is installed in \\'/root/.local/bin\\' which is not on PATH.`\\n  `Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.`\\n  `WARNING: The script plasma_store is installed in \\'/root/.local/bin\\' which is not on PATH.`\\n  `Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.`\\n  `WARNING: The script mako-render is installed in \\'/root/.local/bin\\' which is not on PATH.`\\n  `Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.`\\n  `WARNING: The script google-oauthlib-tool is installed in \\'/root/.local/bin\\' which is not on PATH.`\\n  `Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.`\\n  `WARNING: The script email_validator is installed in \\'/root/.local/bin\\' which is not on PATH.`\\n  `Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.`\\n  `WARNING: The script cmark is installed in \\'/root/.local/bin\\' which is not on PATH.`\\n  `Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.`\\n  `WARNING: The script tabulate is installed in \\'/root/.local/bin\\' which is not on PATH.`\\n  `Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.`\\n  `WARNING: The script nvd3 is installed in \\'/root/.local/bin\\' which is not on PATH.`\\n  `Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.`\\n  `WARNING: The script openapi-spec-validator is installed in \\'/root/.local/bin\\' which is not on PATH.`\\n  `Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.`\\n  `WARNING: The scripts nox and tox-to-nox are installed in \\'/root/.local/bin\\' which is not on PATH.`\\n  `Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.`\\n  `WARNING: The script markdown_py is installed in \\'/root/.local/bin\\' which is not on PATH.`\\n  `Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.`\\n  `WARNING: The script gunicorn is installed in \\'/root/.local/bin\\' which is not on PATH.`\\n  `Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.`\\n  `WARNING: The script fabmanager is installed in \\'/root/.local/bin\\' which is not on PATH.`\\n  `Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.`\\n  `WARNING: The script alembic is installed in \\'/root/.local/bin\\' which is not on PATH.`\\n  `Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.`\\n  `WARNING: The script airflow is installed in \\'/root/.local/bin\\' which is not on PATH.`\\n  `Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.`\",1644419991.966299,1644420449.570959,U030PFH5CRX\\nb7646275-696e-4e51-bade-71d9085a79e8,U030PFH5CRX,,,you need to make sure the scripts folder is present,1644419991.966299,1644420525.117869,U02HB9KTERJ\\n5fe05f6b-7c6e-4e5c-ac39-e325e50c5cdf,,2.0,,\"Not a significant issue, but as a feedback since is there, i added the public learning link in the week2 form, but there is 0 in the leaderboard column. Not sure if the issue was on my side or in the form.\",1644420540.664199,1644420540.664199,U0308MF3KUH\\n08200b55-4c36-40d3-bff9-3df368df6833,U030PFH5CRX,,,<https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_2_data_ingestion/airflow/scripts>,1644419991.966299,1644420565.112849,U02HB9KTERJ\\n60596428-9d06-4aa6-befc-f1dcbfcf9860,U030PFH5CRX,,,\"yes you are right, thank you\",1644419991.966299,1644420604.151809,U030PFH5CRX\\n78618f19-fbb1-4b81-ae61-d5f2d43c7235,U0308MF3KUH,,,\"Did you work on the problems (and include the github repo link)? No link =&gt; no scores\\n\\nJust promoting the course won\\'t help you :joy:\",1644420540.664199,1644420671.783989,U02HB9KTERJ\\n8dfc026e-ff7a-42d1-919e-386724e29765,U0308MF3KUH,,,Please dm me your email,1644420540.664199,1644420672.225109,U01AXE0P5M3\\n60f4178c-b870-4897-ab86-618e1ef6fb67,,5.0,,\"After setting up Postgres with docker, i opened another bash window for pgcli.\\nAfter entering the password for root user the terminal is non responsive\",1644423146.232269,1644423146.232269,U02JE8J1W8P\\na4b05ae2-5908-4d37-8e29-7aa1b09c41a8,U02JE8J1W8P,,,Are you on windows? I think a few windows users reported that as well,1644423146.232269,1644423311.266489,U01AXE0P5M3\\n2f550b42-7082-4285-9756-c1a4a9a1fc90,U02JE8J1W8P,,,What you can do instead is to check the next  video for an alternative to pgcli,1644423146.232269,1644423350.891939,U01AXE0P5M3\\n2a0a3f81-f727-493c-b1b3-7f4d3cac55f1,U02JE8J1W8P,,,\"Oh got it. I\\'ll try that out\\nThanks\",1644423146.232269,1644423687.382249,U02JE8J1W8P\\n4f74b3ad-e462-4a79-9355-37ad33e77a67,U02CK7EJCKW,,,maybe,1644250558.066349,1644423722.794929,U02CK7EJCKW\\n4383f0f0-3bb5-4489-af51-95ee47d5d0b9,U02JE8J1W8P,,,You can just try any other terminal like cmd or bash in vs code. It works there surprisingly,1644423146.232269,1644424173.909589,U02TATJKLHG\\nec966303-902a-43b0-afe7-b7472c904e88,U02BRPZKV6J,,,Did you create a new bucket named `my_bucket`?,1644418295.625149,1644424225.831619,U02TATJKLHG\\n9bb7e73c-7844-45f8-bbdd-7a1aaf659cc8,U02JE8J1W8P,,,Thanks a ton man,1644423146.232269,1644424917.929109,U02JE8J1W8P\\n75ae3a4a-5c47-4600-af99-c514ee652eaa,,2.0,,\"Hell everyone,\\n\\nGot the following error when I run BigQueryInsertJobOperator for green trip data. Any idea how to resolve this? It seems like something to do with new line and carriage return characters.\\n```google.api_core.exceptions.BadRequest: 400 Error while reading table: trips_data_all.green_tripdata_external_table, error message: CSV table references column position 19, but line starting at position:257 contains only 1 columns.```\\nAnd also, got the following error when I tried to read yellow_tripdata_external_table:\\n```Error while reading table: dtc-de-338802.trips_data_all.yellow_tripdata_external_table, error message: Error detected while parsing row starting at position: 0. Error: Bad character (ASCII 0) encountered.```\",1644424946.630289,1644424946.630289,U02BRPZKV6J\\n4ec06bee-b28e-46f9-b628-8d0ffb4abb11,U02BRPZKV6J,,,Thanks a lot <@U02TATJKLHG>!,1644418295.625149,1644425495.177969,U02BRPZKV6J\\n84c74caa-afad-4bff-a5e1-0508384d8d2c,U02QDKU4GTY,,,\"Things are way too different between SQL Server and Big Query. The former is row-based, while the latter is column based, so physically they aren\\'t comparable.\",1644410774.156349,1644425554.411049,U02GVGA5F9Q\\n32db82e5-467c-402e-8f3c-7f361d78e2b5,,3.0,,<@U01AXE0P5M3> I just watched the video of week 2 homework solution. Did you find out why your DAG was returning `success`  without running anything?,1644425657.571219,1644425657.571219,U02T9JQAX9N\\n208e360e-aaae-4130-af92-c6fc90f8860a,U02T9JQAX9N,,,Yes exactly,1644425657.571219,1644426781.361569,U01AXE0P5M3\\ncf7f5a99-97d3-42ea-8355-5a097f57eb3d,U02T9JQAX9N,,,Why then? I didn\\'t face any issue at all,1644425657.571219,1644427419.527109,U02T9JQAX9N\\n1b74303b-a418-4667-a513-d9045321020a,,7.0,,\"While inserting the data to Postgres, I discovered the Jupyter shell was running for a longer time. I queried the DB to see the number of records that had been inserted, and I was surprised it was over 3M records. This is different from the number of records in the csv file. What could have possibly gone wrong?\\n\\nCc: <@U01AXE0P5M3>\",1644428185.308419,1644428185.308419,U02CZA4HX99\\n1592e805-6a69-438e-99a7-5ed5e50da31e,,21.0,,\"I am lagging behind and someone maybe already mentioned it but I think there is an error in the dag file to partition table on git .\\n```        bigquery_external_table_task = BigQueryCreateExternalTableOperator(\\n            task_id=f\"\"bq_{colour}_{DATASET}_external_table_task\"\",\\n            table_resource={\\n                \"\"tableReference\"\": {\\n                    \"\"projectId\"\": PROJECT_ID,\\n                    \"\"datasetId\"\": BIGQUERY_DATASET,\\n                    \"\"tableId\"\": f\"\"{colour}_{DATASET}_external_table\"\",\\n                },\\n                \"\"externalDataConfiguration\"\": {\\n                    \"\"autodetect\"\": \"\"True\"\",\\n                    \"\"sourceFormat\"\": \"\"CSV\"\",\\n                    \"\"sourceUris\"\": [f\"\"gs://{BUCKET}/{colour}/*\"\"],```\\nIt should be PARQUET for the format not CSV\",1644429302.569509,1644429302.569509,U02TC8X43BN\\n576fbe96-e77f-432f-a1b2-5bea73f60ace,U02TC8X43BN,,,\"That\\'s because <@U01DHB2HS3X> had csv files in her bucket.\\n\\n<https://youtu.be/lAxAhHNeGww?t=2333>\",1644429302.569509,1644429540.825789,U0290EYCA7Q\\nd61953f7-8291-41b0-b187-5bf8a9ba5e4f,U02TC8X43BN,,,\"Yes, I already talked about it in the video, that I\\'m using CSV\\'s, but if it\\'s PARQUET for you, please update it to Parquet\",1644429302.569509,1644429637.438579,U01DHB2HS3X\\n56908f00-b91e-4ed5-99df-5f09afd274d5,U02TC8X43BN,,,\"Ah sorry, I kind of watched the video but I must have glossed over the part you mentioned it. I was redoing the step and failing until I saw that. It also says earlier in the file it is expecting parquet so it seemed like an omission.\\nMy bad\\n\\nActually I see there is another error in the flow:\\nI get:\\n\\n``` Parquet column \\'ehail_fee\\' has type DOUBLE which does not match the target cpp_type INT64.```\\n That one I am not sure why it is happening, is it mentioned in the video as well\",1644429302.569509,1644429892.550579,U02TC8X43BN\\nc531656d-65cc-4c7b-8e62-01121d767b5f,U02BRPZKV6J,,,\"Did you change the sourceFormat to PARQUET in externalDataConfiguration? CSV is row oriented, while Parquet is column oriented. This could be the reason.\\n\\n```        bigquery_external_table_task = BigQueryCreateExternalTableOperator(\\n            task_id=f\"\"bq_{colour}_{DATASET}_external_table_task\"\",\\n            table_resource={\\n                \"\"tableReference\"\": {\\n                    \"\"projectId\"\": PROJECT_ID,\\n                    \"\"datasetId\"\": BIGQUERY_DATASET,\\n                    \"\"tableId\"\": f\"\"{colour}_{DATASET}_external_table\"\",\\n                },\\n                \"\"externalDataConfiguration\"\": {\\n                    \"\"autodetect\"\": \"\"True\"\",\\n                    \"\"sourceFormat\"\": \"\"CSV\"\",\\n                    \"\"sourceUris\"\": [f\"\"gs://{BUCKET}/{colour}/*\"\"], ```\",1644424946.630289,1644429970.086389,U0290EYCA7Q\\n1f468214-7218-44f1-b0b2-673f5421b23c,U02TC8X43BN,,,\"I am getting the same for green taxi data. I tried to load via bq tool too, but failed.\\n\\n```(base) % bq load /\\n--source_format=PARQUET /\\ntrips_data_all.green_tripdata_external_table /\\n\"\"<gs://dtc_data_lake_dtc-de-course-339017/green/*.parquet>\"\"\\n\\nWaiting on bqjob_r6697dfa8e3180718_0000017edd5ec338_1 ... (17s) Current status: DONE\\nBigQuery error in load operation: Error processing job \\'dtc-de-course-339017:bqjob_r6697dfa8e3180718_0000017edd5ec338_1\\': Error while reading data, error message: Parquet column \\'ehail_fee\\' has type DOUBLE which does not match the target cpp_type INT64.```\\nI don\\'t see the column here either\\n\\n<https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf>\",1644429302.569509,1644430040.014859,U0290EYCA7Q\\ne6c6209d-f816-4131-a212-e304eaa60c50,U02CZA4HX99,,,Paste the code that you used to insert the data,1644428185.308419,1644430270.780049,U02T9JQAX9N\\nee2fa07e-9941-4287-bb55-723af927539d,U02TC8X43BN,,,\"so it is an issue with the parquet file when it was created?\\nif that is the case maybe instead of selecting all, we can only select the right columns and ignore that one, I will try that\",1644429302.569509,1644430383.578029,U02TC8X43BN\\nbcd292fa-2dfd-464f-8852-4ac475134f21,U02TC8X43BN,,,\"I am gonna load all files one by one, to see which one is corrupted.\",1644429302.569509,1644430531.531069,U0290EYCA7Q\\n25ce4436-e4f2-4223-aa45-7699dd7b77ba,U02QDKU4GTY,,,\"<@U02GVGA5F9Q> one comment here, I worked with Azure Synapse that is columnar organized and it also has clustered and non clustered index.  It\\'s like is the same concept as traditional sql server\",1644410774.156349,1644430840.907979,U02QDKU4GTY\\n9a308ce1-cf3c-4ef7-91c0-417f679f18a6,U02TC8X43BN,,,\"```(base) airflow % bq load --source_format=PARQUET trips_data_all.green_tripdata_external_table \"\"<gs://dtc_data_lake_dtc-de-course-339017/green/green_tripdata_2019-01.parquet>\"\"\\nWaiting on bqjob_r7efbc6ea42e9f825_0000017edfba6efd_1 ... (3s) Current status: DONE\\n\\n(base) airflow % bq load --source_format=PARQUET trips_data_all.green_tripdata_external_table \"\"<gs://dtc_data_lake_dtc-de-course-339017/green/green_tripdata_2019-02.parquet>\"\"\\nWaiting on bqjob_ra60431aa4cca51_0000017edfbaa564_1 ... (2s) Current status: DONE\\n\\n(base) airflow % bq load --source_format=PARQUET trips_data_all.green_tripdata_external_table \"\"<gs://dtc_data_lake_dtc-de-course-339017/green/green_tripdata_2019-03.parquet>\"\"\\nWaiting on bqjob_r776100751b16e9da_0000017edfbad1b4_1 ... (2s) Current status: DONE\\n\\n(base) airflow % bq load --source_format=PARQUET trips_data_all.green_tripdata_external_table \"\"<gs://dtc_data_lake_dtc-de-course-339017/green/green_tripdata_2019-04.parquet>\"\"\\nWaiting on bqjob_rfdd779f30e2e612_0000017edfbafd83_1 ... (3s) Current status: DONE\\n\\n(base) airflow % bq load --source_format=PARQUET trips_data_all.green_tripdata_external_table \"\"<gs://dtc_data_lake_dtc-de-course-339017/green/green_tripdata_2019-05.parquet>\"\"\\nWaiting on bqjob_r4a5ee7c11190f274_0000017edfbb3184_1 ... (2s) Current status: DONE\\n\\n(base) airflow % bq load --source_format=PARQUET trips_data_all.green_tripdata_external_table \"\"<gs://dtc_data_lake_dtc-de-course-339017/green/green_tripdata_2019-06.parquet>\"\"\\nWaiting on bqjob_r17b86930877beef6_0000017edfbb583f_1 ... (2s) Current status: DONE\\n\\n(base) airflow % bq load --source_format=PARQUET trips_data_all.green_tripdata_external_table \"\"<gs://dtc_data_lake_dtc-de-course-339017/green/green_tripdata_2019-07.parquet>\"\"\\nWaiting on bqjob_rb4c26d7d9936864_0000017edfbb8520_1 ... (0s) Current status: DONE\\nBigQuery error in load operation: Error processing job \\'dtc-de-course-339017:bqjob_rb4c26d7d9936864_0000017edfbb8520_1\\': Provided Schema does not match Table dtc-de-\\ncourse-339017:trips_data_all.green_tripdata_external_table. Field ehail_fee has changed type from INTEGER to FLOAT\\n\\n(base) airflow % bq load --source_format=PARQUET trips_data_all.green_tripdata_external_table \"\"<gs://dtc_data_lake_dtc-de-course-339017/green/green_tripdata_2019-08.parquet>\"\"\\nWaiting on bqjob_r4dc46f79755e30f2_0000017edfbc423d_1 ... (0s) Current status: DONE\\nBigQuery error in load operation: Error processing job \\'dtc-de-course-339017:bqjob_r4dc46f79755e30f2_0000017edfbc423d_1\\': Provided Schema does not match Table dtc-de-\\ncourse-339017:trips_data_all.green_tripdata_external_table. Field ehail_fee has changed type from INTEGER to FLOAT\\n\\n(base) airflow % bq load --source_format=PARQUET trips_data_all.green_tripdata_external_table \"\"<gs://dtc_data_lake_dtc-de-course-339017/green/green_tripdata_2019-09.parquet>\"\"\\nWaiting on bqjob_rb9647a730142cdc_0000017edfbc700c_1 ... (2s) Current status: DONE\\n\\n(base) airflow % bq load --source_format=PARQUET trips_data_all.green_tripdata_external_table \"\"<gs://dtc_data_lake_dtc-de-course-339017/green/green_tripdata_2019-10.parquet>\"\"\\nWaiting on bqjob_r516bf500482311a9_0000017edfbc9c48_1 ... (2s) Current status: DONE\\n\\n(base) airflow % bq load --source_format=PARQUET trips_data_all.green_tripdata_external_table \"\"<gs://dtc_data_lake_dtc-de-course-339017/green/green_tripdata_2019-11.parquet>\"\"\\nWaiting on bqjob_r679cf2ea7c06cae8_0000017edfbcc521_1 ... (2s) Current status: DONE\\n\\n(base) airflow % bq load --source_format=PARQUET trips_data_all.green_tripdata_external_table \"\"<gs://dtc_data_lake_dtc-de-course-339017/green/green_tripdata_2019-12.parquet>\"\"\\nWaiting on bqjob_r61323c948556dac0_0000017edfbceed6_1 ... (3s) Current status: DONE```\",1644429302.569509,1644431145.327089,U0290EYCA7Q\\nb6e3668f-40e3-4275-b8e5-f0ccb6f0734f,U02TC8X43BN,,,where did you find this log,1644429302.569509,1644431220.962449,U02TC8X43BN\\nb6e92e0f-2b43-4f3d-ab7a-1e92f87c5a11,U02TC8X43BN,,,\"The partition worked when selecting just the right columns (I think, at least it is green on airflow)\",1644429302.569509,1644431244.067489,U02TC8X43BN\\n9a9e98c0-5371-4874-8381-09467a582df7,U02TC8X43BN,,,\"This is bq tool, which comes with gcloud.\",1644429302.569509,1644431245.779479,U0290EYCA7Q\\n385f0625-eda8-40a0-b2fa-18f651bd4bcd,U02CZA4HX99,,,Maybe you inserted multiple times,1644428185.308419,1644431740.810289,U01AXE0P5M3\\nb29ae314-d9ec-4c18-b102-da480c2dd0ab,U02T9JQAX9N,,,I think because I already run these tasks when preparing the materials. But I\\'m not 100% sure,1644425657.571219,1644431879.344919,U01AXE0P5M3\\nc958312e-5ec9-4e51-98ed-b2fec915ffd2,,6.0,,\"<@U01AXE0P5M3> and others…. Returning after a break, in google cloud storage have the data (yellow taxi data +  fvh data etc), now need to get into BigQuery.  QUESTION: how to efficiently create tables and declare schema?\",1644431940.974499,1644431940.974499,U02U5DPET47\\n7100cdbc-0307-439a-be7c-bbe73ddfb650,U02TC8X43BN,,,\"Also, try by removing the wildcard portion `*.parquet\"\"`. And just specifying `\"\"gs://{bucket}/green/` alone. There was another issue reported where they were facing issues with the wildcard\",1644429302.569509,1644431980.770289,U01DHB2HS3X\\n7f464ce7-c6a9-4558-ac09-a34b578f9077,U02TC8X43BN,,,\"Its actually NaN instead of None for `ehail_fee` in some some files. We need to preprocess it, or have a predefined schema for the bq table.\\n\\n```import pandas as pd\\npd.read_parquet(\\'green_green_tripdata_2019-01.parquet\\', engine=\\'pyarrow\\').head(10)[\\'ehail_fee\\']```\\nOut[11]:\\n```0    None\\n1    None\\n2    None\\n3    None\\n4    None\\n5    None\\n6    None\\n7    None\\n8    None\\n9    None\\nName: ehail_fee, dtype: object```\\nIn\\xa0[12]:\\n```pd.read_parquet(\\'green_green_tripdata_2019-07.parquet\\', engine=\\'pyarrow\\').head(10)[\\'ehail_fee\\']```\\nOut[12]:\\n```0   NaN\\n1   NaN\\n2   NaN\\n3   NaN\\n4   NaN\\n5   NaN\\n6   NaN\\n7   NaN\\n8   NaN\\n9   NaN\\nName: ehail_fee, dtype: float64```\",1644429302.569509,1644431988.142499,U0290EYCA7Q\\ndad37f57-1ca2-4436-93e5-4111ff1e6822,U02BRPZKV6J,,,Thanks for the tips <@U0290EYCA7Q>! I need to investigate in more detail regarding this.,1644424946.630289,1644432045.109309,U02BRPZKV6J\\n23ad4c9e-cc56-4eed-b500-a19e68b5f7d8,U02U5DPET47,,,\"Would think best to do entirely in google cloud, e.g. in say google colab to adapt the upload-data notebook from week 1, and will need to figure out equivalent of squalchemy for BigQuery…\\n\\nany suggestions?\",1644431940.974499,1644432151.942889,U02U5DPET47\\nb39a97ab-f76c-4edc-9e7f-6de0bb005566,,2.0,,\"The question number 4 for week 3 homework, is anybody getting any of the outlined option.\",1644432298.405069,1644432298.405069,U02T0CYNNP2\\nd5436135-0ad0-4af7-a4be-a0be93c2e4c5,,19.0,,\"Hey, I’m following along to video 4.3.1 and am at the point where I’m running my first model. It fails with the following error:\\n```404 Not found: Dataset fresh-shell-338718:trips_data_all was not found in location US```\\nBased on a google search, my specific dataset location (us-west1, in my case) needs to be specified. I’m finding that if you’re using the CLI, this is done with profiles.yml, but I’m using the web-based IDE instead. Where can I specify dataset location?\",1644432387.132949,1644432387.132949,U02U3E6HVNC\\nd0a0ed8f-03ba-4ffc-9e48-d633e0fa52b0,U02TC8X43BN,,,\"And this is actually the DDL statement for external table.\\n\\n```CREATE TABLE `dtc-de-course-339017.trips_data_all.green_tripdata_external_table`\\n(\\n  VendorID INT64,\\n  lpep_pickup_datetime TIMESTAMP,\\n  lpep_dropoff_datetime TIMESTAMP,\\n  store_and_fwd_flag STRING,\\n  RatecodeID INT64,\\n  PULocationID INT64,\\n  DOLocationID INT64,\\n  passenger_count INT64,\\n  trip_distance FLOAT64,\\n  fare_amount FLOAT64,\\n  extra FLOAT64,\\n  mta_tax FLOAT64,\\n  tip_amount FLOAT64,\\n  tolls_amount FLOAT64,\\n  ehail_fee INT64,\\n  improvement_surcharge FLOAT64,\\n  total_amount FLOAT64,\\n  payment_type INT64,\\n  trip_type INT64,\\n  congestion_surcharge FLOAT64\\n);```\\n`SELECT ehail_fee FROM `dtc-de-course-339017.trips_data_all.green_tripdata_external_table` LIMIT 10` outputs `null` for loaded files.\",1644429302.569509,1644432442.982469,U0290EYCA7Q\\n61fa73de-4951-4947-9e3e-a315b2a24284,U02U5DPET47,,,Check week3 videos. Is done with airflow operators,1644431940.974499,1644433040.497139,U0308MF3KUH\\n1d7e61c1-4a73-4664-b0a2-eda97cee1a3d,U02U5DPET47,,,You can check the code here and week 3 videos <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_3_data_warehouse/big_query.sql>,1644431940.974499,1644433321.109159,U01B6TH1LRL\\n9a5fd2c5-4d0f-42b4-acaf-17d67ddddad8,U02QDKU4GTY,,,\"Things are different, yes, but the concept is similar for what a cluster index is at the end. It will define the data will be ordered when storing to retrieve only the relevant blocks when querying.\\n<@U02QDKU4GTY> I haven\\'t used Azure Synapse, that\\'s interesting. I\\'ll read about how they are implementing it.\",1644410774.156349,1644433441.725759,U01B6TH1LRL\\n32e460ea-1ab5-4a89-9e14-f7f1264ff92e,U02TC8X43BN,,,any idea if this happens in all the files or just in some?,1644429302.569509,1644433619.648839,U02UA0EEHA8\\naea3db6e-e8f1-488f-bcef-08d994f55029,,15.0,,\"Still got some issues regarding my docker-compose db\\'s.\\nI can access the docker postgres db through my airflow instance, but I can\\'t get into pgadmin on my localhost, nor pgdatabase on my pgcli.\\n`pgcli -h localhost -p 5432 -U root -d ny_taxi`\\nconnection to server at \"\"localhost\"\" (::1), port 5432 failed: FATAL:  password authentication failed for user \"\"root\"\"\\n\\nDocker-compose file:\\n```version: \"\"3\"\"\\nservices:\\n  pgdatabase:\\n    image: postgres:13\\n    environment:\\n      POSTGRES_USER: root\\n      POSTGRES_PASSWORD: root\\n      POSTGRES_DB: ny_taxi\\n    volumes:\\n      - C:/Users/xxx/Documents/data-engineering-zoomcamp/Luxi/ny_taxi_postgres_data:/var/lib/postgresql/data\\n    ports:\\n      - \"\"5432:5432\"\"\\n    networks:\\n      - airflow\\n    restart: always\\n    \\n  pgadmin:\\n    image: dpage/pgadmin4\\n    environment:\\n      - PGADMIN_DEFAULT_EMAIL=admin@admin.com\\n      - PGADMIN_DEFAULT_PASSWORD=root\\n    ports:\\n      - \"\"8081:8080\"\"\\n\\n\\nnetworks:\\n  airflow:\\n    external:\\n      name: luxi_default```\",1644433771.777729,1644433771.777729,U0319KGEJ13\\n728cbb08-4160-4701-ae33-32195f3be4e8,U02TC8X43BN,,,It happened for two files in 2019. Better to create another task to handle NaN before loading.,1644429302.569509,1644434032.102399,U0290EYCA7Q\\nd602d8c3-3074-4828-83d6-ab261dd0aed4,U02TC8X43BN,,,<https://stackoverflow.com/questions/57209767/converting-nan-floats-to-other-types-in-parquet-format|https://stackoverflow.com/questions/57209767/converting-nan-floats-to-other-types-in-parquet-format>,1644429302.569509,1644434270.430909,U0290EYCA7Q\\n2d7dfd3e-62b4-414d-8770-84d8554136bd,U02TC8X43BN,,,\"Doesn\\'t look like the best solution, as it runs only for green taxi files, i added an except statement when creating the partioned table to drop the ehail_fee column. Is working for the moment.\\nCREATE_BQ_TBL_QUERY = (\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 f\"\"CREATE OR REPLACE TABLE {BIGQUERY_DATASET}.{colour}_{DATASET} \\\\\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 PARTITION BY DATE({ds_col}) \\\\\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 AS \\\\\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 SELECT * EXCEPT (ehail_fee) FROM {BIGQUERY_DATASET}.{colour}_{DATASET}_external_table;\"\")\\n\\nEdit: you can use a different query per colour with an if statement\",1644429302.569509,1644435229.670349,U0308MF3KUH\\n42453d88-1d44-4c6e-abf1-b24a87e0f5f5,U02U3E6HVNC,,,\"It looks like you have multi-region settings, which would still be an issue if you define a location in the profiles.yml.\\nIs it possible that your dbt_name schema is in another location and because you read from the US but write in let\\'s say EU you get such message? You can check from Big query UI, going to the the details of your schemas.\",1644432387.132949,1644435334.646369,U01B6TH1LRL\\n4773ea24-2818-43df-8182-ae85a2bf4311,U0319KGEJ13,,,Maybe pgcli is playing tricks with you? Can you connect from Python/Jupyter?,1644433771.777729,1644435419.381579,U01AXE0P5M3\\n4b148857-1280-4bdd-891e-eb4d39c601ed,U02T0CYNNP2,,,\"If you\\'re not getting the exact answer, select the closest one\",1644432298.405069,1644435563.333089,U01AXE0P5M3\\nbd11d44c-56db-4505-8b57-d9547e7b06a0,U0319KGEJ13,,,\"Sadly enough same problem from python outside of the docker network :confused:\\n\\n```user=\"\"root\"\"\\npassword=\"\"root\"\"\\nhost=\"\"localhost\"\"\\nport=5432\\ndb=\"\"ny_taxi\"\"\\ntable_name = \"\"yellow_taxi_data\"\"\\n\\nengine = create_engine(f\\'postgresql://{user}:{password}@{host}:{port}/{db}\\')\\nengine.connect()\\n\\nsqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at \"\"localhost\"\" (::1), port 5432 failed: FATAL:  password authentication failed for user \"\"root\"\"```\",1644433771.777729,1644435602.379149,U0319KGEJ13\\n4b9c0b18-07a5-4a78-8480-5c88ffc8233e,U02SXQ9L0FJ,,,\"I would say give your best effort but know when to move on. Try to learn the concepts, if you can\\'t implement it, then just at least understand it conceptually and run the solutions given in their github. So then you at least know what a working code/solution looks like.\\n\\nBasically know when to draw the line, because you don\\'t want to want to waste so much time debugging a small issue blocked when you could be progressing in the course and learning more\",1644340781.202059,1644436187.711979,U02T9550LTU\\n7a4ff955-3a61-4e7b-b456-eaafbd9e1b02,U02TC8X43BN,,,I got this issue while running the task using create external table operator.,1644429302.569509,1644436204.277369,U0290EYCA7Q\\nf0ea2fa6-4841-4a8c-b986-21bafeba5f92,U0319KGEJ13,,,\"I was able to access it before I hooked it up to the airflow network, so I\\'ll just wait till all the files have been imported and then remove it from the network I suppose, so I can finish W3 homework :slightly_smiling_face:\",1644433771.777729,1644436221.664189,U0319KGEJ13\\nef2c22d6-aa38-436e-a5d5-d86b3fa7bfde,U02U5DPET47,,,\"In the meantime, a useful tool to be aware of for accessing cloud storage buckets…  “Cloud Storage FUSE is an open source\\xa0<http://fuse.sourceforge.net/|FUSE>\\xa0adapter that allows you to mount Cloud Storage buckets as file systems on Linux or macOS systems. It also provides a way for applications to upload and download Cloud Storage objects using standard file system semantics.”\\n <https://cloud.google.com/storage/docs/gcs-fuse#charges>\",1644431940.974499,1644436696.740649,U02U5DPET47\\nf7b7c020-3c6b-499b-be47-fa85e5e869ad,U02U5DPET47,,,And on the BigQuery side … <https://cloud.google.com/bigquery/docs/tables#python>,1644431940.974499,1644437496.830169,U02U5DPET47\\n8526ee1c-2ebd-4341-bbda-7563f4c79aea,U02U5DPET47,,,\"…it may not be necessary to set this up in Python, but for my future understanding, I’d really like to!\",1644431940.974499,1644437552.969989,U02U5DPET47\\n5486de9a-5372-4b01-910f-dd02058ccb18,U02RTJPV6TZ,,,I faced the same issue and I could not find any solution and started to setup a new VM and other operations.,1644174128.390709,1644438299.815019,U02TPL8JXS5\\n8ba41dad-5812-466f-8b5c-bb6a5d80dd9e,U02QDKU4GTY,,,\"Conceptually yes, they\\'re all about the same. This is true to everything, if you rise the abstraction level you use... But I agree with both, while maintaining my point! :slightly_smiling_face: I haven\\'t used Azure Synapse too (with Azure I\\'m still at the IaaS level), but probably it isn\\'t very different from BQ!\",1644410774.156349,1644440752.245819,U02GVGA5F9Q\\n2aadeb85-6cbb-4e27-93c8-329b7d622c32,U02TC8X43BN,,,\"That is interesting as I get exactly same issue on the really last step, where creating bq table from external table as GerryK\",1644429302.569509,1644441536.594969,U02DD97G6D6\\nf02d5b0e-1a5c-42e1-bff0-d939e0493e32,,1.0,,\"Did anyone else run into the issue with GCSToGCSOperator not including the yellow_tripdata prefix when the files are moved from /raw to /yellow? Not a big deal, but I can\\'t figure out why it would be doing this. Below is my code\\n```\\n    gcs_2_gcs_task = GCSToGCSOperator(\\n    task_id=\"\"gcs_2_gcs_task\"\",\\n    source_bucket=BUCKET,\\n    source_object=\\'raw/yellow_tripdata*.parquet\\',\\n    destination_bucket=BUCKET,\\n    destination_object=\"\"yellow/\"\",\\n    move_object=False\\n)```\\nI know that in the videos we set move_object to True, but after having to redownload all the files for the third time I set it to False to make my life easier. If a file is titled yellow_tripdata_2019-01.parquet it becomes _2019-01.partquet in the yellow/ directory.\",1644443541.970059,1644443541.970059,U02TNEJLC84\\n3d80f723-063d-4177-9210-3c6541a34afc,U02TC8X43BN,,,\"Hi,\\nIt’s possible that certain files, while being converted to parquet, are not retaining the original type for some fields (eg. 10.0 Decimal auto-converts to 10 INT).\\nThese files possibly need to be explicitly type-casted (individually) before pushing to GCS (sorry :disappointed: )\\n\\nIf the number of files having this issue is quite large, I would also suggest uploading CSV’s directly (the way I did it in the video). And create BQ tables based on them. But please note you cannot have mixed formats/schema on a GCS path to create BQ tables upon (as the schema needs to be consistent).\\n\\nFinally, I have just uploaded a quick hack, for those running out of time, to upload CSV’s/Parquets (any format) directly (without Airflow), in order to be ready for Week 4. Please tweak, and use as per your setup.\\n\\n<https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_3_data_warehouse/extras>\\n\\nCC: <@U01AXE0P5M3> <@U01B6TH1LRL>\",1644429302.569509,1644447364.523129,U01DHB2HS3X\\n9f67fbb4-cc36-412f-ba79-dd11ee0d9d91,,10.0,,\"At the start of dbt tutorial, I am running in a location issue.\\n```was not found in location US```\\n if I preview stg_green_tripdata.sql it works but not when I do a dbt-run.\\nMy dataset is in europe-west6, from a github thread I see it has something to do with multi vs single region\\n<https://github.com/dbt-labs/dbt-bigquery/issues/19#issuecomment-635545315>\\nAnd that it could be solved with giving the location details in a profiles.yml\\n<https://docs.getdbt.com/reference/profiles.yml>\\nShould I do that?\",1644447441.422219,1644447441.422219,U02TC8X43BN\\n216befb2-9118-4d3e-a5bf-28d24ebfe417,U026040637Z,,,Resolved. My <gs://bucket/file> information was incorrect. Thanks for your help!,1644288053.633869,1644450214.874409,U026040637Z\\n3a6310ec-1f48-4cec-b5ba-72a87d1055c2,U02TC704A3F,,,\"Until now, only suffering hahaha. Really, I can\\'t understand very well why is so hard to just change a format within a platform that depends on data, I thought would be something way more simpler.\\n\\nI gave up on trying to do this on GCP. I tried a lot of things in Dataflow, the error logs doesn\\'t help at all.\\n\\nThe worst thing is that Dataflow has a template to do exactly what I want, but itsn\\'t working.\\n\\nI will try a couple more things today but if doesn\\'t work, I will give up because I didn\\'t even started week 4 yet.\",1644296434.560629,1644450320.605229,U02TC704A3F\\n9fdc9b94-ebdb-444a-8eca-48b3da593548,U02BRPZKV6J,,,\"<@U02TNEJLC84>\\n\\nThis may help you too\",1644366977.276219,1644450645.514299,U02TC704A3F\\nb13a42dd-4c79-4cd0-9345-b58af68a2a04,U02U3E6HVNC,,,\"In BigQuery, all my data tables show the data location as us-west1 in the details tab. In Google Storage, the location type says \"\"Region\"\" and the location says \"\"us-west1.\"\" It looks like <@U02TC8X43BN> is having a similar problem?\",1644432387.132949,1644455035.969199,U02U3E6HVNC\\nd63184fe-20bf-4cff-8b9d-61d6404efa72,U02UJGGM7K6,,,\"Are we supposed to be doing this on the partitioned or non-partitioned data. I believe my count is right, but the estimated and actual still aren\\'t matching up.\",1644230952.007529,1644456042.011069,U02TNEJLC84\\n54545227-e7d4-4447-a9f2-f120d514f9d3,,5.0,,\"i was following the video for gcs to bigquery and got this error\\n```google.api_core.exceptions.BadRequest: 400 POST <https://bigquery.googleapis.com/bigquery/v2/projects/dezoomcamp22/datasets/trips_data_all/tables?prettyPrint=false>: Invalid value for sourceFormat: .parquet is not a valid value```\\nbigquery_external_table_task = BigQueryCreateExternalTableOperator(\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 task_id=f\"\"bq_{colour}_{DATASET}_external_table_task\"\",\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 table_resource={\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \"\"tableReference\"\": {\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \"\"projectId\"\": PROJECT_ID,\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \"\"datasetId\"\": BIGQUERY_DATASET,\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \"\"tableId\"\": f\"\"{colour}_{DATASET}_external_table\"\",\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 },\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \"\"externalDataConfiguration\"\": {\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \"\"autodetect\"\": \"\"True\"\",\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \"\"sourceFormat\"\": f\"\"{INPUT_FILETYPE}\"\",\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 # \"\"sourceFormat\"\": \"\"parquet\"\",\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \"\"sourceUris\"\": [f\"\"gs://{BUCKET}/{colour}/*\"\"],\",1644456122.637809,1644456122.637809,U02VBG59VQ9\\n775e5175-9eb2-4fe4-b6ad-37669ff14c29,,2.0,,\"Just in case anyone else runs into the issue of the Big Query results being Cached so you don\\'t see the actual amount of data you can add\\n```and RAND() &lt; 2```\\nto your where clause.\",1644456177.655079,1644456177.655079,U02TNEJLC84\\n2a2181d8-772a-4822-8005-5ec571e30d61,U02VBG59VQ9,,,\"I believe you may have over parameterized. In my code it has\\n```\"\"sourceFormat\"\": \"\"PARQUET\"\"```\\nLooks like \"\".parquet\"\" is not a keyword that the programming knows. Try hard coding it or doing a .replace(\\'.\\',\\'\\').upper(). I\\'d keep it simple at first and just try hard coding it and go from there.\",1644456122.637809,1644456351.466319,U02TNEJLC84\\nd72b3850-d7a9-4b7d-8bc7-8b924d0dd44a,U02SXQ9L0FJ,,,Hi <@U02SXQ9L0FJ> please don\\'t feel like you\\'re alone. I and many others have mentioned this previously - I,1644340781.202059,1644456397.020789,U02U5SW982W\\n1e343b4a-2536-4200-9e3f-8d0de9b05d1a,U02SXQ9L0FJ,,,\"I believe <@U02T9550LTU> , <@U02HB9KTERJ>, <@U02S83KSX3L> and <@U02A3AU35LL> offer sound advice. Stay strong!\",1644340781.202059,1644456525.527859,U02U5SW982W\\nb92e4762-b9d9-44fb-868e-a37a156b1894,U02VBG59VQ9,,,\"<@U02TNEJLC84> mike weird it worked but then got this error\\n```google.api_core.exceptions.BadRequest: 400 Error while reading table: trips_data_all.green_tripdata_external_table, error message: Parquet column \\'ehail_fee\\' has type DOUBLE which does not match the target cpp_type INT64.```\\n\",1644456122.637809,1644456709.012349,U02VBG59VQ9\\na8cbe9c5-befb-4ba5-afd8-2cc99bb0f4fe,,2.0,,\"```google.api_core.exceptions.BadRequest: 400 Unrecognized name: tpep_pickup_datetime at [1:86]\\n\\n(job ID: airflow_gcs_2_bq_dag_bq_create_yellow_tripdata_partitioned_table_task_2022_02_09T00_00_00_00_00_55ec420f8c714426e4fb07c2e07608a2)\\n\\n                                                                                    -----Query Job SQL Follows-----                                                                                     \\n\\n    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |\\n   1:CREATE OR REPLACE TABLE trips_data_all.yellow_tripdata             PARTITION BY DATE(tpep_pickup_datetime)             AS             SELECT *  FROM trips_data_all.yellow_tripdata_external_table;\\n    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |\\n[2022-02-10, 01:49:00 UTC] {local_task_job.py:154} INFO - Task exited with return code 1\\n[2022-02-10, 01:49:00 UTC] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check```\\n\",1644458029.042529,1644458029.042529,U02VBG59VQ9\\n62B96A62-402F-4753-A6FD-C8984E579B7E,U02UJGGM7K6,,,You can always approximate.,1644230952.007529,1644460180.333249,U01DFQ82AK1\\n01a8b4e7-1460-4ac0-988c-49b44297da05,U02VBG59VQ9,,,\"<@U02VBG59VQ9> See this\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1644447364523129?thread_ts=1644429302.569509&amp;cid=C01FABYF2RG>\",1644456122.637809,1644461553.146519,U02HB9KTERJ\\n5835351f-2bb2-4636-a570-d7da951a3208,U0319KGEJ13,,,Did you figure it out?,1644433771.777729,1644473388.824829,U01AXE0P5M3\\n1484316c-c480-4884-aecc-57d1df21c4d2,U0319KGEJ13,,,\"Maybe there\\'s something in the logs? Or something already running on this port, e.g another pg?\",1644433771.777729,1644473431.253149,U01AXE0P5M3\\n7ce8a0e9-4eae-4d04-8b46-2d3079669b4b,U02U3E6HVNC,,,\"Were would I see that in the bigquery UI, in the details on my table schema it says I am in europe-west6 in all of them.\",1644432387.132949,1644474779.801659,U02TC8X43BN\\na4450c65-e7ed-45a4-a75b-4283b72178aa,U02U6DR551B,,,\"<@U02T697HNUD>, if I enable the DAG instead of triggering it, will it queue and run the tasks at the next run datetime?  The UI shows I have 37 successes but nothing queued.  And this would be the \"\"correct\"\" approach as opposed to triggering and backfilling manually correct?\",1643873524.132099,1644478252.302449,U02SEB4Q8TW\\n6fbc0a42-4ac8-4f98-951d-45c02ec694b8,U02CZA4HX99,,,\"I thought as much but I used the same code as demonstrated in the video\\n\\n```while True:\\n\\xa0 \\xa0 t_start = time()\\n\\xa0 \\xa0 df.tpep_pickup_datetime =                           pd.to_datetime(df.tpep_pickup_datetime)\\n\\xa0 \\xa0 df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\\n\\n\\xa0 \\xa0 df.to_sql(name=\\'yellow_taxi_data\\', con=engine, if_exists=\\'append\\')\\n\\xa0 \\xa0 t_end = time()\\n\\n\\xa0 \\xa0 print(\\'inserted another chunck, took %.3f second\\' % (t_end - t_start))```\\n<@U01AXE0P5M3>\",1644428185.308419,1644478653.501299,U02CZA4HX99\\nf50104b6-a8d9-4c0c-8784-d4d3b6c90474,U02RSAE2M4P,,,\"<@U01AXE0P5M3> yes maybe. I tried checking for all the containers running on my computer, found the postgres3 container and replaced the name in the .env but still I run into the same problem.\\n\\nAre we supposed to run the week_2 ingestion, locally? I\\'m trying to run the dagIngestionFile on the VM on GCP and I think maybe the pgdatabase is running locally. Or am I missing something?\",1643937705.301049,1643961745.596059,U02RSAE2M4P\\n0c87d314-9892-48b4-a30e-63917189ca95,U02RSAE2M4P,,,You don\\'t need to run it locally,1643937705.301049,1643965111.916929,U01AXE0P5M3\\n5570be5a-634a-43d1-b2da-686398a1a0ee,,7.0,,\"How can I assign more resource to my Docker on Windows? I can\\'t even find the .wslconfig file mentioned in most articles I came across\\n\\nchallenge: Airflow Webserver  keeps restarting\\n\\nEnviroment: wsl2, windows 11\",1643965171.669549,1643965171.669549,U02V90BSU1Y\\nd4d9db5b-3cdb-4c81-aff3-1305c4f39d3c,U02V90BSU1Y,,,You have to create it yourself for the content of the file you can google it,1643965171.669549,1643965244.019339,U02TA7FL78A\\n5c656cb5-f6eb-42ad-8994-e8a33497d985,U02TZ1JCVEC,,,I\\'m not sure what you mean,1643960371.831749,1643965430.658499,U01AXE0P5M3\\n0fc7131e-70d6-417f-9869-d012d85bbae1,U02V90BSU1Y,,,\"Sure, as <@U02TA7FL78A> said. I did it since I noticed more than 8G were being used when executing tasks in parallel and causing the blue screen. 6 tasks in parallel consumed 10G, so be careful.\",1643965171.669549,1643967012.922659,U02LQMEAREX\\nA73740E1-F3F0-4293-B2C7-230E8A8C118A,U02BVP1QTQF,,,Correct. The gist is only to have a VM ready with Conda and Docker as quickly as possible but does not specify how to set up Airflow nor anything else. But it should be ready to follow the rest of instructions for lessons 1 and 2,1643661114.567919,1643967566.180089,U02BVP1QTQF\\n03cdc3cb-2849-4b4c-8aa1-1e4a486c9cb7,U02V90BSU1Y,,,\"u can disable wsl2 , I think it consumes more memory as compared to assigning to docker\",1643965171.669549,1643967696.313329,U02RW07CVTJ\\n834c6b36-3cde-4826-b389-c6c5bb1dc82f,U02TZ1JCVEC,,,<@U02TZ1JCVEC> you are running the same dB as you ran for Week1 material. Whatever data you had inserted then will be here as well.,1643960371.831749,1643967702.734579,U02QK4ZV4UX\\n489d98a8-7f8d-4cdf-aa79-634cfd4a040f,U02UX664K5E,,,\"0. Setup wget command in windows\\n\\t- link: <https://stackoverflow.com/questions/29113456/wget-not-recognized-as-internal-or-external-command/64900250#64900250>\\n\\t- download link: <https://eternallybored.org/misc/wget/>\\ni. copy &amp; paste the .exe file into C:Windows/System32\\nii. wget -h to check the installation\",1642721639.474000,1643967923.943379,U02T697HNUD\\nfa0a07f2-c3f4-4639-ab6a-c78ec2f5af00,,4.0,,Did anyone succeed to run airflow with `docker-compose-nofrills.yml` on Windows/WSL2?,1643968440.423979,1643968440.423979,U030HKR0WK0\\n478aa395-de0f-4312-823d-c254595fa808,U030HKR0WK0,,,\"It has been deprecated. Use the one from video 2.3.4. It is not supposed to be optional; it is the recommended way :slightly_smiling_face:\\n\\nWorks like a charm on my system\",1643968440.423979,1643968998.143359,U02HB9KTERJ\\nb379138d-b09b-48b9-9b7d-867fd94b2dbd,U030HKR0WK0,,,\"Nops... Sorry.\\nI did one similar approach to nofrills.\\nAlexey talked here:\\n<https://www.youtube.com/watch?v=A1p5LQ0zzaQ&amp;list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&amp;index=21>\",1643968440.423979,1643969329.496349,U02CD7E30T0\\n66477f3a-1528-4434-95c4-f1b45c4055ff,U02V90BSU1Y,,,Are you using the full version for Airflow or the light one?,1643965171.669549,1643969457.328999,U02CD7E30T0\\nd3a96207-faf8-4d4b-831c-f7a696ae01a4,,7.0,,\"\\'Morgen <@U01B6TH1LRL>! while trying to upload the csv file (seed), I got the following error:\",1643970122.981149,1643970122.981149,U02UMV78PL0\\n08bf00c8-ce4e-4e7c-8edf-2b999c654b7f,U02UMV78PL0,,,,1643970122.981149,1643970146.901399,U02UMV78PL0\\n817b1916-8be9-4eb0-a4ff-99235861ba00,U02UMV78PL0,,,\"or, better yet:\",1643970122.981149,1643970193.515379,U02UMV78PL0\\nc2b2c600-5653-4585-8fdc-6dd13ef8ddd8,U02UMV78PL0,,,```400 POST <https://bigquery.googleapis.com/upload/bigquery/v2/projects/dtc-de-course-337912/jobs?uploadType=resumable>: Invalid value for type: INT is not a valid value```,1643970122.981149,1643970200.323869,U02UMV78PL0\\nb724b977-5f3f-4ac3-b4db-443bcd30e717,U02UMV78PL0,,,\"so I changed in dbt_project.yml the column type for \"\"locationid\"\" as string. and it worked\",1643970122.981149,1643970273.832279,U02UMV78PL0\\nef4fdb2e-97b0-47b9-af60-dbe27901251e,U030HKR0WK0,,,Forgot to give the due credits. Sorry Luis.,1643968440.423979,1643970368.791109,U02HB9KTERJ\\n6cdca9bc-6a8c-4ffe-83fe-b05a3961f055,,,,\"Just a thought: the (csv) data for January should populate at the end of January, and so it must be processed in February :thinking_face:\",,1643970442.935429,U02HB9KTERJ\\nb0dfc3ae-c873-422e-9ca9-36b2b9ea4998,U02UMV78PL0,,,\"just a heads up for anyone using bigquery: the location of your dataset in profiles.yml should be regional (if originally not multi-regional). as it was returning an error with \"\"EU\"\" (dataset not found)\",1643970122.981149,1643970448.360899,U02UMV78PL0\\n7a8a638e-efbb-4526-9b94-301f2d1f17c8,U02UMV78PL0,,,\"I\\'m sorry Daniel, where are you loading the seed? Because it\\'s meant to be in your repo but I see an URL there.\\nBtw, the content for week 4 will be released on Monday along with some adjustments I still have to push to the repo.\",1643970122.981149,1643970529.465939,U01B6TH1LRL\\n9d40ef96-1ecf-4a1f-a88b-573a59b10526,,20.0,,\"Hello everyone, I got this error while running my assignment, this is for the month of january [Solved]\\n```pyarrow.lib.ArrowInvalid: CSV parse error: Expected 18 columns, got 35: 2,2019-01-10, 12:20:52 UTC9-01-10 12:30:59,1,.88,1,N,186,107,1,7.5,0,0.5,1.66,0,0.3,9.96,VendorID ```\",1643971082.881109,1643971082.881109,U02SPLJUR42\\nf3cae5b3-fbb9-42e0-975a-4e96c2133d0b,U02UMV78PL0,,,\"yes, I am running it from my repo.\\nnot sure if I understood the URL part, the one showing is the API\\'s.\\nthanks for the update!\",1643970122.981149,1643971223.857119,U02UMV78PL0\\nc7e59974-9e9f-43b8-aa85-37b985cacf86,U02SPLJUR42,,,\"If that\\'s for Jan 2020 For Hire Vehicle data, then that\\'s a problem with CSV file, no need to worry :smile:\",1643971082.881109,1643971618.642449,U02TATJKLHG\\n0d68beea-58bb-4d03-b272-e076387b39e7,U02UX664K5E,,,\"Hey <@U02UX664K5E> in my experience this lies on each company\\'s or team\\'s preference. I have a very good experience with such tools, but the goals and resources are different. In my opinion, some of the things to take under consideration:\\n• *Complexity of sources and/or destination*: If you have a variety of sources or a variety of destinations, it could be an easier setup to go for pipeline-as-a-service tools that will have integrations and will simplify the setup, rather than the engineer having to adapt scripts, etc. If those sources also change frequently, there\\'s a good benefit on the schema evolution features that would mean you will immediately have those changes in your dwh and the data loaded without even having to notice. This is a great advantage for scalability of your pipelines.\\n• *Complexity you want to have in the ETL process*: pipeline-as-a-service tools are more limited in this sense, you can\\'t just code it but rather stick to their functions and steps they have. Plus there\\'s no 100% transparency on the actual script, something that in a very technical team may not be wanted plus it limits you as well in the monitoring and observability around those pipelines. \\n• *Technical expertise and roles definition within your team:* Having such tool simplifies the ETL process to a few clicks, this is something that can be extremely useful for smaller teams with less technical expertise or a lack of data engineers with capacity to focus on maintaining the pipelines. We have one of such tools and I, not a data engineer, can create a pipeline in a few seconds and start transforming my data in the data warehouse, what I\\'m actually trained to do. \\n• *Cost*: This is a bit of a grey area because there are so many of these tools, even open source, and it will depend on your pileines, connectors, etc, but is worth mentioning that there\\'s a point where paying on the data engineer time will be more worthy than paying the licenses for such tools (i.e. informatica). The same can happen the other way around, for a very small team setting up the data stack could be cheaper to use one of these tools vs hiring a data engineer\",1643918217.868769,1643971664.585129,U01B6TH1LRL\\n05d5ec3e-0ff2-4364-a323-ebcef9d5716d,U02SPLJUR42,,,\"no, it is for taxi_data\",1643971082.881109,1643974393.594549,U02SPLJUR42\\n785d545b-b40b-4478-bf5c-9134b0c99ec8,U02SPLJUR42,,,Can you share more details? What are you trying to do?,1643971082.881109,1643974679.431329,U02TATJKLHG\\ne922426f-51ee-4fe5-aa78-acdceb097bbc,U02SPLJUR42,,,<@U02SPLJUR42> Looks like the new line char is missing for one line,1643971082.881109,1643974706.921939,U02Q7JMT9P1\\n79c638bc-2121-429e-bc33-e15fcabaab22,U02SPLJUR42,,,did you encounter something like this ??,1643971082.881109,1643974738.922859,U02SPLJUR42\\na7c3ce05-0aba-458c-8ecc-731e4d8c573d,U02SPLJUR42,,,\"i tried it locally and it worked, I don\\'t know why i am facing this issue\",1643971082.881109,1643974762.903669,U02SPLJUR42\\n15c4cca9-e510-4439-9e13-70ba7f26372d,U02SPLJUR42,,,\"<@U02TATJKLHG> Funny thing, I had no problems with FHV 2020-01 data. Had a brief look in BQ - the data looks fine. I had an encoding error in FHV 2020-02 though...\",1643971082.881109,1643974817.601839,U02Q7JMT9P1\\naec50ead-be6b-4f10-968c-28075f50dfde,U02SPLJUR42,,,\"Hello <@U02Q7JMT9P1>, did you encounter something like this??\",1643971082.881109,1643974892.982089,U02SPLJUR42\\nb8e1eb51-071d-42ff-b05f-5827bd9d8d7d,U02SPLJUR42,,,for the first month,1643971082.881109,1643974898.787489,U02SPLJUR42\\n8b17a705-8f77-4eae-8c44-ec66bbfd380d,U02SPLJUR42,,,\"<@U02SPLJUR42> I didn\\'t, but it\\'s obvious from the error message you posted - 35 cols - is almost double of the source 18 -&gt; a carrier return sign got lost and glued two rows together\",1643971082.881109,1643974908.701619,U02Q7JMT9P1\\n84b9523b-fd55-481f-838c-6ad097bd1b76,U02SPLJUR42,,,how can I solve this??,1643971082.881109,1643974932.107629,U02SPLJUR42\\n02538125-09d1-4297-bbdb-2827e65cabb4,U02SPLJUR42,,,\"check the data, try to reload the source file\",1643971082.881109,1643974953.708359,U02Q7JMT9P1\\nd2316585-3ae9-49fb-b005-334504126337,U02SPLJUR42,,,Okay I will try that approach,1643971082.881109,1643974979.569299,U02SPLJUR42\\n08cb7502-c696-4078-8de2-b076bde9b5da,U030HKR0WK0,,,Thanks <@U02CD7E30T0> and <@U02HB9KTERJ>. My docker-compose does not recognize the `x-airflow-common` used in `docker-compose.yml` . Is the version value in this file `version:3`correct? (My docker version is 20.10.12 and is on Windows10/WSL ),1643968440.423979,1643975029.350399,U030HKR0WK0\\n642933ba-e7bc-43f2-89e7-098ba1ca76a9,U02SPLJUR42,,,\"<@U02Q7JMT9P1> Ohh alright. But I guess because people were getting confused on the failure of Jan 2020 FHV data\\'s failure and considering it a blocker, the instructors only asked for 2019 data for homework. I wonder why it didn\\'t fail for you though!\",1643971082.881109,1643976545.035849,U02TATJKLHG\\naa2dc6fc-3c5a-4e8a-9b2b-0a04e5067224,U02SPLJUR42,,,\"<@U02TATJKLHG> Yeah, I am wondering if I did something wrong :sweat_smile: I even managed to overcome the problem with fhv 2020-02 by simply adding `encoding_errors = \\'ignore\\'` parameter when reading from csv. Checked the row where the error was in BQ - data looks good :thinking_face:\",1643971082.881109,1643977031.188949,U02Q7JMT9P1\\ncb6aa688-0b29-44bf-90f4-67d79d57d937,,10.0,,Got FHV 2020-01 data imported without any extra effort on the first try - did I do something wrong? :laughing:,1643977195.789939,1643977195.789939,U02Q7JMT9P1\\n6b4b311c-b2e0-4f62-9299-315cd7343216,U02Q7JMT9P1,,,\"You do it in batches with pandas, right? PyArrow is not as flexible when it comes to reading csv\",1643977195.789939,1643977253.227579,U01AXE0P5M3\\n0488aa44-07df-4c11-a49d-bb4bc7ca4f29,U02Q7JMT9P1,,,\"yep, did conversion of pandas batches into parquet files with `Table.from_pandas` PyArrow method\",1643977195.789939,1643977495.823919,U02Q7JMT9P1\\nf0fe66e8-6375-4ccf-a727-dce5822bb56b,U02SPLJUR42,,,\"<@U02Q7JMT9P1> did you do something like pa.read_csv(\\'csv_file\\', encoding_errros=\\'ignore\\')??\",1643971082.881109,1643977544.407389,U02SPLJUR42\\nd8682d17-d125-4078-8cd1-1193a79ca29b,U02SPLJUR42,,,\"<@U02SPLJUR42> Yes, I needed to add it when I had `UnicodeDecodeError` reading from csv\",1643971082.881109,1643977631.521909,U02Q7JMT9P1\\nf4a8fbf0-87d4-4e68-8732-fc83b660bd4e,U02SPLJUR42,,,\"Okay, I have gone to the documentation but can\\'t find any to ignore the CSV parse error\",1643971082.881109,1643977688.580259,U02SPLJUR42\\nda1bad9a-2997-45aa-beac-22c8c81c97c2,U02SPLJUR42,,,but I had it in Pandas read_csv method,1643971082.881109,1643977710.274179,U02Q7JMT9P1\\n82b31c74-de17-43da-bcfd-1fb4c278ba48,U02RSAE2M4P,,,\"Hi <@U02RSAE2M4P>, please check if your postgres container is running in the same network as the airflow containers.\",1643937705.301049,1643977836.195159,U02SEH4PPQB\\na4a9c403-9acf-41e1-a466-0d7bc1653b19,U02T9550LTU,,,Also remember this is made possible by setting  `catchup = True`,1643940333.121189,1643978114.478559,U02SEH4PPQB\\n5a2c71ec-4437-4df4-a5fe-b763831115c0,,3.0,,\"There seems to be a problem with my dag runs. My start_date is 2019-1-1 and end_date is 2019-12-31.  Schedule interval is \"\"0 6 2 * *\"\". But the dag runs for Jan, Feb, then Dec. Then it jumps to 2022!. We are supposed to change the date, right?\",1643978170.597399,1643978170.597399,U02HB9KTERJ\\n4865eec1-9110-4ffb-a7fd-7cf6c9529109,U02HB9KTERJ,,,<@U02HB9KTERJ> how did you change the date?,1643978170.597399,1643978443.430079,U02SPLJUR42\\nee0d4313-8411-4f97-8fa7-e5f470709480,U02HB9KTERJ,,,clicking on the play button and \\'trigger dag with config\\' option,1643978170.597399,1643978523.348559,U02HB9KTERJ\\n984745b7-353f-4b72-88d6-025de26f95ff,,2.0,,\"while following the week 1 videos I run into this error, I don\\'t know what is wrong because I haven\\'t done anything different. \\'\\'\\' $ winpty docker run -it \\\\\\n&gt;  -e POSTGRES_USER:\"\"root\"\" \\\\\\n&gt;  -e POSTGRES_PASSWORD:\"\"root\"\" \\\\\\n&gt;  -e POSTGRES_DB:\"\"ny_taxi\"\" \\\\\\n&gt;  -v /c/Users/ugoo/datatalktest/2_docker_sql/ny_taxi_postgres_data\"\":\"\"/var/lib/postgresql/data \\\\\\n&gt;  -p 5432:5432 \\\\\\n&gt; postgres:13\\nError: Database is uninitialized and superuser password is not specified.\\n       You must specify POSTGRES_PASSWORD to a non-empty value for the\\n       superuser. For example, \"\"-e POSTGRES_PASSWORD=password\"\" on \"\"docker run\"\".\\n\\n       You may also use \"\"POSTGRES_HOST_AUTH_METHOD=trust\"\" to allow all\\n       connections without a password. This is *not* recommended.\\n\\n       See PostgreSQL documentation about \"\"trust\"\":\\n•        <https://www.postgresql.org/docs/current/auth-trust.html>\",1643978915.537779,1643978915.537779,U02V2G4R796\\n9f27437d-32c5-4825-8457-0a1e09b5c73a,U02HB9KTERJ,,,Manually clicking on update button and refresh seems to make it work (with no other code changes).,1643978170.597399,1643978983.938439,U02HB9KTERJ\\n859e8a40-ff69-40e2-968a-0e7d9ea6671f,U02RSAE2M4P,,,\"<@U02SEH4PPQB> Using  `docker network inspect airflow_default` I can see all the containers running on the \\'airflow_default\\' on my GCP VM.  I can see the airflow-postgres-1 which is for Postgres and I do not need atm. I have another postgres running locally on my computer, where I would to connect according to video 2.3.3. The local postgress is called pgdatabase in the yaml, and I have edited it to add this airflow network to connect to it. However, I keep this error. I\\'m still confused as to why. My .env is also correct.\",1643937705.301049,1643979570.735109,U02RSAE2M4P\\n9012faec-1152-4b8b-84ba-c75b687e0166,U02V2G4R796,,,\"Check syntax where:\\n-v /c/Users/ugoo/datatalktest/2_docker_sql/ny_taxi_postgres_data\"\":\"\"/var/lib/postgresql/data\",1643978915.537779,1643980113.176229,U02QNCUUHEY\\n8839e0b6-c5d9-474f-afd2-aefee1ce7073,U02Q7JMT9P1,,,I used the original PyArrow code with no problems at all.,1643977195.789939,1643980160.645619,U02GVGA5F9Q\\n482c662e-7e13-4ca5-9097-1cd05f304bfc,U02U5DPET47,,,\"Thanks <@U01AXE0P5M3>.  Learning a lot from the class, excited to keep going (and will hope to go back later to try airflow again:-).\\n\\nIs there a summary of the data that we should have uploaded to be ready for week 3 work?\",1643937023.081149,1643980225.989979,U02U5DPET47\\nd59d6e2a-73f4-4af5-904b-1b3e4c7f105c,U02RSAE2M4P,,,\"<@U02RSAE2M4P> I think you need to forward port 5432 since airflow containers are running on the VM while postgres is running locally.\\nYou can also consider running the postgres container on the VM rather than locally.\",1643937705.301049,1643980340.708059,U02SEH4PPQB\\nb104a6ff-1c6f-4e39-8f9d-a85f80e94a0c,U02RSAE2M4P,,,\"Thanks <@U02SEH4PPQB>. I\\'m actually doing your option 2 now. I would see if that works. I\\'ve been on this for hours, but I\\'m learning...so no complaints.\",1643937705.301049,1643980726.260469,U02RSAE2M4P\\n0ff0e300-4fd2-4142-955f-131b661d05a5,U02RSAE2M4P,,,But why do you think I need to forward port 5432 instead of 8080?,1643937705.301049,1643980772.800659,U02RSAE2M4P\\n5cd1c5e1-f584-441e-88cb-f8e335665aa4,U02TVGE99QU,,,\"&gt; ```initdb: error: could not change permissions of directory \"\"/var/lib/postgresql/data\"\": Operation not permitted```\\nWorkaround:\\n\\nThe same answer as with simple docker - we could use volumes, run:\\n```docker volume create --name dtc_postgres_volume_local -d local```\\nand then docker-compose up\\nas i understand, volumes is a propper way to store data locally outside of docker\\n\\nmy docker-compose.yml\\n```# sudo docker volume create --name dtc_postgres_volume_local -d local\\n\\n#version: \"\"3.9\"\"\\nservices:\\n  pgdatabase:\\n      image: postgres:13\\n      environment:\\n        - POSTGRES_USER=root\\n        - POSTGRES_PASSWORD=root\\n        - POSTGRES_DB=ny_taxi\\n      volumes:\\n        - \"\"dtc_postgres_volume_local:/var/lib/postgresql/data:rw\"\"\\n      ports:\\n        - \"\"5432:5432\"\"\\n  pgagmin:\\n      image: dpage/pgadmin4\\n      environment:\\n        - PGADMIN_DEFAULT_EMAIL=admin@admin.com\\n        - PGADMIN_DEFAULT_PASSWORD=root\\n      ports:\\n        - \"\"8080:80\"\"\\n\\nvolumes:\\n  dtc_postgres_volume_local:\\n    external: true```\",1643227187.403500,1643981047.764399,U02R725A716\\n775a001c-82cb-4674-a746-d286432f498b,U02RSAE2M4P,,,<@U02RSAE2M4P> We are both learning . We can forward both port 5432 and 8080 but port 5432 is by default allocated to postgres while port 8080 is allocated to airflow webserver.,1643937705.301049,1643981117.712059,U02SEH4PPQB\\n7c01513b-6dec-4a79-b284-44db97737d75,,3.0,,\"I think this question was asked before, but I couldn\\'t find a proper answer. I don\\'t understand why airflow schedules a run for today even though I specifically included an end_date variable in the DAG. It would be nice to know if someone found a solution to this.\",1643981227.661999,1643981227.661999,U02UVKAAN2H\\n263e00c4-78b2-4744-acc0-2b7eeed1ea17,U02RSAE2M4P,,,\"I think I see the problem, I checked again `docker network inspect airflow_default` and I cannot find the postgres container of pgdatabase on it. So the network basically doesn\\'t know that pgdatabse exist. I don\\'t know why because in the docker-compose.yaml file, I already specified this network name and details.\",1643937705.301049,1643981272.011409,U02RSAE2M4P\\n8e5f4ce9-9d8d-4e2f-951a-32f2f0790efd,U02RSAE2M4P,,,I see this warning also: `WARN[0000] network airflow: network.external.name is deprecated in favor of network.name` when I start the container of the pgdatabase. I think it has something to do with my problem.,1643937705.301049,1643981507.188049,U02RSAE2M4P\\n03844b74-6ab1-4f2d-831d-6aad3028d0d5,U02Q7JMT9P1,,,\"<@U02Q7JMT9P1>  <@U02GVGA5F9Q> Sounds like good progress!  please elaborate — what script did you run, on what files, to what?  (assume to gcp storage &amp; bigquery, but just want to be sure to be clear &amp; learn from your experiences!). Thanks!!\",1643977195.789939,1643981532.312439,U02U5DPET47\\n7bbc309e-6649-4643-9622-a207a15cb8a8,U02UM74ESE5,,,\"Actually, I had run into the same problem. In order to install Wget on Windows:\\n• _download latest binary for windows from <https://eternallybored.org/misc/wget/|eternallybored>._ \\n• _After unzip file, Move wget.exe to your `C:\\\\Program Files\\\\Git\\\\mingw64\\\\bin` or wherever your `Git\\\\mingw64\\\\bin` file._ \\nThats it.  (<https://gist.github.com/evanwill/0207876c3243bbb6863e65ec5dc3f058#wget|Source>)\",1642898827.001000,1643981749.974129,U02S82E4N4S\\n13C51D72-D30D-4BE0-B663-43D191C65AC3,U02UX664K5E,,,<@U01B6TH1LRL> Thank you so much for your in-depth answer. So it seems like a trade off between convenience and flexibility. These no-code tools are amazing for smaller companies to get started quickly without the technical burden but can be very limiting for the more complex needs. ,1643918217.868769,1643982084.734329,U02UX664K5E\\n142d35a9-bddb-4143-8333-6b3f05bc2161,U02V1JC8KR6,,,<@U02QNCUUHEY> Thank you for the explanation.,1643931487.722259,1643982560.168429,U02V1JC8KR6\\n6316b34a-c7fa-429a-8dfa-4c1d5a125905,U02U6DR551B,,,\"If you are running the lightweight version of docker-compose.yaml you need to refresh and auto-refresh, at least this worked for me.\",1643873524.132099,1643983269.771389,U0308MF3KUH\\nd099ddcf-6d1f-466d-b3b8-325463a0f544,U031XAU5UGZ,,,Thank you for the suggestions I will try to follow the course!. <@U02U34YJ8C8> <@U02RE7T06AY> <@U02UX664K5E>,1643912043.072869,1643983425.933349,U031XAU5UGZ\\nde9383f3-7ab5-455b-8490-30dac1280545,U02SPLJUR42,,,\"I had a bad network, hence the task was always retrying. Since the curl command appends to the file, it always add the header everytime. I changed the `&gt;&gt;` to `&gt;`\",1643971082.881109,1643984622.038569,U02SPLJUR42\\n168FA23F-8C50-4FEA-8511-E391BB107C3F,,6.0,,\"Hi everyone.\\nI\\'m having an error with ingesting to gcs. It failed when it get to local_to_gcs_task.  I checked the error and hat I saw is  “Cannot determine path without bucket name”.\\n\",1643987009.755849,1643987009.755849,U02URDLB4LQ\\n303f192c-6e56-4341-a132-1de4226a331e,,5.0,,Hey everyone! Quick question can anyone explain what <@U01DFQ82AK1> means when he talks about denormalizing data in the BigQuery best practices video (3.2.1)?,1643987049.413629,1643987049.413629,U01EKHDMRGT\\na49ccbc6-04ee-492b-94f6-3bdaebe3d549,U02URDLB4LQ,,,Check if you are defining the GCP_GCS_BUCKET properly in the docker-compose.yaml file,1643987009.755849,1643987238.739559,U01EKHDMRGT\\n03d771dd-f871-4292-921e-6c67fd42087e,U02URDLB4LQ,,,\"Like <@U01EKHDMRGT> said: Do you have anything written in the docker-compose.yaml here?\\nMy bucket is `dtc_data_lake_uplifted-nuance-338810` , yours must have a different name.\",1643987009.755849,1643987581.337549,U02CD7E30T0\\n64142D9B-2FBC-4512-9B2D-315BF5137D3C,U02URDLB4LQ,,,Yes ,1643987009.755849,1643987611.243249,U02URDLB4LQ\\nDF349ECA-8D44-4BBB-A728-5B2C925CDC91,U02URDLB4LQ,,,Yes,1643987009.755849,1643987682.894159,U02URDLB4LQ\\n60C953EC-6407-4119-923D-F909C85B51D6,U02URDLB4LQ,,,,1643987009.755849,1643987695.164529,U02URDLB4LQ\\n75af4a07-66ed-4326-8be8-f880e0d220d1,U01EKHDMRGT,,,\"There 4 Normal forms (actually no one use the BCNF) that must be applied in Transactional Tables (OLTP tables).\\nBut in OLAP the table should be a little denormalized (for example, it can go only until 2N form - Order is numeric).\\nMore information here :slightly_smiling_face:\\n<https://www.javatpoint.com/dbms-normalization>\",1643987049.413629,1643987788.987519,U02CD7E30T0\\n09a2e4d5-5304-4c54-83bd-6c5d0dcdd6c6,U02Q7JMT9P1,,,\"I used the original `format_to_parquet` function of Sejalv\\'s `data_ingestion_gcs_dag.py` code, the one that uses PyArrow. It handled all the files of the three DAGs without itches! :wink:\",1643977195.789939,1643987946.975929,U02GVGA5F9Q\\nd1b4aff5-a19d-43e2-abf6-32d60fd4e736,U01EKHDMRGT,,,\"When you have normalized data, you need to perform join operations in order to fetch the right records from different tables. Join operations are expensive. Because they force data shuffling. Data shuffling in simpler terms is sending data to different nodes so that data can be collocated.\\n\\nWhen you have data denormalized, you don\\'t need to perform join operations. Hence you can fetch records faster. Denormalization comes at a cost of increased storage. But storage is far more cheaper in today\\'s age than compute time. Hence denormalization is one of the easier techniques to reduce query execution time.\\n\\nI am not sure how much of this makes sense to you, but you can check normalization, denormalization, data shuffling and further topics to gain more insight. Or may be not, it might be too early to get into these details :D\",1643987049.413629,1643988023.991949,U02TATJKLHG\\n4d35554b-7887-4a0b-b6e0-c0d171f6dce2,U02URDLB4LQ,,,\"Perhaps in the DAG you are not \"\"informing\"\" to the task that is that the bucket.\\nDo you have the DAG like this?\",1643987009.755849,1643988204.511389,U02CD7E30T0\\n5C3F3B4E-6377-493D-9B52-342A87C67141,U02Q7JMT9P1,,,\"<@U02U5DPET47> I have combined the DAG code from week2 with stepwise load from week1. The raw file is loaded as it is, but then is chopped and converted to paquet file in a loop. Thus I avoided airflow running out of resources. The trick here is to handle the exception when pandas iterator runs to the end of the file. What one gets here is a set of parquet files instead of one and has to loop over them to upload to gcs. The final step to create an external table in big query is exactly the same as in the example code we got, one has just to use a wildcard for multiple files in gcs.\\n\\nPlease write me if you have any more specific questions\",1643977195.789939,1643989145.538799,U02Q7JMT9P1\\n55c3777c-44e8-4f29-9b3b-81c242392c80,,4.0,,\"Can we get an extension for the second homework by any chance? :smiling_face_with_tear:  As full time work + following the course, I\\'m having difficulty with catching up. It\\'d be nice if you could accept late submissions with lesser scores or apply a different approach. Thanks in advance :sparkles:\",1643990314.230719,1643990314.230719,U01TKLC0T9N\\n6db37f71-0cdd-46b3-b40c-82fa45a55865,U02U5DPET47,,,\"yes, in the homework description\",1643937023.081149,1643990538.132709,U01AXE0P5M3\\na263b3ef-c783-466d-abcb-5a07c7a7e7c8,U01TKLC0T9N,,,\"They extended the deadline by Monday 7 Feb\\n<https://datatalks-club.slack.com/archives/C02V1Q9CL8K/p1643720403913159>\",1643990314.230719,1643990632.674449,U02U8FXSG1Y\\n862478c3-66e0-441c-aa1d-f06b1ba7e448,U02SXQ9L0FJ,,,Thanks guys!,1643806211.418879,1643990807.071389,U02SXQ9L0FJ\\n4058aea1-4b25-4d22-a3a8-8c4416fee521,U01TKLC0T9N,,,\"You can also take the course at your own pace, you don\\'t have to follow it with everyone\",1643990314.230719,1643991286.438629,U01AXE0P5M3\\n828e5659-43bf-489e-bad0-8603ee2fe9fc,U02UVKAAN2H,,,<@U02UVKAAN2H> if you manually run it then it does that,1643981227.661999,1643991494.883029,U02VBG59VQ9\\n682cc1fa-27ba-4871-bee2-ae333df27d08,U01EKHDMRGT,,,\"<@U02CD7E30T0> <@U02TATJKLHG> (and all!) There is actually a completely normalized form (<http://a.ky|a.k.a. graph normal form)>, it can work practically only with a highly optimized join implementation …. in development in a multicloud native DBMS :-)\\n\\nIntroduction to “Graph Normal Form”: <https://www.relational.ai/blog/graph-normal-form>\\n\\nUnderlying optimal join algorithm:  <https://www.relational.ai/blog/dovetail-join>\",1643987049.413629,1643991555.731429,U02U5DPET47\\neeefb547-f9f1-4d5f-acb7-dd85ba9918ff,U01EKHDMRGT,,,Thank you Margaret!,1643987049.413629,1643991620.235889,U02CD7E30T0\\n984a9e9c-50fd-4f01-9ae0-ef7ed62446ea,U02V90BSU1Y,,,i used the light one and wasn\\'t able to load FVH january 2019 data but it loaded all the rest,1643965171.669549,1643991762.519839,U02VBG59VQ9\\n4b36e402-8322-4b82-a39c-62b007246166,,1.0,,\"hi everyone,how did any of you manage to do terraform configuration?\",1643991777.397959,1643991777.397959,U02T70K8T61\\n484b5b7c-9c89-4382-95fa-fdbd7040f86d,U01TKLC0T9N,,,\"Ahh great to hear that, thanks <@U02U8FXSG1Y> :ok_hand::skin-tone-2::100:\",1643990314.230719,1643991814.079779,U01TKLC0T9N\\nf420799a-fa2e-45f4-9322-a9cd90da1911,U01TKLC0T9N,,,\"I know, but following along with community is always better for me :sweat_smile: Seems like I gotta push my limits more :muscle::skin-tone-2: <@U01AXE0P5M3>\",1643990314.230719,1643991888.974389,U01TKLC0T9N\\nd37b5049-fe07-45bd-aaaa-ac8c23a62640,U02TC704A3F,,,\"<@U02TC704A3F> you need to rename you last step table_id looks like you have same table_id for all you load to big query\\nbigquery_external_table_task = BigQueryCreateExternalTableOperator(\\n\\xa0 \\xa0 \\xa0 \\xa0 task_id=\"\"bigquery_external_table_task_19\"\",\\n\\xa0 \\xa0 \\xa0 \\xa0 table_resource={\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \"\"tableReference\"\": {\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \"\"projectId\"\": PROJECT_ID,\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \"\"datasetId\"\": BIGQUERY_DATASET,\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 *\"\"tableId\"\": \"\"external_table_FVH_19\"\",*\",1643940376.662509,1643991976.959559,U02VBG59VQ9\\na59f65df-b254-47b8-8fb3-3bc1dc4b3abd,U02T70K8T61,,,Just followed video 1.4.1 except used my ProjectID when prompted.,1643991777.397959,1643993610.186689,U02TNEJLC84\\n82fabec3-8161-4b30-bd2e-94b2a6f6234e,,16.0,,\"So my DAGs for week2 appear to work, but when I check the External_Table row count appears to just contain the records from the first parquet file. I went into the worker container and all of the csv files and parquet files are there. Is there a special keyword we need to use in the bigquery_external_table_task task? I\\'ve been rewatching video 3.1.1, but I can\\'t seem to nail down how we need to adapt the code shown to get this to work properly.\",1643993789.434879,1643993789.434879,U02TNEJLC84\\n8a2260f5-ef24-4ff1-a805-fe82b4a6589f,,3.0,,\"Has anyone run into any issues with the BQ machine learning deployment? I got up to the docker run command but i am getting the following error\\n\\n```(base) justinpak@justins-air tip_model % WARNING: The requested image\\'s platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\\n[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/generated_message_reflection.cc:2345] CHECK failed: file != nullptr: \\nterminate called after throwing an instance of \\'google::protobuf::FatalException\\'\\n  what():  CHECK failed: file != nullptr: \\nqemu: uncaught target signal 6 (Aborted) - core dumped\\n/usr/bin/tf_serving_entrypoint.sh: line 3:     9 Aborted                 tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"\"$@\"\"```\\nCould anyone help me troubleshoot?\",1643993990.864529,1643993990.864529,U02TBTX45LK\\n2f190678-aae7-417f-a287-a25e49d05e32,U02TNEJLC84,,,\"have you changed the `table_id` of external table?\\n```bigquery_external_table_task = BigQueryCreateExternalTableOperator(\\n        task_id=\"\"bigquery_external_table_task\"\",\\n        table_resource={\\n            \"\"tableReference\"\": {\\n                \"\"projectId\"\": PROJECT_ID,\\n                \"\"datasetId\"\": BIGQUERY_DATASET,\\n                \"\"tableId\"\": TABLE_NAME_TEMPLATE,\\n            },\\n            \"\"externalDataConfiguration\"\": {\\n                \"\"sourceFormat\"\": \"\"PARQUET\"\",\\n                \"\"sourceUris\"\": [f\"\"gs://{BUCKET}/raw/{PARQUET_FILE_NAME}\"\"],\\n            },\\n        },\\n    )```\",1643993789.434879,1643994504.043999,U030FNZC26L\\n83240c8e-61f1-4029-80ed-dbc407377dce,U02TNEJLC84,,,which `TABLE_NAME_TEMPLATE = \\'yellow_taxi_{{ execution_date.strftime(\\\\\\'%Y_%m\\\\\\') }}\\'`,1643993789.434879,1643994538.062459,U030FNZC26L\\n367BE65D-39B6-40AA-B6B2-D18374E82DFD,U02TNEJLC84,,,\"<@U02TNEJLC84> if you are using BQ query to create the table , please pay attention to that fact that parquet file in url should include * to pull all data \\nSomething similar to \\nYellow_taxi-2019-*.parquet \",1643993789.434879,1643994701.209889,U02AGF1S0TY\\n9eccf364-a17c-441a-82d6-ebc0033f55a0,U02TNEJLC84,,,<@U030FNZC26L> Are we creating a separate table for each file? I was thinking that we were creating one large table containing all the data from all of the files in the time span.,1643993789.434879,1643994737.797879,U02TNEJLC84\\n9CE76308-6185-4D08-B043-B032C14B2A39,U02TNEJLC84,,,No one table in BQ with all data from gcs,1643993789.434879,1643994762.949889,U02AGF1S0TY\\nd160a57b-3fb1-4193-bb36-a957f93d6782,U02TNEJLC84,,,I created one for each one. I didn\\'t know it will append all data.,1643993789.434879,1643994850.983819,U030FNZC26L\\nfa31a24e-54d3-4937-9259-43952707e2bb,U02TNEJLC84,,,<@U02AGF1S0TY> If we do that then won\\'t the data get duplicated? Ex: say we\\'re doing 1960-01-01 to 1960-03-01. First run will create a Parquet file for 1960-01-01 and load it. Second run will create a file for 1960-02-01 and then load the data from files 1960-01-01 and 1960-02-01 and so now there are duplicate records for 1960-01-01 and so on.,1643993789.434879,1643994917.546239,U02TNEJLC84\\n1864F7E1-FF16-4C8D-96D3-9D14231854CF,U02TNEJLC84,,,\"No it didn\\'t happen that way ..\\nI don\\'t remember the exact syntax The instructional video did use * to refer to all Individual files for each month in 2019 and 2020 ..\",1643993789.434879,1643995144.853739,U02AGF1S0TY\\nc301cd32-3595-4835-a420-1201f0e0780d,U02TNEJLC84,,,\"Just to clarify, homework for week 2 requires uploading parquet files to GCS only. Loading data from the data lake to BigQuery will be done in Week 3 as far as I understand.\",1643993789.434879,1643995240.682499,U02SUH9N1FH\\n197A819E-298A-49BB-8F29-33579C25E820,U02TNEJLC84,,,That is right .I was referring to week 3 videos,1643993789.434879,1643995318.114149,U02AGF1S0TY\\na4a63b4a-6a25-4e9a-8186-d19c0679ef0f,U02TNEJLC84,,,Ah fair enough :thumbsup:,1643993789.434879,1643995336.169529,U02SUH9N1FH\\nab356dec-db54-4962-9f6a-fd2b431f0f6d,U02TNEJLC84,,,\"<@U02TNEJLC84> You might have to try out something like the `BigQueryUpsertTableOperator`\\nbecause the `BigQueryCreateExternalTableOperator`\\nmight just create table once if you pass the same id and ignore subsequent request (that\\'s what I think is happening). You can check the Upsert Operator below if you want to try creating tables via Airflow, else this\\'ll be done in week3 from Big Query UI.\\n\\n<https://github.com/apache/airflow/blob/3e9828022b03b60d9e112f1f64340a528c8407e3/airflow/providers/google/cloud/operators/bigquery.py#L1798>\",1643993789.434879,1643996051.160989,U02TATJKLHG\\n09b39de1-e8e7-4a7f-bd6b-cb19223ddfe1,U02TNEJLC84,,,\"&gt; :man-facepalming::man-facepalming::man-facepalming: Oh, dang. I\\'ve been working since yesterday trying to get the load to work. I\\'m slow sometimes. I\\'ll wait and hopefully they will give more details than what is in the videos already posted. Just did a test run with the * method and did line counts on the two files it should have loaded. The count is WAY too high so something isn\\'t right there.\",1643993789.434879,1643996054.340609,U02TNEJLC84\\na6ba3b4a-3292-40d2-8fb3-4ba29e06ff79,U02TNEJLC84,,,<@U02TNEJLC84> SELECT\\xa0distinct\\xa0Count(*)\\xa0FROM\\xa0`dezoomcamp22.trips_data_all.external_table_19`,1643993789.434879,1643997586.601499,U02VBG59VQ9\\n30129d35-a58e-4ef5-8d53-135f49da0280,U02TNEJLC84,,,\"i got around 7,667,792\",1643993789.434879,1643997622.447709,U02VBG59VQ9\\n75bb2238-bc3e-4c00-b95e-f2129251907d,U02TNEJLC84,,,this includes 2019 and 2020 data FYI,1643993789.434879,1643997694.797499,U02VBG59VQ9\\nc2929b6d-3d4c-4dda-8ed3-7f9531f1f6c1,,6.0,,\"Hey all, for some reason my localIngestionDag is getting an \"\"AttributeError: \\'DataFrame\\' object has no attribute \\'tpep_pickup_datetime\\'\"\" error? Would appreciate any help! I am running the Dag using a clone from the github but still for some reason this error is happening\",1643999043.697849,1643999043.697849,U02T9550LTU\\n3d2c596b-666c-44a9-8720-9eaf2e0cad5f,U02T9550LTU,,,Change the column name you are using.,1643999043.697849,1643999827.529669,U02TNEJLC84\\n8358431f-abde-49bb-941e-33b1afb5061d,U02T9550LTU,,,\"```*** Reading local file: /opt/airflow/logs/LocalIngestionDag/ingest/2022-02-04T18:35:50.320890+00:00/1.log\\n[2022-02-04, 18:35:56 UTC] {taskinstance.py:1032} INFO - Dependencies all met for &lt;TaskInstance: LocalIngestionDag.ingest manual__2022-02-04T18:35:50.320890+00:00 [queued]&gt;\\n[2022-02-04, 18:35:56 UTC] {taskinstance.py:1032} INFO - Dependencies all met for &lt;TaskInstance: LocalIngestionDag.ingest manual__2022-02-04T18:35:50.320890+00:00 [queued]&gt;\\n[2022-02-04, 18:35:56 UTC] {taskinstance.py:1238} INFO - \\n--------------------------------------------------------------------------------\\n[2022-02-04, 18:35:56 UTC] {taskinstance.py:1239} INFO - Starting attempt 1 of 1\\n[2022-02-04, 18:35:56 UTC] {taskinstance.py:1240} INFO - \\n--------------------------------------------------------------------------------\\n[2022-02-04, 18:35:56 UTC] {taskinstance.py:1259} INFO - Executing &lt;Task(PythonOperator): ingest&gt; on 2022-02-04 18:35:50.320890+00:00\\n[2022-02-04, 18:35:56 UTC] {standard_task_runner.py:52} INFO - Started process 112 to run task\\n[2022-02-04, 18:35:56 UTC] {standard_task_runner.py:76} INFO - Running: [\\'***\\', \\'tasks\\', \\'run\\', \\'LocalIngestionDag\\', \\'ingest\\', \\'manual__2022-02-04T18:35:50.320890+00:00\\', \\'--job-id\\', \\'112\\', \\'--raw\\', \\'--subdir\\', \\'DAGS_FOLDER/data_ingestion_local.py\\', \\'--cfg-path\\', \\'/tmp/tmpmvj8ir01\\', \\'--error-file\\', \\'/tmp/tmp0nsh3rgf\\']\\n[2022-02-04, 18:35:56 UTC] {standard_task_runner.py:77} INFO - Job 112: Subtask ingest\\n[2022-02-04, 18:35:56 UTC] {logging_mixin.py:109} INFO - Running &lt;TaskInstance: LocalIngestionDag.ingest manual__2022-02-04T18:35:50.320890+00:00 [running]&gt; on host e57ba8607bbd\\n[2022-02-04, 18:35:56 UTC] {warnings.py:110} WARNING - /home/***/.local/lib/python3.7/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing \\'execution_date\\' from the template is deprecated and will be removed in a future version. Please use \\'data_interval_start\\' or \\'logical_date\\' instead.\\n  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))\\n\\n[2022-02-04, 18:35:56 UTC] {taskinstance.py:1426} INFO - Exporting the following env vars:\\nAIRFLOW_CTX_DAG_OWNER=***\\nAIRFLOW_CTX_DAG_ID=LocalIngestionDag\\nAIRFLOW_CTX_TASK_ID=ingest\\nAIRFLOW_CTX_EXECUTION_DATE=2022-02-04T18:35:50.320890+00:00\\nAIRFLOW_CTX_DAG_RUN_ID=manual__2022-02-04T18:35:50.320890+00:00\\n[2022-02-04, 18:35:56 UTC] {logging_mixin.py:109} INFO - yellow_taxi_2022_02 /opt/***/output_2022-02.csv 2022-02-04T18:35:50.320890+00:00\\n[2022-02-04, 18:35:56 UTC] {logging_mixin.py:109} INFO - connection established successfully, inserting data...\\n[2022-02-04, 18:35:56 UTC] {taskinstance.py:1700} ERROR - Task failed with exception\\nTraceback (most recent call last):\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1329, in _run_raw_task\\n    self._execute_task_with_callbacks(context)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1455, in _execute_task_with_callbacks\\n    result = self._execute_task(context, self.task)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1511, in _execute_task\\n    result = execute_callable(context=context)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py\"\", line 174, in execute\\n    return_value = self.execute_callable()\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py\"\", line 185, in execute_callable\\n    return self.python_callable(*self.op_args, **self.op_kwargs)\\n  File \"\"/opt/airflow/dags/ingest_script.py\"\", line 22, in ingest_callable\\n    df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/pandas/core/generic.py\"\", line 5487, in __getattr__\\n    return object.__getattribute__(self, name)\\nAttributeError: \\'DataFrame\\' object has no attribute \\'tpep_pickup_datetime\\'\\n[2022-02-04, 18:35:56 UTC] {taskinstance.py:1277} INFO - Marking task as FAILED. dag_id=LocalIngestionDag, task_id=ingest, execution_date=20220204T183550, start_date=20220204T183556, end_date=20220204T183556\\n[2022-02-04, 18:35:56 UTC] {standard_task_runner.py:92} ERROR - Failed to execute job 112 for task ingest\\nTraceback (most recent call last):\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/task/task_runner/standard_task_runner.py\"\", line 85, in _start_by_fork\\n    args.func(args, dag=self.dag)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/cli_parser.py\"\", line 48, in command\\n    return func(*args, **kwargs)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/cli.py\"\", line 92, in wrapper\\n    return f(*args, **kwargs)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py\"\", line 298, in task_run\\n    _run_task_by_selected_method(args, dag, ti)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py\"\", line 107, in _run_task_by_selected_method\\n    _run_raw_task(args, ti)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py\"\", line 184, in _run_raw_task\\n    error_file=args.error_file,\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py\"\", line 70, in wrapper\\n    return func(*args, session=session, **kwargs)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1329, in _run_raw_task\\n    self._execute_task_with_callbacks(context)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1455, in _execute_task_with_callbacks\\n    result = self._execute_task(context, self.task)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\"\", line 1511, in _execute_task\\n    result = execute_callable(context=context)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py\"\", line 174, in execute\\n    return_value = self.execute_callable()\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py\"\", line 185, in execute_callable\\n    return self.python_callable(*self.op_args, **self.op_kwargs)\\n  File \"\"/opt/airflow/dags/ingest_script.py\"\", line 22, in ingest_callable\\n    df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/pandas/core/generic.py\"\", line 5487, in __getattr__\\n    return object.__getattribute__(self, name)\\nAttributeError: \\'DataFrame\\' object has no attribute \\'tpep_pickup_datetime\\'\\n[2022-02-04, 18:35:56 UTC] {local_task_job.py:154} INFO - Task exited with return code 1\\n[2022-02-04, 18:35:56 UTC] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check```\",1643999043.697849,1643999838.974499,U02T9550LTU\\n6c1a72ca-579e-4add-bbad-e09df6d338db,U02T9550LTU,,,\"Check what the column name is in the dataframe.\\n```AttributeError: \\'DataFrame\\' object has no attribute \\'tpep_pickup_datetime\\'```\\nIt\\'s different for the FHV data.\",1643999043.697849,1643999915.101159,U02TNEJLC84\\nbbedd3df-f31b-41be-b174-cd872c4fe0ed,U02T9550LTU,,,\"<@U02TNEJLC84>, i\\'m following this video: <https://www.youtube.com/watch?v=s2U8MWJH5xA&amp;list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&amp;index=20>\\n\\nso that column should be in there... this is ingetion locally for yellow taxi data into postgres\",1643999043.697849,1643999949.239009,U02T9550LTU\\n8d7ca498-26c3-4b15-b02c-6ec61d43ea87,U02T9550LTU,,,Did you test the code in your own local Jupyter Notebook first?,1643999043.697849,1644000148.853829,U02TNEJLC84\\nb346809a-e6e8-4c78-97a9-86945c5c388a,,4.0,,\"Hi! Regarding HW 3, question 4, I\\'ve got slightly different count to the available options in the Google Form. At first I doubted about the data integrity of my Big Query tables. Did my ingestion DAGs failed at some point? Then I realized the following:\\n\\n- For dates `BETWEEN \\'2019-01-01\\' AND \\'2019-03-31\\'`, the resulting count gives me 26643\\n- For dates `BETWEEN \\'2019-01-01\\' AND \\'2019-03-30\\'` (note *one day less*), the resulting count gives me 26558\\n\\nJust bringing this heads-up in case someone is blocked here :smiley:\",1644000821.071479,1644000821.071479,U02S2TZRBL7\\nf04882fd-eccc-4e35-804c-b51975ec7e43,U02S2TZRBL7,,,\"I just made the exact same message before seeing yours\\nexcept I was getting 26647\\nActually I am still getting 26560, weird\",1644000821.071479,1644001216.914429,U02TC8X43BN\\nc67c531c-873e-424a-bfd7-af362fa599f0,U02V1JC8KR6,,,Try to make start_date earlier. The first scheduler will run at initial start date + schedule interval (1 month),1643931487.722259,1644001245.438659,U02UNB4G739\\nddb47703-edec-4157-a940-11a3aa266b25,U02U5DPET47,,,\"You can transfer from my GCP if you want, I will give access\",1643937023.081149,1644001450.850159,U02UNB4G739\\na89f6956-b631-4ef7-8bc7-be1168601cc8,U02UVKAAN2H,,,Try the DAG code from Alexey video data ingestion to local. It worked for me somehow,1643981227.661999,1644001689.619849,U02UNB4G739\\n9a9ec211-7f1b-4834-8167-1febf7a36beb,U02UVKAAN2H,,,Thanks!,1643981227.661999,1644001710.792109,U02UVKAAN2H\\n0b41d28f-11fd-4649-a4bb-f7777e4ea64a,U02S2TZRBL7,,,\"I think results will also depend on which variable we are using in the `WHERE` clause. I went with this:\\n\\n`WHERE DATE(dropoff_datetime) BETWEEN \\'2019-01-01\\' AND \\'2019-03-31\\' ...`\\n\\nOne can even go one step further and ask: how is a trip defined to be _\"\"between 2019/01/01 and 2019/03/31\"\"_ ?\\nShould we consider `pickup_datetime` only,  `dropoff_datetime` only, or may be a combination of both?\",1644000821.071479,1644001911.817419,U02S2TZRBL7\\n93dc0064-4ff0-4472-b477-9500e919c47b,U02QK4ZV4UX,,,\"Access how and from where? If the DAGs are running fine, I\\'m not sure what the issue is.\",1643869666.937519,1644004064.797289,U02U3E6HVNC\\na9e29500-846d-4969-a224-b3fdfb73d3d2,U02Q7JMT9P1,,,\"what do you mean by one step <@U01AXE0P5M3>, do you mean we will have just one task??\",1643925402.979699,1644004820.563979,U02SPLJUR42\\nbce4046f-666a-4780-9ab6-c0b94d3e06f7,U02Q7JMT9P1,,,Or more. It really depends on your situation,1643925402.979699,1644005687.996889,U01AXE0P5M3\\n1de63bb5-14f6-48b6-9a89-566a1c673a27,U02V2G4R796,,,And also how you pass env variables,1643978915.537779,1644006309.029839,U01AXE0P5M3\\n0b4b0bfe-8324-49d9-a8dc-2dd0eec87d2d,,5.0,,\"Hi all, regarding Airflow.\\nI\\'m unsure why my macro isn\\'t working correctly, it\\'s not converting itself into a string.\\nCode:\\n```local_workflow = DAG(\\n    \"\"YellowTripDataDAG\"\",\\n    schedule_interval=\"\"0 6 2 * *\"\",\\n    start_date=datetime(2021, 1, 1)\\n)\\n\\nFILENAME = \\'yellow_tripdata_{{ execution_date.strftime(\\\\\\'%Y-%m\\\\\\') }}.csv\\'\\n\\n@task\\ndef extract_data():\\n    <http://logging.info|logging.info>(f\"\"--------------------------{FILENAME}----------------\"\")\\n\\nwith local_workflow:\\n    extract_data = extract_data()```\\nLogs:\\n``` INFO - --------------------------yellow_tripdata_{{ execution_date.strftime(\\'%Y-%m\\') }}.csv----------------```\\n\",1644009935.094509,1644009935.094509,U0319KGEJ13\\nf1a1421a-0eac-4b21-be3c-2793d3c6a9e5,U0319KGEJ13,,,I\\'m running the DAG manually through the webserver. So I would expect the execution_date to be `2022-02`,1644009935.094509,1644010045.893509,U0319KGEJ13\\na3f9ce6e-334e-4eb2-9d36-92cb00dee8c6,U0319KGEJ13,,,<@U0319KGEJ13> it\\'s the way jinja templates work. All is correct in log. You can read more at <https://airflow.apache.org/docs/apache-airflow/stable/concepts/operators.html#jinja-templating>,1644009935.094509,1644011878.791669,U02QNCUUHEY\\n23778489-f87c-4db4-8aa0-107f25cd34be,U02T9550LTU,,,yes.. running into other issues... I was wondering if there was a way to test/debug from airflow?,1643999043.697849,1644011956.096109,U02T9550LTU\\n76edb250-03b1-444b-af3e-2d9a277f8574,U0319KGEJ13,,,\"So if I understand it correctly, Jinja templates only work in combination with Operators, not with tasks / python code itself, it\\'ll get transformed to a string after it\\'s passed through the Operator?\",1644009935.094509,1644012240.013499,U0319KGEJ13\\n4851b33b-03d5-40c6-943f-9dba4f6bb024,U0319KGEJ13,,,Yes,1644009935.094509,1644012723.500599,U02QNCUUHEY\\n92d5558e-79e8-4a3f-adfb-d714dc7128fe,U0319KGEJ13,,,Moving it to an operator did indeed do the trick :slightly_smiling_face: Thanks!,1644009935.094509,1644012911.487149,U0319KGEJ13\\n3293fd6e-8070-43b0-8358-67b7ab6bf6f3,,3.0,,\"Hi! In DE Zoomcamp 2.3.3 - Ingesting Data to Local Postgres with Airflow, when I try to run docker-compose of week 1 it gives me the following error:\\n``` FATAL:  data directory \"\"/var/lib/postgresql/data\"\" has invalid permissions\\n2_docker_sql-pgdatabase-1  | 2022-02-04 23:53:56.636 UTC [83] DETAIL:  Permissions should be u=rwx (0700) or u=rwx,g=rx (0750).```\\nWhen I use\\n```winpty docker run -it \\\\\\n    -e POSTGRES_USER=\"\"root\"\" \\\\\\n    -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n    -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n    -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n    -p 5432:5432 \\\\\\n    --network=airflow_default \\\\\\n    --name pgdatabase \\\\\\n    postgres:13```\\nit works. I can use docker run but what might be the issue with docker-compose in this case?\",1644019141.832489,1644019141.832489,U02R09ZR6FQ\\n472bff45-c516-4c48-b7e3-dade495e658d,U02R09ZR6FQ,,,docker-compose running the .yaml file from week2 works fine,1644019141.832489,1644019340.685919,U02R09ZR6FQ\\n6fb46db2-e140-44c1-864e-3c9a4f3b1202,U02R09ZR6FQ,,,\"Sorry <@U02UKLHDWMQ>, I only saw this now! Glad to know you it was solved!\",1643816806.173399,1644019923.751949,U02R09ZR6FQ\\n365136af-dd88-4b92-903a-fa7b1ccb462e,,1.0,,\"Hey all, in week 3 homework we only used data from FHV trips, am i correct?\",1644023529.462119,1644023529.462119,U02RA8F3LQY\\na0cf202b-4d1d-4247-a5a7-899e5adcd4e7,U02TC704A3F,,,Thanks everyone for the help!,1643940376.662509,1644024848.430919,U02TC704A3F\\n7442246e-9176-4b77-ac20-2fc49969dbfd,,3.0,,Any suggestions for what the `Failed to expand table` error when running a query means? A google search did not return any useful hits. Thanks.,1644028397.488689,1644028397.488689,U026040637Z\\n0c26f13d-a279-4451-ba88-a874354e2919,U026040637Z,,,did you successfully create an external table for zones?,1644028397.488689,1644028854.312939,U02RA8F3LQY\\n509ea9fd-1259-45dc-8543-e783a778cd15,,6.0,,\"For running postgres in week 1 did anyone else have password issues. What I mean is, did anyone else have a issues with root not being accepted?\",1644033110.180109,1644033110.180109,U02SXQ9L0FJ\\nC40F5286-6733-4CCB-99F9-82FF7E3B2B24,U02SXQ9L0FJ,,,\"I actually did, so I ended up uninstalling everything (bc I previously had it installed with a different password and whatnot) and restarted the entire course. Strangely, when I copied and ran the code from GitHub, instead of typing it out, it worked :thinking_face: which part are you at? \",1644033110.180109,1644034234.161949,U02BTB7H2Q3\\nc38e1590-a03a-471d-9cce-5fbbc6936e61,U02S2TZRBL7,,,I had the same problem. I just went with the one with `2019-03-30` as the answer but I think the one with `2019-03-31` should have been the right answer,1644000821.071479,1644036070.559219,U02TATJKLHG\\n720c9131-d37c-4e6c-93d6-b39acf68f129,U02SXQ9L0FJ,,,yea.... ^ i also did the same thing,1644033110.180109,1644042805.397219,U02T9550LTU\\na65c5b2a-b015-4809-a4a3-5cee381563d0,U02SXQ9L0FJ,,,even mapping a docker container with pg to port 5431 didn\\'t help?,1644033110.180109,1644045008.591879,U01AXE0P5M3\\n7b70dc14-6aa8-4d09-9f2e-f5f89cd1592a,U02SXQ9L0FJ,,,interesting,1644033110.180109,1644045016.903579,U01AXE0P5M3\\n586a9ca8-67a0-41cf-9b20-4e0228d73fff,U02RA8F3LQY,,,yes,1644023529.462119,1644045044.983789,U01AXE0P5M3\\n1a1023b4-f7c7-42d3-bdc6-5780ca443cdf,U02R09ZR6FQ,,,\"maybe week 2 is a bit too early, but indeed, if plain docker gives you problems, try fast-forward a few videos and run it with docker-compose\",1644019141.832489,1644045106.434109,U01AXE0P5M3\\nfd0e691d-361a-4ff7-9bb0-3d21828a8efa,U02S2TZRBL7,,,I\\'ll check it this weekend. perhaps the issue could be how we get the data for the video (with transfer service) and here (with airflow),1644000821.071479,1644045165.418269,U01AXE0P5M3\\n3025a265-966f-41c3-8c9a-650f3b64d121,U02TBTX45LK,,,\"it seems that you need a computer with a different processor (amd64, not arm)\",1643993990.864529,1644045223.948809,U01AXE0P5M3\\n29a7d586-1391-43af-956e-2f400cb88ab5,U02TBTX45LK,,,\"is it tensorflow serving? try googling \"\"tensorflow serving arm64 docker image\"\"\",1643993990.864529,1644045264.623249,U01AXE0P5M3\\needd88ae-fa8e-468c-8ced-b03ff4c35498,,11.0,,\"I\\'m running into an error on the homework:\\n```\\'Execution Date\\' FAILED: The execution date is 2022-02-05T07:17:28.953173+00:00 but this is after the task\\'s end date 2021-01-01T00:00:00+00:00.```\\nBecause i wanted to set the range for the downloads.  Any ideas on how to fix this? I feel like it would be aneasy fix and I I was wondering how everyone  was able to set the range for the downloads.\",1644047251.049729,1644047251.049729,U02T9550LTU\\n6003a615-5c3a-4b0f-9dd8-b3f012b99385,U02T9550LTU,,,\"```    \"\"start_date\"\": datetime(2019, 1, 1),\\n    \"\"end_date\"\": datetime(2021, 1, 1),```\",1644047251.049729,1644047331.258099,U02T9550LTU\\n5baa40fb-f88a-4c14-9c2a-c192a535fa61,U02T9550LTU,,,\"Set\\n```catchup=True```\\n\",1644047251.049729,1644047521.891649,U02QNCUUHEY\\n246ef3df-9f73-4a98-a461-5baad6816017,U02U5SW982W,,,It was just that last step in that doc Alexey. Thanks for pointing this out.,1644208872.428749,1644221290.434919,U02U5SW982W\\n6005c279-a2d6-4e48-9a50-b263a35f91a9,U02SPLJUR42,,,<@U02HB9KTERJ>can you guide me on how you used cloud network??,1644158033.659209,1644221839.834579,U02SPLJUR42\\nc196a04a-e87b-4898-b2b2-59209dc59fad,U02RSAE2M4P,,,\"I keep getting this error in my logs on VM.\\n\\nI already created the .google\\\\credentials\\\\google_credentials.json in my root on the VM.\\n```google.auth.exceptions.DefaultCredentialsError: File /.google/credentials/google_credentials.json was not found.```\",1644189271.291429,1644223117.536269,U02RSAE2M4P\\nf7dec24a-8baa-4b34-bd44-a18500f3eee0,U02U5K0RFK8,,,\"Hi <@U02U5K0RFK8> I run into the same issue before while I was trying to access the Airflow UI on 8080.\\nMy workaround was to edit the .env file, at the `_AIRFLOW_WWW_USER` section I changed the values like this\",1644203421.832589,1644226194.334309,U031T71PEL8\\nb0adbdaa-2817-4cd9-bf8d-c6e29e5e9536,U02RSAE2M4P,,,\"I already tried doing this\\n`/home/samson/.google/credentials/:/.google/credentials:ro`\\non the VM but it still doesn\\'t work.\",1644189271.291429,1644226271.006959,U02RSAE2M4P\\n31b023c1-d649-4c8e-ad99-7288f036636b,,1.0,,\"Hello, I use the VM from gcp with 16g  but i have a problem with memory on docker with airflow(i have only 3.9 g dedicated for docker in my VM )so how I can increase memory with shell commands in Ubuntu ? thank you in advance\",1644226978.024679,1644226978.024679,U02DBNR22GN\\n69e8e1f6-dc74-4e4b-b251-5b4a657d238b,,4.0,,\"Hi! Any reason why my table count for external_table is always the same, even after `bigquery_external_table_task` is successful?\\n\\n*compose.yml -  Setting the env variable for GCP*\\n\\n```    GCP_PROJECT_ID: \\'dtc-de-course-339017\\'\\n    GCP_GCS_BUCKET: \"\"dtc_data_lake_dtc-de-course-339017\"\"```\\n*DAG Task - Getting the env variable, and setting big query dataset*\\n```BIGQUERY_DATASET = os.environ.get(\"\"BIGQUERY_DATASET\"\", \\'trips_data_all\\')\\nPROJECT_ID = os.environ.get(\"\"GCP_PROJECT_ID\"\")\\nBUCKET = os.environ.get(\"\"GCP_GCS_BUCKET\"\")\\n\\n    bigquery_external_table_task = BigQueryCreateExternalTableOperator(\\n        task_id=\"\"bigquery_external_table_task\"\",\\n        table_resource={\\n            \"\"tableReference\"\": {\\n                \"\"projectId\"\": PROJECT_ID,\\n                \"\"datasetId\"\": BIGQUERY_DATASET,\\n                \"\"tableId\"\": \"\"external_table\"\",\\n            },\\n            \"\"externalDataConfiguration\"\": {\\n                \"\"sourceFormat\"\": \"\"PARQUET\"\",\\n                \"\"sourceUris\"\": [f\"\"gs://{BUCKET}/raw/{parquet_file}\"\"],\\n            },\\n        },\\n    )```\\n```airflow@34dc5bbcf58b:/opt/airflow/logs/data_ingestion_gcs_dag_homework_q1/bigquery_external_table_task/2021-05-02T06:00:00+00:00$ more 1.log\\n[2022-02-07 10:30:23,297] {taskinstance.py:1032} INFO - Dependencies all met for &lt;TaskInstance: data_ingestion_gcs_dag_homework_q1.bigquery_external_table_task scheduled__2021-05-02T06:00:00+00:00 [qu\\neued]&gt;\\n[2022-02-07 10:30:23,340] {taskinstance.py:1032} INFO - Dependencies all met for &lt;TaskInstance: data_ingestion_gcs_dag_homework_q1.bigquery_external_table_task scheduled__2021-05-02T06:00:00+00:00 [qu\\neued]&gt;\\n[2022-02-07 10:30:23,342] {taskinstance.py:1238} INFO -\\n--------------------------------------------------------------------------------\\n[2022-02-07 10:30:23,343] {taskinstance.py:1239} INFO - Starting attempt 1 of 2\\n[2022-02-07 10:30:23,344] {taskinstance.py:1240} INFO -\\n--------------------------------------------------------------------------------\\n[2022-02-07 10:30:23,391] {taskinstance.py:1259} INFO - Executing &lt;Task(BigQueryCreateExternalTableOperator): bigquery_external_table_task&gt; on 2021-05-02 06:00:00+00:00\\n[2022-02-07 10:30:23,401] {standard_task_runner.py:52} INFO - Started process 13202 to run task\\n[2022-02-07 10:30:23,411] {standard_task_runner.py:76} INFO - Running: [\\'***\\', \\'tasks\\', \\'run\\', \\'data_ingestion_gcs_dag_homework_q1\\', \\'bigquery_external_table_task\\', \\'scheduled__2021-05-02T06:00:00+00:\\n00\\', \\'--job-id\\', \\'176\\', \\'--raw\\', \\'--subdir\\', \\'DAGS_FOLDER/data_ingestion_gcs_dag_hw1.py\\', \\'--cfg-path\\', \\'/tmp/tmpfp25um5q\\', \\'--error-file\\', \\'/tmp/tmpa13_ipk5\\']\\n[2022-02-07 10:30:23,415] {standard_task_runner.py:77} INFO - Job 176: Subtask bigquery_external_table_task\\n[2022-02-07 10:30:23,596] {logging_mixin.py:109} INFO - Running &lt;TaskInstance: data_ingestion_gcs_dag_homework_q1.bigquery_external_table_task scheduled__2021-05-02T06:00:00+00:00 [running]&gt; on host d\\na56ca004a21\\n[2022-02-07 10:30:23,801] {warnings.py:110} WARNING - /home/***/.local/lib/python3.7/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing \\'execution_date\\' from the templat\\ne is deprecated and will be removed in a future version. Please use \\'data_interval_start\\' or \\'logical_date\\' instead.\\n  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))\\n\\n[2022-02-07 10:30:23,855] {taskinstance.py:1426} INFO - Exporting the following env vars:\\nAIRFLOW_CTX_DAG_OWNER=***\\nAIRFLOW_CTX_DAG_ID=data_ingestion_gcs_dag_homework_q1\\nAIRFLOW_CTX_TASK_ID=bigquery_external_table_task\\nAIRFLOW_CTX_EXECUTION_DATE=2021-05-02T06:00:00+00:00\\nAIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-05-02T06:00:00+00:00\\n[2022-02-07 10:30:23,863] {crypto.py:82} WARNING - empty cryptography key - values will not be stored encrypted.\\n[2022-02-07 10:30:27,378] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=data_ingestion_gcs_dag_homework_q1, task_id=bigquery_external_table_task, execution_date=20210502T060000, start_\\ndate=20220207T103023, end_date=20220207T103027\\n[2022-02-07 10:30:27,462] {local_task_job.py:154} INFO - Task exited with return code 0\\n[2022-02-07 10:30:27,576] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check\\nairflow@34dc5bbcf58b:/opt/airflow/logs/data_ingestion_gcs_dag_homework_q1/bigquery_external_table_task/2021-05-02T06:00:00+00:00$```\\n\\n*I see parquet files are getting uploaded into bucket.*\\n\\n```(base) % gsutil ls -lr <gs://dtc_data_lake_dtc-de-course-339017/>\\n<gs://dtc_data_lake_dtc-de-course-339017/raw/>:\\n 137625296  2022-02-07T06:57:49Z  <gs://dtc_data_lake_dtc-de-course-339017/raw/yellow_2019-01.parquet>\\n 128380769  2022-02-07T07:08:21Z  <gs://dtc_data_lake_dtc-de-course-339017/raw/yellow_2019-02.parquet>\\n 144113501  2022-02-07T07:20:16Z  <gs://dtc_data_lake_dtc-de-course-339017/raw/yellow_2019-03.parquet>\\n 136706305  2022-02-07T07:28:49Z  <gs://dtc_data_lake_dtc-de-course-339017/raw/yellow_2019-04.parquet>\\n 137418656  2022-02-07T07:45:53Z  <gs://dtc_data_lake_dtc-de-course-339017/raw/yellow_2019-05.parquet>\\n 126726779  2022-02-07T07:56:52Z  <gs://dtc_data_lake_dtc-de-course-339017/raw/yellow_2019-06.parquet>\\n 115534598  2022-02-07T08:06:25Z  <gs://dtc_data_lake_dtc-de-course-339017/raw/yellow_2019-07.parquet>\\n 111332831  2022-02-07T08:14:35Z  <gs://dtc_data_lake_dtc-de-course-339017/raw/yellow_2019-08.parquet>\\n 120286338  2022-02-07T08:27:04Z  <gs://dtc_data_lake_dtc-de-course-339017/raw/yellow_2019-09.parquet>\\n 130950231  2022-02-07T08:38:26Z  <gs://dtc_data_lake_dtc-de-course-339017/raw/yellow_2019-10.parquet>\\n 124810768  2022-02-07T08:50:42Z  <gs://dtc_data_lake_dtc-de-course-339017/raw/yellow_2019-11.parquet>\\n 125360614  2022-02-07T09:03:33Z  <gs://dtc_data_lake_dtc-de-course-339017/raw/yellow_2019-12.parquet>\\n 116228546  2022-02-07T09:12:50Z  <gs://dtc_data_lake_dtc-de-course-339017/raw/yellow_2020-01.parquet>\\n 113958025  2022-02-07T09:38:44Z  <gs://dtc_data_lake_dtc-de-course-339017/raw/yellow_2020-02.parquet>\\n  54378226  2022-02-07T09:44:29Z  <gs://dtc_data_lake_dtc-de-course-339017/raw/yellow_2020-03.parquet>\\n  26074630  2022-02-06T14:50:02Z  <gs://dtc_data_lake_dtc-de-course-339017/raw/yellow_tripdata_2021-01.parquet>\\nTOTAL: 16 objects, 1849886113 bytes (1.72 GiB)```\\n*Count remains the same, and shows the count for yellow_tripdata_2021-01.parquet.* \\n\\n*My assumption is the table already exists, and BigQueryCreateExternalTableOperator doesn\\'t do anything.*\\n\\n\\n```(base) % bq query --use_legacy_sql=false \\\\\\n\\'SELECT\\n\\xa0 \\xa0COUNT(1)\\n\\xa0FROM\\n\\xa0 \\xa0`dtc-de-course-339017`.trips_data_all.external_table\\'\\nWaiting on bqjob_r720608f2c98f7fc4_0000017ed3917ab6_1 ... (0s) Current status: DONE\\n+---------+\\n|   f0_   |\\n+---------+\\n| 1369765 |\\n+---------+```\",1644227045.368479,1644227045.368479,U0290EYCA7Q\\ne0492e0b-d901-4732-a8c7-1751412bbac1,U02SPLJUR42,,,I followed video 1.4.1. You don\\'t need to install conda - just docker and docker-compose.,1644158033.659209,1644227747.148359,U02HB9KTERJ\\nf9d86c96-9f54-4d3c-a9a4-d85350c7e603,U0290EYCA7Q,,,Maybe we need to use this operator `BigQueryUpsertTableOperator` instead?,1644227045.368479,1644229147.282789,U0290EYCA7Q\\n4ea5a6d3-8315-4aca-8163-c48c9e54b679,U02SEH4PPQB,,,Thank you <@U02BKFQ6DNG> for the elaborate explanation. I now understand,1644160040.904359,1644230011.513809,U02SEH4PPQB\\nf7d9cf11-2566-4320-a7f9-ea890ceeba5a,,9.0,,\"Hello there! Got a question about week 3 homework (BigQuery SQL):\\nIn Question 4: What is the count, estimated and actual data processed for query which counts trip between 2019/01/01 and 2019/03/31 for dispatching_base_num B00987, B02060, B02279.\\nThere are given 4 options:\\n`Count: 0, Estimated data processed: 0MB, Actual data processed: 600MB`\\n`Count: 26558, Estimated data processed: 400 MB, Actual data processed: 155 MB`\\n`Count: 26558, Estimated data processed: 155 MB, Actual data processed: 400 MB`\\n`Count: 26558, Estimated data processed: 600 MB, Actual data processed: 155 MB`\\nBut for some reason my answers don\\'t match any of them, could you help me to find out what\\'s wrong? :arrow_down:\",1644230952.007529,1644230952.007529,U02UJGGM7K6\\nc20cbae7-86c5-4be2-8bad-9bbe0703cc7a,U02UJGGM7K6,,,\"I created 3 tables: non partitioned, partitioned, and partitioned and clustered  all together,  and tried to compare performance running the following queries:\",1644230952.007529,1644230965.605139,U02UJGGM7K6\\na62fd82f-d7e5-4c16-9a18-ee5360e855ac,U02UJGGM7K6,,,\"# Task 4 queries\\n```SELECT COUNT(*) FROM\\nde-zoomcamp-10.nytaxi.table_fhv_tripdata\\nWHERE DATE(dropoff_datetime) BETWEEN \\'2019-01-01\\' AND \\'2019-03-31\\'\\n      AND\\n      dispatching_base_num in (\\'B00987\\', \\'B02060\\', \\'B02279\\');```\\n-- Query complete (2.3 sec elapsed, 643 MB processed)\\n\\n```SELECT COUNT(*) FROM\\nde-zoomcamp-10.nytaxi.partitioned_table_fhv_tripdata\\nWHERE DATE(dropoff_datetime) BETWEEN \\'2019-01-01\\' AND \\'2019-03-31\\'\\n      AND\\n      dispatching_base_num in (\\'B00987\\', \\'B02060\\', \\'B02279\\');```\\n-- Query complete (0.5 sec elapsed, 400.1 MB processed)\\n\\n```SELECT COUNT(*) FROM\\nde-zoomcamp-10.nytaxi.partcluster_table_fhv_tripdata\\nWHERE DATE(dropoff_datetime) BETWEEN \\'2019-01-01\\' AND \\'2019-03-31\\'\\n      AND\\n      dispatching_base_num in (\\'B00987\\', \\'B02060\\', \\'B02279\\');```\\n-- Query complete (0.4 sec elapsed, 138.1 MB processed)\",1644230952.007529,1644230981.328099,U02UJGGM7K6\\n96cb01dc-9224-4f4b-b4dd-396bde795cec,U02UJGGM7K6,,,Count in my case is always 26643,1644230952.007529,1644231054.421009,U02UJGGM7K6\\n108735f6-f918-46d8-80fb-aa83dfe068c4,U02DBNR22GN,,,\"You don\\'t. It will use all the available memory. You can limit only. 4GB was the maximum amount of memory a 32bit system could access. Assuming there are no 32bit VMs anymore, there must be something limiting the available memory inside the VM.\",1644226978.024679,1644232331.216149,U02GVGA5F9Q\\n8C11C151-E11F-4D1B-8D32-3DF39F620596,U02UJGGM7K6,,,\"Hi Ilya, i will first focus on getting count correct. That depends on your initial data source from csv load. As your data is more, It can be that you have multiple files with same data. A distinct on a unique key might help to dig deeper. \",1644230952.007529,1644233134.940249,U01DFQ82AK1\\naaa6ca9a-279e-4188-b298-fb30d7ee6ea9,U02U5SW982W,,,Everything is good now?,1644208872.428749,1644234457.115139,U01AXE0P5M3\\n192b3435-027b-4d5e-a9a4-1e0d015eb34f,U0290EYCA7Q,,,That\\'s a very long message  - could you please put most of it in the thread?,1644227045.368479,1644234548.803219,U01AXE0P5M3\\na7557be5-5815-4920-b892-a3ec1e88f48d,U0290EYCA7Q,,,\"In the task to upload data into external table. we use BigQueryCreateExternalTableOperator, which is used for creating a table.\\nDo we need to load all nyc trip data into external table using airflow task? Just parquet is enough?\",1644227045.368479,1644234824.132369,U0290EYCA7Q\\n6b8eb725-e33e-4015-a717-d6277819f339,,1.0,,Pgcil,1644234994.820359,1644234994.820359,U031W3YLT0U\\nb39bce2d-445e-4b1a-9628-1e8a3369b340,U02U5SW982W,,,Please am having issues tr,1644208872.428749,1644235611.841669,U031W3YLT0U\\n4b2b5523-2618-4eea-96e6-86b0a17d52d1,U031W3YLT0U,,,pgcli*,1644234994.820359,1644235880.513909,U031RREL78V\\n81daa3a4-cda8-49b4-9415-5943fb2aedd8,U02TMEUQ7MY,,,\"hi this is what am getting when trying to run it $ pip show pgcil\\nWARNING: Package(s) not found: pgcil\",1642864750.357300,1644237648.913199,U031W3YLT0U\\ne7813357-c984-42d5-a1f9-6b5382d501fa,U02UJGGM7K6,,,\"My count was also 26643. If you change the date to 03-30-2019, then the count will be 26558. Either my/your implementation of the date range is wrong  or the author of the question made a mistake while writing the query.\",1644230952.007529,1644237912.888429,U02UVKAAN2H\\n2d987ffc-1048-4018-831a-89a7d5d85345,U01UMAXUPSQ,,,\"There already some very good resources online that keep getting updated with new releases of both BQ, SF and others competitors.\\nWhen we evaluated migration to Snowflake at my company, I found t<https://hevodata.com/learn/snowflake-vs-redshift-vs-bigquery/|his one in particular >as one of the most complete ones. It\\'s sponsored but still very insightful and covers a lot of aspects of the comparison\",1644172067.855989,1644238214.743169,U01B6TH1LRL\\ndb235687-97b9-4e87-baa1-1eae23997adf,,2.0,,Are there links for office hours today?,1644238618.971109,1644238618.971109,U021RS6DVUZ\\n1bd5c0e6-67ae-463a-ba31-b6e70a9d325e,U021RS6DVUZ,,,The link will be published once we are live in the <#C02V1Q9CL8K|announcements-course-data-engineering> channel,1644238618.971109,1644240673.759319,U01B6TH1LRL\\n4bff1f4f-66c9-4818-a298-dd5171c0fd0d,U021RS6DVUZ,,,Thanks,1644238618.971109,1644243586.907849,U021RS6DVUZ\\ndd98d047-62d9-4f75-b09b-4deb577b0505,U02DD97G6D6,,,\"Thanks for the suggestions Olga and N\\'uno, unfortunately mounting the workflow-utils folder did not resolve my issue. I have tried a couple of other things as well, as suggested in the module_management. The only thing I did not try is the create a package for the work_utils and use the module as an external module and install it through pip. Not sure what is the best way for such cases.\",1643887552.482999,1644244282.831219,U02DD97G6D6\\n2eb2ba6a-5b24-45a5-a9b1-187b660e7260,U02DD97G6D6,,,\"<@U01AXE0P5M3> do you have any suggestions, or what else I might be missing except mounting the mounting the directory\",1643887552.482999,1644244333.594899,U02DD97G6D6\\n338aae84-6e52-49fa-ae06-f050ef56fcf4,,1.0,,I\\'m trying to set up a VM using the video that Alexey did in week 1.. when I install Anaconda in the VM and I check `less .bashrc` I do not see the lines that Anaconda added.. When I run `python`I think its the normal python that I have in my local environment that is running.. How do I solve this issue?,1644245746.018899,1644245746.018899,U02T9JQAX9N\\n9c36c511-c9c8-4697-bcaa-46605bc36727,U02U5SW982W,,,\"<@U02U5SW982W> are you still facing this issue? I had this issue like you, and turns out it was because I was running VSCode locally, instead of being on WSL Remote. Not sure if this applies to you!\",1644208872.428749,1644246561.882849,U02T941CTFY\\nc9b284b7-762f-440e-abc1-aaf8ee295fff,,7.0,,\"Hi guys) I\\'m little bit far behind the schedule :slightly_smiling_face: but anyway I have a question. I\\'ve trouble to understand what is\\n```5432:5432\\xa0The default container postgres port mapped (fowarded) to the default localhost postgres port\\xa05432```\\nWhere is container postgres port and default localhost postgres port? If I write 5431:5432 then what will be container postgres port in this case? Also, what is localhost? If I run docker on my laptop, is my computer considered to be localhost? Why do we need such mapping?\\nI was working on topic *Dockerizing the Ingestion Script* and a few questions arose)\\n```winpty docker run -it --network=pg-network taxi_ingest:v001 --user=root --password=root --host=pg-database --port=5432 --db=ny-taxi --table_name=yellow_taxi_trips --url=${URL}```\\nAgain, what port do we take here? container postgres port or localhost postgres port ? And how this port is connected with Postgres container we created? Do we have 3 containers now: 1st - postgres, 2nd - pgAdmin, 3d - for pipeline of data ingesting? Why do we need so many containers? Why can\\'t we use one container :smiley: ? I don\\'t have previous experience with docker, probably, I\\'m asking the obvious questions:smile:\",1644246663.591679,1644246663.591679,U02TCDCDS9H\\n29ef2c60-53d7-4ff2-9f4f-102332099a74,U02T9JQAX9N,,,Solved. Thanks,1644245746.018899,1644246709.620739,U02T9JQAX9N\\nF75652BE-7852-4447-81ED-CF387FC735B3,U02TCDCDS9H,,,,1644246663.591679,1644247138.772129,U02AGF1S0TY\\n3FB64A13-4F34-4857-B572-C84A3E9C2083,U02TCDCDS9H,,,\"docker container run -p 8080:80 -d nginx\\nPort 80 of the Nginx container is exposed to the outside world on host port 8080.\",1644246663.591679,1644247171.018199,U02AGF1S0TY\\nA9331E12-E783-48D4-9398-1D42E21BD2AA,U01UMAXUPSQ,,,:pray:,1644172067.855989,1644247312.162299,U01UMAXUPSQ\\nAD6F06E8-B394-48F9-B54D-4D40C80140EE,U02TCDCDS9H,,,Port mapping essentially takes care of connecting to the container through the host machine ,1644246663.591679,1644247341.992069,U02AGF1S0TY\\nF0C11CD2-B476-4F41-942A-8F25B3D9F7C8,U02TCDCDS9H,,,\"the outside world cannot connect to a Docker container by default. So we use port mapping .  after port mapping in above case , \\ncurl -I 0.0.0.0:8080 , or curl localhost:8080 \\nwe should be able to connect to the Nginx server \\nHope this helps \",1644246663.591679,1644247622.401599,U02AGF1S0TY\\n769a3314-cc40-48bd-8d10-93f139e69071,U02TCDCDS9H,,,\"Ok, thank you) Can you, please, answer the question about containers? Why do we have such many containers? When do we need to create a new container and when we can use one container?\\nAlso, why do we need specific *name* for each image, postgres and pg-admin, in network? Why can\\'t we use the names of images when we want , for example, pgAdmin be able to see postgres?\\nWhy do we assign for the host the postgres\\' name in network  *--host=pg-database*? Why don\\'t we use, again, the name of image postgres:13? What is the difference?\",1644246663.591679,1644248592.319959,U02TCDCDS9H\\n071c9120-0d56-489d-a48b-f2530448dff6,U02TCDCDS9H,,,your questions about containers and images has been explained here - <https://www.youtube.com/watch?v=tOr4hTsHOzU&amp;list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&amp;index=13>,1644246663.591679,1644248894.775009,U02RREQ7MHU\\n7A9C2E04-EEA9-408F-88CF-9A0F80041C6D,U02TCDCDS9H,,,\"containers provide isolated environment for the code and it\\'s dependencies . We have used a container to run Postgres database in a container . The pgadmin container facilitates interactions with the database with a GUI . You may also use CLI tools like pgcli to interact with the DB container , just that pgadmin provides a good interface ..rather than installing pgadmin we chose to run the application in a container and put the DB and pgadmin in same network so that they can connect . Containerized env is flexible and preferred deployment solution \",1644246663.591679,1644249222.507099,U02AGF1S0TY\\nf8fc49bd-db3d-49d4-9688-7f05c874d042,,,,\"What is best practice for deploying statefull applications such as messaging systems (Kafka, Pulsar, etc.) or databases (MongoDB, Postgress, etc.)? Is there an uncomplicated solution for the automatic restart of these applications and documentation of the crash as well as the reasons, e.g. Kubernetes for stateless applications?\",,1644249528.694529,U02SB1T3J2G\\n6d24ba3e-f090-49a1-a494-a8d087d113d3,,8.0,,\"Dear All, I have followed DE Zoomcamp from 2.1.1 to 2.3.2 and was able to run the airflow DAG and was able to import the data to my GCP cloud storage.However,my bigquery table external_table is empty.Where could it possibly go wrong? I have run the DAG successfully with no error.\",1644250260.131359,1644250260.131359,U02U8QJLZL4\\n3e1c7a41-9869-4a56-afc7-fc1c348bee07,,4.0,,Is there any way to create a GCP  account without credit card?,1644250558.066349,1644250558.066349,U02CK7EJCKW\\n882b7019-63c2-44c1-907d-08289c9389b8,,3.0,,<https://awesomedataengineering.com/>,1644251441.216099,1644251441.216099,U01B6TH1LRL\\n8d5b09de-5db5-4394-a3a7-59c03bf64281,U01B6TH1LRL,,,Thank you <@U02TATJKLHG> :tada: :raised_hands:,1644251441.216099,1644251649.716759,U01B6TH1LRL\\n7748884f-ef13-4a86-aaef-b52d3311d3f1,U01B6TH1LRL,,,this is amazing. Thanks :+1:,1644251441.216099,1644252552.712569,U030FNZC26L\\n60791c36-5c28-47cb-a38a-90c1b994961c,,5.0,,\"Hello guys! I;m getting errors trying to upload  the data to postgre on docker container.\\n\\nHere\\'s the docker file for postgres:\\n```docker run -it \\\\\\n\\xa0 -e POSTGRES_USER=\"\"root\"\" \\\\\\n\\xa0 -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n\\xa0 -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n\\xa0 -v c:/Users/USER/Documents/2_docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n\\xa0 -p 5432:5432 \\\\\\n\\xa0 postgres:13```\\nI ran the above code in the git terminal and got the error below:\\n```WARNING: The requested image\\'s platform (linux/amd64) does not match the detected host pla\\ntform (windows/amd64) and no specific platform was requested\\nThe files belonging to this database system will be owned by user \"\"postgres\"\".\\nThis user must also own the server process.\\n\\nThe database cluster will be initialized with locale \"\"en_US.utf8\"\".\\nThe default database encoding has accordingly been set to \"\"UTF8\"\".\\nThe default text search configuration will be set to \"\"english\"\".\\n\\nData page checksums are disabled.\\n\\nfixing permissions on existing directory /var/lib/postgresql/data ... ok\\ncreating subdirectories ... ok\\nselecting dynamic shared memory implementation ... posix\\nselecting default max_connections ... 100\\nselecting default shared_buffers ... 128MB\\nselecting default time zone ... Etc/UTC\\ncreating configuration files ... ok\\nrunning bootstrap script ... 2022-02-07 17:02:09.057 UTC [44] LOG:  could not link file \"\"p\\ng_wal/xlogtemp.44\"\" to \"\"pg_wal/000000010000000000000001\"\": Permission denied\\n2022-02-07 17:02:09.064 UTC [44] FATAL:  could not open file \"\"pg_wal/000000010000000000000\\n001\"\": No such file or directory\\nchild process exited with exit code 1\\ninitdb: removing contents of data directory \"\"/var/lib/postgresql/data\"\"\\ntime=\"\"2022-02-07T18:02:11+01:00\"\" level=error msg=\"\"Error waiting for container: failed to s\\nhutdown container: container 14bac8a41a7cd9afe22b763b1c89e52319895c974006e4cbc744d43df0c6b\\n455 encountered an error during hcsshim::System::waitBackground: failure in a Windows syst\\nem call: The virtual machine or container with the specified identifier is not running. (0\\nxc0370110): subsequent terminate failed container 14bac8a41a7cd9afe22b763b1c89e52319895c97\\n4006e4cbc744d43df0c6b455 encountered an error during hcsshim::System::waitBackground: fail\\nure in a Windows system call: The virtual machine or container with the specified identifi\\ner is not running. (0xc0370110)\"\"```\",1644253376.469599,1644253376.469599,U02TWBYQKKP\\nb2f91a93-c881-4223-9744-bd391b9a569d,U01B6TH1LRL,,,Thanks! this is really nice.,1644251441.216099,1644253645.749889,U0308865C0H\\n28aa2acf-2a1a-4678-8564-761f9aee7be4,,1.0,,\"Also, there is a \"\"less overwhelming\"\" learning roadmap by the Seattle Data Guy.\",1644253789.528329,1644253789.528329,U02TATJKLHG\\nd7d0c6e9-8ae1-4214-a631-a39140389919,U02TWBYQKKP,,,how do you run it? from WSL? from terminal?,1644253376.469599,1644254071.228869,U01AXE0P5M3\\n4df7c88b-7b5e-4564-9b21-60693bb1210b,U02TWBYQKKP,,,would renting a cloud VM be an option for you?,1644253376.469599,1644254109.796219,U01AXE0P5M3\\n2352f1dd-d81e-4051-8f28-ad471c65fbce,,1.0,,When’s the week 3 homework due? I’m running a bit behind I think.,1644254532.375189,1644254532.375189,U02U34YJ8C8\\n6106c87a-1864-4d17-9356-c7b052a37b57,U02U34YJ8C8,,,next week on Monday,1644254532.375189,1644254685.998639,U01AXE0P5M3\\n590dc675-1418-4bc2-9eb7-19d1e683d39a,,,thread_broadcast,\"Any pointer could be appreciated, my airflow log has no error.\",1644250260.131359,1644255633.852439,U02U8QJLZL4\\n210bbd30-59c9-4480-b776-71db580ba57a,U02U5SW982W,,,\"Thanks <@U02U5SW982W> for asking this question and also Thanks <@U01AXE0P5M3> for reply with solution, i was also not able to get exact cause of this issue until i came across this question.\",1644208872.428749,1644255951.834459,U02SWSTT11D\\nE543EE5F-7789-400B-AB1E-42A92014165B,U02U8QJLZL4,,,\"<@U02U8QJLZL4> please check parameters for  object_name in the airflow DAG ..if the airflow is running ok , I had similar issue with parameterization of target table ..also \\nGo into ur docker container  using docker exec -it cont-id bash \\nand confirm your csv and parquet files are being created without a hitch \",1644250260.131359,1644256128.199299,U02AGF1S0TY\\n1b10b633-9a9c-4bb0-8f31-b8b53c63ebe2,U01AXE0P5M3,,,please pip install pgcli is not working. am using windows 10,1642511128.453300,1644256239.129079,U031W3YLT0U\\n70e62e18-003c-4ac6-bb94-70d857521ccc,U02U8QJLZL4,,,\"What if you delete the table, and run the DAG again?\",1644250260.131359,1644256593.129779,U0290EYCA7Q\\nc95152f8-4ec9-443d-b016-727ebbfa6a9a,,5.0,,I just set up my VM today and I want to try out week 2 assignment.. I am worried about terraform.. Should I just copy my whole folder from local to the VM or I should clone the repo I created on Github,1644256919.318639,1644256919.318639,U02T9JQAX9N\\nc977040a-cae0-44fe-9b2e-099b3d4f58da,U02T9JQAX9N,,,My Github repo does not contain the whole terraform config files beacuse I wasn\\'t tracking some of them,1644256919.318639,1644256985.573429,U02T9JQAX9N\\n97e9f89d-4bd7-4fc0-8fb3-0f9be95c57df,U02U8QJLZL4,,,\"<@U02AGF1S0TY> Thanks for responding. \"\"object_name\"\": f\"\"raw/{parquet_file}\"\",no change here.Airflow running fine.I have seen inside docker container and could see both csv and parquet files created there.\",1644250260.131359,1644257071.590389,U02U8QJLZL4\\nf6b134b6-1683-4155-aebc-b0d03b406a81,U02TWBYQKKP,,,Ran it from the terminal,1644253376.469599,1644257085.448899,U02TWBYQKKP\\n2e95fa22-1538-42b4-adf9-1f7e24c73f1e,U02TWBYQKKP,,,renting a cloud vm. How much does that cost?,1644253376.469599,1644257139.400429,U02TWBYQKKP\\nbf6bab01-9139-4000-a6ba-60544a4dc664,,1.0,,\"please am having issues, pip install pgcli is not working on my windows 10\",1644257271.502199,1644257271.502199,U031W3YLT0U\\n7AB35D72-CD06-472E-9803-7636E285C3D9,U02U8QJLZL4,,,Try and print {parquet_file} or  other env variables  within dag and check in dag logs and see if they are ok ,1644250260.131359,1644257289.645719,U02AGF1S0TY\\n278a9d48-1660-449b-ad48-7e8efafba0da,U02U8QJLZL4,,,\"BigqueryCreateExternalTable operator is actually for creating a table.\\n\\nPlease try upsert operator - This will create a table, if it doesn\\'t exist.\\n\\n```    bigquery_upsert_table_task = BigQueryUpsertTableOperator(\\n        task_id=\"\"bigquery_upsert_table_task\"\",\\n        dataset_id=BIGQUERY_DATASET,\\n        table_resource={\\n            \"\"tableReference\"\": {\\n            \"\"projectId\"\": PROJECT_ID,\\n            \"\"tableId\"\": \"\"fhv_table\"\",\\n            },\\n            \"\"externalDataConfiguration\"\": {\\n                \"\"sourceFormat\"\": \"\"PARQUET\"\",\\n                \"\"sourceUris\"\": [f\"\"gs://{BUCKET}/raw/{parquet_file}\"\"],\\n             },\\n         },\\n     )```\",1644250260.131359,1644257394.946479,U0290EYCA7Q\\ne4805c4a-91b7-4ac9-ae99-99ee5c9fc7b8,U02TATJKLHG,,,I would also recommend Seattle Data Guy in general :raised_hands:,1644253789.528329,1644257585.591899,U01B6TH1LRL\\n2dcd094e-4dca-4982-8a9a-04a43d74d6bc,U031W3YLT0U,,,can you share the error in the thread?,1644257271.502199,1644257665.638069,U01B6TH1LRL\\nb36f0e78-e968-41f7-a28a-2d56a174529c,U02TC704A3F,,,<@U02U8CB58G3> Thanks!  And indeed congrats too on being head :slightly_smiling_face:.,1644097257.301339,1644258350.185749,U02U5DPET47\\n6c09c99e-2b32-4d26-8186-7495879065c7,U02T9JQAX9N,,,You don\\'t need terraform for week 2 assignment,1644256919.318639,1644258374.935559,U01AXE0P5M3\\nd869bad0-0ecd-4155-a2c4-eb5cf6895601,U02U8QJLZL4,,,Thanks <@U0290EYCA7Q> and <@U02AGF1S0TY> - checking now with your suggestion,1644250260.131359,1644259277.735469,U02U8QJLZL4\\nb7e8f16d-dbaf-4783-8654-81b9776515c0,,13.0,,\"[Week 3] I’m stuck with my DAG trying to move files with the GCSToGCSOperator.\\n\\nI’ve run previous DAGs to download and process the CSV files for FHV and yellow taxis, so I know that Airflow works and is able to connect to GCP.\\n\\nHowever, when I run my DAG to move the files in my bucket, Airflow says that my DAG was successfully run but the files have not moved at all and no new folders have been created. I do not know how to proceed because I do not jnow how to debug this issue.\",1644259304.367409,1644259304.367409,U02BVP1QTQF\\nbd4da86e-4b36-455c-a63f-cbc2f7490ae1,U02BVP1QTQF,,,\"This is my DAG:\\n\\n```import os\\nimport logging\\n\\nfrom airflow import DAG\\nfrom airflow.utils.dates import days_ago\\n\\nfrom airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator, BigQueryInsertJobOperator\\nfrom airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator\\n\\nPROJECT_ID = os.environ.get(\"\"GCP_PROJECT_ID\"\")\\nBUCKET = os.environ.get(\"\"GCP_GCS_BUCKET\"\")\\npath_to_local_home = os.environ.get(\"\"AIRFLOW_HOME\"\", \"\"/opt/airflow/\"\")\\nBIGQUERY_DATASET = os.environ.get(\"\"BIGQUERY_DATASET\"\", \\'trips_data_all\\')\\n\\nDATASET = \"\"tripdata\"\"\\nTAXI_TYPES = {\\'yellow\\': \\'tpep_pickup_datetime\\', \\'fhv\\': \\'Pickup_datetime\\', \\'green\\': \\'lpep_pickup_datetime\\'}\\nINPUT_PART = \"\"raw\"\"\\nINPUT_FILETYPE = \"\"parquet\"\"\\n\\ndefault_args = {\\n    \"\"owner\"\": \"\"airflow\"\",\\n    \"\"start_date\"\": days_ago(1),\\n    \"\"depends_on_past\"\": False,\\n    \"\"retries\"\": 1,\\n}\\n\\n# NOTE: DAG declaration - using a Context Manager (an implicit way)\\nwith DAG(\\n    dag_id=\"\"gcs_2_bq_dag\"\",\\n    schedule_interval=\"\"@daily\"\",\\n    default_args=default_args,\\n    catchup=False,\\n    max_active_runs=1,\\n    tags=[\\'dtc-de\\'],\\n) as dag:\\n\\n    for taxi_type, ds_col in TAXI_TYPES.items():\\n\\n        gcs_2_gcs_task = GCSToGCSOperator(\\n            task_id=f\\'move_{taxi_type}_{DATASET}_files_task\\',\\n            source_bucket=BUCKET,\\n            source_object=f\\'{INPUT_PART}/{taxi_type}_{DATASET}*.{INPUT_FILETYPE}\\',\\n            destination_bucket=BUCKET,\\n            #destination_object=f\\'{taxi_type}/{taxi_type}_{DATASET}*.{INPUT_FILETYPE}\\',\\n            destination_object=f\\'{taxi_type}/\\',\\n            move_object=False\\n        )\\n\\n        gcs_2_gcs_task```\",1644259304.367409,1644259401.401199,U02BVP1QTQF\\n2e301f4c-8de3-4736-8de6-14d99a013f04,U02U8QJLZL4,,,\"also big query table is not super important right now - make sure you have the data in the bucket, and we\\'ll deal with creating tables in week 3\",1644250260.131359,1644259471.382639,U01AXE0P5M3\\nf4d143ea-115a-44ea-8818-3fe86668a736,U02BVP1QTQF,,,\"I had exact same issue today when recording the solution for homework 2 :see_no_evil:\\n\\nI tried many things, but the only thing that worked was logging to one of the airflow containers (I used webservice) and executing this:\\n\\n```airflow dags backfill yellow_taxi_data_v1 --reset-dagruns -s 2019-01-01 -e 2021-02-01```\",1644259304.367409,1644259571.319919,U01AXE0P5M3\\n604f992a-1ea8-4b42-9e02-6e7159f8e528,U02BVP1QTQF,,,but I was on the verge of nuking my VM and creating a new one - because no other ways seemed to work. Airflow keeps history somewhere and I couldn\\'t do anything to make it forget it,1644259304.367409,1644259745.670389,U01AXE0P5M3\\nb1256f88-cd31-424e-a417-f0a121bde5a4,U02TWBYQKKP,,,\"something like 20 cents per hour. But if you have that $300 of free credits that google gives to new accounts, you don\\'t need to spend anything\",1644253376.469599,1644259835.021759,U01AXE0P5M3\\nfcdf6585-bee2-4941-9645-c7f60eb6519f,U02BVP1QTQF,,,\"Ok, so I just logged in to my airflow_airflow-webserver container and run this:\\n\\n```airflow dags backfill gcs_2_bq_dag --reset-dagruns -s 2019-01-01 -e 2021-02-01```\\nIt seems to be doing something but I don’t know what lol. The “backfill process” says it’s doing 763 runs (!) and even though my DAG isn’t triggered it seems to be updating stuff, but the contents of the bucket aren’t changing\",1644259304.367409,1644260738.771749,U02BVP1QTQF\\n330d150a-19eb-4dee-8fb1-f149c64fbfbf,U02BVP1QTQF,,,<https://medium.com/nerd-for-tech/airflow-catchup-backfill-demystified-355def1b6f92>,1644259304.367409,1644261628.342659,U0290EYCA7Q\\n1813b309-ee67-489f-b9c8-f362a72cd6e7,U02T9JQAX9N,,,\"Hmm.. Does that imply that Terraform is just used to set up the infrastructure we\\'re using and it is not needed again after that is done, even when the infrastructure is being used\",1644256919.318639,1644261657.438049,U02T9JQAX9N\\n58852b3c-c026-4c26-963d-94e0f1ddb15d,U02T9JQAX9N,,,\"yes, that\\'s correct\",1644256919.318639,1644262490.953169,U01AXE0P5M3\\nb4048013-c8b1-4fbe-aca0-790bfce7820a,U02BVP1QTQF,,,763 is certainly too much - it\\'s like daily for 2 years?,1644259304.367409,1644262542.502969,U01AXE0P5M3\\n169208d9-3606-498c-831d-cdf880d53853,U02BVP1QTQF,,,\"I think so, but I’m not sure why. The schedule is set to days_ago(1)\",1644259304.367409,1644262734.848459,U02BVP1QTQF\\n663f510e-5b76-4c4a-8c43-47fcca630329,U02BVP1QTQF,,,I’m going to nuke my vm and start from scratch. Local deployment did not work either,1644259304.367409,1644262758.920549,U02BVP1QTQF\\na7d5bf39-2db2-4008-94bb-2fc15b69138b,U02T9JQAX9N,,,Thanks,1644256919.318639,1644262816.176559,U02T9JQAX9N\\n101d67f0-d838-4c49-86d0-0ae0f32d3ca9,U02BVP1QTQF,,,and schedule_interval?,1644259304.367409,1644262888.325639,U01AXE0P5M3\\n5f232d35-ef6f-457e-87e4-c66d9c4a9804,U02BVP1QTQF,,,@daily,1644259304.367409,1644262964.662099,U02BVP1QTQF\\nd98f6b4c-c54f-4a54-a8f8-162f019d09b0,U02BVP1QTQF,,,\"I still can’t answer why Airflow works differently for me, and differently for other configs :white_frowning_face: I’ll have to read about this.\\n\\nI don’t see a point on using backfilling because a single DAG run should be sufficient in moving all the specified data at once.\\n\\nAs a workaround, I would also suggest clearing everything up in `airflow/logs` (Airflow history), and might as well work in a different set of docker containers where only this DAG exists (week 3 setup)\",1644259304.367409,1644264707.322789,U01DHB2HS3X\\n98c3e2d5-6bf5-4753-b225-49a58c3edfd5,U0290EYCA7Q,,,\"Hi <@U0290EYCA7Q>, yes, an external table only needs to pick up the latest/updated data based on the Storage path specified. And the schema/table should not need to be explicitly updated (in case of External ones). But if you notice the path you\\'re pointing to in source_uri\\'s in upto the parquet file. Hence, it is only taking that particular parquet file everytime. Please change the URI to point it to only upto \"\"raw\"\", and you should be fine. Also BQ was never the focus of week 2, but you\\'ll learn more about it in Week 3.\\n\\nApart from that, we need the data for 2019-2020 for Week 3 &amp; 4 exercises.\",1644227045.368479,1644265444.319969,U01DHB2HS3X\\nae4b82ba-20dc-4c76-8eae-dbab0d4ea33d,,5.0,,\"[Week 1] Hi everyone, I\\'m having some trouble figuring out how to set the linux root directory for wsl. I\\'m a bit behind from the past two weeks but trying to catch up now.\\n\\nEssentially, my problem is as I am getting my environment for wsl and docker set up, the recommended practices from both the docker site and microsoft site is to have your files you are working on stored in the wsl linux fiel system instead of having it being mounted in the windows file system. It seems when I have everything set up, my wsl root directory is actually mounted on windows. I have tried to look up how to  make it use the linux directory but I\\'m not seeing any info on how to do it. Any help would be appreciated.\\n\\ni have included screenshots of the recommendations and also a screen shot of what my directory looks like on my terminal.\",1644265727.094329,1644265727.094329,U02QMQWRMRS\\n61ffd987-9ea0-4e8e-890c-9823d492e0c4,U02BVP1QTQF,,,\"I just created a new VM and set up Airflow with just the simple DAG I posted above and it still doesn’t work…\\n\\nI’m guessing that if it were a permissions issue it would return an error but that is not the case. I’m honestly at a loss.\\n\\nI’m going to play a bit with the scheduling and the filenames and see if it works.\",1644259304.367409,1644265770.193909,U02BVP1QTQF\\n8747e674-81dd-4437-bcbb-8e0765943e26,U02U5K0RFK8,,,\"You\\'re right, Michael. Thank you. I\\'ll make the change in the repo. Or if possible, could you please create a PR for this?\",1644205357.104949,1644266168.601019,U01DHB2HS3X\\n6a3b02d5-f04a-4531-9af9-9d17825378d8,U02BVP1QTQF,,,\"Ok, so I’m a stupid idiot. My `source_object` string was not properly formatted and wasn’t parsing any files. I just fixed it and it works, but the destination filenames are all wonky, so I need to work on that.\\n\\nThank you for your patience!\",1644259304.367409,1644266191.816549,U02BVP1QTQF\\n30ddcc50-d6a3-458d-b2a0-736808201e7b,U02QMQWRMRS,,,What\\'s wrong with mounting? That\\'s how I usually use it,1644265727.094329,1644266773.937259,U01AXE0P5M3\\ne718b213-9bb7-4247-a1de-92127430caae,U02QMQWRMRS,,,\"Now if you do cd, you\\'ll get to your actual WSL home folder, and you can do anything you\\'d like there\",1644265727.094329,1644266838.737319,U01AXE0P5M3\\n72bbdbb7-396f-41a0-95a1-aafa1da3c4cc,U02QMQWRMRS,,,\"Oh ok if that\\'s how you have then it\\'s good enough for me. I wasn\\'t sure because it said in the best practices screenshots I added that, it wasn\\'t a goof idea to have the project files stored in the mounted windows directory because it would slow it down or something like that. I did not want to run into any problems down the line\",1644265727.094329,1644267169.251719,U02QMQWRMRS\\nd000317d-d8a7-4f13-a482-996fa7f89dd0,U02QMQWRMRS,,,this path is apparently the root linux directory as the person who wrote the docker tutorial for wsl had it. This is where it is located on my computer.,1644265727.094329,1644267416.803119,U02QMQWRMRS\\nd1f275a3-d7bc-4940-8378-b494e7105851,,3.0,,\"Is there one place in the Github repo that explains all of the tools we need to install for this course (and installation guides)? I tried to look for one in the repo, but it isn\\'t easily evident for me.\",1644269611.405819,1644269611.405819,U031DV1AP5F\\n1b19a156-cd9a-4194-a259-dc2aac137ba6,U031DV1AP5F,,,\"I just added this section in the readme:\\n\\n<https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/README.md#tools>\",1644269611.405819,1644269874.238329,U01AXE0P5M3\\n71b66896-aa0f-4e6c-a7c3-c9b83772c27b,U031DV1AP5F,,,\"oh actually we already have a section about that in week 1!\\n\\n<https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup#environment-setup>\\n\\nI forgot that I added it :smiley:\",1644269611.405819,1644269961.577389,U01AXE0P5M3\\n4be002b8-3215-468e-92a5-ffa1bf4410e4,U031DV1AP5F,,,\"Ah, I was reading through section 1 from the top and only skimmed to the bottom and missed it. Thank you!\",1644269611.405819,1644270089.936169,U031DV1AP5F\\n74a46dea-191c-45c5-8628-092c970c3ebe,U02V90BSU1Y,,,<@U02V90BSU1Y> Did you set the corresponding IAM roles to as described <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/2_gcp_overview.md#initial-setup|here>?,1644152686.645389,1644271492.764899,U02CGKRHC9E\\nc184567d-3b10-44ed-8ff6-3d1d4df45f69,,,,\"When I started this course I was determined that I would take copious detailed notes of all the YouTube video\\'s for the course, read all the extra resources and maintain a blog of my reflections and all the things that I had learnt. But that was approximately 3 weeks ago when I was younger, and full of hope. :grin:(adapted from something my sister just sent me).\",,1644272632.143189,U02U5SW982W\\nab5627e1-d115-4e4c-8c60-a9ed47d20a27,,2.0,,\"Hi everyone, when following the \"\"DE Zoomcamp 3.3.3 - Integrating BigQuery with Airflow (+ Week 2 Review)\"\" I realized CSV file format is used within the configuration of  \"\"bigquery_external_table_task\"\", however I have only uploaded the parquet version of files in week2 homework, therefore that cause issues with the given task definition. You can find in the thread how to fix the issue.\",1644275746.416569,1644275746.416569,U02DD97G6D6\\n42608ae1-9789-4997-b044-110649ab0625,U02DD97G6D6,,,\"the initial task definition is like that:\\n\\n```bigquery_external_table_task = BigQueryCreateExternalTableOperator(\\n     task_id=f\"\"bq_{colour}_{DATASET}_external_table_task\"\",\\n      table_resource={\\n                \"\"tableReference\"\": {\\n                    \"\"projectId\"\": PROJECT_ID,\\n                    \"\"datasetId\"\": BIGQUERY_DATASET,\\n                    \"\"tableId\"\": f\"\"{colour}_{DATASET}_external_table\"\",\\n      },\\n     \"\"externalDataConfiguration\"\": {\\n                    \"\"autodetect\"\": \"\"True\"\",\\n                    \"\"sourceFormat\"\": \"\"CSV\"\",\\n                    \"\"sourceUris\"\": [f\"\"gs://{BUCKET}/{colour}/*\"\"],\\n                },\\n     },\\n )```\\nTo make it work for Parquet files,\\n• sourceFormat should be changed to PARQUET\\n• autodetect specification should be removed. (As by the Airflow documentation, autodetect does not needed for Parquet and ORC types. I personally tested specifiying autodetect also caused issue and column names are not propagated correctly )\",1644275746.416569,1644275855.047869,U02DD97G6D6\\n6714147b-0c8a-452c-8c30-2700683f7707,,3.0,,\"Hi, when I click week 3 homework form link, an \"\"Internal Error\"\" message is shown.\\nHomework\\'s link will still be made available?\",1644280475.080249,1644280475.080249,U030F0YHDAM\\n54a80feb-b0d0-4f84-be6e-75788d2894cb,U030F0YHDAM,,,i am experiencing the same too,1644280475.080249,1644283136.487399,U02ULMHKBQT\\n6b73e8ae-d4f5-48a0-9c2f-a88797230dd0,U02UEE4MBEG,,,\"Had a similar issue, had an extra space after the colon in the -v flag. Also, had extra spaces after the = signs. Didn\\'t realize that these commands were sensitive to spaces. Was very frustrating until I realized that.\",1642748652.027100,1644284187.994339,U031DV1AP5F\\n48781fb7-1f18-40e1-9187-ebb16fecb49d,,11.0,,\"Hi, I just ran into an error with wk2 homework.\\nstart_date is set to 2019,1,1 but execution date is reading\\n2022-02-08\",1644286226.784259,1644286226.784259,U02QS4BD1NF\\ned0a28af-cab0-47dc-8396-20d97e9faa9c,U02QS4BD1NF,,,Is your catchup parameter set to False? It should be set to True or remove catchup setting from your dag as it is by default True.,1644286226.784259,1644286511.703899,U02DD97G6D6\\nb43d3dd7-27f3-46b3-8205-5b6b528dcb36,U02QS4BD1NF,,,its set to True,1644286226.784259,1644286602.375809,U02QS4BD1NF\\n0c3cf217-2cea-45bb-9b51-67dad30a85fd,U02QS4BD1NF,,,\"Hmm not sure, but could you send your settings dag creation\",1644286226.784259,1644286688.767599,U02DD97G6D6\\nc7e44750-7258-4c67-afd0-32f22263e1b0,U02DD97G6D6,,,Nice call,1644275746.416569,1644286804.813079,U02TC704A3F\\n767bcbc5-93e9-4a81-a72f-8c0b17d0f0cd,U030F0YHDAM,,,It works for me (Europe). Might be that there was an internal problem in google forms. Or the form was not distributed to all regions fast enough.,1644280475.080249,1644287130.824389,U02UP3KN3SQ\\n690d737a-9b03-47e7-bfb5-655b50e4a253,U02QS4BD1NF,,,\"In my case, I was using backfill to let Airflow now, that I want to run everything.\\n\\nI ran the following command:\\n```airflow dags backfill --start-date 2019-01-01 --end-date 2021-01-01 yellow_tripdata_ingestion_gcs_dag```\\nNote:\\n• You have to execute this command on the airflow sheduler. I\\'m not aware of a way to execute this over the UI.\\n• My dag\\'s ID is \\'yellow_tripdata_ingestion_gcs_dag\\'. Your\\'s might have a different name.\\n• The end date is 2021-01-01 by purpose. Because the last data interval, which you would like to execute is from 2020-12-01 to 2021-01-01.\\n<https://airflow.apache.org/docs/apache-airflow/stable/dag-run.html#backfill>\",1644286226.784259,1644287335.336829,U02UP3KN3SQ\\nb9d9eb40-3883-407a-859a-d39b2dc58063,U030F0YHDAM,,,\"Yeah, it is working now :thumbsup:\",1644280475.080249,1644287360.958309,U02DD97G6D6\\n96879066-1ab3-43c5-b70e-2f8de7d083cb,U02QS4BD1NF,,,\"As far as I understand catchup handling the backfilling but I didnt experience the issue, therefore no idea. But might be airflow didnt take up the latest changed version of dag if you set the catchup false at first place, if that is the case renaming dag id should help.\",1644286226.784259,1644287451.988719,U02DD97G6D6\\ne2fbf168-0152-4a89-9a84-0b32d4b02f57,,4.0,,\"Hello! I have a question about week 3 homework. In question 1 I obtained this count: 23130810.\\nThat is my gcs data for fhv vehicles data. And also this is the info of my non-partitioned table of this.\",1644287510.558709,1644287510.558709,U02TJ69RKT5\\n3ce75fd5-f924-4896-b85d-3e93841171f8,U02QS4BD1NF,,,\"Thank you.\\nI deleted the Dag and restarted it. Its working properly now.\\n\\nI think it was a general problem with my airflow, because the local ingestion dag that worked properly also had similar issue moments ago\",1644286226.784259,1644287909.738369,U02QS4BD1NF\\n7a253bf3-8a9b-40b0-9692-3b4d404ca17b,,6.0,,\"Has anyone else received the error `Access Denied: BigQuery BigQuery: Permission denied while globbing file pattern.`I am trying to create external tables from the data in Google Storage, which had been transferred from AWS using the transfer service. The transfer service was successful, and I can see that the data is present in this bucket location.\",1644288053.633869,1644288053.633869,U026040637Z\\n7039C2CE-2440-42A3-9770-E88F5E9721F8,U026040637Z,,,Please check the IAM permission for your user via console ,1644288053.633869,1644288151.573139,U01DFQ82AK1\\n766e1b1a-01d7-4d89-a62c-29977fe659c6,U02TP858APK,,,I also have the same issue with the code never finishing running and having to force it to quit. Glad to know that the terminal in VS code works.,1643196570.265400,1644288404.956949,U031DV1AP5F\\n95802c65-0b92-4da1-9c55-ef2ab243b789,U02TJ69RKT5,,,\"Hello, can you see the size difference. I think something went wrong in your export? How did you export the data?\",1644287510.558709,1644292287.283029,U01DFQ82AK1\\necd8192b-79cf-45d1-b16f-0ec28d80a710,U02TJ69RKT5,,,Ah I see you have parquet? Best to compare once you load data with csv also,1644287510.558709,1644292353.990899,U01DFQ82AK1\\n78ba3e4e-e528-40d1-b433-0a61ac0dc064,U02UJGGM7K6,,,\"Ah, I see the problem. It was at my end. I used the query with `WHERE dropoff_datetime BETWEEN \\'2019-01-01\\' AND \\'2019-03-31\\'` . Try this, if this solves the issue\",1644230952.007529,1644292557.213059,U01DFQ82AK1\\n9d3e90e2-45fc-4c3a-8194-ee5a3efe99ac,U02CK7EJCKW,,,\"Use a debit card :slightly_smiling_face: Unfortunately, you need one even to activate a trial period.\",1644250558.066349,1644293089.290209,U02HB9KTERJ\\nD38A6922-D4FB-4137-B0AE-A8D16C4F94F8,U02U5K0RFK8,,,Thank you. I will create a PR.,1644205357.104949,1644294299.781659,U02U5K0RFK8\\nD847005D-311C-48FD-BC92-C1D9CE1A49C6,U02U5K0RFK8,,,\"Thank you, I will try that!\",1644203421.832589,1644294413.741279,U02U5K0RFK8\\nea63f481-c27e-4bf1-8a37-ffb2d94dc210,,5.0,,\"Hi everyone\\n\\nI\\'m in some kind of a side quest right now.\\n\\nCan someone help me in converting csv files, that are already in Cloud Storage, to parquet format? I want to know how accomplish this using GCP, I know that I can use pyarrow in local machine but I\\'m trying to figure out how to do this inside the platform.\\n\\nIsn\\'t related to the classes or homework, however I feel like is something that a Data Engineer would have to do in daily basis, am I right?\\n\\nI also think this would be a good topic for a micro-workshop :smiley:\",1644296434.560629,1644296434.560629,U02TC704A3F\\n65c9cb33-9917-4656-aba0-afadb653e949,U02TJ69RKT5,,,\"I have the same parquet file sizes as you <@U02TJ69RKT5>. Just my BQ table size is 1,59 GB instead.\\n\\nI ran the following query for you to list the number of rows per month. You could use the same query in your table to check, if there are months of data missing:\\n```SELECT FORMAT_DATE(\"\"%b %Y\"\", pickup_datetime) AS ym, count(*) AS cnt FROM `&lt;gcp project id&gt;.trips_data_all.fhv_tripdata` GROUP BY ym;```\",1644287510.558709,1644296783.674709,U02UP3KN3SQ\\n8356F8F6-8376-42F1-8F7B-0C5824EB741B,U02TC704A3F,,,\"Hi, you can have multiple solutions here\\n• CSV to BigQuery, BigQuery exported to parquet\\n• You can use this <https://cloud.google.com/blog/products/data-analytics/running-format-transformations-with-cloud-dataflow-and-apache-beam|https://cloud.google.com/blog/products/data-analytics/running-format-transformations-with-cloud-dataflow-and-apache-beam> example to use dataflow + apache beam \",1644296434.560629,1644299304.381499,U01DFQ82AK1\\n2E7898EC-9DBC-4327-B915-37C74816369C,U02TC704A3F,,,\"Also take a look at Google cloud Data fusion, it might have some inbuild connectors for the same\\n<https://cloud.google.com/blog/topics/developers-practitioners/bridge-data-silos-data-fusion|https://cloud.google.com/blog/topics/developers-practitioners/bridge-data-silos-data-fusion>\",1644296434.560629,1644299654.499239,U01DFQ82AK1\\n91c546e4-d73d-42fb-a67c-afacfec0d6eb,U026040637Z,,,<@U026040637Z> did you get the permissions fixed.. i tried providing access to the second user..but no dice :disappointed: <@U01DFQ82AK1> any suggestions?,1644288053.633869,1644300668.979769,U02SMBGHBUN\\nd1f5709b-4d61-4512-8153-98cc58576d66,,1.0,,I couldn\\'t get to the course content for the first two weeks. Can I still do the homework for them or move on?,1644301287.424989,1644301287.424989,U02JE8J1W8P\\n2c36080c-9d57-4f38-85e5-6d88c4ec5c96,U02JE8J1W8P,,,You can (and should - if you have time). But you won\\'t be able to submit the results,1644301287.424989,1644302290.101339,U01AXE0P5M3\\n17c6290b-0a45-4770-89cb-0a5fdfee350b,U02TEERF0DA,,,\"Should this work for a GCP VM?  Initially I tried to port forward in VSCode but it didn\\'t work so I tried the SSH browser client in GCP.  That also didn\\'t work since I am not sure if that VM key exists.  As well, is the user_name and vm_ip parameters supposed to be replaced with the service account user name and internal IP or external IP?\",1642781895.135700,1644302646.471369,U02SEB4Q8TW\\n937e221f-a545-4ec2-9636-59c6a245736d,U026040637Z,,,\"For folks who were stuck like me :slightly_smiling_face:\\nThe error will keep appearing even after giving every possible access, if the said bucket does not exist. I had Parquet files and was trying to load an non-existing CSV file. But the error was misleading\",1644288053.633869,1644303095.528129,U02SMBGHBUN\\n37DE5645-78D9-4B1C-A057-CA2229FE0BD9,U026040637Z,,,If you are using admin or owner account you should have all the permissions. ,1644288053.633869,1644304368.805579,U01DFQ82AK1\\nb82c0e0f-164c-4462-9f22-65491db76f5e,U02R09ZR6FQ,,,\"Hi <@U02R09ZR6FQ>, were you setting up Airflow in a GCP VM?  I am wondering how and/or where the credentials should be placed in the VM since I think that might be the issue after having trouble accessing the localhost Airflow after forwarding in VSCode and in GCP.\",1643816806.173399,1644305124.009669,U02SEB4Q8TW\\n86c96e0a-4def-4148-aa64-76bef6e23f73,U02TATJKLHG,,,\"yeah with that assumption of course, I also checked if min and max on the partition column work, but they do not :slightly_smiling_face:\",1644906567.045219,1644912396.741739,U02UA0EEHA8\\nd1698eae-1c77-49aa-af37-b8167fc66de8,U030FNZC26L,,,\"Depends on what you want to show, you can use grafana\",1644902599.749989,1644915788.789909,U0308MF3KUH\\nd29c9dfe-4375-41dd-adbe-0a79024b2efb,U02TA6MJZHR,,,\"storage admin\\nstorage object admin\\nviewer\",1644884972.605279,1644915972.968929,U02TA6MJZHR\\n7932423e-cb55-4b14-a239-7a49d086a7ae,U02UZ493J56,,,\"<@U02UVKAAN2H> that\\'s interesting, that would explain why in many cases just bigquery admin is enough and for other it is\",1644861631.604849,1644916894.007329,U01B6TH1LRL\\nae89fefb-80ac-486e-90aa-bedf08e5415f,U02HB9KTERJ,,,hey <@U02HB9KTERJ> have in mind that you may need to grant more access roles to your service account if you want to read directly from the external tables. Checkout this thread -&gt; <https://datatalks-club.slack.com/archives/C01FABYF2RG/p1644861631604849>,1644841666.691949,1644916975.541249,U01B6TH1LRL\\n1cf57ec0-5cfb-4b2b-b38c-24380e37fd84,U02DD97G6D6,,,\"You need to modify the roles assigned to the service account, you can always delete and start from scratch ofc if you find it easier\",1644792118.370219,1644917060.381509,U01B6TH1LRL\\n7bb8b7b1-534a-4978-8c29-c9d1248987ed,,2.0,,\"In Q4 Week 3 (count, estimated data, actual data... ), was it clear that the date mentioned was `dropoff_datetime` and not `pickup_datetime`? I used the latter!\",1644917363.454799,1644917363.454799,U02HB9KTERJ\\n0ed47d83-a50f-4064-9f65-96a5941f2349,U02HB9KTERJ,,,\"Also, I used the `DATE`  function to cast, and it changed the count from 26558 to 26643...\",1644917363.454799,1644917609.761059,U02HB9KTERJ\\nbc185e4b-3089-483b-b3b3-6820ea514a7b,U02HB9KTERJ,,,Thanks a lot. I created a native table (but dropped the `ehail_fee` column because of the bug).,1644841666.691949,1644917732.277609,U02HB9KTERJ\\nd36ed035-a5e1-4d32-83ec-edd3bd60db09,U02S2TZRBL7,,,\"I\\'ve seen you all figured out already, but one small explanation on why it works with debug and not while running a model. The debug will only check that you can reach the dwh, where the model run will create a table in a schema in one region (EU) as select from another table in another region (erope-west6), that\\'s why it only appears in that moment.\\nI take the location prompt may be either old or generic, I\\'ve seen that this with the location is a very common thing to run into with bigquery :disappointed:\",1644876320.850049,1644917835.036909,U01B6TH1LRL\\n162ab6fd-7eba-44d5-a0d6-b0086c075c2e,U02S2TZRBL7,,,Thanks much Victoria.,1644876320.850049,1644918107.174319,U02NSF7RYP4\\n34849ea6-8ef0-4a29-99b4-26cf203503b6,U02S2TZRBL7,,,Your tutorial videos are really helpful. :thank_you:,1644876320.850049,1644918184.960999,U02NSF7RYP4\\n9e3ec989-921c-4c8d-a944-ea056eb68078,U02HB9KTERJ,,,\"There wasn\\'t difference in result, I used both to compare.\",1644917363.454799,1644918498.957489,U02QNCUUHEY\\nc0dad3d0-be47-4040-996e-39a0d4b8e063,U02TATJKLHG,,,\"Also, why can\\'t we partition by a `string` column type? <@U01DFQ82AK1>\",1644906567.045219,1644918923.420949,U02TATJKLHG\\n51f82e6f-af62-417a-bfc0-612d4cf96856,,8.0,,\"You can now add diagrams as a code block in your markdown files in GitHub :tada:\\n\\nCould be a great news for people here who are writing a lot of md files :smile:\\n<https://github.blog/2022-02-14-include-diagrams-markdown-files-mermaid/>\",1644919012.575939,1644919012.575939,U02TATJKLHG\\n24d93db4-115e-4284-9c9b-17ffa79ae679,U02TATJKLHG,,,is BQ specific as per the first message in the thread here :slightly_smiling_face:,1644906567.045219,1644919552.983009,U02UA0EEHA8\\ncc6c4cf9-9a7f-4db7-aa0c-9acc44e5b8aa,U02TATJKLHG,,,Yeah kinda got that but it still confuses me as to why partitioning would not be allowed over string datatype,1644906567.045219,1644919886.195349,U02TATJKLHG\\nbd416c3e-54ec-42c7-9f09-bb19c4f5abba,U02TATJKLHG,,,\"Hi Ankur, I am not 100% sure about that. But my guess is, that it would be hard for BQ to determine how many and where partitions will exists. In TIMESTAMP partition BQ can determine min and max based on DATE (month or year), for IntegerRange we specify a range bucket `RANGE_BUCKET(customer_id, GENERATE_ARRAY(0, 100, 10))` , in this case we also specify min and max range. I think we cannot do the same for String type.\\n\\nIf we think about how partitions are used to store and query data, it make sense that specific region of storage is somehow pre-aligned with fix values, example data segment block1 -&gt; IntegerRange 0-10, block2 -&gt; IntegerRange 11-20, .... If we cannot fix this, BQ would always have to read all the data defeating the purpose of partitioning.\\nAll that said, _i have to say I did not find this anywhere officially this is just my educated guess._\",1644906567.045219,1644920781.286419,U01DFQ82AK1\\needd74a8-3d47-4a92-a5a9-49abc07a42fc,U02TATJKLHG,,,\"Thanks Ankush, interesting thought and makes sense as well :smile:. Will try to read more around this and see if something official can be found.\",1644906567.045219,1644921100.114439,U02TATJKLHG\\nfa695eaa-0456-4547-ba52-888a6181d8b4,U02TATJKLHG,,,\"I do agree with both <@U02UA0EEHA8> and <@U02TATJKLHG>.\\nWe were supposed to find the best strategy: best in terms of cost and best for this particular dataset. Since using `RANGE_BUCKET` was possible and very intuitive in my case, I believe the correct answer should be the one with the lowest cost, which in my tests was _~1.1 sec elapsed, 47.9 MB processed_ for strategy 2 vs. ~_1.3 sec elapsed, 71.2 MB processed_ for strategy 3. (cache off, 5 it.).\\nBTW; even if the dataset change in time - why don\\'t we use `MIN / MAX` on SR_Flag in our query so there is no need to hardcode it? (thinking loudly)\",1644906567.045219,1644921762.424829,U02UKBMGJCR\\nca0fe570-3aab-4232-a760-ab53b1d24bb9,U030FNZC26L,,,\"thanks, I\\'ll check it.\",1644902599.749989,1644922334.320659,U030FNZC26L\\n8d8458ab-54d6-41b7-8ec8-c90b4d479939,U02TATJKLHG,,,\"<@U02TATJKLHG>\\nIt gives an error during docker-compose build.\",1644493645.074519,1644922526.709399,U02HB9KTERJ\\nbeed7fdd-f4d5-41e7-a85a-6be9ec03845a,,9.0,,Question 1 week 4. I get definitely different results. I believe since different data preprocessing. Could somebody tell me the right count of records in yellow and green databases?,1644922786.198149,1644922786.198149,U0301PJRDNV\\n21b01303-eb3b-49c3-98f6-77aaeb0a4bee,,,thread_broadcast,<https://datatalks-club.slack.com/archives/C02V1Q9CL8K/p1644920834019199?thread_ts=1644897190.028899&amp;cid=C02V1Q9CL8K>,1644906567.045219,1644923231.527889,U01DFQ82AK1\\nb0df71ca-46e7-44a0-913e-1e9b8a84e949,U02TATJKLHG,,,\"Hmm, that is strange.\\n\\nCan you just try and run this -\\n\\n```docker build --tag dbt/bigquery  --target dbt-bigquery .```\\nand build with docker just to see everything is installing fine.\",1644493645.074519,1644923559.706589,U02TATJKLHG\\nf47aa117-734c-4823-b4c2-dcd18bf4fc39,U02TATJKLHG,,,Exactly same error...,1644493645.074519,1644923867.878749,U02HB9KTERJ\\n337c5200-26ac-428c-8af6-6e6b5f5caf4f,U02TATJKLHG,,,\"Problem is with this line\\n\\n```##\\n# dbt-core\\n##\\nFROM base as dbt-core\\nRUN python -m pip install --no-cache-dir \"\"git+<https://github.com/dbt-labs/${dbt_core_ref}#egg=dbt-core&amp;subdirectory=core>\"\"\\n##```\",1644493645.074519,1644923913.235349,U02HB9KTERJ\\n25e57dcf-6414-4873-905f-a84dcbb3eb5d,U02TATJKLHG,,,Even running this (pip install) command on my local machine gives the same error,1644493645.074519,1644923932.083619,U02HB9KTERJ\\nb772df1b-5615-450e-bfe2-2d6297808bae,U02TATJKLHG,,,\"<@U01DFQ82AK1> is it sense in using partitioning when almost all values empty and distribution of others is uneven?\\n~~~\\nSELECT SR_Flag, count(*)\\nFROM `dtc-de-339117.trips_data_all.fhv_tripdata_partitoned_clustered`\\ngroup by SR_Flag\\norder by count(*) desc\\n\\n-- null 36701264\\n--1  2369823\\n--2  1874968\\n--3  749388\\n--4  229875\\n--5  87381\\n~~~\",1644906567.045219,1644924109.556629,U02QNCUUHEY\\nfe439074-249a-4332-934e-640ca1d5072b,U02TATJKLHG,,,\"It might still make sense to cluster them, as cluster will just internally sort them between partitions, but this is definitely a case to avoid using partitioning as most data will end up in few partitions\",1644906567.045219,1644924225.653819,U01DFQ82AK1\\n6DA46BB1-5C32-4FE6-BA53-B05BB9CFB00B,U02TATJKLHG,,,HOLY CRAP YES! I\\'ve wanted this feature for ages! Thanks for the update!,1644919012.575939,1644924349.424809,U02BVP1QTQF\\n73304e74-74f5-4905-83bc-0535035bd357,U02TATJKLHG,,,\"Yes, I understand about clustering but not about partitioning. Almost all data would be in the first partitions.\",1644906567.045219,1644924354.948239,U02QNCUUHEY\\n5c05d011-602e-4be8-bf03-dabe39d5542d,U02TATJKLHG,,,\"Waiting for the all new and fancy week 4 notes from you <@U02BVP1QTQF> now, hahaha!\",1644919012.575939,1644924691.069529,U02TATJKLHG\\nDBE51108-D649-4BF3-99E8-68382EB12F11,U02TATJKLHG,,,\"Since GitHub didn\\'t support any markdown diagrams, I\\'ve kept myself from learning mermaid because they wouldn\\'t show properly and would have to fall back to images, but I\\'ve got no excuse anymore :rolling_on_the_floor_laughing:\",1644919012.575939,1644924873.716969,U02BVP1QTQF\\nfa90e2aa-b1b4-4127-bb57-9ca9ab2e686d,U02TATJKLHG,,,\"Haha, I didn\\'t know what mermaid was until today lol. But sounds like this is going to be really helpful.\",1644919012.575939,1644925145.321289,U02TATJKLHG\\na39c8fa8-ba6d-402f-a506-2f4066299556,U02TATJKLHG,,,\"I tried this command and it worked for me -\\n\\n```python -m pip install --no-cache \"\"git+<https://github.com/dbt-labs/dbt-bigquery@v1.0.0#egg=dbt-bigquery>\"\"```\\nNot able to understand why this fails for you\",1644493645.074519,1644925859.921249,U02TATJKLHG\\na6f690bd-4b49-4dfd-ab8c-34e76a4096d8,U02TATJKLHG,,,\"Yeah, this should work. apparently it is the ARG stuff that is causing the problem.\\n\\nI replaced all ${..} with their values inside the dockerfile and it is working fine\",1644493645.074519,1644925906.039559,U02HB9KTERJ\\ne0ccf659-60cf-4b0a-9b60-6f0b97bb0d03,U02TATJKLHG,,,\"There are a couple more tools for diagrams in markdown, like WaveDrom, js-sequence and flowcharts.js. I learnt about them because I traditionally take notes with pen and paper because it allows me to merge text and drawings seamlessly and was frustrated with both the slowness of writing by hand and the clumsiness of drawing on a computer and compositing them into a document. I think there’s still a lot of work regarding tooling for merging drawings into documents, but diagram tools like mermaid are a great step forward for replacing pen and paper in favor of computer notetaking, at hte cost of a slight learning curve.\",1644919012.575939,1644925920.571019,U02BVP1QTQF\\n1a6d0f00-61c9-489e-9b4d-4499e44354df,U02TATJKLHG,,,You shouldn\\'t have to replace the ARG values manually. Can you maybe just recopy the Dockerfile from the official repo?,1644493645.074519,1644925960.892939,U02TATJKLHG\\nbedd8e80-2447-41e6-99b6-c295098af4ef,U02TATJKLHG,,,\"Yes, I am aware. Debugging things one at a time. I\\'ll try that too, BTW, your `docker-compose.yaml`  is also incorrect in the docker-setup.md file. Mind fixing it? Thanks :slightly_smiling_face:\",1644493645.074519,1644926044.488729,U02HB9KTERJ\\n0faf20ba-57fc-4389-a465-b9dcd8221984,U02TATJKLHG,,,\"Yes I figured that the indentation is incorrect. I will fix it, thanks!\",1644493645.074519,1644926227.473199,U02TATJKLHG\\n1d814a6e-b23c-43c9-93b3-334a492aafd3,U02TATJKLHG,,,\"Nopes, the original Dockerfile doesn\\'t work either. I just tried to install dbt-core and removed others; didn\\'t work. I\\'ll keep the manual editing for now!\",1644493645.074519,1644926564.396929,U02HB9KTERJ\\nf057ffec-2b84-4a87-9127-12bb1b79cd53,U02TATJKLHG,,,\"Both are same btw, I just wanted to make sure that you are not facing issues because of any errors I made while copying the file :smile:. Maybe you can share the link to you Dockerfile here just incase someone comes searching for a solution to a similar error?\",1644493645.074519,1644926986.624329,U02TATJKLHG\\ndac7b3d0-f3f4-4174-bb26-ee2e7bf66723,U02TATJKLHG,,,\"This would surely be a great thing for integrating small and simple diagrams seamlessly in your md files. A complex diagram as code would surely be intimidating, but I think this can really help when we just want to build upon the diagram without having to leave the markdown editor everytime.\",1644919012.575939,1644927365.695649,U02TATJKLHG\\nc4c33384-ea4f-4aef-8c51-2345428077fa,U02TATJKLHG,,,\"Prefer clustering over partitioning under the following circumstances:\\n• Partitioning results in a small amount of data per partition (approximately less than 1 GB).\\n• Partitioning results in a large number of partitions beyond the\\xa0<https://cloud.google.com/bigquery/quotas#partitioned_tables|limits on partitioned tables>.\\n• Partitioning results in your mutation operations modifying most partitions in the table frequently (for example, every few minutes).\\n\",1644906567.045219,1644929425.772589,U0290EYCA7Q\\n68C83913-AD01-46DC-9141-85C4C9C95184,U030FNZC26L,,,You can use Apache Superset which is an open-source viz tool: <https://superset.apache.org|https://superset.apache.org>,1644902599.749989,1644929677.562009,U02UX664K5E\\n9efc22a8-4471-42a5-b791-a14b29620541,U02TATJKLHG,,,\"Yes, I understood your point. Didn\\'t meant that all of your files are error-prone! \\'Only the paranoid survive\\' - so I had to try either.\",1644493645.074519,1644930185.388389,U02HB9KTERJ\\n8a18dd42-cb70-425a-92d7-1b39942f4f27,,1.0,,\"Hey \\n\\nI registered today for the data engineer zoomcamp. I wanted to know if I am able to catch up with all the videos and homework till week 7,  will I get a chance to work on  a self project, peer reviewing projects and finally getting a certificate?\",1644931121.791109,1644931121.791109,U0330DCCDN1\\n59E64A56-BBE3-4121-8FF3-55E2D292F5BB,,1.0,,<@U01AXE0P5M3> <@U01B6TH1LRL> What would be the best approach to incorporate some data exploration at the beginning of our pipeline? I thought of running a Jupyter notebook on a GCP VM and read the CSVs from Cloud Storage to do some data exploration.  ,1644932809.642579,1644932809.642579,U02UX664K5E\\nf91d26c2-0d33-43c0-83aa-e8035501772f,U02UX664K5E,,,That\\'s what I\\'d do!,1644932809.642579,1644932933.247899,U01AXE0P5M3\\n2564fb55-1642-47e8-b5f1-19df1b30339f,U0330DCCDN1,,,To get a certificate you\\'ll only need to work on a project (and do peer reviewing),1644931121.791109,1644932994.663129,U01AXE0P5M3\\n10ce9348-c90b-4540-9ccb-af67dc1a3a66,U0301PJRDNV,,,\"I\\'ll have a second look, but I\\'d rather not disclose the results otherwise it defeats the whole purpose\",1644922786.198149,1644933985.251239,U01B6TH1LRL\\n8eb1ad13-f6ab-4774-8a77-1fe7825e2772,U02TATJKLHG,,,\"<@U02QNCUUHEY> putting theory aside, have you tried to perform tests? I believe the cost will vary depending on what kind of information are you trying to pull out from database\",1644906567.045219,1644935660.716339,U02UKBMGJCR\\n8c024ec3-8c45-4e11-9ef0-1175e129a932,U02TATJKLHG,,,I made tests for clustering. Partitioning is not only about speed but about supporting it too.,1644906567.045219,1644935925.022439,U02QNCUUHEY\\nb747947e-5532-4b91-a47d-eb801fa32ca7,U02TATJKLHG,,,What\\'s your results in tests? How many attempts have you done?,1644906567.045219,1644936025.003209,U02QNCUUHEY\\nae12a0d2-fe68-46b3-8919-913183ef7fe0,,5.0,,\"Hello,\\nI am facing a problem where my docker container crashes at the time of reading csv files from yello taxi data\\n`table = pv.read_csv(src_file)`\\nI am able to read files from 2021-01.csv which is 24MB.\\nAs the files under 2019-01.csv is around 646MB, `read_csv` fails and the container crashes .\\nI have allocated 8GB and 4CPUs to the kubernetes.\",1644936410.672209,1644936410.672209,U02Q7HGRB0F\\n91941cee-84c6-4224-8bcf-6674a206924a,U02Q7HGRB0F,,,Kubernetes? What do you want to do?,1644936410.672209,1644936612.984909,U01AXE0P5M3\\nace764bf-dd05-405e-8d60-265db11dd60d,U02VBG59VQ9,,,\"Yeah this worked for me ^^\\n\\nI ended up having to make a two different SQL statements to convert the external tables to a partitioned table.\\n\\n```    CREATE_BQ_TBL_QUERY = (\\n            f\"\"CREATE OR REPLACE TABLE {BIGQUERY_DATASET}.{colour}_{DATASET} \\\\\\n            PARTITION BY DATE({ds_col}) \\\\\\n            AS \\\\\\n            SELECT * FROM {BIGQUERY_DATASET}.{colour}_{DATASET}_external_table;\"\"\\n        )\\n\\n        if colour == \\'green\\':\\n            CREATE_BQ_TBL_QUERY = (\\n                f\"\"CREATE OR REPLACE TABLE {BIGQUERY_DATASET}.{colour}_{DATASET} \\\\\\n                PARTITION BY DATE({ds_col}) \\\\\\n                AS \\\\\\n                SELECT * EXCEPT (ehail_fee) FROM {BIGQUERY_DATASET}.{colour}_{DATASET}_external_table;\"\"\\n            )```\",1644456122.637809,1644937413.082199,U02TEERF0DA\\n05d77ed0-1dd6-4fd6-bef5-53897ab092d3,U02VBG59VQ9,,,I ended up having this exact same error. I needed to drop the tables I have previously created in BQ and recreate them. Then this error went away.,1644458029.042529,1644937463.604399,U02TEERF0DA\\nc033ccd6-6c8d-4886-bf4c-4c68bba36541,U02Q7HGRB0F,,,\"Sorry, i meant the docker desktop engine.\",1644936410.672209,1644937468.341669,U02Q7HGRB0F\\ndf076670-12c5-4244-9bc4-cf6d17fb9332,U0301PJRDNV,,,\"<@U01B6TH1LRL> I don’t request the answers. I just want to ensure that we work with the same initially data)))\\ngreen_tripdata_external_table and yellow_tripdata_external_table\\nI suppose they are not a part of week 4 homework. Right?\",1644922786.198149,1644937678.296119,U0301PJRDNV\\n59617bd2-00d6-4f3c-92af-746b499388f9,U02TATJKLHG,,,\"Not enough to make a judgement whether partitioning makes sense in all real-world cases for integer columns with a lot of `NULL` values, but still enough to see the small cost difference for fhv dataset and my example query.\",1644906567.045219,1644938472.355679,U02UKBMGJCR\\nf95738ba-7d03-4c39-b363-7baa5ec495b4,U02TATJKLHG,,,\"which of course does not mean your, different, query would have the same cost and performance\",1644906567.045219,1644938587.608259,U02UKBMGJCR\\naaded662-8384-49b4-8bfe-5a16b768d6ef,,5.0,,\"hi guys, regarding DBT location `was not found in location US` issue, could i confirm that both development schema Eg. `dbt_thomas` and `trips_data_all` must be pointing to a US location Eg. `us-east1` ? Because now both of them are pointing to the same `asia-southeast1` and it’s throwing me error\\n```404 Not found: Dataset de-zoomcamp-339108:trips_data_all was not found in location US```\",1644939005.399959,1644939005.399959,U02ULMHKBQT\\nb82cacf6-0aea-460c-bacb-9480640ea8be,U030FNZC26L,,,Thank you <@U02UX664K5E>. It\\'s really nice.,1644902599.749989,1644939302.157729,U030FNZC26L\\n16921e59-9d24-4b9c-992b-7ed5438c56df,U02ULMHKBQT,,,\"just did a re-run on the DBT cloud UI and it works fine now. to answer my own question: looks like as long as both point to the same location, it should work, US or non-US is fine.\",1644939005.399959,1644939330.628679,U02ULMHKBQT\\n1a8022af-b6ba-4aed-a594-dea2948bbbb5,U02Q7HGRB0F,,,Maybe you could give it more ram?,1644936410.672209,1644939338.114279,U01AXE0P5M3\\n9f962340-ea0b-4a68-a35a-cce1db57e143,U02Q7HGRB0F,,,Our you can chunk this file and save each chunk separately. This is what some students did,1644936410.672209,1644939380.374999,U01AXE0P5M3\\n22a22ec3-08e8-4407-b5fa-a69ee42d2c03,U02ULMHKBQT,,,\"TL;DR: Check your `profile.yml` file.\\n\\nI had the very same issue yesterday (more details: <https://datatalks-club.slack.com/archives/C01FABYF2RG/p1644876320850049>)\",1644939005.399959,1644939470.209899,U02S2TZRBL7\\nb356631d-58e7-4371-b583-db0903183b29,U02TATJKLHG,,,If it helps... <https://github.com/mmg10/dbt_docker/blob/main/Dockerfile>,1644493645.074519,1644939829.580899,U02HB9KTERJ\\n63675432-9f8f-4c3a-879b-038967b8dc06,U02ULMHKBQT,,,\"hey <@U02ULMHKBQT> I explained in several threads above, BQ doesn\\'t allow multiregion queries, meaning that the creation of the table in your dev schema (dbt_name) and the select from the source schema (trips_data_all) should happen in the same region, independent of which one is it.\\n<@U02S2TZRBL7> he\\'s working from the cloud UI so no profiles.yml defined as you had locally.\",1644939005.399959,1644940659.664499,U01B6TH1LRL\\nfeaf840e-83ba-457f-88ff-0a12bfc0b00a,U02ULMHKBQT,,,\"Oh! My bad, sorry. I thought initial post was referring to local set up :sweat_smile:\",1644939005.399959,1644940839.656479,U02S2TZRBL7\\n29defa47-c818-4643-a597-4c1e45882cf9,,2.0,,\"Hey, when i run `dbt test accepted_values_stg_green_tripdata_Payment_type…` and `accepted_values_stg_green_tripdata_Payme nt_type…` are failing. Did anyone else have this problem?\\n```No matching signature for operator IN for argument types INT64 and {STRING} at [24:23]```\",1644944035.464559,1644944035.464559,U02TAA1LDQT\\n2c4fe9b2-e8a8-4452-85f1-53e71ac9283c,U02TAA1LDQT,,,\"looks like you\\'re missing the quote line. I explain why it\\'s needed in the video, but I encourage you to check the compiled code of the test how it looks now and even try to run it manually in BQ\",1644944035.464559,1644945837.131319,U01B6TH1LRL\\nba19169a-2fbf-4b82-b319-9c369458c6b9,U032Q68SGHE,,,\"I think as long as we don\\'t use this column to show insights or visualize data, it won\\'t be a problem.\",1644881953.727679,1644945853.020559,U032Q68SGHE\\ndc0cc6e2-902c-42d0-979d-bbd2e804b249,U02TAA1LDQT,,,thanks,1644944035.464559,1644946592.077669,U02TAA1LDQT\\nf6be0c5b-1684-449b-88dd-d67c4f5f794e,U02DBNR22GN,,,\"Hi <@U02LQMEAREX> and <@U02DBNR22GN> ! I\\'m having this issue and have one question:\\n\\nAfter adding the *Viewer role*, did you have to re-generate the json credentials?\",1644785599.998209,1644954588.302049,U02S2TZRBL7\\n6dd29d0e-a5e9-46d3-8fea-b92e007e7902,U02DBNR22GN,,,No Just adding the Viewer role,1644785599.998209,1644954882.521629,U02DBNR22GN\\nd4f1fbc8-e0b4-4304-95a2-5168be5f1697,U032Q68SGHE,,,Thank you so much <@U032Q68SGHE>!,1644881953.727679,1644954899.936849,U02S2TZRBL7\\n10bdd978-2c30-48c2-b4df-e8fe1b3b40fe,U02DBNR22GN,,,Thank you <@U02DBNR22GN>!,1644785599.998209,1644955043.255759,U02S2TZRBL7\\n6ec8ce19-9209-4956-bd91-a7b1793032a2,U0301PJRDNV,,,\"I\\'m also having a count value different to all available options in the google form.\\n\\n*More context:*\\n• I\\'m running dbt locally\\n• My green and yellow source data were created from the parquet files; not the csv\\n• For `stg_green_tripdata.sql` model I have to `cast(0 as numeric) as ehail_fee` because otherwise I was getting error when compiling `fact_trips.sql` model. See here for more details: <https://datatalks-club.slack.com/archives/C01FABYF2RG/p1644881953727679>\",1644922786.198149,1644957313.280889,U02S2TZRBL7\\nb54ab90a-4101-4d1a-b7d4-58151f5f4b39,,3.0,,Hello everyone! FHV data has full duplicates. Which combinations of fields to use for the primary key? How do you think?,1644957970.026469,1644957970.026469,U02T65GT78W\\n9cc82f7a-dc87-467c-89bd-65a4777d2fd7,U02DBNR22GN,,,\"Same for me. Just add the role. Sorry for the delay :sweat_smile: As far as I know, the credentials authenticate the service account it was generated for, the roles provide authorization to access and use the resources of the project. <https://cloud.google.com/docs/authentication/getting-started>\",1644785599.998209,1644961257.414919,U02LQMEAREX\\nfefc45d5-ead2-4a32-b325-8af8cb32d8dd,U02T65GT78W,,,I counted the records with no deduplication for the homework,1644957970.026469,1644961280.739309,U01B6TH1LRL\\n86be7acf-0567-4119-9d22-7b3f42a45ed5,U032Q68SGHE,,,\"We could also convert to float when reading the csv to generate a parquet table in airflow ingestion dag (to GCS). `pv.read_csv(src_file, convert_options=pv.ConvertOptions(column_types = {\\'ehail_fee\\': \\'float64\\'}))`\\nBigQuery will keep the numeric format and dbt models match with no problem.\",1644881953.727679,1644962175.346569,U02LQMEAREX\\n2652fca5-26ca-49ac-a745-da90703013fc,U02DD97G6D6,,,\"please i am also having this issue, anyway around it\",1644792118.370219,1644962386.703319,U02T0CYNNP2\\nc05a6107-5bcd-4b7a-b032-304cc4a960da,U02DD97G6D6,,,for this add cloud storage admin role to your service account,1644792118.370219,1644964837.863459,U02T0CYNNP2\\n738ff432-4d86-49a7-a73e-fa1458454c31,U030FNZC26L,,,\"I tried superset. It\\'s amazing and has very useful features. I can use it to create a dashboard for myself. I can connect to BQ, query, create beautiful charts, update the chart, ...\",1644902599.749989,1644965463.694419,U030FNZC26L\\n93c4df0e-a337-43af-99a5-feb653d21ecd,,7.0,,What happens If I can\\'t get to submit week 4 assignment?  Just don\\'t seem to completely get a hold of dbt.:face_with_head_bandage:,1644967397.653989,1644967397.653989,U02RSAE2M4P\\n708FC550-1EEB-4BB2-8832-06736E74382D,U030FNZC26L,,,\"<@U030FNZC26L> Glad to hear that, did you deploy Superset on GCP or locally? \",1644902599.749989,1644968127.235519,U02UX664K5E\\n509a4d20-94bb-4279-a188-90df40e7aede,U02RSAE2M4P,,,\"Homework even though scored aren\\'t graded, so you can continue the course without submitting any homework as I\\'m doing. If you really want the certificate, then you just need to complete the project.\",1644967397.653989,1644968319.465419,U02UX664K5E\\n7150ecfa-9907-421d-83b9-2c715cbcc437,,4.0,,\"[Week 4] Hello! I tried to run my dbt project, and I obtained this, and that is my table in BigQuery:\",1644968876.461069,1644968876.461069,U02TJ69RKT5\\n904f4abd-c13c-4759-a947-78d414481d64,,1.0,,\"Is Week 4 homework ready to be done? I\\'m asking because is written \"\"WIP\"\" (Work in Progress?) in the title of the .md file inside the repo.\",1644972230.414909,1644972230.414909,U02TC704A3F\\ndb1a8f43-15c9-4c64-a25a-acf842f6950f,,11.0,,\"When I try to run dbt with the stg_yellow_tripdata.sql I get the error.\\n```Compilation Error in model stg_yellow_tripdata (models/staging/stg_yellow_tripdata.sql)```\\n```Model \\'model.my_new_project.stg_yellow_tripdata\\' (models/staging/stg_yellow_tripdata.sql) depends on a source named \\'staging.yellow_tripdata\\' which was not found```\\nThis is what my schema.yml looks like\\n```version: 2\\n\\n\\nsources:\\n    - name: staging\\n    \\n      database: dazzling-pier-338723\\n\\n      schema: trips_data_all\\n\\n      tables:\\n        - name: green_tripdata_external_table\\n        - name: yellow_tripdata_external_table```\\nstg_green_tripdata.sql runs fine and I double checked that the names listed above are my tables in Big Query. Has anyone run into this? Is anyone familiar with how to resolve?\",1644973280.026019,1644973280.026019,U02TNEJLC84\\n2ada752b-b0d3-4f20-ba21-2fba5e47f0b2,U02TNEJLC84,,,\"In your schema.yml, under tables, you should have yellow_tripdata, not yellow_tripdata_external_table\",1644973280.026019,1644973490.902959,U02U3E6HVNC\\nbf075090-14eb-45e5-9279-8777618082c1,U02TNEJLC84,,,\"Had the same issue and was naming problems.\\n\\nIn SQL file I was looking for {{\\xa0source(\\'staging\\',\\'yellow_tripdata\\')\\xa0}} and I had named my external tables with \"\"yellow_tripdata_external_table\"\" because that\\'s how I do it in BQ.\\n\\nCan you send us the stg_yellow_tripdata.sql code?\",1644973280.026019,1644973901.301029,U02TC704A3F\\nacefb5a8-b967-45eb-a8fb-64e7426e95ce,U02TNEJLC84,,,\"Thanks for the reply. I changed up the name in both stg_yellow_tripdata.sql and schema.yml, but now it is stating that yellow_tripdata is not located in the US which is fine. I\\'ll trouble shoot that later. My confusion right now is why green works named as green_tripdata_external_table and yellow does not. They are named green_tripdata_external_table and yellow_tripdata_external_table in big query.\",1644973280.026019,1644973930.200399,U02TNEJLC84\\n38347faa-8c74-4b51-8215-491178a3f3af,U02TNEJLC84,,,Here is the code: <https://pastebin.com/QSDgAQwU>,1644973280.026019,1644973984.172409,U02TNEJLC84\\nc1f49345-2685-4e4d-bbc8-e7bc25a13f44,U02TNEJLC84,,,Here are my tables in Big Query:,1644973280.026019,1644974046.322609,U02TNEJLC84\\n19c19cd2-8363-49e6-89fd-aa77eac4ee9b,U02TNEJLC84,,,\"Ooohh boy, had this problem too with locations. DBT by default will try US.\\n\\nI fixed this going under projects/my-project/connection inside dbt cloud in account settings and inserting manually.\",1644973280.026019,1644974239.987969,U02TC704A3F\\n09922696-2665-4af0-8aed-261705f745e7,U02TNEJLC84,,,\"<@U02TNEJLC84>, in line 7 of this pastebin code, there\\'s the issue. You should change the source to yellow_tripdata_external_table and it\\'s done.\",1644973280.026019,1644974390.805389,U02TC704A3F\\n2fdc0034-012f-44aa-9be3-0b8e945101e5,U02TNEJLC84,,,\"<@U02TC704A3F> Thank you thank you thank you friend! I will give that a shot. Unfortunately, it was giving me a git error and I chose reclone repository which seems to have blown my entire project away. :partycrying: Four days of work blown away. Going to start from scratch tomorrow. Guess that\\'s what I get for working on this while exhausted. Thank you again for your help and time. I\\'ll try to spot check my code better next time. Doh!\",1644973280.026019,1644974614.189019,U02TNEJLC84\\nab649cc7-7267-41d7-ad98-f3663117db21,U02TNEJLC84,,,\"No problem my friend\\n\\nSad moment now hahaha, but good luck tomorrow, things will be better\",1644973280.026019,1644974743.371689,U02TC704A3F\\nfc5606ae-22b5-4013-b79e-2a54ccca33a4,,,,\"Please i need help with this error.    Error while reading data, error message: Parquet column \\'ehail_fee\\' has type DOUBLE which does not match the target cpp_type INT64.\",,1644980887.935749,U02U5G0EKEH\\n6a626d39-1d2d-4c64-9536-cb512f0d8066,,1.0,,,1644980981.580359,1644980981.580359,U02U5G0EKEH\\n3335629b-6c0a-46f7-8a61-4fc1c796be9b,U02ULMHKBQT,,,thank you both for clarifying and help!,1644939005.399959,1644981840.157089,U02ULMHKBQT\\n5fcb6274-0480-4892-b734-036d06e8be65,U02Q7HGRB0F,,,Thanyou Alexey..,1644936410.672209,1644987356.843289,U02Q7HGRB0F\\n86b6c487-882f-48f9-ad28-9409483028bf,U02RSAE2M4P,,,Somewhere Alexey mentioned that you can skip dbt without any consequences (although I don\\'t recommend),1644967397.653989,1644989880.177399,U02HB9KTERJ\\n6d2f3c4d-7810-4563-8ad3-ae0db8af7b12,U02RSAE2M4P,,,<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1644864132354879>,1644967397.653989,1644989952.588609,U02HB9KTERJ\\nb48e328f-5aa9-4fd0-a43a-116eea9028f8,U02RSAE2M4P,,,Try using dbt cloud....,1644967397.653989,1644990632.315339,U02T0CYNNP2\\n20c580eb-a295-4d0b-bc0f-7de84beda5a1,U02U5G0EKEH,,,\"<https://app.slack.com/client/T01ATQK62F8/C01FABYF2RG/thread/C01FABYF2RG-1644881953.727679>\\n\\nThis should help you\",1644980981.580359,1644992607.746309,U02T941CTFY\\n8d07d3e6-21f1-44e2-a305-b489d1cc22e3,U02TATJKLHG,,,\"<@U02TATJKLHG> We don\\'t get a GUI using this, right? just the command to run the model.. <https://github.com/ankurchavda/data-engineering-zoomcamp/blob/main/4_dbt/docker-setup.md#commands>\",1644493645.074519,1644992857.811459,U02HB9KTERJ\\n72f1a44b-be5f-41b5-bf2a-ef0f45885bd5,U02TA6MJZHR,,,\"have you verified that your bucket is correct? `dtc_data_lake_pivotal-surfer-336713` looks like the default values that were the docker compose files, probably need to change it.\\n\\nthe service account email is `[service account name]`@`[gcp project id]`.<http://iam.gserviceaccount.com|iam.gserviceaccount.com>. I\\'m guessing your GCP project is `zoomcamp-de-338716` . and it\\'s likely your bucket is something like `dtc_data_lake_zoomcamp-de-338716` , assuming you followed the terraform instructions\",1644884972.605279,1644993068.937339,U02UARNPAN5\\ne7fd1f5b-09f2-44a4-a736-6c0b708e2e48,,5.0,,\"```Hello guys, not sure why dbt test is not picking my tests defined under staging folder. got schema.yml defined with tests. \\n\\n\\ndbt test\\n\\n06:41:38  Running with dbt=1.0.1\\n06:41:38  Found 4 models, 0 tests, 0 snapshots, 0 analyses, 376 macros, 0 operations, 1 seed file, 2 sources, 0 exposures, 0 metrics\\n06:41:38\\n06:41:38  [WARNING]: Nothing to do. Try checking your model configs and model specification args\\n\\n\\n\\nPS E:\\\\data_talks_club\\\\dbt_ws\\\\dbt_demo&gt; dbt build\\n06:46:45  Running with dbt=1.0.1\\n06:46:45  Found 4 models, 0 tests, 0 snapshots, 0 analyses, 376 macros, 0 operations, 1 seed file, 2 sources, 0 exposures, 0 metrics\\n06:46:45\\n06:46:48  Concurrency: 2 threads (target=\\'dev\\')\\n06:46:48\\n06:46:48  1 of 5 START view model dbt_dataset.stg_green_tripdata.......................... [RUN]\\n06:46:48  2 of 5 START view model dbt_dataset.stg_yellow_tripdata......................... [RUN]\\n06:46:52  1 of 5 OK created view model dbt_dataset.stg_green_tripdata..................... [OK in 4.06s]\\n06:46:52  3 of 5 START seed file dbt_dataset.taxi_zone_lookup............................. [RUN]\\n06:46:53  2 of 5 OK created view model dbt_dataset.stg_yellow_tripdata.................... [OK in 4.82s]\\n06:47:04  3 of 5 OK loaded seed file dbt_dataset.taxi_zone_lookup......................... [INSERT 265 in 12.03s]\\n06:47:04  4 of 5 START table model dbt_dataset.dim_zones.................................. [RUN]\\n06:47:10  4 of 5 OK created table model dbt_dataset.dim_zones............................. [CREATE TABLE (265.0 rows, 12.1 KB processed) in 6.18s]\\n06:47:10  5 of 5 START table model dbt_dataset.fact_trips................................. [RUN]\\n06:47:28  5 of 5 OK created table model dbt_dataset.fact_trips............................ [CREATE TABLE (194.0 rows, 15.1 GB processed) in 17.83s]\\n06:47:28\\n06:47:28  Finished running 2 view models, 1 seed, 2 table models in 43.50s.\\n06:47:28\\n06:47:28  Completed successfully\\n06:47:28\\n06:47:28  Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5```\",1644993278.066569,1644993278.066569,U02NSF7RYP4\\n00a2d753-dc05-47ad-87d5-fefe7d74e739,U02TATJKLHG,,,\"Hey man, yes not a GUI like the cloud. You can serve the docs and see a lineage graph/documentation and stuff ideally. But I haven\\'t been able to do that. They are available on port 8080 in docker and I forwarded that port to my machine but still it says \\'the site can\\'t be reached\\'.\\n\\nI asked this here - <https://datatalks-club.slack.com/archives/C01FABYF2RG/p1644817851455599>\",1644493645.074519,1644993427.493829,U02TATJKLHG\\ne122e107-f374-4d95-9e38-264f864db22b,,2.0,,\"Any body who\\'s suddenly facing issues with `Max retries exceeded with url: /token` from their local setup, especially in India, something has weirdly changed with the ISPs which is causing gcloud to fail. I was trying to debug this since last night until I finally stumbled upon this <https://stackoverflow.com/a/71106347/5193334|stackoverflow answer> just two days old. Change your ISP or setup a VPN and it\\'ll work. Maybe switch to a mobile hotspot or something.\",1644993561.588799,1644993561.588799,U02TATJKLHG\\na2717379-6b8d-4505-afe2-82c1a7d0779f,U030FNZC26L,,,Thanks. It\\'s in a VM on GCP.,1644902599.749989,1644993575.175659,U030FNZC26L\\na9d3bda9-0193-46da-aedb-25c1c49aec52,U0301PJRDNV,,,\"for wk4 q1, i am having a diff answer from those options as well. PS: i have also done `cast(0 as numeric) as ehail_fee` in my `green.sql` file.\",1644922786.198149,1644994062.396759,U02ULMHKBQT\\n9dbe0911-e4fa-4543-834d-af3cf8d7e8e2,U02NSF7RYP4,,,Please paste your error logs in the thread in a code block,1644993278.066569,1644994257.990969,U02TATJKLHG\\n9c2fc430-07d1-444e-845b-15356381e8fd,U0301PJRDNV,,,\"<@U0301PJRDNV> to answer your question- my counts for initial data for green trip is 7778101, and yellow is 109047518. hope by sharing this, we can assure each other that we are on the same page before we proceed.\",1644922786.198149,1644994537.965329,U02ULMHKBQT\\n3fd242fb-a57c-4e8d-991c-83479fd2ea69,U02RSAE2M4P,,,\"The only consequence for skipping is less scores at the end, but for the certificate it doesn\\'t matter\",1644967397.653989,1644995749.773179,U01AXE0P5M3\\nf137df28-ccff-446a-b89e-1cc42537dabf,U02U6DR551B,,,Hi <@U02U6DR551B> did you try simply defaulting everything to the US? I had quite a lot of problems with this but resolved it eventually as shown in my blog <https://learningdataengineering540969211.wordpress.com/2022/02/19/week-4-de-zoomcamp-4-3-1-location-location-location-dbt-part-2/|post>. Not sure if it will help or if it\\'s the same problem. You will also need to make sure that you point to your new dataset that you created in the US.,1645343764.071639,1645346843.248249,U02U5SW982W\\nade22c89-2a28-489f-9a35-4e33e38e9e81,,7.0,,\"TLDR. *If you got the correct answer for homework 4 question 1 please let me know and comment.*\\n\\nHey guys; The deadline for Week 4 homework submission is approaching inexorably, although despite many attempts I was unable to get any of the provided answers to Question 1.\\nWe theoretically use the same dataset, followed exactly the same steps to ingest data from the same source, created tables using the same SQL queries but still I have seen plethora of threads here on Slack with different counts.\\nIt\\'s crazy because there is no place for random results in such case, this is REALLY SIMPLE example of task to solve after all.\\nI really would like to find a reason why our results differ, but there are many factors and I know from experience that the reason may be something that I would never pay attention to.\",1645346916.280559,1645346916.280559,U02UKBMGJCR\\n95d6512a-3849-4587-8d32-ba18ad2df231,U02UKBMGJCR,,,Hi <@U02UKBMGJCR> I\\'m thinking there could be quite a few reasons but NO I do not get any of the answers provided in the multiple choice possibilities. Mine is 61636205. Go figure ...,1645346916.280559,1645347130.066919,U02U5SW982W\\n53b82648-ccea-4f48-bf34-219298ee1ff1,U02UKBMGJCR,,,And yes - I\\'m like a headless chook at the moment wondering how I\\'m going to get the homework in. If I don\\'t get results that agree with Question 1 then all the others are pretty much out the door as well...,1645346916.280559,1645347239.252669,U02U5SW982W\\n5c9059e8-82e9-424f-af83-2675815ab8bb,U02UKBMGJCR,,,\"I\\'m checking the answers and it seems for other questions there\\'s more consensus on what the right answer is. So you\\'re probably good with other questions\\n\\nFor now I\\'ll suggest to skip question one - and we\\'ll discuss internally what to do with it. It seems the answers we\\'re getting are pretty much all over the place\",1645346916.280559,1645350137.530119,U01AXE0P5M3\\n81fb386d-aae2-474a-9494-caa0c71c1743,U02UKBMGJCR,,,e.g. these are the answers to Q1,1645346916.280559,1645350181.191779,U01AXE0P5M3\\n1496b801-eee8-49a9-83aa-5c10d32b5efa,U02UKBMGJCR,,,and this is how the other questions look like,1645346916.280559,1645350205.551849,U01AXE0P5M3\\nc9ba99e4-d0dc-4200-b5f1-dc354eb56983,U02U6DR551B,,,I recommend <@U02U5SW982W> solution and it worked,1645343764.071639,1645351256.578719,U02RA8F3LQY\\n4e9a20bd-05c4-4867-a4cc-7ad2670c4596,U02UKBMGJCR,,,Hey Alexey; This is related to question 1 only. FHV part is reproducible in 100%.,1645346916.280559,1645351470.202979,U02UKBMGJCR\\n1e22aaa0-8b58-4369-83ee-09c689f854f8,U02UKBMGJCR,,,Ok well I\\'m hoping that is something <@U01AXE0P5M3>,1645346916.280559,1645351600.959439,U02U5SW982W\\n38df6ea4-9086-4d38-87de-cd303ae51d3b,U02U6DR551B,,,Hopefully it works for some people <@U02RA8F3LQY>,1645343764.071639,1645351673.873629,U02U5SW982W\\n9E3A3C65-19F3-407E-91ED-60AB81306918,,2.0,,\"Hi!\\n\\nQuick question: is it essential to complete the week 4 HW (the DBT topic) in order to proceed with this course? Or can I walk through week 4 lectures some time later? \",1645354553.711379,1645354553.711379,U02Q2B9P15M\\nd5f86f3c-ac00-48f6-89bc-7a379221a3ae,,3.0,,does any one\\'s count matches in hw4?,1645354614.611159,1645354614.611159,U02U6DR551B\\n496d7b4f-9dd5-468f-aa9e-336826cf361e,U02Q2B9P15M,,,<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1644864132354879?thread_ts=1644864132.354879&amp;cid=C01FABYF2RG|https://datatalks-club.slack.com/archives/C01FABYF2RG/p1644864132354879?thread_ts=1644864132.354879&amp;cid=C01FABYF2RG>,1645354553.711379,1645355186.818679,U0308MF3KUH\\n954DFE69-CC84-4CE8-98A0-75C1F6C3358A,U02Q2B9P15M,,,\"<@U0308MF3KUH> oh, great! Thank you Gerry\",1645354553.711379,1645355351.403139,U02Q2B9P15M\\nb585f3b2-be80-41a3-870b-962adae4c10a,USLACKBOT,,,\"above problem got solved, when I did same thing in virtual machine in gcp\",1644401511.198979,1645357100.754349,U0300EGP2EL\\n1157b6db-ec07-48f6-afcd-75080eb510a0,USLACKBOT,,,problem is with RAM I guess,1644401511.198979,1645357109.931159,U0300EGP2EL\\nC0706005-4BD2-48E0-9F18-16AD4F3BB520,U02U5SW982W,,,\"Other than the first question, which is all over the place, the other questions were pretty straightforward. What are you having issues with?\",1645338532.693739,1645358058.850199,U02BVP1QTQF\\n46af0238-12d3-4f7b-8509-195f0de0573d,U02T9JQAX9N,,,Thanks.. I\\'ll try this,1645310364.645229,1645358816.572009,U02T9JQAX9N\\n96a554a2-22b8-4d0e-bfe3-b6531e9c3547,U030FNZC26L,,,\"I used this page and the course VM we created before:\\n<https://superset.apache.org/docs/installation/installing-superset-using-docker-compose>\",1644902599.749989,1645359679.031649,U030FNZC26L\\n8060BD65-9272-4E49-B4CC-5DBE405DE70D,U02U6DR551B,,,\"For the Q1, it doesn\\'t match on my end \",1645354614.611159,1645359706.828339,U02TBCXNZ60\\n06769f64-c3db-45d1-9a99-c2f79726dbc7,U02U5SW982W,,,Can you explains what walls are you running into?,1645338532.693739,1645359795.807349,U01B6TH1LRL\\n9b25ac92-c78c-44e5-936f-1827dd6e5211,U02RA8F3LQY,,,\"They had some issues last week, it looked like it had some downtime\",1645340086.281699,1645359824.107629,U01B6TH1LRL\\n04354f00-9192-4920-9255-87d26b5ba8c6,U02T9JQAX9N,,,\"Looks like you\\'re not using data stored in bq but in an external table, so you need to give permission to the bucket as well go the service account\",1645310364.645229,1645359896.025479,U01B6TH1LRL\\n0d9e77db-d514-4d5a-bca1-c11f78ec5d5a,U02T9JQAX9N,,,Which permission would I give to the bucket.. is it `Storage Object ADMIN`,1645310364.645229,1645360526.508499,U02T9JQAX9N\\nfc7d24f1-fe4f-490d-9a7e-2b18eae31ede,U02T9JQAX9N,,,\"And it\\'s a bit weird.. because while I was working in `staging`  my yellow and green tripdata are both external tables, yet they worked with `dbt run`\",1645310364.645229,1645360630.593579,U02T9JQAX9N\\n5e141947-de83-4fad-bce7-2e1dc779e3a3,U02T9JQAX9N,,,I have added the `Storage Object Admin` permission to the `dbt-service-account` yet it\\'s still the same thing,1645310364.645229,1645360816.143299,U02T9JQAX9N\\nfaad8853-a93f-4c25-baf2-8be0c82a8dd2,U02S9JS3D2R,,,\"*Source*: parquet files from BQ, uploaded by Airflow DAGs.\\n*trips_data_all*:\\n• green_tripdata: 7 778 101; \\n• yellow_tripdata: 109 047 518.\\n*production*:\\n• stg_green_tripdata: 6 304 783;\\n• stg_yellow_tripdata: 56 100 630;\\n• fact_trips: 62 405 413.\\nBut! These numbers doesn\\'t matter, because every time I run `dbt bulld` command or run a job, I get a new result :slightly_smiling_face: For now it\\'s 61 450 554. Maybe if I will re-run job all time, I can get correct result. kek.\",1645039735.829189,1645361156.498859,U02RC9GPNG0\\ncabdbfd6-86a8-41cc-8e3b-585781c9a5bb,U02T9JQAX9N,,,I decided to try `dbt build` and it worked. Why it worked is beyond me right now..,1645310364.645229,1645361785.595579,U02T9JQAX9N\\n69af3202-aa4b-44e2-9c19-a9bccd1c4558,U02U6DR551B,,,Neither me,1645354614.611159,1645361952.252409,U02C2M83SE5\\n054c38f7-23d0-4004-a4f5-55d98f0da432,U02T9JQAX9N,,,Hmm.. another update.. when I tried `dbt run` again after `dbt build` it worked.. I removed the `Storage Object Admin` role that I had initially put and neither `build` nor `run` worked. On adding it back.. `dbt run` didn\\'t work until I had done `dbt build` first,1645310364.645229,1645362200.067019,U02T9JQAX9N\\n7f5c75ea-71e9-4da4-9857-60a7b95d36ea,,2.0,,My main branch on dbt suddenly changed to read-only... How do I change it back,1645363248.686729,1645363248.686729,U02T9JQAX9N\\n02f19750-831d-4588-8597-9338d7b36b21,,1.0,,My fact_trips has only 192 records! Is it normal?,1645363499.583259,1645363499.583259,U02HB9KTERJ\\nd61974cc-55fd-4c4a-8671-9702f1711369,U02HB9KTERJ,,,\"Sorry, forgot to set the var is_test_run to false.\",1645363499.583259,1645363797.179889,U02HB9KTERJ\\n5173c882-d356-4ff4-9bc5-3bfd3a77b32d,U02T9JQAX9N,,,\"I do think the real issue was because in my `dim_zones.sql` file.. I didn\\'t change the config macro to `table`.. it was still `view` . Now that I changed it, I removed the `Storage Object Admin` permission and it worked normally.\",1645310364.645229,1645364683.896219,U02T9JQAX9N\\nc0b1b6a4-963e-4615-b427-8ab9df977491,U02T9JQAX9N,,,\"Since you are on main branch, it doesn\\'t allow to change. Just create a new branch to keep going\",1645363248.686729,1645365232.658739,U02C2M83SE5\\nf41ca0d7-3952-4f9c-a3c5-bc2246b8a8ce,U02T9JQAX9N,,,Yeah.. thanks.. I\\'ve done that,1645363248.686729,1645365493.874859,U02T9JQAX9N\\n7f8957ff-47a3-43df-9ad2-055ba4a58b8e,U02URV3EPA7,,,\"Yeees!!! Now works. :muscle:\\n\\nI applied <@U0308MF3KUH>\\'s  suggestion and put `pytrends` in _PIP_ADDITIONAL_REQUIREMENTS\\n\\nThank you very much!\",1645291417.326719,1645365691.220749,U02URV3EPA7\\n2A344F90-A7CD-420B-AECF-2F8AAAFA06CB,U02U6DR551B,,,\"<@U02TBCXNZ60> what about the rest of the questions? If yes, can you share staging table counts?\",1645354614.611159,1645371417.296499,U02U6DR551B\\n9020be88-15a1-4e13-b99f-ed60ff3ed582,,1.0,,\"I\\'m facing this error when running `dbt build`\\n\\n```15:33:59  15 of 15 START table model dbt_toluwani.fact_trips.............................. [RUN]\\n15:34:02  15 of 15 ERROR creating table model dbt_toluwani.fact_trips..................... [ERROR in 3.10s]\\nDatabase Error in model fact_trips (models/core/fact_trips.sql)\\n  Error while reading table: composed-facet-339115.trips_data_all.external_green_tripdata, error message: Parquet column \\'ehail_fee\\' has type DOUBLE which does not match the target cpp_type INT64.\\n  compiled SQL at target/run/taxi_rides_ny/models/core/fact_trips.sql```\",1645371837.400189,1645371837.400189,U02T9JQAX9N\\na86cded2-1fbf-4c6a-8ffd-a059ff996241,U02T9JQAX9N,,,ignore the `ehail_fee` column while importing to SQL/BigQuery etc.,1645371837.400189,1645372711.559099,U02HB9KTERJ\\na831c3b6-6a66-44c2-9224-b85f934a150b,,5.0,,\"I\\'m facing this error while running `dbt build` from the `dm_monthly_zone_revenue` model\\n```16:11:54  16 of 17 START table model dbt_toluwani.dm_monthly_zone_revenue................. [RUN]\\n16:11:55  16 of 17 ERROR creating table model dbt_toluwani.dm_monthly_zone_revenue........ [ERROR in 1.68s]\\nDatabase Error in model dm_monthly_zone_revenue (models/core/dm_monthly_zone_revenue.sql)\\n  A valid date part name is required at [17:33]\\n  compiled SQL at target/run/taxi_rides_ny/models/core/dm_monthly_zone_revenue.sql```\\nI\\'m using dbt cloud\",1645373750.829869,1645373750.829869,U02T9JQAX9N\\ne760ba8f-96b8-46d6-9f06-d4a26f8ea6da,U02T9JQAX9N,,,\"This is my sql file\\n```{{ config(materialized=\\'table\\') }}\\n\\nwith trips_data as (\\n    select * from {{ ref(\\'fact_trips\\') }}\\n)\\n    select \\n    -- Revenue grouping \\n    pickup_zone as revenue_zone,\\n    date_trunc(pickup_datetime, \\'month\\') as revenue_month, \\n    --Note: For BQ use instead: date_trunc(pickup_datetime, \\'month\\') as revenue_month, \\n\\n    service_type, \\n\\n    -- Revenue calculation \\n    sum(fare_amount) as revenue_monthly_fare,\\n    sum(extra) as revenue_monthly_extra,\\n    sum(mta_tax) as revenue_monthly_mta_tax,\\n    sum(tip_amount) as revenue_monthly_tip_amount,\\n    sum(tolls_amount) as revenue_monthly_tolls_amount,\\n    sum(ehail_fee) as revenue_monthly_ehail_fee,\\n    sum(improvement_surcharge) as revenue_monthly_improvement_surcharge,\\n    sum(total_amount) as revenue_monthly_total_amount,\\n    sum(congestion_surcharge) as revenue_monthly_congestion_surcharge,\\n\\n    -- Additional calculations\\n    count(tripid) as total_monthly_trips,\\n    avg(passenger_count) as avg_montly_passenger_count,\\n    avg(trip_distance) as avg_montly_trip_distance\\n\\n    from trips_data\\n    group by 1,2,3```\",1645373750.829869,1645373810.622739,U02T9JQAX9N\\nd9c76a7c-6ecc-4565-ba72-0588e70982a7,U02T9JQAX9N,,,\"```--Note: For BQ use instead: date_trunc(pickup_datetime, \\'month\\') as revenue_month, ```\\nIs this part correct? If using BQ, uncomment this line and comment the one above\",1645373750.829869,1645374865.407609,U02HB9KTERJ\\ne8549c87-e98d-4d05-835a-78ea86a5dcb4,,8.0,,\"I\\'ve searched the Slack but I can\\'t find anything about this problem. Why would I be getting this error in the dbt console? I built the green table from .csv files, and it has the right number of lines.\\n```Database Error in model fact_trips (models/core/fact_trips.sql)\\n  Column 8 in UNION ALL has incompatible types: BOOL, STRING at [26:5]\\n  compiled SQL at target/run/taxi_rides_ny/models/core/fact_trips.sql```\\nAt this point I have overwritten all of my own files with the ones in the repo, so I have no idea why this would not be working.\",1645380005.596629,1645380005.596629,U02UBQJBYHZ\\ndb4ca3c7-a7fd-404e-90c9-b39b3c94585f,U02UBQJBYHZ,,,apparently in one of the tables it treats the boolean column as string... maybe try casting it to boolean?,1645380005.596629,1645383113.640779,U01AXE0P5M3\\n394b9ee0-d7c5-4be4-a2bb-55b0db93be5d,U02U5SW982W,,,\"Hi <@U02BVP1QTQF> and <@U01B6TH1LRL> it just seemed to take forever to resolve the issue of location for me, then I hit the snag of having only the test data in my BQ (I should have been checking as I went yes) - plus dbt sometimes just responding in odd ways that I wasn\\'t expecting (various not available errors and other weird things) but then it would resolve. I guess that\\'s hard when you\\'re learning because you don\\'t know what the error is. Is it a genuine error or is it just a glitch or what is it? When it works it\\'s great - it really is - but when it doesn\\'t it\\'s a bit of a horror show (I guess that\\'s just tech). I think I\\'m getting through it. I\\'m hoping <@U02DPPNQ8V6> is right and the other questions for homework are pretty straightforward. Thanks for checking in guys :pray:\",1645338532.693739,1645384670.564429,U02U5SW982W\\n7b7f458d-8d33-4db5-91ce-872a9b1f41b6,U02UBQJBYHZ,,,\"Hi. You definitely need to check the column types in your BQ tables (yellow, green). If I\\'m not mistaken, the problem is in the `store_and_fwd_flag` column. If this is the case, then you should cast the columns to the same type, e.g. to `boolean`, as  Alexey suggested.\",1645380005.596629,1645386143.425279,U02DNSC6Z52\\nafd91452-b45d-471e-a1e6-e900276d043d,,,,\"[Week 4]  I have a separate GitHub branch for the dbt build job and I am running into this issue.  Originally I thought it was because I left the `week_4_analytics_engineering` folder in there but I removed it and still have this issue.\\n\\n```Cloning into \\'/tmp/jobs/44885646/target\\'...\\nSuccessfully cloned repository.\\nListing tags in repo...\\nChecking out to week_4_analytics_engineering...\\nFailed to checkout to specified revision.\\ngit checkout week_4_analytics_engineering\\nfatal: \\'week_4_analytics_engineering\\' could be both a local file and a tracking branch.\\nPlease use -- (and optionally --no-guess) to disambiguate```\",,1645386738.265939,U02SEB4Q8TW\\nd6c9b38e-244f-40a5-bdd1-e6bcc590c977,U02UBQJBYHZ,,,I can\\'t cast as boolean from dbt. Only integer and numeric.,1645380005.596629,1645389581.962369,U02UBQJBYHZ\\n441fbba1-6488-41e9-92c4-331f99cb869e,U02U5SW982W,,,\"I had some issues following the videos because I made the mistake of setting up schemas in different regions and because dbt is quite dense (I mean, everything we’ve seen so far in this course is super dense lol), but once I managed to solve all of my issues, the homework is derivative of the work we’ve seen during the lesson videos. The answer I got for Q1 did not match any of the available answers in the form but all other questions matched the answers.\\n\\nI share the sentiment of having a mini panic attack when learning something new and stumbling into errors and not knowing how things are supposed to work yet, but I think that’s part of the deal of working with computers :sweat_smile: .  Keep up the good work!\",1645338532.693739,1645389836.602049,U02BVP1QTQF\\n39088576-37c5-452e-a7af-c2ec3aea7e49,U02UBQJBYHZ,,,Also it tells me the problem is incompatibility between bool and int.,1645380005.596629,1645390022.723189,U02UBQJBYHZ\\nda99ea95-42b7-4495-afa2-d4532054af79,U02UBQJBYHZ,,,OK works if I cast to string. Why does this fail for me and not for anyone else?,1645380005.596629,1645390252.569439,U02UBQJBYHZ\\n96de73bf-55da-4929-8d68-7d7520101c16,U02U5SW982W,,,You too <@U02BVP1QTQF> - you are powering ahead as always!:grinning:,1645338532.693739,1645390472.184639,U02U5SW982W\\n8613a27c-28e2-4885-bb71-7643599b8154,,25.0,,\"So, in dbt cloud environment, I solved the problem of incompatible store_and_fwd_flag types between yellow and green data sets, and I deleted the starter files in the example folder. I committed my files and then I started the job in production environment. The problem came back and so did the starter files in the example folder. What step did I skip?\",1645396335.802449,1645396335.802449,U02UBQJBYHZ\\nd6fe492b-fab1-484b-b87b-f187b20d2149,,5.0,,\"I can\\'t get back to my dbt development environment, and the production environment is running old code. I was up-to-date with commits.\",1645398282.526149,1645398282.526149,U02UBQJBYHZ\\ne17ac42d-2a3a-418f-bc25-d07a18e165ed,U02UBQJBYHZ,,,Hi Cris - you did a Pull Request from GitHub?,1645396335.802449,1645398307.798299,U02U5SW982W\\n04397f94-6f07-4ca8-8f74-c23af324ed7b,U02UBQJBYHZ,,,\"I did a commit before I ran. I did not do a pull request, but the code was up to date and I wasn\\'t intending to change it.\",1645396335.802449,1645398539.330179,U02UBQJBYHZ\\nae94389d-f362-4017-8f8e-71cabfafadb1,U02UBQJBYHZ,,,I\\'d do a pull request but i can\\'t get back to the development environment now.,1645396335.802449,1645398558.054329,U02UBQJBYHZ\\ne9047d1e-a40d-42bb-80a2-dd2d55b29908,U02UBQJBYHZ,,,Hi <@U02UBQJBYHZ> what do you mean you can\\'t get back to your development environment? I just wonder if you\\'ve done a silly thing like me (probably not) and haven\\'t got your code in your GitHub repo up to date. Mine is a slightly different problem but the solution might be the same. I outlined my problem and steps in my blog <https://learningdataengineering540969211.wordpress.com/home/|post>.,1645398282.526149,1645398640.225369,U02U5SW982W\\n404de9c3-8e6b-4cef-84c5-d5aff0561981,U02UBQJBYHZ,,,\"Sorry, what do you mean you \\'can\\'t get back to the development environment?\",1645396335.802449,1645398671.601509,U02U5SW982W\\n82d6c22c-33bc-4cd3-ab0a-54db3d958a5d,U02UBQJBYHZ,,,In the main navigation bar of dbt top left when you click on it do you have Environments listed? And then when you click on that what do you see?,1645396335.802449,1645398721.529389,U02U5SW982W\\n009f6bb9-b9ca-45fb-8a8b-37bc6084adae,U02UBQJBYHZ,,,\"So right, the code I thought I had committed isn\\'t there. But I had done a pull request, and a commit. I don\\'t know why it didn\\'t go through.\",1645398282.526149,1645398753.048149,U02UBQJBYHZ\\n16f00fa4-78ad-4b70-9a03-1ebd70729b84,U02UBQJBYHZ,,,So did I just lose ten hours of work?,1645398282.526149,1645398775.395319,U02UBQJBYHZ\\nac2da64f-61a8-450f-9264-ea53ed60d35b,U02UBQJBYHZ,,,\"When I go on dbt all I see is the production environment page. If I click on development, it does nothing.\",1645398282.526149,1645398831.395109,U02UBQJBYHZ\\nc393c18b-cfd3-4167-9354-a297dd537d08,U02UBQJBYHZ,,,\"Yes I do, and I see Development and Production. But nothing happens when I click on Development.\",1645396335.802449,1645399183.454449,U02UBQJBYHZ\\n8f6ab8b8-3953-485e-ba1e-f6b8f43748e1,U02UBQJBYHZ,,,Well it goes to an empty page.,1645396335.802449,1645399200.374899,U02UBQJBYHZ\\ncf7f89b9-e6b0-474e-bfd0-4279abda3e4e,U02UBQJBYHZ,,,\"OK it came back. There is \"\"Develop\"\" on that menu.\",1645396335.802449,1645399259.405459,U02UBQJBYHZ\\n4ec0f728-de07-467e-b189-da7192e63719,U02UBQJBYHZ,,,Didn\\'t lose anything. I\\'ll do the commit again.,1645396335.802449,1645399314.140889,U02UBQJBYHZ\\nfb8bd5e1-d4e4-4395-9df0-82004ec49a53,U02UBQJBYHZ,,,\"Wait a minute, do I have to go to my github repo and merge?\",1645396335.802449,1645399361.979289,U02UBQJBYHZ\\n9329fd88-1ba1-44d6-b82a-d6f02bf33a0c,U02UBQJBYHZ,,,Hi <@U02UBQJBYHZ> I find it a bit flaky sometimes. As in if I say click on the Navigation menu and then on Develop it will just show a blank screen of death (ie. do nothing). If I then go and click on the project in the top dropdown box (image 1) and then click on my project and then hit develop on the main navigation bar it will then weirdly come up. Not sure if this is what is happening to you?,1645398282.526149,1645399419.953729,U02U5SW982W\\n8710ab71-1975-45c2-852f-23b44e3dbce6,U02UBQJBYHZ,,,Absolutely,1645396335.802449,1645399451.483579,U02U5SW982W\\nfed6d1ad-ecec-47cd-ad4d-9f51e57e50e8,U02UBQJBYHZ,,,Or else the changes you have made in Development will not go over to production because this is where production is getting it\\'s info from,1645396335.802449,1645399484.085609,U02U5SW982W\\n3e05b356-265c-4a4b-9bd9-bba3488c645b,U02UBQJBYHZ,,,You can change it in development and re-run all your jobs in production until the cows come home - until it\\'s updated in your GitHub repo you won\\'t see any changes. That\\'s the point I guess - separation of production and development.,1645396335.802449,1645399554.890679,U02U5SW982W\\nfdd1655e-d99c-452f-aaa5-35fcbf3993c9,U02UBQJBYHZ,,,But I committed from the dbt console.,1645396335.802449,1645399698.291229,U02UBQJBYHZ\\n37a1d84e-3a2a-457d-ac81-ff8751336e79,U02UBQJBYHZ,,,The first time I did that I didn\\'t have to merge anything.,1645396335.802449,1645399722.514299,U02UBQJBYHZ\\nc7716707-8c62-4bd2-8563-941ba50fb90e,U02UBQJBYHZ,,,When you got to your GitHub repo do you see the changes that you put in your code here in the main GitHub branch?,1645396335.802449,1645399897.043339,U02U5SW982W\\na1ca4a8c-3818-4d36-964f-7fdc4aa0617d,U02UBQJBYHZ,,,\\'go to\\' that should be,1645396335.802449,1645399913.661529,U02U5SW982W\\nd6a61435-e836-406d-8954-ef8757fc80fd,U02UBQJBYHZ,,,\"No I hit the \"\"pull request\"\" button a couple of times until I got a \"\"merge\"\" button. I\\'m not used to using git this way. I only ever push my own changes, and I never have to merge anything. But I guess in real life you have to actually do the extra step.\",1645396335.802449,1645399965.252109,U02UBQJBYHZ\\n48e95b87-67c3-478f-b697-25a77c490936,U02UBQJBYHZ,,,My changes are in there now.,1645396335.802449,1645399984.439039,U02UBQJBYHZ\\nbcd1ed1a-d795-418a-b45f-a93bf1d87461,U02UBQJBYHZ,,,Success! :slightly_smiling_face:,1645396335.802449,1645400164.003719,U02UBQJBYHZ\\n610bdeff-6bad-46a9-a3e7-1ab26fb70a1b,U02UBQJBYHZ,,,Awesome! Great job :clap:,1645396335.802449,1645400185.124669,U02U5SW982W\\n81e30798-a6c0-4686-a1fa-d366f1e40728,U02UBQJBYHZ,,,Thanks so much! I\\'m not sure I would have figured that out. I\\'m not sure I did figure it out LOL. I just went to github and clicked buttons until it merged my code.,1645396335.802449,1645400253.957069,U02UBQJBYHZ\\nf22c13e6-d700-4e40-9716-a1fbda2aec44,U02UBQJBYHZ,,,I\\'m not really used to the intricacies of GitHub either. A lot of the time I don\\'t find it terribly straightforward :confused:,1645396335.802449,1645400264.113629,U02U5SW982W\\n18af0feb-c0c1-439e-80a8-4e1db99584a1,U02UBQJBYHZ,,,I just use it as a place to store my latest code. I don\\'t make branches for my own code.,1645396335.802449,1645400308.369289,U02UBQJBYHZ\\nc87c7e8d-a5bf-451e-a26c-dc4a3f029e66,U02UBQJBYHZ,,,Me too Cris :slightly_smiling_face: and yes I just kept hitting buttons until it seemed to change and give me what I wanted in dbt to push the changes back. I\\'m very technical like that :wink:,1645396335.802449,1645400401.433659,U02U5SW982W\\n4346e7e1-e973-40f4-88b7-7b6bec75050b,,4.0,,\"If I forgot the name of my gcp_bucket , where can I find it?\",1645402111.670169,1645402111.670169,U02T2DX4LG6\\n56da7fed-186b-4b9c-8fd1-9997191e4a38,U02T2DX4LG6,,,,1645402111.670169,1645402660.741269,U02T2DX4LG6\\n04fefede-6716-4f70-9b75-2e736e707182,U02T9550LTU,,,oh  thanks i will try that out!,1645148944.544799,1645402983.845409,U02T9550LTU\\nac783d1f-fe53-4a9e-8e1f-c9649b3c9090,U02T2DX4LG6,,,Hi <@U02T2DX4LG6> I had the same issue as well. I outline how to get this in my blog <https://wordpress.com/post/learningdataengineering540969211.wordpress.com/278|post> (see about halfway down - Preliminaries) . You basically just need to sign into GCP.,1645402111.670169,1645403211.482139,U02U5SW982W\\ndf05aec0-cfaf-4e20-bfd4-07816bd97770,U02T2DX4LG6,,,thanks Sandy!,1645402111.670169,1645403798.988059,U02T2DX4LG6\\nd90f001b-2c5c-4a49-8ef6-8837582c803f,U02T2DX4LG6,,,No problems :),1645402111.670169,1645404323.473179,U02U5SW982W\\n8c98429f-d1ee-43ee-8a8a-2bbb5ce31224,,8.0,,\"Does anyone get relationship warnings after building `fact_fhv_trips.sql`?\\n\\n```02:15:21  Completed with 2 warnings:\\n02:15:21  \\n02:15:21  Warning in test relationships_stg_fhv_tripdata_dropoff_locationid__locationid__ref_taxi_zone_lookup_ (models/staging/schema.yml)\\n02:15:21    Got 28151 results, configured to warn if != 0\\n02:15:21  \\n02:15:21    compiled SQL at target/compiled/taxi_rides_ny/models/staging/schema.yml/relationships_stg_fhv_tripdata_3f485d67da05e9c09d76ca36534fb599.sql\\n02:15:21  \\n02:15:21  Warning in test relationships_stg_fhv_tripdata_pickup_locationid__locationid__ref_taxi_zone_lookup_ (models/staging/schema.yml)\\n02:15:21    Got 208 results, configured to warn if != 0\\n02:15:21  \\n02:15:21    compiled SQL at target/compiled/taxi_rides_ny/models/staging/schema.yml/relationships_stg_fhv_tripdata_dbb8b1821bf273d4db815dab2430b4ca.sql\\n02:15:21  \\n02:15:21  Done. PASS=19 WARN=2 ERROR=0 SKIP=0 TOTAL=21```\\nIt seems like the `pickup_locationid` and `dropoff_locationid` in my `stg_fhv_tripdata.sql` isn’t being found in `taxi_zone_lookup.csv`?\",1645409756.360849,1645409756.360849,U02UAHJHJ20\\n0e7518a8-1984-4c4d-998a-51108188531e,,1.0,,\"Hello, All.\\nFor #homework4, should the following be considered valid\\nvalues for dispatching_base_num in FHV Data?\\nI’m not able to get the totals for the homework questions.\\n\\nDISPATCHING_BASE_NUM\\n----------------------\\ndispatching_base_num\\n\\\\N\\nPaul Solotshi\\n\\n\\nThe typical values are:-\\ndispatching_base_num\\nB00860\\nB01065\\nB02395\\nB02510\\nB02563\",1645410950.187189,1645410950.187189,U02UUU3PUDA\\ndce700c3-85ed-4639-9088-805c6357c54c,U02UUU3PUDA,,,Hi <@U02UUU3PUDA> they look fine to me...,1645410950.187189,1645414176.871959,U02U5SW982W\\n2d03e41d-d13f-480e-a25a-d95af41a2f85,U02UAHJHJ20,,,Hi <@U02UAHJHJ20> is that just because the build is running the tests?,1645409756.360849,1645414423.333819,U02U5SW982W\\n408DCC07-DECE-407B-A423-2E80BEFE07F9,U02UAHJHJ20,,,<@U02U5SW982W>  yes it\\'s from the tests. It\\'s just bugging me that this warning doesn\\'t pop up for the yellow or green trip data pickup and dropoff location IDs. I could easily ignore or take off the warnings though,1645409756.360849,1645414946.459919,U02UAHJHJ20\\n1a95d5bd-f9a8-4e1d-beed-9c798b985b52,U02UAHJHJ20,,,Hmmm... ... I see what you mean <@U02UAHJHJ20>,1645409756.360849,1645415276.271929,U02U5SW982W\\n5e38d570-fdda-4e28-b4b5-04023a071322,,11.0,,Homework Question 4 - does anyone not get any of the answers?,1645415350.125939,1645415350.125939,U02U5SW982W\\n6dc93a30-cfc8-46cd-96d2-211de89f831c,U02U5SW982W,,,\"Just wondering if I\\'m alone in not getting any of the answer? I have one of the answers for Question 3 (I can only assume that I am right). I\\'m wondering if there is something wrong with my code for the join... but I thought I had followed it pretty much just as Victoria had it: `{{\\xa0config(materialized=\\'table\\')\\xa0}}`\\n\\n`with\\xa0fhv_data\\xa0as\\xa0(`\\n\\xa0\\xa0\\xa0\\xa0`select\\xa0*,`\\xa0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0`\\'fhv\\'\\xa0as\\xa0service_type`\\xa0\\n\\xa0\\xa0\\xa0\\xa0`from\\xa0{{\\xa0ref(\\'staging_fhv_tripdata\\')\\xa0}}`\\n`),`\\xa0\\n\\n`dim_zones\\xa0as\\xa0(`\\n\\xa0\\xa0\\xa0\\xa0`select\\xa0*\\xa0from\\xa0{{\\xa0ref(\\'dim_zones\\')\\xa0}}`\\n\\xa0\\xa0\\xa0\\xa0`where\\xa0borough\\xa0!=\\xa0\\'Unknown\\'`\\n`)`\\n`select`\\xa0\\n\\xa0\\xa0\\xa0\\xa0`fhv_data.dispatching_base_num,`\\xa0\\n\\xa0\\xa0\\xa0\\xa0`fhv_data.pickup_datetime,`\\xa0\\n\\xa0\\xa0\\xa0\\xa0`fhv_data.dropoff_datetime,`\\n\\xa0\\xa0\\xa0\\xa0`fhv_data.PULocationID,`\\n\\xa0\\xa0\\xa0\\xa0`fhv_data.DOLocationID,`\\n`\\xa0\\xa0\\xa0\\xa0<http://fhv_data.SR|fhv_data.SR>_Flag,`\\n`from\\xa0fhv_data`\\n`inner\\xa0join\\xa0dim_zones\\xa0as\\xa0pickup_zone`\\n`on\\xa0fhv_data.PULocationID\\xa0=\\xa0pickup_zone.locationid`\",1645415350.125939,1645415504.484269,U02U5SW982W\\n212caa60-68c8-4577-9ac9-d75142b65661,U02U5SW982W,,,\"My record count comes back with 23,554,618? This isn\\'t even close to any of the answers that are provided here...\",1645415350.125939,1645415628.365489,U02U5SW982W\\na4bbe4ca-43ce-4b7d-a796-48f28b39751a,U02U5SW982W,,,<https://datatalks-club.slack.com/archives/C02V1Q9CL8K/p1645363883000419>,1645415350.125939,1645416004.574989,U02SQQYTR7U\\nfd00f7bb-d9f0-4182-af55-15d067ce9dc0,U02U5SW982W,,,Hi <@U02SQQYTR7U> - thank you for that but I believe that only applied to homework 4 question 1 and not homework 4 question 4? Am I right?,1645415350.125939,1645416323.208579,U02U5SW982W\\nc01cd56e-323d-4825-bab3-cc4eed49251d,U02U5SW982W,,,owh sorry I can\\'t read detail you question,1645415350.125939,1645416370.204589,U02SQQYTR7U\\n0cc6b9bb-5946-4a26-a383-8d6fbb108796,U02U5SW982W,,,from my side I already got same count with the choice,1645415350.125939,1645416436.237779,U02SQQYTR7U\\n5cb4dfe0-a2a7-4f65-8e26-cbd739715efd,U02U5SW982W,,,Ah ok <@U02SQQYTR7U> - I\\'m just wondering if there is anything wrong with my join in my code. That\\'s all I can think of at the moment...,1645415350.125939,1645416603.187709,U02U5SW982W\\n349e1d47-52a5-4e90-82f3-a64ae95b7bd6,,3.0,,\"Question about ETL vs ELT. Is it true to say that ETL works against Data Warehouses usually, and ELT works against Data Lakes?\",1645417366.901519,1645417366.901519,U02U5L97S6T\\nc941d572-52bd-4a1f-bccd-14de231428b7,U02UAHJHJ20,,,Hi <@U02UAHJHJ20> - did you make sure that kept only records with known pickup and dropoff locations? Like in your join when you create the table? I ask because I didn\\'t do this properly.,1645409756.360849,1645417538.356229,U02U5SW982W\\n023ff38b-8063-4e84-b8e2-19e825fa7baf,U02U5SW982W,,,Hi <@U02SQQYTR7U> - I believe I have found my problem. At least my code for the join was incorrect. I\\'m pretty sure that was it. Thanks for trying to help :slightly_smiling_face:,1645415350.125939,1645417963.741359,U02U5SW982W\\n048BF640-ABA9-46DE-B115-40A5EE32174F,U02UAHJHJ20,,,\"<@U02U5SW982W> Yes I do suspect that it may be because my fhv data has null values for pickup and dropoff location IDs. After excluding it, the error still shows. This all happens in my staging fhv script and not in the fact fhv script \",1645409756.360849,1645419957.823599,U02UAHJHJ20\\ncbc19bd7-dcc8-4a87-862d-626e76fb12b5,U02UAHJHJ20,,,<@U02UAHJHJ20> when you are doing this build are you in development?,1645409756.360849,1645421184.310579,U02U5SW982W\\n2146bba1-79dd-4f96-a4fa-027931380795,U02UAHJHJ20,,,\"That is, have you fixed it in development and then done the build there and there is an error here in the development? Or do you do the fix in development, run it and it works here in the development environment but not in production? I just ask because when I\\'ve fixed something in development for some silly reason I\\'ve thought that the fix will just come through to my production environment, which of course is wrong. Duh.\",1645409756.360849,1645421401.237499,U02U5SW982W\\n624c3d6d-f711-47a6-a5e0-4c7f1a99fd03,U02UAHJHJ20,,,And you have run it with the test variable as false? As in the question? ie. What is the count of records in the model stg_fhv_tripdata after running all models *with the test run variable disabled (:false)*,1645409756.360849,1645421683.906569,U02U5SW982W\\n8dda548d-cf4f-4bfd-bebd-191f934797c3,,,thread_broadcast,\"Hi <@U02U5L97S6T> - this is directly from Ankush\\'s video \\'ETL is a data warehouse solution, whereas ELT is a data lake solution.\\' (I took transcript notes which I will include below if you want the detail in written form). ELT is used for large amounts of data - which is pretty much a data lake. So, yes I believe that you are right. Transcript <https://learningdataengineering540969211.wordpress.com/2022/01/29/week-2-de-zoomcamp-2-1-1-data-lake/|link> in my blog.\",1645417366.901519,1645422687.885049,U02U5SW982W\\ne657abb0-136e-4065-a39a-6674d08654b6,,1.0,,\"Hey all, I\\'m enjoying the course so far. Still in week1... How much time do we have for projects?\\n\\nAnd are projects done in groups?\",1645429178.222289,1645429178.222289,U0343G3CH32\\nf591ac53-8e1c-4c5f-b145-8008841d0e0b,,2.0,,\"Hey everyone, just joined the channel and the zoomcamp. Can anyone update with the zoomcamp?\",1645171598.201219,1645171598.201219,U033PKGGFS7\\n4befa786-b733-462d-a0c4-68054f9d4e22,,16.0,,\"Hello all, just started with the course. i\\'m in `1.2.2 - Ingesting NY Taxi Data to Postgres` . when i\\'m trying this command `pgcli -h localhost -p 5432 -u root -d ny_taxi` getting this error. `connection to server at \"\"localhost\"\" (::1), port 5432 failed: FATAL:  role \"\"root\"\" does not exist`\",1645172408.560079,1645172408.560079,U0343G3CH32\\n1c2d8295-9542-47d7-93b1-883763e2202e,U0343G3CH32,,,I\\'m not sure why? any one faced it before?,1645172408.560079,1645172429.817729,U0343G3CH32\\n7c4d25c0-2ba1-49e7-9e5d-8e25b770b558,U0343G3CH32,,,\"i tried brew install pgcli. getting this error instead `FATAL:  role \"\"root\"\" does not exist`\",1645172408.560079,1645173043.283289,U0343G3CH32\\n063246d5-f7df-404d-a777-4b26d09b7725,U033PKGGFS7,,,\"What do you mean by \"\"update\"\"? If you want to get started, all the information is in the course repo\",1645171598.201219,1645173135.471799,U01AXE0P5M3\\nbb1c5bee-1061-416b-9504-9871d15e8111,U02UA0EEHA8,,,\"I still have the same problem. I used csv files and parquet separately. Unfortunately still getting a lower number than what is in the videos. I also think the problem lies with the input, but I have no idea how to solve it.\",1645171121.759679,1645174039.000509,U02UVKAAN2H\\nf80cf135-c796-45fb-b8c4-36f392b70848,U02CD7E30T0,,,\"Hi Luis, sorry for the late message. This was just a quick hack to get you all started with it, but like Alexey said, please tweak it as per your convenience. If you feel you should download directly, there\\'s also some python code from our original weeks for passing a `curl` command to the web URL, that you can use here\",1645010628.987459,1645174408.435929,U01DHB2HS3X\\n6964530b-d7b8-45d9-af7e-9969790c35f0,U0343G3CH32,,,Did you create a postgre database via Docker before trying to connect to it?,1645172408.560079,1645175364.858479,U02U1GSKX2T\\nc1ef76cc-d2ad-4fed-9a41-d3737e1e6e89,U02T9550LTU,,,\"Hi! I faced the same problem when I used SELECT * when making the fhv_fact_trips.sql.\\nApparently BigQuery renames the columns to avoid duplicated names when making a query. However, if you check the actual SQL code that dbt rund, the code is a CREATE ... TABLE statement, which does not allow duplicated columns and also fails in BigQuery. I fixed it by setting column names in the SELECT FORM statement, in the same way it is done in the videos. Something like...\\nfhv_tripdata.pickup_locationid as pickup_locationid,\\nfhv_tripdata.dropoff_locationid as dropoff_locationid,\\netc...\",1645148944.544799,1645175567.498139,U02LQMEAREX\\n017ab6ef-1705-4083-bb0e-9c4edc8e570c,U0343G3CH32,,,yes,1645172408.560079,1645175623.372539,U0343G3CH32\\n3a71f5ba-c6fa-45b8-b8be-09e0bdfe003b,U0343G3CH32,,,\"Got it, can you copy and past the command you used here please? Will help in debugging\",1645172408.560079,1645175670.369089,U02U1GSKX2T\\n450168dc-6510-483f-9625-b28347cecb40,U0343G3CH32,,,\"```docker run -it \\\\\\n  -e POSTGRES_USER=\"\"root\"\" \\\\\\n  -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n  -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n  -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n  -p 5432:5432 \\\\\\n  postgres:13```\\n\",1645172408.560079,1645175722.186809,U0343G3CH32\\n1ac3b57e-5be5-48ad-a353-ad5cc1c50738,,2.0,,\"I know this is not the right channel but... it is the right period to share this here.\\nAccording with this article mostly of the DE are Analytics Engineer:\\n<https://medium.com/coriers/they-quit-a-faang-after-4-months-9b0333df7b4e>\",1645175852.583289,1645175852.583289,U02CD7E30T0\\n710bc6e7-fbde-4e8e-9352-5c6877481f69,U02CD7E30T0,,,\"Hello Seja, no problem :slightly_smiling_face: I Could workaround because it is a simple script. I just wrote this post because someone could be blocked.\",1645010628.987459,1645175944.670019,U02CD7E30T0\\nf50cd623-36de-4e27-bb91-66a522a9acba,U0343G3CH32,,,Hmm this seems good. And when you ran that command you got the same output as the video?,1645172408.560079,1645176444.722149,U02U1GSKX2T\\nf9acf49a-d8a9-4573-8770-c54dda858fe8,U02UA0EEHA8,,,\"Hi!\\nI agree. I have used parket files for both yellow and green, and followed Victoria\\'s code, keeping the `borough\\xa0!=\\xa0\\'Unknown\\'` when using dim_zones table, and got a number corresponding to one of those in the form.\\nIn order to avoid the `ehail_fee` column type problem when using some \"\"green\"\" parket files, you have some choices (I modified Airflow dag and worked, other solutions have been provided by other colleagues but I did not tested them):\\n• Drop ehail_feel column since it is not really used. For instance when creating a partitioned table from the external table in BigQuery\\xa0\\nSELECT * EXCEPT (ehail_fee) FROM…\\xa0\\n• Modify stg_green_tripdata.sql model using this line cast(0 as numeric) as ehail_fee.\\n• Modify Airflow dag to make the conversion and avoid the error.\\xa0\\npv.read_csv(src_file, convert_options=pv.ConvertOptions(column_types = {\\'ehail_fee\\': \\'float64\\'}))\",1645171121.759679,1645176494.094879,U02LQMEAREX\\n8601f05a-e132-4b9e-840c-cc9ed2baf932,U0343G3CH32,,,yes,1645172408.560079,1645176927.085409,U0343G3CH32\\n62ec7bc8-36d8-4520-91a1-a14157437134,U0343G3CH32,,,,1645172408.560079,1645176972.023769,U0343G3CH32\\n8cd23063-63fe-4c60-87f8-6e7e2936040a,U0343G3CH32,,,\"Oh... so I am not sure but it could be because capital `-U` is being used. can you try with lowercase?\\n`pgcli -h localhost -p 5432 -u root -d ny_taxi`\",1645172408.560079,1645177259.983899,U02U1GSKX2T\\n1ff31ec6-cfc0-4976-8f50-24d327077df3,U0343G3CH32,,,\"yeah, tried with small u before... same error\",1645172408.560079,1645178804.992809,U0343G3CH32\\n25c15ad6-8f53-46d3-a550-529c950c7e3e,U0343G3CH32,,,\"Oh.. sorry can\\'t be of much help as I am working on windows so not sure about MacOS. Although I found this <https://stackoverflow.com/questions/16973018/createuser-could-not-connect-to-database-postgres-fatal-role-tom-does-not-e/16974197#16974197|stackoverflow answer>. In this if you head over to the \"\"*Update/Modification (For Mac):\"\"*. This could help.\\n\\nEdit: Also found this <https://www.dbrnd.com/2017/01/postgresql-error-fatal-database-role-root-does-not-exist-in-linux/|website>. Which explains a bit on why this issue might arise. Although it is for Linux, I believe it should also work for MacOS too.\",1645172408.560079,1645179028.483249,U02U1GSKX2T\\ne7a8da14-6293-49eb-a144-1c726786de97,U0343G3CH32,,,thanks for the help satvik.,1645172408.560079,1645179425.136799,U0343G3CH32\\n12358086-c198-4ac2-b06b-0f813299d67d,U02UA0EEHA8,,,\"still, the count \"\"should\"\" be the same :smile:  <@U01B6TH1LRL> how do we deal with it? (sorry for the tag)\",1645171121.759679,1645183588.632929,U02UA0EEHA8\\ndb7ffa3f-39a3-4dd7-b120-c7ca85955f8e,U02CD7E30T0,,,Lol I was lurking on r/dataengineering when the guy posted this,1645175852.583289,1645184873.248369,U02TATJKLHG\\nea54e0c1-1cba-4fd8-8331-fb9c58ec78d6,,,,\":wave: Hello, team!\",,1645186080.135549,U033A2TUN7R\\nd0ed132d-23a7-4141-b767-5fc965bc2ba3,U0343G3CH32,,,\"Hello <@U0343G3CH32>, check if you made a standalone installation of postgres in your system on top of the one running in docker. I had a similar issue and I resolved it by closing the standalone postgres and only having the postgres in docker running.\",1645172408.560079,1645187966.558129,U02SEH4PPQB\\nd42aa320-861e-4f2d-97e3-9f3dbfea450e,,10.0,,\"Something really funky happened to my schemas in BigQuery.\\n\\nMy raw data contains all the trips for yellow and green taxis but when I query the staging schemas generated by dbt, the only contain a handful of days. The staging schema for yellow taxis only contains trips for one day and the schema for green taxis only has 5 days.\\n\\nI will review and rerun my models but I was wondering if someone else has encountered this issue.\",1645188792.791639,1645188792.791639,U02BVP1QTQF\\n7fbf5b71-0988-40bf-8db3-c2317af0a6cb,U02BVP1QTQF,,,\"I am sure you might have checked this but just to confirm, did you run with `--var \\'is_test_run: false` ?\",1645188792.791639,1645188929.147159,U02TATJKLHG\\n5C748473-5681-4BBA-AC3B-078CC10FD2FA,U02BVP1QTQF,,,\"You know what, that\\'s probably it. I am not in front of my computer right now but I will update you as soon as I get back\",1645188792.791639,1645188999.978879,U02BVP1QTQF\\nf83d1507-0723-4562-a0e2-5ebdeea2ed40,U02BVP1QTQF,,,\"Happened to me too, lol.\",1645188792.791639,1645189046.600859,U02TATJKLHG\\n32b608e1-fd6f-4330-9f01-7ccb8cd16b40,U02S9JS3D2R,,,\"61602986.\\n\\nNot matching any options lol. Do we have a general consensus here on what the correct answer should be? :joy:\",1645039735.829189,1645190151.523669,U02TATJKLHG\\na286ffd3-4cc7-413e-a613-78e7f4e26ea2,U02BVP1QTQF,,,Happened to me too but I resolved it in the same way as <@U02TATJKLHG> put it.,1645188792.791639,1645192344.176829,U02SEH4PPQB\\n6371eb07-5f24-48db-87fa-f2568b959d7b,,12.0,,\"```While deploying dbt job to production, it only creates taxi_zone_lookup model. And complaining about rest of the models. Should not it be deploying all models in production dataset as well.  ```\",1645192902.010689,1645192902.010689,U02NSF7RYP4\\ne1e38f89-21e9-48d2-90e4-0380e482774d,U02NSF7RYP4,,,\"This message took 5 scrolls on my 21 inch monitor. Please feel for my brothers on a mobile phone and paste the error logs in the thread. :people_hugging:\\n\\nAlso paste just the relevant logs in a code block.\",1645192902.010689,1645193113.250619,U02TATJKLHG\\n56e6d64a-0f6d-45d9-9fee-28c5d8cc3882,,3.0,,\"Hey, does anyone know why in the first question of homework 4 it mentions filtering for 2019 and 2020? I thought we were only using the data from these years anyway.\",1645193136.004579,1645193136.004579,U02TAA1LDQT\\n45c2128a-2137-4e61-b198-0431691adbaf,U02NSF7RYP4,,,ok bro. did it. :grinning:,1645192902.010689,1645193305.308449,U02NSF7RYP4\\n519c866d-99f9-4480-a5c4-4c3b74d9b212,U02NSF7RYP4,,,\"Thank you! Just one more thing, remove the logs from the message and paste here in the thread, that\\'s the recommended way :smile:\",1645192902.010689,1645193431.936199,U02TATJKLHG\\nd88cf6e8-8175-446f-adce-c90c8796b743,U02NSF7RYP4,,,github link <https://github.com/Master7786/dbt>,1645192902.010689,1645193440.632959,U02NSF7RYP4\\n59a4600f-06ce-4bde-86c1-388986ad634f,U02NSF7RYP4,,,\"```Completed with 11 errors and 0 warnings:\\n13:57:45  \\n13:57:45  Runtime Error in test not_null_dm_monthly_zone_revenue_revenue_monthly_total_amount (models/core/schema.yml)\\n13:57:45    404 Not found: Table data-terraform-338612:production.dm_monthly_zone_revenue was not found in location europe-west6\\n13:57:45    \\n13:57:45    Location: europe-west6\\n13:57:45    Job ID: c0151ff2-3120-4a00-9495-a306a0691a38\\n13:57:45    \\n13:57:45  \\n13:57:45  Runtime Error in test accepted_values_stg_green_tripdata_Payment_type__False___var_payment_type_values_ (models/staging/schema.yml)\\n13:57:45    404 Not found: Table data-terraform-338612:production.stg_green_tripdata was not found in location europe-west6\\n13:57:45    \\n13:57:45    Location: europe-west6\\n13:57:45    Job ID: 5130883a-d23e-41c8-8036-b4fac919326c\\n13:57:45    \\n13:57:45  \\n13:57:45  Runtime Error in test not_null_stg_green_tripdata_tripid (models/staging/schema.yml)\\n13:57:45    404 Not found: Table data-terraform-338612:production.stg_green_tripdata was not found in location europe-west6\\n13:57:45    \\n13:57:45    Location: europe-west6\\n13:57:45    Job ID: 268b70de-477c-419f-9ae6-ab65a1979418\\n13:57:45    \\n13:57:45  \\n13:57:45  Runtime Error in test accepted_values_stg_yellow_tripdata_Payment_type__False___var_payment_type_values_ (models/staging/schema.yml)\\n13:57:45    404 Not found: Table data-terraform-338612:production.stg_yellow_tripdata was not found in location europe-west6\\n13:57:45    \\n13:57:45    Location: europe-west6\\n13:57:45    Job ID: 5bd0b10e-3fdf-4e55-b981-74f0533b0269\\n13:57:45    \\n13:57:45  \\n13:57:45  Runtime Error in test not_null_stg_yellow_tripdata_tripid (models/staging/schema.yml)\\n13:57:45    404 Not found: Table data-terraform-338612:production.stg_yellow_tripdata was not found in location europe-west6\\n13:57:45    \\n13:57:45    Location: europe-west6\\n13:57:45    Job ID: 1f5a6034-6bb1-46d8-aace-d97bdb7276e0\\n13:57:45    \\n13:57:45  \\n13:57:45  Runtime Error in test relationships_stg_green_tripdata_Pickup_locationid__locationid__ref_taxi_zone_lookup_ (models/staging/schema.yml)\\n13:57:45    404 Not found: Table data-terraform-338612:production.stg_green_tripdata was not found in location europe-west6\\n13:57:45    \\n13:57:45    Location: europe-west6\\n13:57:45    Job ID: 08ee5168-7ee4-4924-b6c2-e329d3385fca\\n13:57:45    \\n13:57:45  \\n13:57:45  Runtime Error in test relationships_stg_green_tripdata_dropoff_locationid__locationid__ref_taxi_zone_lookup_ (models/staging/schema.yml)\\n13:57:45    404 Not found: Table data-terraform-338612:production.stg_green_tripdata was not found in location europe-west6\\n13:57:45    \\n13:57:45    Location: europe-west6\\n13:57:45    Job ID: 62ac29b3-6e8a-44d5-9f33-040d95413e48\\n13:57:45    \\n13:57:45  \\n13:57:45  Runtime Error in test unique_stg_green_tripdata_tripid (models/staging/schema.yml)\\n13:57:45    404 Not found: Table data-terraform-338612:production.stg_green_tripdata was not found in location europe-west6\\n13:57:45    \\n13:57:45    Location: europe-west6\\n13:57:45    Job ID: 3e606efe-c446-4c9f-97e3-83459daaf338\\n13:57:45    \\n13:57:45  \\n13:57:45  Runtime Error in test unique_stg_yellow_tripdata_tripid (models/staging/schema.yml)\\n13:57:45    404 Not found: Table data-terraform-338612:production.stg_yellow_tripdata was not found in location europe-west6\\n13:57:45    \\n13:57:45    Location: europe-west6\\n13:57:45    Job ID: fa98cf6e-3770-406e-98a3-2cb4a8db1ebf\\n13:57:45    \\n13:57:45  \\n13:57:45  Runtime Error in test relationships_stg_yellow_tripdata_Pickup_locationid__locationid__ref_taxi_zone_lookup_ (models/staging/schema.yml)\\n13:57:45    404 Not found: Table data-terraform-338612:production.stg_yellow_tripdata was not found in location europe-west6\\n13:57:45    \\n13:57:45    Location: europe-west6\\n13:57:45    Job ID: c73dd776-9e21-42ec-a4da-9e2dd6d0a9f5\\n13:57:45    \\n13:57:45  \\n13:57:45  Runtime Error in test relationships_stg_yellow_tripdata_dropoff_locationid__locationid__ref_taxi_zone_lookup_ (models/staging/schema.yml)\\n13:57:45    404 Not found: Table data-terraform-338612:production.stg_yellow_tripdata was not found in location europe-west6```\\n\",1645192902.010689,1645193471.278269,U02NSF7RYP4\\ne581580f-f5e5-431b-830f-f31d0fd01330,U02NSF7RYP4,,,\"See if this helps\\n\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1644432387132949>\",1645192902.010689,1645193526.434019,U02TATJKLHG\\n195129d2-2f07-464e-b852-944c484bfab1,U02NSF7RYP4,,,Thanks Ankur. let me check,1645192902.010689,1645193554.353949,U02NSF7RYP4\\n7ce85a3e-6651-4f61-b7e5-d6731a718d97,U02TAA1LDQT,,,We\\'ve been using 2019-2020 data throughout,1645193136.004579,1645193601.853239,U02TATJKLHG\\nf5933294-2012-45c3-922c-3d803aeb8403,U02NSF7RYP4,,,i can run all models on staging dataset. problem is while deploying to production.,1645192902.010689,1645193678.840729,U02NSF7RYP4\\n255613d9-b12b-4d9c-9750-fc3beaa4d027,U02TAA1LDQT,,,I know that but why then would we need to filter the statement on these dates?,1645193136.004579,1645193709.080319,U02TAA1LDQT\\nd0fbc9c0-a41f-4d6a-b198-3da92b047e31,U02NSF7RYP4,,,In your `profiles.yml` what location have you provided under the prod target?,1645192902.010689,1645193769.330419,U02TATJKLHG\\n4008983c-9936-407f-b239-0f06954fb1d1,U02TAA1LDQT,,,\"Victoria explained in the data studio video as to why we are doing it. There are outlier records that are going up to ~2080. So we just ignore the \"\"bad\"\" data\",1645193136.004579,1645193922.107539,U02TATJKLHG\\n2a2bed9c-0fdd-4ed7-a310-2fa3e5ab241e,U02NSF7RYP4,,,europe-west6,1645192902.010689,1645193997.877509,U02NSF7RYP4\\n0a965ced-f72e-445f-8662-03cefea1cbc9,U02NSF7RYP4,,,\"From what I\\'ve seen in other threads, there is likely a difference in the location in your gcs bucket, your trips_data_all and production somewhere\",1645192902.010689,1645194532.085659,U02TATJKLHG\\n357ea1b0-89fa-47a6-bf19-5c3b179dd4e7,U02NSF7RYP4,,,ok. let me check,1645192902.010689,1645194587.263359,U02NSF7RYP4\\n8a34cd24-39ed-4d5d-87d3-f6f3fb079244,U0343G3CH32,,,<@U02SEH4PPQB> thank you very much. That worked,1645172408.560079,1645197776.887529,U0343G3CH32\\nc1a7258d-a559-491a-93a7-3f6877f771e5,U0343G3CH32,,,\"Glad to hear so <@U0343G3CH32>, Happy coding!!:blush:\",1645172408.560079,1645198055.592279,U02SEH4PPQB\\n1fd438f0-72f0-42c9-b478-c8de9a32fcce,U0297ANJF6F,,,\"Okay all, I have a resolution, at least to my issue. It was due to the external tables being in eastus-1 and not in US multi-region. I believe this is because we are making a table as opposed to a view so the data actually has to be written from useast-1 to US multi-region which isn\\'t allowed. My resolution was:\",1645138166.157909,1645198256.480829,U02TNEJLC84\\n33a672b6-1487-4029-a281-e9193c67fb9a,U0297ANJF6F,,,\"1. Create new bucket that is US multi-region\\n2. Create three transfer jobs to move the fhv, green and yellow taxi data to the new bucket\\n3. Create a new US multi-region dataset in BigQuery\\n4. Run three queries to create the new external tables using the new US multi-region bucket\\n5. Update dbt cloud schema.yml to point to the new data set\\n6. Reran the dbt run command and BAM! it worked\\nIf I have time tonight I\\'ll try to throw together a quick video walk through of this.\",1645138166.157909,1645198294.872709,U02TNEJLC84\\n47cfcb5e-04e4-40a2-af6c-999efd1905fd,,7.0,,\"Just putting this in the main chat in case anyone else runs into and issue when running the fact_trips step in dbt.\\n```Database Error in model fact_trips (models/core/fact_trips.sql)\\n  Access Denied: BigQuery BigQuery: Permission denied while globbing file pattern.\\n  compiled SQL at target/run/ny_taxi_rides/models/core/fact_trips.sql```\\nI see a lot of answers floating around that this is a permissions issue which it may be for some people, but for me it had to do with my bucket being in a specific region rather than multi region. I tested this by running the code generated to the target directory in dbt in BigQuery itself and got\\n```Cannot process data across locations: us-east1,US```\\nSteps to resolve this can be found in this thread: <https://datatalks-club.slack.com/archives/C01FABYF2RG/p1645138166157909> Hopefully this saves some peeps some time.\\n\\nUpdate: Video walk through for how to resolve. <https://youtu.be/kL3ZVNL9Y4A>\",1645198942.177789,1645198942.177789,U02TNEJLC84\\n08977949-fa76-4def-93d8-c4b47d3664d0,,3.0,,\"hello,\\nI was able to run the example model model successfully and the tables/vies were created in my dataset.\\nBut I get the below error when I run the commands \"\"dbt run\"\" and \"\"dbt test\"\"\\n\\nNote: \"\"dbt debug\"\" completed successfully\\n\\n16:27:39  Running with dbt=1.0.1\\n16:27:39  Unable to do partial parsing because profile has changed\\n16:27:39  Unable to do partial parsing because a project dependency has been added\\n16:27:40  Encountered an error:\\nCompilation Error in test accepted_values_stg_green_trip_data_Payment_type__False___var_payment_type_values_ (models/staging/schema.yml)\\n  \\'NoneType\\' object is not iterable\\n\\n  &gt; in macro test_accepted_values (tests/generic/builtin.sql)\\n  &gt; called by macro default__test_accepted_values (macros/generic_test_sql/accepted_values.sql)\\n  &gt; called by test accepted_values_stg_green_trip_data_Payment_type__False___var_payment_type_values_ (models/staging/schema.yml)\",1645202465.420209,1645202465.420209,U02V90BSU1Y\\n2fa1d8d9-7892-457a-8454-c7f14a48c18a,U02V90BSU1Y,,,Thank you.,1645088736.844399,1645202522.119609,U02V90BSU1Y\\nb63e0f2b-3735-4893-ad82-062cd87eb248,U033PKGGFS7,,,Thanks. I meant where do I get started :sweat_smile:,1645171598.201219,1645203106.516189,U033PKGGFS7\\n3080a713-52df-4d0b-ab97-13728e6a322f,U02BVP1QTQF,,,\"It was indeed the `is_test_run` var, I had totallly forgotten about it… :sweat_smile:\\n\\nI’m facing a new issue, though. I added the `dm_monthly_zone_revenue.sql` model file to my core folder and modified `schema.yml`  to include it (as a matter of fact, my dbt folder is pretty much a copy of the repo). I modified the `date_trunc` line as stated in the comment in order to have BigQuery accept it, but when I try running the model, I get the following error: `A valid date part name is required` . I’ve been trying to find a solution online to no avail.\",1645188792.791639,1645203188.287409,U02BVP1QTQF\\n1d855638-8367-480c-a5b5-e5963f305598,U02BVP1QTQF,,,\"Is it written this way?\\n\\n`date_trunc(pickup_datetime, month) as revenue_month`\",1645188792.791639,1645203312.897729,U02TATJKLHG\\ncd9ae593-4089-439d-9562-9e4415cbfabb,U02V90BSU1Y,,,\"Do you have this in your `dbt_project.yml` file?\\n\\n```vars:\\n  payment_type_values: [1,2,3,4,5,6]```\",1645202465.420209,1645203751.101959,U02TATJKLHG\\ncaced8bf-3df7-49db-b589-545ce46aa4c2,U02BVP1QTQF,,,\"…I’m such an idiot. I wrote `date_trunc(pickup_datetime, \\'month\\') as revenue_month` with the quotes surrounding “month”.\\n\\nThank you!\",1645188792.791639,1645203985.308409,U02BVP1QTQF\\n6d614934-56d4-4738-9937-168a083c13ca,U02BVP1QTQF,,,Haha I think the course is getting the better of us. 4 weeks of grind and I be like :male_zombie:,1645188792.791639,1645204079.647419,U02TATJKLHG\\nfd961814-a305-40be-aa19-1861a572069a,U02BVP1QTQF,,,\"You bet, my brain is toast lol\",1645188792.791639,1645204176.144839,U02BVP1QTQF\\nb04c2e60-d6b1-4e71-a292-2030b20709cc,,16.0,,\"Hello everyone. As someone that is starting the course and going slowly, does anyone know what the expenses are for the duration of the course? I mainly ask because I\\'ve used GCP in the past so I don\\'t have the free trial active anymore and would like to have an idea. Can we go through the course with GCP free tier limits, or is it expected that we\\'ll go beyond those limits?\",1645210766.400019,1645210766.400019,U031HNNSW3A\\neca14317-030b-49ca-a2cc-a812875d99f5,U02TJ69RKT5,,,\"That happens in whatever model I run. I deleted this project and create a new one and for now, is running.  I think it is because I use a south American location, but really I didn\\'t find anything about this error. :man-facepalming:\",1644968876.461069,1645210891.200609,U02TJ69RKT5\\n841893d6-2ac6-4be1-b033-06ae1f89dc6c,U031HNNSW3A,,,Welcome <@U031HNNSW3A> just create a second google account. You can use the same phone number and still get the free credits. I was in the same situation.,1645210766.400019,1645211055.168819,U02TNEJLC84\\n05cfc393-c3c8-4c72-a429-800e28e714d0,U031HNNSW3A,,,\"great to hear! Thanks I will try it, I thought it would be limited by credit card number or something like that\",1645210766.400019,1645211102.206139,U031HNNSW3A\\n1fa02313-065a-4e34-b457-8ecc2d23be71,U031HNNSW3A,,,\"Yeah, that would make sense and I thought the same, but it was pretty straight forward and saw the credits right at the top when I logged into the new account. Hit me up if you have any issues setting that up.\",1645210766.400019,1645211188.211929,U02TNEJLC84\\n2efeb1dd-0d31-4963-bcd6-728aec4f4e40,U031HNNSW3A,,,\"I finished 4 weeks of course work and so far I have spent only 6 euros from my free credits. Even if you don\\'t get an account, I think it is affordable.\",1645210766.400019,1645211196.989639,U02UVKAAN2H\\na9bb3f30-48b6-45a3-907c-93450fd7c060,U031HNNSW3A,,,thank you I will give it a shot! I also thought that it\\'d be okay as long as it is affordable and for a course I figured the free tier wouldn\\'t be exceeded by much if at all. I am always paraniod though that I leave something running and it charges me some unexpected ammount,1645210766.400019,1645211311.996949,U031HNNSW3A\\n5550da67-d85e-4bf5-98a4-6b730e580ba0,U031HNNSW3A,,,<@U02TNEJLC84> So I just tried it and it seems it won\\'t let me. Says they couldn\\'t do it and to contact support,1645210766.400019,1645212026.931579,U031HNNSW3A\\nd36e646a-e271-4ad1-af2b-40421d0c2ed8,U031HNNSW3A,,,\"Not sure if it\\'s important but if you want to use the same bank card as previously, you need first to remove it from the previous account. I did that and managed to get another free trial just one week ago\",1645210766.400019,1645213090.114209,U01AXE0P5M3\\ne533758f-ff1f-477e-a9b0-3404de8c2891,U031HNNSW3A,,,oh that\\'s good advice. So you can remove billing from your original account no problem?,1645210766.400019,1645213135.221439,U031HNNSW3A\\n06334dfd-be36-4eb3-a2b8-e4bcb4db3afe,U031HNNSW3A,,,\"I did that, yes\",1645210766.400019,1645213167.511229,U01AXE0P5M3\\ne4b220ed-9880-4420-992f-21c04b901b14,U031HNNSW3A,,,\"awesome thanks, I will try\",1645210766.400019,1645213179.700629,U031HNNSW3A\\n95f2aad7-219b-41ac-984c-101916f64a51,U031HNNSW3A,,,\"<@U01AXE0P5M3> can I ask how you did it? as in, did you replace it with another payment method in your original account? it won\\'t let me remove it unless I supply a different payment method\",1645210766.400019,1645213606.415669,U031HNNSW3A\\n5DEAE3D4-2F32-482E-985A-A9212530148A,,23.0,,I\\'m a bit behind. Is it possible to get through the DBT stuff this weekend or would it take me longer?,1645213874.310119,1645213874.310119,U02U34YJ8C8\\n8a8d7884-3e44-4c57-8508-2882ff470910,U031HNNSW3A,,,\"It did let me remove it, but I guess I didn\\'t have anything active in my previous account\",1645210766.400019,1645213949.843029,U01AXE0P5M3\\n703cae4e-932c-430b-9289-58227599e45a,U02U34YJ8C8,,,\"I think it\\'s doable, but I ran into SO many problems with dbt it\\'s not even funny. Just a heads up. This slack channel will be your friend for troubleshooting.\",1645213874.310119,1645214270.103549,U02TNEJLC84\\nad0e83fa-64b1-479c-91c3-9e223c06af93,U0297ANJF6F,,,\"Thanks guys,  I just came back from university, I will try all the suggested solutions tonight.\\nAlmost I did same, created the `all_taxi_trips` dataset in US region because  dbt-cloud is automatically created the dataset in the US region(I wasn\\'t all able to find any configuration in dbt-cloud to create a dataset in desired region).\\nI also think it\\'s require  some additional permission from big query.\",1645138166.157909,1645214368.650159,U0297ANJF6F\\nd83093ea-6351-41b7-b902-ae523d6163cd,U031HNNSW3A,,,Mine both appear to be the same credit card. Sign out of EVERYTHING google. Then go to gmail and create an account. Use your phone number like usual and then claim the GCP credit.,1645210766.400019,1645214428.121359,U02TNEJLC84\\na1b53839-cbee-4c73-b436-795ef0b4d861,U031HNNSW3A,,,<https://cloud.google.com/free>,1645210766.400019,1645214486.436699,U02TNEJLC84\\nf9fd4a7f-bb47-4925-a962-a8c57011689c,U02U34YJ8C8,,,\"If you don\\'t let yourself stuck to homework Q1, then it is doable over the weekend. I spent way too much time to solve the issue I had with that question, and still couldn\\'t find an explanation.\",1645213874.310119,1645214570.727869,U02UVKAAN2H\\nc46c7b44-1284-4dda-91fd-38294d753497,U02U34YJ8C8,,,\"Hi <@U02U34YJ8C8>, <@U02TNEJLC84> is right in that it is do-able. I too am running into many problems - which is relating specifically to location. Maybe if you have your BigQuery in a location like the US or EU then maybe it\\'s not such a huge problem. For me I\\'m going to have to find a work-around for this before I can move forward.\",1645213874.310119,1645217563.639529,U02U5SW982W\\nfd707c4a-d4fc-4971-9ba4-250ff0c7d5e5,U02U34YJ8C8,,,<@U02U5SW982W> Hope this helps if you haven\\'t seen it yet. <https://datatalks-club.slack.com/archives/C01FABYF2RG/p1645198942177789>,1645213874.310119,1645218197.234469,U02TNEJLC84\\n24e55358-2229-45b3-aaa3-b8ce17bba2bf,U0297ANJF6F,,,\"`Run three queries to create the new external tables using the new US multi-region bucket`\\nFor this step above , easy way is to create a Transfer service , That can transfer data from one GCP bucket to another GCP bucket.\",1645138166.157909,1645218545.457869,U0297ANJF6F\\n139f49e9-a871-432c-90dd-fd4129ff14c7,U02U34YJ8C8,,,We are on the same boat <@U02U34YJ8C8> ! I think we can do it! <@U02TNEJLC84> I\\'m glad there is lot of issue as that is the only way for me to learn! :D,1645213874.310119,1645218979.546989,U02UX664K5E\\nb3229d73-8d5e-4911-be95-2ea95ca879f0,U02U34YJ8C8,,,Hi <@U02TNEJLC84> I did see that. Did you have to create and copy your dataset in BigQuery to get around the problem? Because of course you can\\'t change the region once you have created it. Out of curiosity what multi-region did you set your database up in?,1645213874.310119,1645219608.851449,U02U5SW982W\\nfc482aa2-cfef-4a1e-bed8-ebdc9ae5b57a,U0297ANJF6F,,,\"I tried all 6 steps, but getting same error.\\nI don\\'t understand  why it is creating `dim_zones` `taxi_zone_lookup`  `stg_green_tripdata` and `stg_yellow_tripdata`\\nOnly error is for the fact_trips model  and when also when i run dbt test\",1645138166.157909,1645219761.646109,U0297ANJF6F\\n0fd1d2af-d285-4d6b-8b79-c2f681b44615,U02U34YJ8C8,,,<@U02U5SW982W> US (multi-region). Created new bucket in correct region. Moved data there and then created new dataset/tables in big query.,1645213874.310119,1645220932.138539,U02TNEJLC84\\n477fad0f-25da-4bcd-b7bc-13ce49b41669,U02U34YJ8C8,,,Yep - that\\'s exactly what I\\'m doing right now. Thanks for that - fingers crossed it will work.,1645213874.310119,1645222255.760759,U02U5SW982W\\nebf7bab0-8e4a-4fad-a4b5-fb5a69e17315,U02U34YJ8C8,,,Just wondering if you had any major dramas with dbt after you sorted that issue out?,1645213874.310119,1645222298.283029,U02U5SW982W\\n4a9c2d5f-74e8-4757-889d-14a7e195b24c,U02TNEJLC84,,,\"I think I figured out this  problem. When I create a model with `{{\\xa0config(materialized=\\'view\\')\\xa0}}`  all the models are being created. I just changed the `fact_trips.sql` configuration to the view , everything worked.\\nBut when  I tried to create the materialized table for all the models , I got same error (`Permission denied while globbing file pattern.`) for all models\",1645198942.177789,1645222642.067599,U0297ANJF6F\\ne76dd7db-621d-4519-aa8a-2ca62821f7de,,9.0,,\"Hello guys, i have a problem building the dbt project. can someone help me?\\n```Server error: RPC server failed to compile project, call the \"\"status\"\" method for compile status: Compilation Error in model stg_green_tripdata (models/staging/stg_green_tripdata.sql)\\nModel \\'model.taxi_rides_ny.stg_green_tripdata\\' (models/staging/stg_green_tripdata.sql) depends on a source named \\'staging.green_tripdata\\' which was not found```\",1645223234.222399,1645223234.222399,U02RA8F3LQY\\nf982683e-0161-491b-b28c-651a94501a90,U02RA8F3LQY,,,,1645223234.222399,1645223284.208709,U02RA8F3LQY\\nd798a66c-649d-46af-b113-6e2937d7ae29,U02RA8F3LQY,,,\"Hi Fikri, I had the exact same problem as outlined in my <https://learningdataengineering540969211.wordpress.com/|blog> - it wasn\\'t something silly like not calling the \\'sources\\' sources in your yml file was it?\",1645223234.222399,1645224027.886799,U02U5SW982W\\n0cfdd2f0-ccee-406e-8ba0-1701da6f81c8,U02RA8F3LQY,,,\"Ohh i see, thanks for the help Sandy it solved!\",1645223234.222399,1645226139.416809,U02RA8F3LQY\\n7ab6341b-925a-4369-a109-fec0eb8b3f66,U02RA8F3LQY,,,No worries <@U02RA8F3LQY> I\\'m hoping you can avoid the location issue with dbt...,1645223234.222399,1645226400.056749,U02U5SW982W\\nd7b995bd-22ba-42ac-9125-80e5a1e4fe70,U02U34YJ8C8,,,Hey <@U02U5SW982W> haven\\'t made it much further. If you\\'re still struggling with the glob issue I hope this helps. <https://youtu.be/kL3ZVNL9Y4A>,1645213874.310119,1645229416.916709,U02TNEJLC84\\n627bb942-68b1-475b-bf2b-18efadf46182,U0297ANJF6F,,,<@U02QW460CTH> Cause the parquet files that drive the external tables aren\\'t in the region. I threw this together real quick. Hope it helps. <https://youtu.be/kL3ZVNL9Y4A>,1645138166.157909,1645229486.942559,U02TNEJLC84\\n88a3ec60-0410-467b-861b-7165dcddbc03,U02U34YJ8C8,,,Awesome - thanks for that <@U02TNEJLC84> - I\\'m still working on copying everything over. I\\'m feeling a bit like the reverse of the Midas touch at the moment - where everything turns to sh&amp;^ :wink:,1645213874.310119,1645229536.399489,U02U5SW982W\\na2f6d3b0-f300-49a2-964d-019014987c88,U02U34YJ8C8,,,You are not alone friend. Sums up my experience with dbt so far. <https://tenor.com/view/sideshow-bob-rakes-simpsons-gif-4149558>,1645213874.310119,1645229680.177859,U02TNEJLC84\\nb6d6ef48-3aae-4b0c-be49-0bd45543d678,U02U34YJ8C8,,,:rolling_on_the_floor_laughing:,1645213874.310119,1645230771.753369,U02U5SW982W\\ndb32c195-0f93-47aa-aef7-10881e7411e4,U02U34YJ8C8,,,Boy oh boy did I need that,1645213874.310119,1645230780.460159,U02U5SW982W\\ne148e0e2-ee47-4940-b085-999cf48cb1ee,U02U34YJ8C8,,,H... that may have come out wrong. I did not a whack in the head with a rake lol...  but rather the laugh at the misfortunes of a fictional character,1645213874.310119,1645231610.026429,U02U5SW982W\\nec7d7577-98c0-427b-bc70-d32d6dfcbd1b,U02RA8F3LQY,,,did you solve the location issue?,1645223234.222399,1645231890.451289,U02RA8F3LQY\\n696099bf-8cfa-4b65-8a53-4a2526160a89,U02U34YJ8C8,,,I gotcha. Sometimes I feel like I need both. :slightly_smiling_face: Good luck. You got this!!!,1645213874.310119,1645232159.906969,U02TNEJLC84\\nc3ecd140-3f73-4a19-8690-ca5061fc7c1b,U02U34YJ8C8,,,Absolutely me too. And thank you. You too :).,1645213874.310119,1645232884.681659,U02U5SW982W\\n3e223817-b265-469a-9862-b5d245fd5a91,U02RA8F3LQY,,,Still going <@U02RA8F3LQY>... copying everything over in my GCP. You have any issues?,1645223234.222399,1645234085.409229,U02U5SW982W\\n64d00b30-4948-410d-ab61-3920ab2e68d7,U02RA8F3LQY,,,\"yeah just the same as your issue on your blog, i tried to create new dataset \"\"trips_data_all\"\" and set location to US still not working\",1645223234.222399,1645235702.002949,U02RA8F3LQY\\n3c239670-5484-4f2b-9d44-a11f94709770,U02RA8F3LQY,,,Just updated my blog <@U02RA8F3LQY>,1645223234.222399,1645235800.255489,U02U5SW982W\\n6bafe1fe-49fa-454e-928f-8a20d36f7822,U02RA8F3LQY,,,Did you get the copy to work okay in GCP?,1645223234.222399,1645235855.023439,U02U5SW982W\\nb3031c3b-538f-4ff7-ab21-92f2b712e563,,10.0,,\"hello everyone I still confused about question number 1 at homework week 4\\nthe cluse said\\n\\n&gt; You\\'ll need to have completed the \"\"Build the first dbt models\"\" video and have been able to run the models via the CLI. You should find the views and models for querying in your DWH.\\nso that mean we need do transform green and yellow data trip for same as on the video ? and see the results from dbt console ?? OR only see result from video \"\"Build the first dbt models\"\" ??\\n\\nthanks\",1645237507.252609,1645237507.252609,U02SQQYTR7U\\n48242056-13bf-4ee3-9fb4-77f3bca8a23f,U02S9JS3D2R,,,61602985 :joy:,1645039735.829189,1645237713.897729,U026040637Z\\n3930be64-bab8-4734-a492-3b2cf068d847,U02SQQYTR7U,,,Hi <@U02SQQYTR7U> have you actually gone through and done the videos yet? The DE Zoomcamp 4.5.1?,1645237507.252609,1645240073.378349,U02U5SW982W\\na04c5577-5d34-41b8-afbf-cfc61126094d,U02SQQYTR7U,,,yes I did it,1645237507.252609,1645240107.262509,U02SQQYTR7U\\nb6a3d460-f817-45fd-ac45-0267515615ca,U02SQQYTR7U,,,but I can\\'t see the answer from the video,1645237507.252609,1645240129.613739,U02SQQYTR7U\\n9dd43034-e65e-4bf7-8bc9-6526e2a850e2,U02SQQYTR7U,,,So you have a model called fact_trips in your GCP datawarehouse?,1645237507.252609,1645240260.040859,U02U5SW982W\\n9ec90a15-1ee2-46b7-9445-5b194f26916f,U02SQQYTR7U,,,\"owh sorry I think I missed\\nyou mean do same thing on this video \"\"Build the first dbt models\"\" on our DBT  and GCP datawarehouse ?\",1645237507.252609,1645240441.479609,U02SQQYTR7U\\n4b274eab-2c0b-4791-90a4-36a593859af0,U02SQQYTR7U,,,Absolutely <@U02SQQYTR7U>. At least that is my understanding.,1645237507.252609,1645240497.059509,U02U5SW982W\\n285d3c66-1075-4516-8180-caaeda141800,U02SQQYTR7U,,,And that is what I\\'m doing at the moment. Although it is more of a drama than I would like. I\\'m busy trying to overcome various issues but hopefully all will come well soon.,1645237507.252609,1645240579.514219,U02U5SW982W\\n7c48d8ab-794b-49a5-ac37-9bab0d245572,U02SQQYTR7U,,,\"ok thanks <@U02U5SW982W>\\nactually I still set the DBT\",1645237507.252609,1645240639.993489,U02SQQYTR7U\\n3739df21-831c-4680-b079-1093b496b7bd,U02SQQYTR7U,,,:thumbsup:,1645237507.252609,1645240681.050309,U02U5SW982W\\n5d1ad015-3bcf-462a-acdd-e857e4c0e170,U02SQQYTR7U,,,thank you,1645237507.252609,1645240692.728609,U02SQQYTR7U\\na193c483-f541-44a8-b6c3-f7aa745a2727,,2.0,,\"This is the state of docker but 0.0.0.0:8080 is not loading,(just started week 2)\",1645242703.658159,1645242703.658159,U02RH0V5K33\\na6a2c818-bb6d-4bf9-983e-e74ed9f2cb23,,3.0,,\"Hello. In week 4, when I created the seeds/taxi_zone_lookup.csv and ran \"\"dbt seed\"\", dbt didn\\'t recognize it as a seed file (I\\'m working with dbt cloud).\\nI tried in my dbt_project.yml adding to seed configuration the attribute \"\"+enabled: true\"\" but still didn\\'t work. Every time that I ran \"\"dbt seed\"\" or \"\"dbt seed --full-refresh\"\" I obtained this:\",1645242815.271579,1645242815.271579,U02TJ69RKT5\\nebcd1ab9-93c0-4381-b9c8-591fd5ab7a8b,U02RH0V5K33,,,Try localhost:8080,1645242703.658159,1645242990.541739,U02HFP7UTFB\\n72d47fc0-c410-4937-b06e-86dca3e39af9,U02RH0V5K33,,,thanks,1645242703.658159,1645243323.122579,U02RH0V5K33\\n5c30f056-b61e-419b-8c45-da24794cbee3,,1.0,,\"[week 2] I am getting this error in DAG, and I have the ingest_script.py file, what is wrong? How do I repair this DAG\\n```Broken DAG: [/opt/airflow/dags/dag_local/data_ingestion_local.py] Traceback (most recent call last):\\n  File \"\"&lt;frozen importlib._bootstrap&gt;\"\", line 219, in _call_with_frames_removed\\n  File \"\"/opt/airflow/dags/dag_local/data_ingestion_local.py\"\", line 10, in &lt;module&gt;\\n    from ingest_script import ingest_callable\\nModuleNotFoundError: No module named \\'ingest_script\\'```\",1645243694.371979,1645243694.371979,U02RH0V5K33\\nef5c6e01-bcea-42e4-9a56-4559a44926bf,U02TNEJLC84,,,\"Thanks a lot! \\n\\nCan you please create a PR with a link to this  video?\",1645198942.177789,1645256182.843199,U01AXE0P5M3\\n8a940b09-58a4-4bfc-a72e-caf6088631b1,U02C1H2P3HQ,,,\"You may be connected to the wrong database. This can happen if the data is not properly persisted.\\n\\nAlexey created multiple postgres database containers but because they are persisted to the same directory, the data is always there.\\n\\nCheck the running containers and be sure you are connected to the right one.\",1644133063.305329,1644134726.535689,U02V90BSU1Y\\n882e0437-bbe7-49c3-90ee-af291906f358,U02V90BSU1Y,,,OK will try that now.,1644103450.874419,1644134795.596469,U02V90BSU1Y\\ne526136c-d851-47ce-89a3-d650313802dd,U02C1H2P3HQ,,,i saw that you\\'re opening postgres database not ny_taxi,1644133063.305329,1644134925.317569,U02RA8F3LQY\\ne559b161-3ee5-476a-8a78-bbcb15172f25,U02T9550LTU,,,\"Thank you everyone for your help and suggestions! :raised_hands: By changing catchup=True _*and also*_ deleting all dag runs, i noticed that the start date in the top right changed properly and it worked!\\n\\nI thought i was gonna take the L on this homework, but I should be able to complete it now thanks to your help :raised_hands:\",1644047251.049729,1644135657.137319,U02T9550LTU\\na89dca54-1a67-466c-b5cc-ab54ec57764b,,8.0,,Good day all. My free trial on GCP has expired. I\\'ve been trying to create another account and start my setup all over. However it seems I can\\'t use the same debit card for billing on different accounts. Has anyone experienced this? How did you resolve this?,1644136107.540759,1644136107.540759,U02SZARNXUG\\nf68144da-787e-4213-a996-ebcc43c88419,U02TC704A3F,,,Hmmm... <@U02TC704A3F> is someone showing off? :stuck_out_tongue_winking_eye:,1644097257.301339,1644136350.879849,U02U5SW982W\\n7f8f72c8-22e1-4e93-9f2c-b078d6123de4,U02TC704A3F,,,But hey congratulations :clap:it must be nice to have finished :grinning:,1644097257.301339,1644136395.690729,U02U5SW982W\\n58d61c6a-d2b1-44ae-963e-05d32f2484c0,U02V90BSU1Y,,,Thank You <@U02T8HEJ1AS> This worked for me.,1644103450.874419,1644136492.925539,U02V90BSU1Y\\n12650cbc-22c0-4531-a3c1-9858b9620431,U02C1H2P3HQ,,,\"Yes, This is true\",1644133063.305329,1644136556.027849,U02V90BSU1Y\\nb1c5e858-d672-4b55-8ce9-f3b7d11d75ea,U02SZARNXUG,,,\"We\\'re almost in the same situation, ours expires in 1 week. I was hoping that we could use the same bank card... I guess a virtual card could be an option?\",1644136107.540759,1644137585.264029,U01AXE0P5M3\\n6d46121c-5138-45b4-8100-d9d3f3a11545,,4.0,,\"I get errors when I  run \\'terraform apply\\'\\n\\nAlso, I noticed <@U01DHB2HS3X> in video 2.31 (Airflow enviroment with docker ) put the google_credentials.json files in .google_credentials folder and  while <@U01AXE0P5M3> put same credentials in a .gc folder in video (Setting up enviroment for google cloud)\\n\\nNote: I am working on a compute instance and which of these should I use\\nalso, nothing happens when I run \\'docker-compose build\\' from airflow directory.\\n\\nPlease help\",1644137598.640829,1644137598.640829,U02V90BSU1Y\\n2f57e271-3f5f-42b6-bea7-edee992adcf0,U02V90BSU1Y,,,You should follow what Sejal did to be able to use airflow in week 2,1644137598.640829,1644137661.526489,U01AXE0P5M3\\n6e4f2db4-df5f-4cab-ac73-805d8ef1ff9b,U02V90BSU1Y,,,Please note that I am working on VM and already set the file to .gc file,1644137598.640829,1644138131.993619,U02V90BSU1Y\\n16e9484e-9164-4590-8ec8-853b83f307bb,U02SZARNXUG,,,\"I guess so, too. I\\'m thinking of using my brother\\'s card but I I want that to be the last resort. Hoping there would be another walk around.\",1644136107.540759,1644138295.055599,U02SZARNXUG\\n7c7db3c0-6626-4220-933b-9181ec1a22ab,U02SZARNXUG,,,I\\'ve tried using another card..same error as before,1644136107.540759,1644139175.890129,U02SZARNXUG\\nd38ed5cc-e4aa-4a5e-ab15-98ff313c0097,,16.0,,\"docker-compose not working,  it does not do anything\\n\\nI have downloaded the official docker set up file from the below URL\\n\"\"curl -LfO \\'<https://airflow.apache.org/docs/apache-airflow/stable/docker-compose.yaml>\"\"\",1644140999.329219,1644140999.329219,U02V90BSU1Y\\ned24db1d-798c-4506-a3e0-8525cd771302,U02UKLHDWMQ,,,Yes docker is running,1644110549.437789,1644141246.612859,U02UKLHDWMQ\\n14bb842f-2e47-4617-a0ac-ff16985bc45c,U02UKLHDWMQ,,,Using a VM,1644110549.437789,1644141267.702669,U02UKLHDWMQ\\nb8fabe08-cd06-4b80-bcd8-67d93cc90885,U02V90BSU1Y,,,You can move it to another location,1644137598.640829,1644142492.443749,U01AXE0P5M3\\nbb68785f-b448-4a1d-8afe-f2646b83034d,U030FNZC26L,,,\"Ok. I\\'d appreciate if you have a free moment to leave a comment, especially regarding the user in the container, because I am curious how it should be solved according to the state-of-the-art :+1::skin-tone-3:\",1643646196.837139,1644145381.113909,U02UKBMGJCR\\ne714315b-07c2-4b31-85b3-2ac9615bf836,U030FNZC26L,,,I have no idea to be honest. I followed a different approach and it seems to be working fine,1643646196.837139,1644145489.753569,U01AXE0P5M3\\n09a61b62-0345-4608-884e-5f66d21b3baf,U02V90BSU1Y,,,That\\'s weird. Maybe try removing all local docker images and try building again?,1644140999.329219,1644145613.436319,U01AXE0P5M3\\nd666f631-bd04-4924-818e-b7392f0a68e1,U030FNZC26L,,,\"Hi <@U02UKBMGJCR>! Woah, this has been well analyzed &amp; detailed. :clap: But it seems like it comes down to choosing between either AIRFLOW_UID or the root user, right? Also, it is still strange though why the nofrills setup worked on my MacOS.\",1643646196.837139,1644146017.640929,U01DHB2HS3X\\ne335c586-f743-42ae-95fb-1760a71f1aeb,U02U5SW982W,,,\"this command supposedly clears everything:\\n```docker system prune --volumes```\\n\",1644056241.427939,1644146230.160669,U02GVGA5F9Q\\n2b6e6485-ed5a-4037-9fbb-674df6389a1c,U02V90BSU1Y,,,\"I am running from a fresh VM set up by following the (setting up enviroment for google cloud) video.\\n\\nThere is no local image of airflow on the VM.\\n\\nscreenshot below shows the Images I have\",1644140999.329219,1644146466.261909,U02V90BSU1Y\\n4acbc7ea-daeb-48cf-b990-70164cfc6e4a,U02SZARNXUG,,,If you use another card with another gmail account it should work. At least I thought it would... I\\'ll probably try that when my account expires. My only fear is that all my virtual credit cards use my name.,1644136107.540759,1644146629.872759,U02GVGA5F9Q\\n12f656d6-facc-4c95-82e9-e97e3dcd05d3,U02V90BSU1Y,,,Having same issue,1644140999.329219,1644146644.720249,U02UKLHDWMQ\\n4f544880-0c6d-4e78-97e1-88f18bfc29f7,U02BRPZKV6J,,,Thanks <@U02S2TZRBL7> and <@U02UX664K5E>! It\\'s very helpful!,1643929919.420949,1644146697.105549,U02BRPZKV6J\\n7a104e35-2f9e-4cf1-88de-2704cc66e851,U02V90BSU1Y,,,Mine the first day I tried it. Now it\\'s no longer coming up and am using a VM.,1644140999.329219,1644146718.282909,U02UKLHDWMQ\\ne19b860a-4b96-48e3-a4bd-91ddedb580fb,U02BRPZKV6J,,,\"My SQL Query ended up like this because the data in my Google Cloud Storage is in Parquet after week 2 DAG Transformation.\\n```CREATE\\xa0OR\\xa0REPLACE\\xa0EXTERNAL\\xa0TABLE\\xa0`dtc-de-338802.yellow_taxi_trip.external_yellow_tripdata`\\nOPTIONS\\xa0(\\n\\xa0\\xa0format\\xa0=\\xa0\\'parquet\\',\\n\\xa0\\xa0uris\\xa0=\\xa0[\\'<gs://dtc_data_lake_dtc-de-338802/raw/yellow_tripdata_2019-*.parquet>\\',\\xa0\\'<gs://dtc_data_lake_dtc-de-338802/raw/yellow_tripdata_2020-*.parquet>\\']\\n);```\\n\",1643929919.420949,1644146777.105529,U02BRPZKV6J\\n1babd887-f77e-49a2-9390-6bac36f0789e,U02V90BSU1Y,,,Don\\'t you have to change the downloaded docker compose yaml file?,1644140999.329219,1644146785.979229,U02GVGA5F9Q\\n4a80a4d3-1e92-41f5-9541-15b2f8b2e2d7,U02V90BSU1Y,,,\"does the docker-compose work at all? I mean for other projects. for example for this:\\n<https://docs.docker.com/compose/gettingstarted/>\",1644140999.329219,1644146889.408909,U030FNZC26L\\nbcaf559a-17fa-46e7-894f-f5f3e48dc33b,U02V90BSU1Y,,,is this a v2 docker compose?,1644140999.329219,1644146958.008839,U02GVGA5F9Q\\n62e81ddc-a369-406d-a32a-532be25542e7,U02R2PU9NLD,,,\"I already submitted a while ago, do I need to do it again?\",1644084893.400949,1644147058.736319,U02UMV78PL0\\n85d2ee22-9038-4837-956d-abf2c2c1d4bd,U02RZEKRBHD,,,why am i gettting this error? Can someone help with that,1643230715.425600,1644147068.824199,U031FT3K2AZ\\n44b184a9-432e-4cdd-bcc5-a9310bba3d99,U02V90BSU1Y,,,the command with v2 is `docker compose CMD` . I\\'m not sure `docker-compose` works here.,1644140999.329219,1644147093.727609,U02GVGA5F9Q\\nb891af2a-1b47-4b87-9382-bf86542d39e6,U02V90BSU1Y,,,\"I think it works only with docker desktop? when on linux you install compose separately, docker doesn\\'t know what to do with this \\'compose\\' thing\",1644140999.329219,1644147204.541849,U01AXE0P5M3\\n4913d3aa-2f45-415e-9c27-e2b228947f52,U02V90BSU1Y,,,I\\'m on Linux and there\\'s no docker desktop for Linux. We install the engine and everything works from the command line. There are some GUI apps to mimic the desktop client.,1644140999.329219,1644147300.484529,U02GVGA5F9Q\\n68bc7792-1cf1-45f9-980d-d6ca3e27292a,U02R2PU9NLD,,,would be nice - but I can\\'t update it yet. I\\'ll write here when I do,1644084893.400949,1644147329.843899,U01AXE0P5M3\\n0106c9a9-8c2f-46b4-b396-931b178acff8,U02V90BSU1Y,,,What\\'s the result of `docker compose version` ?,1644140999.329219,1644149206.448879,U02GVGA5F9Q\\n2f692ac4-93dc-4409-925d-48c4376a3841,U030FNZC26L,,,\"Another thing is, Docker doesn’t seem to be a good choice for setting up local Airflow env, because (as you’ve pointed out), both of them are resource-intensive. Better to use a Cloud-hosted / Managed version next time\",1643646196.837139,1644150649.361729,U01DHB2HS3X\\n48dd84dc-d9e6-4c59-91e2-e3eadf10a8f0,U02V90BSU1Y,,,\"result below:\\nDocker Compose version v2.2.3\",1644140999.329219,1644152595.704599,U02V90BSU1Y\\n9ef11060-9187-4ba3-ab30-439ff9162dba,,2.0,,\"Terraform apply throws error\\nHow do I resolve please?\",1644152686.645389,1644152686.645389,U02V90BSU1Y\\n28735a24-87aa-4b7f-8920-5415526ab9a2,U02V90BSU1Y,,,\"Did you try to remove the \"\"-\"\" from your command? This is just a thought...\",1644140999.329219,1644152689.483789,U02GVGA5F9Q\\n1c4c50ff-e9c4-4d14-a296-d85e85bba4d6,U02V90BSU1Y,,,\"no I didn\\'t, I will retry that\",1644140999.329219,1644153668.310219,U02V90BSU1Y\\nbdceef7b-9991-49c0-97ba-77e2def34927,U030FNZC26L,,,\"&gt; But it seems like it comes down to choosing between either AIRFLOW_UID or the root user, right?\\nSince we can override the $AIRFLOW_UID it\\'s rather a question which user (UID) to use in Airflow\\'s containers: host UID, airflow (UID 50000) or root. It seems to me we were supposed to use the host UID , however the reason is a little bit unclear: `\"\"Otherwise the files created in\\xa0dags,\\xa0logs\\xa0and\\xa0plugins\\xa0will be created with root user.\"\"`\\nI think it\\'s irrelevant when comes to dags and plugins - we (host) are adding files there, not container. When comes to logs - it\\'s the opposite but we can still view them on host.\\n\\nOn the other hand, shall we use different container user and different user for Airflow? (airflow user != webserver user)\",1643646196.837139,1644155129.189129,U02UKBMGJCR\\n63027231-3eee-4aa5-95af-5931d13e44a3,U030FNZC26L,,,\"&gt; Better to use a Cloud-hosted / Managed version next time\\nI totally agree on that. Especially from the instructors perspective it\\'s much more efficient to have all participants using exactly the same environment :slightly_smiling_face:\",1643646196.837139,1644155479.236809,U02UKBMGJCR\\n5e28ced1-1602-47ec-8bf7-ec038ae525eb,,,,\"A good comparison between Google Cloud Composer (fully managed workflow orchestration) and Airflow on Docker and on-premise\\n<https://stackoverflow.com/a/62771505>\",,1644157248.242569,U030FNZC26L\\ncd7cde1c-e387-4c0d-84c3-f0180700d2c4,,4.0,,\"Hello I am currently in a place with a poor network, I will need to know if uploading just two months in 2019 taxi data can suffice for week3 task which is for bigquery.\",1644158033.659209,1644158033.659209,U02SPLJUR42\\n2d67e913-e693-47a3-ba0f-76bc2c083690,U02SPLJUR42,,,Maybe you can use transfer service or get a cloud VM?,1644158033.659209,1644159062.244279,U01AXE0P5M3\\naed61905-0598-4654-9afd-20d29b8f055e,,5.0,,\"Hello, i was checking out this <https://github.com/datastacktv/data-engineer-roadmap?fbclid=IwAR2mK-E9H8oDNMuM7QjOVluPNnf_GOBs7MrEOwXKJ0BFIwa-9jBdNECL1vc|data engineering road map>, but I don\\'t understand the  role of `messaging` . Could anybody with the exposure or experience kindly explain. Thanks\",1644160040.904359,1644160040.904359,U02SEH4PPQB\\n7669f5b8-31ae-4147-a9dd-b471984a97d6,U02SEH4PPQB,,,It\\'s similar to streaming. Something sends messages to some message broker and other services read messages and process them,1644160040.904359,1644160152.301419,U01AXE0P5M3\\n30402f89-9fa6-45dd-9c5b-bc65168c84a8,U02SEH4PPQB,,,\"Thanks <@U01AXE0P5M3>, is it the same as sensors in Airflow?\",1644160040.904359,1644160352.861729,U02SEH4PPQB\\n36fbcbd6-ffc2-46b7-8d2e-5c970699df3f,U02SEH4PPQB,,,\"Rather an arguable map from my point of view. There are plenty of DEs what has switched from software engineering. Those shouldn\\'t need CS fundamentals. For complete newcomers, I\\'d put DB and DWH knowledge before all other skills.\",1644160040.904359,1644162929.853369,U02Q7JMT9P1\\n133a2a3c-29b5-437e-9c1b-bf2fe1e2de08,,2.0,,\"In the updated code repo, upload to gcs bucket is done through bash and gcloud sdk. This requires the Google credentials json. Which code change in which file is required to move the json from local to the docket container?\",1644165146.111579,1644165146.111579,U02QK4ZV4UX\\n5e8a090b-8d46-4c6d-952a-8e2c8fa80325,,1.0,,:wave: I’m here! What’d I miss?,1644166367.689319,1644166367.689319,U031XBKQZGU\\n4a3442a5-714b-4287-bbc8-a54fd4cf128b,U02RZEKRBHD,,,I’m not sure what the name of the directory is where you are calling `docker run` but the error suggests the name of that directory needs to be lowercase. If the name of the directory - and I’m guessing from the output of your terminal is `MINGW64` you’ll have to change that to `mingw64`. See if that works :thumbsup:,1643230715.425600,1644166800.162449,U02TEERF0DA\\n2718fa64-9de2-41ba-9f62-c619d3108280,,1.0,,\"Hello everyone,\\nis it compulsory to take the last two lessons of week 2 (2.4.1 and 2.4.2) about Transfer service?\\nI did the Ingesting Data to GCP with Airflow correctly through Sejal\\'s video (2.3.2) but I find it extremely confusing to follow the first lesson of Ankush as it has a completely different configuration on BigQuery, and I only imported the data from yellow_tripdata_2021-01.csv (as in the video 2.3.2)\",1644169654.578609,1644169654.578609,U02UZ493J56\\n286b5812-412f-478c-a907-9dc687ebab80,U02UZ493J56,,,\"It\\'s a good to know kind of a video and you\\'ll only learn if you try the things shown in the video. However, so far the data transferred using that service has not been used so you can skip that if you feel you don\\'t want to invest time into that.\",1644169654.578609,1644170870.540439,U02TATJKLHG\\nE98A775F-2AF9-42B9-BF32-B871DF89C99E,,2.0,,\"Hi, I’m wondering it there will be any comparison of BQ vs Snowflake - understanding any differences between providers (other than price ) would be really useful?  thanks\",1644172067.855989,1644172067.855989,U01UMAXUPSQ\\nc91d7321-18a7-4870-a382-d0b537bf10b2,,1.0,,\"Hi all currently looking for some solutions, got locked out of my VM instance , can\\'t nolonger ssh using my windows local machine, tried some of the online available solutions and they are just not working,\\n$ ssh -i gcp2 DELL@IpdadressXX\\n*error, .DELL@ipadressXXX: Permission denied (publickey).*\\ni tried the following but they are still failing\\n1. generated new keys, they still failed\\n2. tried the gcp web console it failed\\n3. Tried editing firewall settings too\\nIssue happened after upgrading my disc space from 30GB to 100GB\",1644174128.390709,1644174128.390709,U02RTJPV6TZ\\n252c675b-4938-4b07-8363-94af24c62f64,,,,\"For week 3 homework, do we have to put all the FHV and yellow_taxi data into GBQ and create clusters and partitions before starting the homework?\",,1644178520.154609,U02UAHJHJ20\\n65fb7778-d159-4ca3-9f6b-cbc325fb3ca5,,4.0,,\"I always have timeout error during `local_to_gcs_task` execution. Did anyone have this problem? My connection is stable and I can download data from the server, but can\\'t upload it to gcp.\",1644178793.908769,1644178793.908769,U02V0TZKJBA\\n3f8222d4-cbde-4161-b9df-e96bb809a44a,U02V0TZKJBA,,,May be you check the path directory to Gcp like the gs://{}/raw/{} in addition the environment variable setups u specified in your docker-compose.yml file,1644178793.908769,1644180600.632729,U02RTJPV6TZ\\n3373857d-389a-4088-9afc-6cf8d6c69fff,U02QK4ZV4UX,,,\"<https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_2_data_ingestion/airflow/1_setup_official.md#pre-reqs>\\n\\nyou should follow this\",1644165146.111579,1644180875.119519,U01AXE0P5M3\\n3d4b5dc6-37db-4f35-b827-ceaa055b0681,U02V90BSU1Y,,,\"Did you already get it to work? Otherwise, maybe it\\'s already build, so you should just run it? `docker-compose up -d`\",1644140999.329219,1644181384.663679,U0319KGEJ13\\n886aec4c-3970-4f3f-b2fc-55503ef9cbbf,U02V0TZKJBA,,,<@U02RTJPV6TZ> Where GCP directory should be? You mean path to credentials?  Also where gs://{}/raw/{} should be?,1644178793.908769,1644183001.889649,U02V0TZKJBA\\necf8b6fb-7e1e-4af7-89a6-827f6f2f832a,,3.0,,\"Hello all, this question might sound stupid, but can\\'t wrap my head around it. How do I iterate over the months from the URL of the yellow taxi data? I\\'m thinking of a while loop, but not sure that\\'s best in this case. Is it?\",1644183431.161959,1644183431.161959,U02RSAE2M4P\\nE2C0749D-436D-412C-B004-70F8C86488ED,U02TMP4GJEM,,,I tried what <@U02TZ71470X>  did. It seems intuitive but I still can\\'t get it to work unfortunately. Not sure what else to do,1643061968.489400,1644183629.162499,U02TJ1V22EM\\nE89B3CEF-60EF-434B-BCC7-FC10E571E8B8,U02TMP4GJEM,,,,1643061968.489400,1644183724.174719,U02TJ1V22EM\\n41BA0D2A-A0E0-476F-B40D-952486C0E5D6,U02TMP4GJEM,,,Everything runs fine but I don\\'t see a table ,1643061968.489400,1644183746.433469,U02TJ1V22EM\\n54195e44-5fc0-4007-b302-0e8e5b7f3b00,U02RSAE2M4P,,,\"Hi <@U02RSAE2M4P>, you actually do not need to write a loop. If you take a sample link e.g  for January 2019, it looks like this `<https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2019-01.csv>` , the last section of the link `2019-01` is what we need to change progressively to get all the data. So what can make this section dynamic?, the answer is the `execution date` based on the scheduler. In this case we did set the schedule to `@monthly,` with the first run being `2019-01-01`  followed by `2019-02-01`  and so on. This means that anytime the dag runs, the the `execution_date` variable will hold`2019-01` , `2019-02`  and so on. Thus to make the links to the data dynamic, we have to incorporate the `execution_date`  variable into the links using `jinja templating` as follows: `<https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_{{execution_date}}.csv>`\",1644183431.161959,1644185110.903539,U02SEH4PPQB\\n929181f9-3504-42e5-94c5-801dab283b94,,,,\"Hi All, hope everyone is having a great weekend! Wanted to share this resource I had come across. Hope you guys find it useful.\\n<https://www.startdataengineering.com>\",,1644186645.342769,U02UM74ESE5\\n2631c0ba-bb92-4539-a966-7fec11d7887a,U02RSAE2M4P,,,\"Thanks a lot for the explanation. very clear.\\n\\nI have a question though, even though I set the start_date to 2019-01-01, the execution date starts from today, why is that? Is it because of me manually triggering the DAG? How can I fix this?\",1644183431.161959,1644186889.738599,U02RSAE2M4P\\nd9555910-3f05-418a-adf1-1327896aa8ad,,3.0,,\"Hello all,\\n\\nI\\'m trying to do HW 2 on my gcp vm, the vm doesn\\'t allow me create a directory for the google credentials. Has anyone tried this?\",1644189271.291429,1644189271.291429,U02RSAE2M4P\\ncfa53510-cd84-4621-8d8c-2fd100c8b34a,U02SZARNXUG,,,\"Hi all. Maybe my experience helps, I have 2 GCP accounts, with the same card and I\\'ve had no problems using the free credits with both of them (I started the 2nd one after the first one expired).  I am currently using the 2nd one for the zoomcamp.\",1644136107.540759,1644190874.158869,U02U414GXSA\\n01d6f928-fcc7-4280-8466-334ac35fd3bb,U02RSAE2M4P,,,I fixed this. I set catchup=True and deleted all the previous dag runs.,1644183431.161959,1644192306.756359,U02RSAE2M4P\\n,,,channel_join,<@U032KB7NTRN> has joined the channel,,1644195238.895619,U032KB7NTRN\\n17c73007-bd99-40be-b796-e35e1ed61712,,2.0,,\"Hi All, I am having a couple of problems I can\\'t seem to figure out. When I run docker-compose on my local machine and then try to access the airflow gui on port 8080, I can\\'t login. It keeps saying my login is invalid. When I do it on a virtual machine from my google cloud account it works fine. I am using the exact same .env and docker-compose.yaml files on both machines, so the login and password are exactly the same.  The second problem is that the last character of my bucket name is getting cut off when I run the dag for week 3. This worked fine for week 2, so I don\\'t know why the last character is getting cut off, but it causes the dag to fail (if I hard code the bucket name in the dag.py file instead of trying to retrieve it from the .env file, it works correctly).  It is suspicious to me that the bucket name is 32 characters, with the last one being cut off, but like I said, everything worked fine for week 2 without the last character being dropped.\",1644203421.832589,1644203421.832589,U02U5K0RFK8\\n91f7a625-6209-43e2-93e6-6c718d9175d1,,3.0,,\"For Week 3, should the \\'gcs_to_bq_dag.py\\' work without any modifications? I tried running it and my files were moved successfully from the \\'raw\\' folder to the \\'yellow\\' folder, but the file names were not what was expected.  The names look like this: \\'yellow_tripdata*.parquet_2019-01.parquet\\'.  So there seems to be a problem with the asterisk in the following line of the dag file: destination_object=f\\'{colour}/{colour}_{DATASET}*.{INPUT_FILETYPE}\\'\",1644205357.104949,1644205357.104949,U02U5K0RFK8\\nf577e1f6-f5d5-4a5f-b111-3f2790fcfbcd,U02SEH4PPQB,,,\"<@U02SEH4PPQB> Sensors is more like a poller. Messaging is source,storage and also consumer. Source emits an event and that gets put into a queue. This is reliable storage and some cases distributed as well. Consumer then consumes the message from the queue. Here is where Airflow sensor comes in. It can just poll the queue and indicate if messages are there. But a consumer also does some processing based on the message.\",1644160040.904359,1644207918.622979,U02BKFQ6DNG\\n4A6371C8-5710-454A-A626-758BF6A01D44,U02SZARNXUG,,,Thanks santiago. For me it didn\\'t work so I have used my brother debit card n email to get this thing done. ,1644136107.540759,1644208010.572769,U02V2G3NK8U\\nf2469ea1-847e-4f79-ab7f-74ce588d657d,U02SZARNXUG,,,\"I managed to do it (i.e. use the same card).\\n\\nI removed the payment option from my old google account*; waited for 2-3 days, then signed in with my other email account (and same card)\\n\\n* first remove billing (which almost closes your gcp account, then remove card from <http://pay.google.com|pay.google.com>.\\n\\nP.S. Wasn\\'t expecting our instructor \\'to be in the same boat\\' :-)\",1644136107.540759,1644208570.977229,U02HB9KTERJ\\n703eda83-a70d-4e4a-aeb8-cf3de37d02f4,U02V90BSU1Y,,,What is important is that you map the folder correctly in the docker-compose file.,1644137598.640829,1644208692.931079,U02HB9KTERJ\\n430e1a9b-8cf2-4c8c-8d74-d25c4e60c9db,U02SPLJUR42,,,Cloud network is the best solution; it won\\'t cost you much either. It was ~$0.3 for couple of hours for me.,1644158033.659209,1644208798.826759,U02HB9KTERJ\\n37ff093e-3e53-4b9b-9dd6-fd46866a36a0,,26.0,,\"local_to_gcs_task keeps failing\\n```File /.google/credentials/google_credentials.json was not found.```\\nThis is bizarre because it was working only the other day. The other tasks before it are okay. I\\'ve checked my directory structure where I\\'m running airflow and I can see that it is there:\",1644208872.428749,1644208872.428749,U02U5SW982W\\n895c891d-cedf-4e76-a6aa-bd481c398688,U02U5K0RFK8,,,\"When I changed the destination object in the dag.py file to the following the file names were as expected (i.e. I removed \\'*.{INPUT_FILETYPE}\\'). `destination_object=f\\'{colour}/{colour}_{DATASET}\\',` File name after transfer to \\'yellow\\' folder: yellow_tripdata_2019-01.parquet (unchanged from the file name in the \\'raw\\' folder).\",1644205357.104949,1644208962.282759,U02U5K0RFK8\\ndce43f25-4558-4b96-87c5-2aff0ad829d0,,,thread_broadcast,\"This is the results of my directory structure:sandy@sandy-ODYSSEY-X86J4105:~/git_clones/data-engineering-zoomcamp/project/airflow$ ls -a\\n.  ..  dags  dags_adjusted  dags_local  docker-compose.yaml  Dockerfile  .env  .google  logs  nohup.out  plugins  requirements.txt\\nsandy@sandy-ODYSSEY-X86J4105:~/git_clones/data-engineering-zoomcamp/project/airflow$ cd .google\\nsandy@sandy-ODYSSEY-X86J4105:~/git_clones/data-engineering-zoomcamp/project/airflow/.google$ ls\\ncredentials\\nsandy@sandy-ODYSSEY-X86J4105:~/git_clones/data-engineering-zoomcamp/project/airflow/.google$ cd credentials\\nsandy@sandy-ODYSSEY-X86J4105:~/git_clones/data-engineering-zoomcamp/project/airflow/.google/credentials$ ls\\ngoogle_credentials.json\\nsandy@sandy-ODYSSEY-X86J4105:~/git_clones/data-engineering-zoomcamp/project/airflow/.google/credentials$\",1644208872.428749,1644208985.742579,U02U5SW982W\\ne65df7bc-a5c5-4181-b0a8-f81a2c39df3c,U02U5SW982W,,,Anyone have a similar problem? Spending so much time ... going crazy...,1644208872.428749,1644209012.517849,U02U5SW982W\\n99ce0e54-a9a8-4285-bc68-9659c96d94c6,U031XBKQZGU,,,\"The first two weeks :slightly_smiling_face: - and some fun.\\n\\nNot that you can\\'t catch up.\",1644166367.689319,1644209859.311739,U02HB9KTERJ\\n38ae6d0b-c8f4-45d7-8647-9b0b77db6224,U02V0TZKJBA,,,Make sure you created the resource (from week 1 : terraform apply) since that creates the bucket. No need to change this bucket url; it worked fine for me.,1644178793.908769,1644209978.275919,U02HB9KTERJ\\n704df32a-8d2e-45cf-bd79-370c78f33a28,U02RSAE2M4P,,,I think I used the `sudo`  command for this.,1644189271.291429,1644210052.262239,U02HB9KTERJ\\n71a477a6-19b2-41f5-a7fb-008604f284ad,U02U5SW982W,,,Do you have your credentials checked into the git?,1644208872.428749,1644210519.608359,U02TATJKLHG\\n8a506979-1734-4d90-9c17-e81c83eee0df,U02U5SW982W,,,I\\'m not sure what you mean <@U02TATJKLHG>? If you mean is the file there in this loction with my credentials then yes.,1644208872.428749,1644211471.908599,U02U5SW982W\\n08f82cf6-4326-4a31-ae23-f8c1a0d2fb0f,U02U5SW982W,,,I mean have you also checked in the credentials to your remote git repository? Because they seem to be in your code folder. And it would not be advisable to check-in them so was just checking that you haven\\'t shared the creds publicly on git,1644208872.428749,1644211616.431569,U02TATJKLHG\\nd92bb69c-66bd-470e-998e-822e9fa8a457,U02U5SW982W,,,Ah I see <@U02TATJKLHG> no I haven\\'t.,1644208872.428749,1644211909.769469,U02U5SW982W\\nde5bbf7b-d7ab-42b0-81fa-87a10c705879,U02U5SW982W,,,But thanks for the headsup,1644208872.428749,1644211928.579699,U02U5SW982W\\nd928630f-7ced-450f-a0f7-748d81a14e3d,U02U5SW982W,,,\"Also, can you share you docker-compose.yaml part where you have set the env vars and the volume mounts\",1644208872.428749,1644213148.376159,U02TATJKLHG\\n22e703c0-bcbe-4d97-bfb2-2a30444c439b,U02U5SW982W,,,sure thing,1644208872.428749,1644213377.749119,U02U5SW982W\\n8de1b62f-e3ba-4653-9383-6cbddc12e173,U02U5SW982W,,,\"build:\\n    context: .\\n    dockerfile: ./Dockerfile\\n  environment:\\n    &amp;airflow-common-env\\n    AIRFLOW__CORE__EXECUTOR: CeleryExecutor\\n    AIRFLOW__CORE__SQL_ALCHEMY_CONN: <postgresql+psycopg2://airflow:airflow@postgres/airflow>\\n    AIRFLOW__CELERY__RESULT_BACKEND: <db+postgresql://airflow:airflow@postgres/airflow>\\n    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0\\n    AIRFLOW__CORE__FERNET_KEY: \\'\\'\\n    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: \\'true\\'\\n    AIRFLOW__CORE__LOAD_EXAMPLES: \\'false\\'\\n    AIRFLOW__API__AUTH_BACKEND: \\'airflow.api.auth.backend.basic_auth\\'\\n    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}\\n    GOOGLE_APPLICATION_CREDENTIALS: /.google/credentials/google_credentials.json\\n    AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT: \\'google-cloud-platform://?extra__google_cloud_platform__key_path=/.google/credentials/google_credentials.json\\'\\n    GCP_PROJECT_ID: \\'dtc-de-339104\\'\\n    GCP_GCS_BUCKET: \"\"dtc_data_lake_dtc-de-339104\"\"\\n\\n  volumes:\\n    - ./dags_adjusted:/opt/airflow/dags\\n    - ./logs:/opt/airflow/logs\\n    - ./plugins:/opt/airflow/plugins\\n    - ~/.google/credentials/:/.google/credentials:ro\\n\\n  user: \"\"${AIRFLOW_UID:-50000}:0\"\"\\n  depends_on:\",1644208872.428749,1644213393.822249,U02U5SW982W\\nb978df84-502f-404c-9287-69383a34b3bd,U02U5SW982W,,,Can you exec in your airflow worker in docker and see if the creds file is actually present in you airflow home path?,1644208872.428749,1644213517.107659,U02TATJKLHG\\n5a97a398-eb92-4940-886b-0eec3e8aa38c,U02U5SW982W,,,According to this line `~/.google/credentials/:/.google/credentials:ro`  your creds in your local system should be at `~/.google/credentials/`  this path for the volume to be able to pick it up,1644208872.428749,1644213619.519159,U02TATJKLHG\\nf03c744a-e038-47d2-964a-85fca8db5d84,U02QK4ZV4UX,,,\"<@U01AXE0P5M3> I checked the worker container and found the json file is available. but still recieved the following error.\\n```[2022-02-07, 05:54:36 UTC] {subprocess.py:74} INFO - Running command: [\\'bash\\', \\'-c\\', \\'gcloud auth activate-service-account --key-file=/opt/***/google_credentials.json &amp;&amp;         gsutil -m cp /opt/***/output_2019-04.parquet <gs://dtc_data_lake_ny-taxi-de-zoomcamp/raw>\\']\\n[2022-02-07, 05:54:36 UTC] {subprocess.py:85} INFO - Output:\\n[2022-02-07, 05:54:37 UTC] {subprocess.py:89} INFO - ERROR: (gcloud.auth.activate-service-account) Unable to read file [/opt/***/google_credentials.json]: [Errno 2] No such file or directory: \\'/opt/***/google_credentials.json\\'\\n[2022-02-07, 05:54:37 UTC] {subprocess.py:93} INFO - Command exited with return code 1\\n[2022-02-07, 05:54:37 UTC] {taskinstance.py:1700} ERROR - Task failed with exception```\",1644165146.111579,1644213746.776059,U02QK4ZV4UX\\n4909ab6a-cccc-43af-854a-ffaf502e66f4,U02U5SW982W,,,\"Hi <@U02TATJKLHG> yes it is see below: `sandy@sandy-ODYSSEY-X86J4105:~/git_clones/data-engineering-zoomcamp/project/airflow/.google/credentials$ cd ~/.google/credentials`\\n`sandy@sandy-ODYSSEY-X86J4105:~/.google/credentials$ ls`\\n`google_credentials.json`\\n`sandy@sandy-ODYSSEY-X86J4105:~/.google/credentials$` \",1644208872.428749,1644213829.643049,U02U5SW982W\\n5e1068d7-b5b2-4181-bd8a-5638c7a5d22c,U02U5SW982W,,,I\\'ll just check my airflow home path in the airflow worker now,1644208872.428749,1644213879.968879,U02U5SW982W\\n76a392c8-6a82-414f-84ea-0f1e6a57a9ae,U02U5SW982W,,,\"Is this what you mean? `sandy@sandy-ODYSSEY-X86J4105:~/git_clones/data-engineering-zoomcamp/project/airflow$ docker exec -it e044f6246a8e bash`\\n`default@e044f6246a8e:/opt/airflow$ ls`\\n`airflow-worker.pid  airflow.cfg  dags  logs  plugins  requirements.txt  webserver_config.py  yellow_tripdata_2019-01.csv`\\n`default@e044f6246a8e:/opt/airflow$ ls -a`\\n`.  ..  airflow-worker.pid  airflow.cfg  dags  logs  plugins  requirements.txt  webserver_config.py  yellow_tripdata_2019-01.csv`\\n`default@e044f6246a8e:/opt/airflow$` \",1644208872.428749,1644214131.376709,U02U5SW982W\\n30093628-814a-4bfa-b756-9d73f154cce9,U02U5SW982W,,,In this case it doesn\\'t look like it is...,1644208872.428749,1644214145.947499,U02U5SW982W\\n094c646d-cd07-4574-a00e-fc2159cf7d93,U02U5SW982W,,,How does this get changed?,1644208872.428749,1644214161.248849,U02U5SW982W\\n1fdbf053-6101-4dea-b374-cc3e8b2edb1a,U02U5SW982W,,,Okay actually it\\'s not in `/opt/airflow/` but in  `cd /`,1644208872.428749,1644215213.437639,U02TATJKLHG\\n18c356b8-8907-40ff-9814-ca113a788286,U02U5SW982W,,,\"Hmmm... I think I\\'m chasing my tail here. I\\'m just trying Sejal\\'s other alternative right now to see if I have any success...`local_to_gcs_task = BashOperator(`\\n        `task_id=\"\"upload_to_gcs_task\"\",`\\n        `bash_command=f\"\"gcloud auth activate-service-account --key-file={path_to_creds} &amp;&amp; \\\\`\\n        `gsutil -m cp {path_to_local_home}/{dataset_file} gs://{BUCKET}\"\",`\\n\\n    `)`\",1644208872.428749,1644215831.983209,U02U5SW982W\\n22d7c70f-2191-421d-9a29-33117fc61153,U02V0TZKJBA,,,\"<@U02HB9KTERJ> I\\'ll check that, thanks\",1644178793.908769,1644216609.172159,U02V0TZKJBA\\ndb0e11f1-2923-4de2-9c7f-1659e00dbb81,,3.0,,\"Hi! Do I understand correctly that in homework for week 2, it is necessary/possible to change the `data_ingestion_gcs_dag.py` script using the methods that Alexey did in `data_ingestion_local.py`?\",1644217373.501249,1644217373.501249,U02RC9GPNG0\\n7c122b8f-376b-4ca1-b9d2-417a03a2124f,U02RC9GPNG0,,,\"Yes, exactly\",1644217373.501249,1644217468.931779,U01AXE0P5M3\\n8e78d9f6-6165-44a9-902d-a74d0761501e,U02RC9GPNG0,,,\"Alternatively you can create a new file for that, but I think it\\'s simpler to do it with the existing one\",1644217373.501249,1644217514.905509,U01AXE0P5M3\\na74bc1d7-056d-4c98-bee0-e9338e3b3d35,U02RC9GPNG0,,,Thanks! :),1644217373.501249,1644217591.734299,U02RC9GPNG0\\n1a47ddc8-17a0-4ac6-bb66-0ce1f7ecd0b5,U02U5SW982W,,,\"<https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_2_data_ingestion/airflow/1_setup_official.md#file-googlecredentialsgoogle_credentialsjson-was-not-found|https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_2_data_ingestion/airflow/1_setup_official.md#file-googlecredentialsgoogle_credentialsjson-was-not-found>\\n\\nMaybe this will help?\",1644208872.428749,1644217969.124939,U01AXE0P5M3\\n336b6a33-de3d-49d2-b4c8-ea8c817f4841,U02U5SW982W,,,Thanks Alexey - I\\'ll go over those steps again.,1644208872.428749,1644218494.921009,U02U5SW982W\\n3b030763-b2d7-46f7-8520-aca5a7fa439a,U030FNZC26L,,,\"I am not sure on the query side, but BQ supports streaming inserts. But they can be costly\",1644824603.281529,1644826493.362999,U01DFQ82AK1\\n889b8130-e2ef-47a7-ae3e-29271fdbada5,U030FNZC26L,,,\"yeah, I use pub/sub and dataflow to insert data into BQ. But I want to also add visualization. Assume you have a temperature sensor and you want to visualize data live. I tried data studio, it\\'s free but I faced some problems. It randomly gives me an error that cannot connect to BQ.\",1644824603.281529,1644826790.024029,U030FNZC26L\\n3098b9e6-cb27-47b6-aeab-4cd41910f764,,1.0,,\"qn1 and qn 2 week3 seem to sound the same for me, am i missing something\\nQuestion 1:\\nWhat is count for fhv vehicles data for year 2019\\nCan load the data for cloud storage and run a count(*)\\n\\nQuestion 2:\\nHow many distinct dispatching_base_num we have in fhv for 2019\\nCan run a distinct query on the table from question 1\",1644830394.574129,1644830394.574129,U02RTJPV6TZ\\nbc06ceb9-230d-4063-bed3-da8c39e2035d,U02DD97G6D6,,,This is the detailed error log,1644792118.370219,1644831250.591839,U02DD97G6D6\\n59904e05-84af-4625-81ce-745813fc9b69,,9.0,,:cry:,1644831446.111549,1644831446.111549,U01AXE0P5M3\\n081f6a6a-814f-4453-8e4c-f7d4a11d1dbe,U02TBTX45LK,,,\"I\\'ll update the setup with a note on adding viewer. I wonder, do you also have bigquery admin?\",1644636171.927259,1644831666.298589,U01B6TH1LRL\\n08386bb3-60d7-4db5-b0ac-6f415ce1c395,U02V9V9NLJG,,,\"Have you seen the setup instructions from the repo? -&gt; <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_4_analytics_engineering/dbt_cloud_setup.md>\\nI create a service account, then generate the json key and use that to upload in dbt cloud to set up the auth\",1644630270.056449,1644831720.736459,U01B6TH1LRL\\n747b16f0-8c1f-4e35-a42d-16e45ff835fc,U02DD97G6D6,,,The error seems related to accessing the `stg_fhv_tripdata` as when I change it to `stg_green_tripdata` it works without a problem. But not sure stg_fhv_tripdata is created without issue as previous step.,1644792118.370219,1644831882.831909,U02DD97G6D6\\n4168782a-7a1e-4cd0-bc09-b1b3ee4aa78f,U02U5G0EKEH,,,Hey <@U02DBNR22GN> go to your service account and check the rights there and make sure it\\'s bigquery admin or has the other rights listed in the setup,1644527800.138399,1644831937.863999,U01B6TH1LRL\\n80e43917-ebaa-4fe4-af10-8e21a615452a,U030FNZC26L,,,\"Can you share the error.\\nSo from BQ there is no streaming output, so you would need to query BQ again and again. For that you might want to explore time series queries\\nsomething like this <https://medium.com/google-cloud/time-series-analytics-with-bigquery-f65867c1ce74>\",1644824603.281529,1644832077.307159,U01DFQ82AK1\\n4a1de003-33b8-4fb5-905f-820659af03e6,U02TBTX45LK,,,I had bigquery admin access for the account but I am not sure if i re-uploaded my\\xa0`Service Account JSON` after adding bigquery admin access,1644636171.927259,1644832089.955469,U02UDRBE3RS\\n6d5ddbdc-3e28-447d-b5a0-8e092de037a3,U02RTJPV6TZ,,,\"they are different. Q2 is about counting unique records, while Q1 is about total count\",1644830394.574129,1644832170.511309,U01AXE0P5M3\\ne0c9e859-bdc0-4b35-b005-32d195e81906,U030FNZC26L,,,this is the error,1644824603.281529,1644832277.258669,U030FNZC26L\\nc9b48a12-826b-41f3-83ea-1bae06cf6edb,U02DD97G6D6,,,And it is even more interesting when I change materialized configuration to `{{\\xa0config(materialized=\\'view\\')\\xa0}}`  it works. But it does not work for table. <@U01B6TH1LRL> do you have any idea what I am missing here,1644792118.370219,1644832449.209949,U02DD97G6D6\\n53e7b137-63a5-4715-8e1a-6a60d41e9e78,U02DD97G6D6,,,\"You need to check your service account rights, easiest way to go would be to gran bigquery admin\",1644792118.370219,1644832586.780989,U01B6TH1LRL\\n11cfbada-6f60-4798-8d2a-ab2290ce7cee,,2.0,,\"Ok sorry silly question, I went to do other stuff during the weekend and I am coming back to dbt. I was trying to run a job in production and now I keep getting the following error:\\n```The project production has not enabled BigQuery.```\\nIt was all working fine the other day and suddenly nothing and I really don\\'t get why. Did I change or don\\'t understand a vital setting, is it an internet issue (I am moving places).\",1644832676.588429,1644832676.588429,U02TC8X43BN\\n8d8c67c5-8866-4547-bcf2-09166446a726,U01AXE0P5M3,,,Time to start a crowd-funding campaign on <http://patreon.com|patreon.com> :wink:,1644831446.111549,1644832715.852979,U02Q7JMT9P1\\n8fb173c4-e3d2-4232-bc0f-53b0a36a617e,U01AXE0P5M3,,,and/or create another account? :thinking_face:,1644831446.111549,1644832973.938959,U01AXE0P5M3\\n640eea27-9e8e-4db9-a708-d36dea7da0fc,U01AXE0P5M3,,,that\\'s actually my second one... I\\'m running out of bank cards,1644831446.111549,1644833002.108829,U01AXE0P5M3\\n624f9411-a05f-4d7c-8fd2-431504961166,U02DD97G6D6,,,Yes I think I already did that when I checked the service account I am seeing it like that (I dont see admin role here :disappointed: ),1644792118.370219,1644833397.737419,U02DD97G6D6\\n77f10f5c-d257-482b-a0df-d3ac840a74af,U02TC8X43BN,,,\"Can you run the compiled code directly from BQ? a quick google search points to not having enables the bigquery API, not sure how it could have been disabled without changing the account :woman-shrugging:\\n<https://stackoverflow.com/questions/41448606/error-processing-job-project-has-not-enabled-bigquery>\",1644832676.588429,1644834342.888929,U01B6TH1LRL\\n495c0201-bf38-4539-8a22-3f7fa224385c,U02SFFC9FPC,,,Removed the back slashes and ran the command on a single line of code rather than allowing it to be run though c/paste,1643931419.667799,1644834358.487899,U02SFFC9FPC\\nf267dd57-b30f-45ce-a377-d836ee0073cd,U030FNZC26L,,,Can you check access via IAM?,1644824603.281529,1644835533.225119,U01DFQ82AK1\\n641ca812-3a41-4333-a0d5-d901fd9e12fe,U01AXE0P5M3,,,I think you can do with same bank card and different email. I think i made a second gcloud account with the same bank account,1644831446.111549,1644836804.753039,U0308MF3KUH\\ndb461329-bb7c-4ec2-bfcd-08811eae6dd4,U030FNZC26L,,,\"good question. I\\'m not sure if I\\'m doing it right or not. I couldn\\'t find any role for data studio. But for my email that I used for this gcp account, I have the following roles:\",1644824603.281529,1644837016.946089,U030FNZC26L\\nb3b961c9-7ac3-4f85-b400-8e02bdfaa01d,U02UX664K5E,,,\"Didn\\'t check the logs, but otherwise the tasks seemed OK for me.\",1644811740.957389,1644837522.374309,U02HB9KTERJ\\n43f7f550-5054-41e9-8096-d3fc613a3286,U02TBTX45LK,,,\"In my case,  I had BigQuery Admin before adding Viewer role.\",1644636171.927259,1644838111.502209,U02LQMEAREX\\n975563f3-b728-441a-ae1c-b04852f5fa0c,U02TBTX45LK,,,\"That\\'s strange, I *only* have big query admin on my service account and had no problems at all.\",1644636171.927259,1644840573.310149,U01B6TH1LRL\\n65d1e600-68c1-487b-bcc6-ae2e59e59f37,U02TBTX45LK,,,\"btw, I added a note in the cloud setup regarding this, to add viewer role as well to avoid such issue\",1644636171.927259,1644840605.952779,U01B6TH1LRL\\nA629CA2C-8A1F-4C46-AD72-5567BCA37412,U02TBTX45LK,,,I am guessing it is more re uploading the Service Account JSON file after I changed my access that helped ,1644636171.927259,1644840936.606689,U02UDRBE3RS\\n9d01af71-e8f4-412f-9b1a-fb8372a1e960,,3.0,,\"Will external tables suffice for Week 4 (and onwards)? Or do I need to create native (by using BigQueryInsertJobOperator)? I am asking coz I already have the yellow data uploaded in two stages - in Week 2 and 3. Now, I am doing just for Green (19/20).\",1644841666.691949,1644841666.691949,U02HB9KTERJ\\n21932F2C-3007-4055-80A1-621FF2546FF9,U01AXE0P5M3,,,<@U01AXE0P5M3> please start a Patreon or Buy Me A Coffee account for DTC. ,1644831446.111549,1644843325.859299,U025P29E8D9\\n46383a52-dc04-4ca6-a523-3666e858ba5a,U02TC8X43BN,,,\"It was actually something really silly.\\nOnce again I copied the file from git to not have to write all the descriptions and schema.yml on git is written to work with postgress while the bigquery is commented out.\\nI did not realize and I guess the error is kind of specific enough in a way but the \"\"production\"\" in it threw me off.\",1644832676.588429,1644844740.726109,U02TC8X43BN\\nc9cb5b0e-6d5a-4591-9a6a-cebe6e8152b1,U030FNZC26L,,,\"I also see the table schema in DataStudio:\\nand for credentials, I selected myself (which I think is the same as the one in IAM roles). It\\'s weird that it works some of the time.\",1644824603.281529,1644845548.836949,U030FNZC26L\\na59659cf-d7df-496c-be8e-0a33ac46560a,U02TATJKLHG,,,Anybody who can help with this? :confused: I want to check the UI and lineage graphs,1644817851.455599,1644845719.579179,U02TATJKLHG\\n6caded42-c428-4fde-b542-bf18db642a76,U030FNZC26L,,,Looks like it is working,1644824603.281529,1644845880.858389,U01DFQ82AK1\\n03c07cc0-9f92-4ca5-a118-0b8d22caaf0f,U030FNZC26L,,,\"Maybe be googling the exact error might help, never used data studio\",1644824603.281529,1644845903.909099,U01DFQ82AK1\\nd4be3cc9-ae15-4002-bcb9-d8b8024a4d63,,1.0,,Why do we have to create an external table before creating a native table in BigQuery? Can\\'t we just create a native table straightaway,1644849017.296719,1644849017.296719,U02T9JQAX9N\\na9535b28-fabe-4673-b2e0-0f5e6cf1388e,U01AXE0P5M3,,,\"There\\'s a way to support me personally via github - but I\\'m spending all these money on DTC anyways :slightly_smiling_face:\\n\\n<https://github.com/sponsors/alexeygrigorev>\",1644831446.111549,1644850796.419939,U01AXE0P5M3\\n4bc2eac1-3ba3-4c99-a740-df36564c4961,,1.0,,\"Hi All, I\\'m having a weird issue with airflow. It seems to be giving me incorrect import errors, ones that have appeared previously but which I have fixed. Here\\'s an example of my current error raised by airflow and the appriopriate code fragment from the DAG I\\'m trying to use:\\n\\nAs you can see, the bq_ext_2_part_task is defined in the file, and there even is no line number 67 anymore! If I break the code in some other way, the import error will update with the newer error, but fixing the error does not make the import error go away.\\n\\nDoes anyone have an idea why this is happening?\\n\\nEDIT: If I create a new DAG file and copy the content from the DAG that is getting an Import Error, the new DAG file is imported successfully. What gives?\",1644850871.679379,1644850871.679379,U02V1JC8KR6\\n8c1169f3-7727-4291-92b5-cacaf983427b,U01AXE0P5M3,,,<mailto:alexey_grigorev_second_account_for_learning@gmail.com|alexey_grigorev_second_account_for_learning@gmail.com> ? :stuck_out_tongue:,1644831446.111549,1644850933.057619,U02CD7E30T0\\nf54c69a3-a8b7-4921-b90f-5e9a0f7abd5f,U02TATJKLHG,,,Try with Port mapping `-p 8080:80`,1644817851.455599,1644851156.315769,U01DFQ82AK1\\nf68fdc33-9fef-4c26-a4cf-1945d65ebce0,U02T9JQAX9N,,,\"You can do that, check this out <https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv#bq>\\nI showed external table to show the usage of external tables also\",1644849017.296719,1644851328.643709,U01DFQ82AK1\\n42fa2bc8-deab-45a7-8d6a-344a2b89a629,U01AXE0P5M3,,,works with the same card!,1644831446.111549,1644852209.056389,U01AXE0P5M3\\n1e60d4ee-4add-4fa1-916a-7e21ebb1cff0,U01AXE0P5M3,,,that\\'s actually my 3rd free trial... I hope google will not ban my main gmail account :see_no_evil:,1644831446.111549,1644852257.907379,U01AXE0P5M3\\ndf0297e8-a66a-43cb-a9b6-12d1b68e2811,,3.0,,\"Another question regarding something I might be missing.\\nWe put a default var is_test_run as TRUE to limit results to 100.\\nWhen we make the production jobs, are we supposed to pass the variable as False, or does it do it automatically?\\nI see my jobs only got 200 rows so not quite ideal for analytics.\\nIs it\\ndbt run  \\'is_test_run: false\\'\\nor do we add an environment variable for production?\",1644852864.943879,1644852864.943879,U02TC8X43BN\\n4bd17e22-407d-4097-a465-2413d97bd69e,,4.0,,\"Just submitted my HW for week 3 and have to admit, it was rather frustrating for me. After puzzling but yet doable challenges of Airflow DAG creating of week 2, I was banging my head against the wall fighting with data type errors of Big Query. I had to redo DAGs from week 2 to force the schema for yellow taxi data and could create a partitioned table in the end, but the trick did not work for FHV data - I was stumbling upon \"\"\\'pickup_datetime\\' has type BYTE_ARRAY which does not match the target cpp_type INT64\"\" error no matter what. So, I gave up in the end and uploaded all csv files to the cloud storage and created tables with SQL queries. It was fast and smooth.\\n\\nMy question to more experienced DEs, what\\'s the point of packing csv data in parquet format if there is very little control on the data quality and querying it later from the data lake will crash because of one bad row? What\\'s the point of the schema-on-read concept if it leads to such problems?\",1644853447.563519,1644853447.563519,U02Q7JMT9P1\\n859f7172-d65e-4d86-94b5-94b0f57e8f38,U02TC8X43BN,,,\"I switched the default to FALSE before running to prod, but I believe you can add it to the job definition here for example (after `dbt run` )\",1644852864.943879,1644853522.827569,U02UA0EEHA8\\n8ce59ab7-3f0f-433e-8fa4-2890d8cfff2d,U02Q7JMT9P1,,,\"This was also surprising for me, unfortunately I did not try with parquet format. Can you provide me a file to test it out\",1644853447.563519,1644854422.313409,U01DFQ82AK1\\ne8500cd4-6f1a-4448-b241-bfeaed1ee7bf,U02Q7JMT9P1,,,<@U01DFQ82AK1> Do you mean CSV? It was FHV from 2019-03 on (tried several months). My gut feeling says the reason might be errors during downloading CSVs or because I had to convert them to parquet in chunks (otherwise Airflow was running out of resources). I can share the parquets with you later when I will figure out the best way,1644853447.563519,1644856276.709819,U02Q7JMT9P1\\n99ea6a55-04a2-48de-bff5-bae39bd7adf4,U02DBNR22GN,,,I added the viewer role and it works for me thank u,1644785599.998209,1644856553.584709,U02DBNR22GN\\n83642ac1-c91a-4e57-ae7b-5a01cd591de7,U02TC8X43BN,,,\"My idea behind it was to show how to use variables, this was a very clear way to show both ways of setting and defining one. In production, I did not clarify but I do dbt run\\xa0\\'is_test_run: false\\' because we want to keep all of the data.\\nA better implementation of such limit would be to use the<https://docs.getdbt.com/reference/dbt-jinja-functions/target/| target variable >where we could actually know if the model is running in a development environment and we should limit or if it\\'s running in production and then we take the whole amount of data.\",1644852864.943879,1644858443.153769,U01B6TH1LRL\\nb0541072-7c5c-4b95-b293-94ecef783918,U02TC8X43BN,,,Feel free to also delete the line or switch up like Igor did,1644852864.943879,1644858473.693629,U01B6TH1LRL\\n2c153f6a-0baf-4cbb-8efa-436c65c253fa,U02HB9KTERJ,,,As long as you have a table in BQ to select from as the source of your models you will be able to run the dbt project.,1644841666.691949,1644858622.320019,U01B6TH1LRL\\n67587b68-0bdd-4980-8a38-89ae892babbf,U02DD97G6D6,,,just modify it and then re generate the json key and upload it in dbt,1644792118.370219,1644858696.427849,U01B6TH1LRL\\n0ac7dda8-82bd-4527-a55a-fa93c567d659,U02Q7JMT9P1,,,I had to use transfer service to download files from s3 bucket.,1644853447.563519,1644859059.273029,U0290EYCA7Q\\nc81e320c-7c45-400c-a542-da688b858328,U02TBTX45LK,,,I added the Viewer Role and it solved my problem,1644636171.927259,1644859373.577939,U02DBNR22GN\\n66de14b4-195c-445f-97c3-4261501f2b4b,,,,\"Incase anyone is still having issues with uploading of data from week2, my work around the issue was to setup airflow on a VM and run the dags.\\nThis way I was able to avoid download error due to speed or network fluctuations bcos I\\'m now using the Internet of the vm and the process ran very fast.\\n\\nCheck video 1.4.1 for setting up vm and installing docker on it. Then copy your airflow folder to the vm and run the docker compose file to setup airflow.\\n\\n<https://datatalks-club.slack.com/archives/C02V1Q9CL8K/p1644517912430799?thread_ts=1644517912.430799&amp;cid=C02V1Q9CL8K|https://datatalks-club.slack.com/archives/C02V1Q9CL8K/p1644517912430799?thread_ts=1644517912.430799&amp;cid=C02V1Q9CL8K>\",,1644860216.734869,U02QS4BD1NF\\n27d76384-7124-4fdd-9800-4cba1c012f35,U02TBTX45LK,,,I’m having the same problem so will try an add Viewer Role. Its strange though because i thought that comes under Admin.,1644636171.927259,1644860380.087969,U02TAA1LDQT\\n12f8d84f-1e01-4b5e-bc50-31c6795b465c,U02Q7JMT9P1,,,<@U01DFQ82AK1> <https://drive.google.com/drive/folders/1qYwvIcqQ7VLkI4inqA6YwSIp58rHwnAv?usp=sharing> External table on these two parquet files throws an error,1644853447.563519,1644860450.787699,U02Q7JMT9P1\\n65eeeb74-595e-42cd-996d-e089bd372b85,U02TATJKLHG,,,\"I have this line in my docker-compose.yaml but it didn\\'t work\\n```    ports:\\n      - \"\"8080:8080\"\"```\\nI also passed as you suggested but It\\'s not working even with that\",1644817851.455599,1644861205.469779,U02TATJKLHG\\na484d389-2d7d-4504-8ab9-dc64205db2d7,U02TBTX45LK,,,\"I am having the same problem, even by adding Viewer role: maybe I am doing something wrong in this last step, could you provide me in details how you added the viewer role?\",1644636171.927259,1644861354.004039,U02UZ493J56\\nd714554f-f4e7-4b21-99c0-285f0b054cd8,,7.0,,\"I am having this issue when running the fact_trips model:\\n```Access Denied: BigQuery BigQuery: Permission denied while globbing file pattern.```\\nI have seen that some users solved it by adding the Viewer role to the service account in use. I did it (and also generated a new json file and uploaded it in the dbt cloud), but I still have this problem. Could you provide me more details on how to do this last step? Maybe I am doing it wrong. Thank you!\",1644861631.604849,1644861631.604849,U02UZ493J56\\n5f6d8d6f-d105-4a5d-8304-533ffe406372,,1.0,,Do we need dbt for next weeks? Can we skip if is out of interest?,1644864132.354879,1644864132.354879,U0308MF3KUH\\n7b3f17c7-9e7b-4981-8e7b-ace834079f98,U02UZ493J56,,,\"Try to look at your code, you probably trying to cross select between two different data set. ensure the dataset is same.\",1644861631.604849,1644864562.358139,U02U5G0EKEH\\n3357ec89-3f2d-4a9a-a5f9-b40f3db927bb,,6.0,,Is there are any solution for below \\'permission\\' issue?,1644864676.992129,1644864676.992129,U02CK7EJCKW\\n0d43ce18-d5c3-4894-aeb3-cc3d463c2d37,,,,I\\'m currently using windows wsl2,,1644865170.941919,U02CK7EJCKW\\ndfe2a6c8-886d-464b-afde-6ea666505524,U02CK7EJCKW,,,Did you add the public key to metadata? Is the identity file pointing to the private key?,1644864676.992129,1644865228.175809,U01AXE0P5M3\\n59900eee-652d-43e2-9b63-bae0f87a2247,U02CK7EJCKW,,,\"yes, identity file pointing private key which is in .ssh folder, but I didn\\'t add public key in GCP metadata\",1644864676.992129,1644865386.227849,U02CK7EJCKW\\n5eb55de7-643c-4caf-a40f-933dc9c68857,U02CK7EJCKW,,,\"Okay, I see that you use wls. Since VS code lives on your \"\"main\"\" windows, you need to configure ssh there. It won\\'t look into your ssh config in wsl\",1644864676.992129,1644865406.682599,U01AXE0P5M3\\n43fd3bd8-0c04-445a-a10c-6283b933106a,U02CK7EJCKW,,,Please follow the video tutorial step by step or check the notes from Alvaro in this channel,1644864676.992129,1644865464.752379,U01AXE0P5M3\\n4670499c-ab5f-47e0-b3f4-efcfff011b9b,U02U3E6HVNC,,,\"Aah that makes so much sense, thank you!\",1644432387.132949,1644865528.426649,U02U3E6HVNC\\n1f148d93-1cfb-4d2b-b5ff-a86f79d5514f,U0308MF3KUH,,,Yes you can skip it,1644864132.354879,1644865634.204359,U01AXE0P5M3\\n88c56e26-8a42-4e54-81fc-cd52a3f078da,U02CK7EJCKW,,,\"Ok, Let me try again. Thank You\",1644864676.992129,1644865712.398899,U02CK7EJCKW\\nc07ac926-4992-4e36-84e9-d743f0c6ad50,U02UZ493J56,,,Everything seems to be good in my code,1644861631.604849,1644865727.307759,U02UZ493J56\\nB6277FED-BD41-422C-887C-7402ACEB76D9,U02V1JC8KR6,,,\"<@U02V1JC8KR6>Just a small cross check ..Have you saved the file in VS code after editing ? \\n\\nMay be just exit airflow and restart \\n\",1644850871.679379,1644866679.604809,U02AGF1S0TY\\nf7534333-2bbc-4571-8c23-b804658b299f,U02CK7EJCKW,,,You are right the problem was I  didn\\'t config .ssh in windows. Solved Thanks :smiley:,1644864676.992129,1644867465.965779,U02CK7EJCKW\\nd0142b2f-4afe-4ed7-8bf6-6af0a1f6572f,U02DD97G6D6,,,\"Would it make sense to recreate from scratch and upload that to dbt? Otherwise which one I should modify, the principles of the roles?\",1644792118.370219,1644875120.664159,U02DD97G6D6\\n4f3c19f8-4ca5-402a-887a-3f3c642bcb3f,U02UZ493J56,,,\"I had the same issue and the problem was with the tables in trips_data_all dataset (green_tripdata and yellow_tripdata). They were external tables (sourced from the bucket). So I created new tables using these external ones, the new ones were not external anymore. Just make sure you have the same file names at the end, or change them in dbt files.\",1644861631.604849,1644875237.787889,U02UVKAAN2H\\n5e70f2d7-c651-4544-aa09-2d243e012244,U02UZ493J56,,,Additional note: External tables show the source location under the details. The ones you should have will have schema and no source address.,1644861631.604849,1644875561.012039,U02UVKAAN2H\\n0f3b029c-2913-451f-8f27-b41f1015270c,,11.0,,\"[WEEK 4] Hi! I\\'ve read this issue several times but none of the proposed solutions seem to work for me.\\n*Error:*\\n```21:43:55  Runtime Error in model stg_green_tripdata (models/staging/stg_green_tripdata.sql)\\n21:43:55    404 Not found: Dataset deng-338919:trips_data_all was not found in location EU\\n21:43:55    \\n21:43:55    Location: EU\\n21:43:55    Job ID: 772ae51b-c4ea-4483-9b10-a6645c447c09```\\nI have checked my source dataset (ie: *trips_data_all* ) and my development schema (ie: *dbt_jcanovas* ) and both are in `europe-west6`\\nBut I still have the location error. Other items of interest:\\n\\n• I\\'m running dbt locally as a python package and when I run `dbt debug` all checks passed successfully \\n• This is how my `profiles.yml` looks like:\\n```deng:\\n  outputs:\\n    dev:\\n      dataset: dbt_jcanovas\\n      fixed_retries: 1\\n      keyfile: /home/jm/.google/credentials/google_credentials_dbt.json\\n      location: EU\\n      method: service-account\\n      priority: interactive\\n      project: deng-338919\\n      threads: 4\\n      timeout_seconds: 300\\n      type: bigquery\\n  target: dev```\\nAny ideas? TIA\",1644876320.850049,1644876320.850049,U02S2TZRBL7\\nbe83feaa-719e-4c75-a7f8-f2e9a1c4e1ba,U02S2TZRBL7,,,\"Hey, have you tried using  \"\"europe-west-6\"\" instead of EU in your profiles.yml?\",1644876320.850049,1644877260.270319,U01B6TH1LRL\\n2b0a4a62-dcd6-42db-9c48-09e32fcc45eb,U02S2TZRBL7,,,\"Also, for messages, try to keep it short and then detail in the thread so it doesn\\'t take the whole screen\",1644876320.850049,1644877305.490659,U01B6TH1LRL\\ncab5fb4a-cace-4604-bb8e-8e7bbbf1bbe8,U02S2TZRBL7,,,\"Thanks <@U01B6TH1LRL> , that solved the issue!\\n\\nWeird behavior IMO because when I run `dbt init` for the first time, it was prompting options interactively to the console (project name, connector, path to json credentials, etc).\\nFor location, there were 2 options only: US or EU... never thought about this.\",1644876320.850049,1644879035.424839,U02S2TZRBL7\\n08f8f5d5-2c2b-4c19-a647-284753a86c2f,,7.0,,\"[WEEK 4] Just in case someone found this error \"\"error message: Parquet column \\'ehail_fee\\' has type DOUBLE which does not match the target cpp_type INT64.\"\" in fact_trips.sql running dbt. The workaround I found (not the best solution) was modify stg_green_tripdata.sql model using this line *cast(0\\xa0as\\xa0numeric)\\xa0as\\xa0ehail_fee.*\",1644881953.727679,1644881953.727679,U032Q68SGHE\\nab3749fe-f20f-401d-bfe4-ee9b9a39673a,,5.0,,\"{week 2} please i have this error when trying to run the dag to upload my parquet files to gcs. I\\'ve tried revising the permissions to the project id but nothing. I\\'d appreciate any help i can get,\",1644884972.605279,1644884972.605279,U02TA6MJZHR\\n9a90ec23-097e-4e94-94a6-03c19d0a6986,U02S2TZRBL7,,,Thanks for the feedback :smiley:! Added :white_check_mark:,1644635944.221919,1644886860.287319,U02S2TZRBL7\\nc0fa569b-709a-454b-bcf3-74ae24c174a5,U032Q68SGHE,,,\"Thanks, I will try this out. This column \\'ehail_fee\\' is killing me\\n\\nedit 1 - It worked, true lifesaver. But how this affects the table? I\\'ve seen that this column is all null values in BQ.\",1644881953.727679,1644890374.290399,U02TC704A3F\\nfcec76cc-6b74-475d-a282-e45455f7fa97,U02TA6MJZHR,,,what permissions does that service account have? you should be able to see those permissions on the <https://console.cloud.google.com/iam-admin/iam> page,1644884972.605279,1644892482.053119,U02UARNPAN5\\na0be84ad-5832-4ca0-a72f-49b0b611cf6d,,9.0,,\"Hi everyone.\\n\\nI\\'m thinking about and working on a data pipeline for the course project and want to ask your opinion about the tools and services. Assume the task can be scaled (maybe to even several million sensors) and used by real users in the real world, and not just a course project.\\nLet me explain the task:\\n• I have 1000 sensors that send data 10 times a day but not at a specific interval. Assume 10 random times in a day for each sensor --&gt; 1000 * 10 data points in total.\\n• Each sensor data would be 20 string words or a JSON with several key-value pairs like sensor id, timestamp, value, ...\\n• There are also 1000 users (one for each sensor) that want to visualize their own sensor data.\\n• The price is important. So I want to do it at the lowest cost.\\nWhat I do right now:\\n• I use Google Cloud IoT core and one pub/sub topic to get sensor data (which is simulated using several VMs on GCP to send data using some Python scripts).\\n• Then I use the cloud function to be triggered when new data is published on the topic. I tried Google Dataflow, but it is more expensive than cloud function, so I switched to cloud function.\\n• Then this cloud function inserts the data into BigQuery.\\n    ◦ Would it be cheaper to save data in GCS and then load it into BigQuery? Because I see some people say this is cheaper. Note that a few seconds is not critical for me.\\n    ◦ Or maybe saving data in GCS, skipping BQ, and somehow loading data for each user in Python?!\\n• Now I want to add some visualizations for 1000 users to see their own data in near-real-time and with max a few seconds delay.\\n    ◦ I tried Data Studio, but it has some limitations. It\\'s refresh rate and also getting disconnected from BQ.\\n    ◦ I\\'m looking for something free or cheap.\\n    ◦ I\\'m thinking of using Streamlit and deploying it using Google App Engine, but I\\'m not sure if I have to deploy 1000 apps for 1000 users or if it is possible to do it in another way. I know the app engine will scale automatically. I think I have to run a SQL for each user based on his sensor ID (which I know) when he opens the app or the link, right?\\n    ◦ Basically, it would be one SQL query for each user to get his/her data from BQ and show it in the dashboard.\\n    ◦ Any other recommendations would be appreciated.\",1644902599.749989,1644902599.749989,U030FNZC26L\\n6e6d8654-3eae-4ef0-b3e1-a0e4c5f85f49,U02S2TZRBL7,,,\"Getting same error with EU location. tried with \"\"europe-west-6\"\" but getting error on dbt debug-\",1644876320.850049,1644903831.044879,U02NSF7RYP4\\n91b91aff-841a-42cc-9cd1-d09c9c604783,U02S2TZRBL7,,,\"Configuration:\\n  profiles.yml file [OK found and valid]\\n  dbt_project.yml file [OK found and valid]\\n\\nRequired dependencies:\\n - git [OK found]\\n\\nConnection:\\n  method: service-account\\n  database: data-terraform-338612\\n  schema: dbt_dataset\\n  location: europe-west-6\\n  priority: interactive\\n  timeout_seconds: 300\\n  maximum_bytes_billed: None\\n  execution_project: data-terraform-338612\\n  Connection test: [ERROR]\\n\\n1 check failed:\\ndbt was unable to connect to the specified database.\\nThe database returned the following error:\\n\\n  &gt;Database Error\\n  Invalid value for location: europe-west-6 is not a valid value\",1644876320.850049,1644903831.790619,U02NSF7RYP4\\n25b83274-d036-4384-a0af-be25f7eaf4c7,U02ULGHNT33,,,how do you run that ? <@U0290EYCA7Q>,1642939459.070600,1644904494.907239,U02T70K8T61\\n742115a9-aff9-4079-abc8-a72ea2a90e9a,U02S2TZRBL7,,,ok. Its europe-west6 and not europe-west-6 . Also had to delete dbt dataset from bigquery which was created with EU location previously.,1644876320.850049,1644904741.493999,U02NSF7RYP4\\n02f2d509-76c3-4df6-9ec6-6715835ba7a5,U02S2TZRBL7,,,its working now.,1644876320.850049,1644904756.933119,U02NSF7RYP4\\n2d404f08-c22b-4f55-8f71-f0cd153c8844,U02S2TZRBL7,,,\"05:55:32  1 of 3 START table model <http://dbt_dataset.my|dbt_dataset.my>_first_dbt_model......................... [RUN]\\n05:55:32  2 of 3 START view model dbt_dataset.stg_green_tripdata.......................... [RUN]\\n05:55:34  2 of 3 OK created view model dbt_dataset.stg_green_tripdata..................... [OK in 2.11s]\\n05:55:37  1 of 3 OK created table model <http://dbt_dataset.my|dbt_dataset.my>_first_dbt_model.................... [CREATE TABLE (2.0 rows, 0 processed) in 5.47s]\\n05:55:37  3 of 3 START view model <http://dbt_dataset.my|dbt_dataset.my>_second_dbt_model......................... [RUN]\\n05:55:43  3 of 3 OK created view model <http://dbt_dataset.my|dbt_dataset.my>_second_dbt_model.................... [OK in 5.53s]\",1644876320.850049,1644904799.583279,U02NSF7RYP4\\n49bf4c5b-016c-4933-a2f6-149534be09cf,,21.0,,\"In question 5 in the week three solution, let\\'s say `SR_Flag` would\\'ve been `integer` , which it is in my case since I used the parquet files, would it make sense to partition by `SR_Flag` and cluster by `dispatching_base_num` ? Also, why can\\'t one partition by `string` type columns?\",1644906567.045219,1644906567.045219,U02TATJKLHG\\n06538a21-7996-46c2-9178-8e60345fc444,U02TATJKLHG,,,\"Currently, BQ supports partition by\\n• *Time-unit column*: Tables are partitioned based on a\\xa0`TIMESTAMP`,\\xa0`DATE`, or\\xa0`DATETIME`\\xa0column in the table.\\n• *Ingestion time*: Tables are partitioned based on the timestamp when BigQuery ingests the data.\\n• *Integer range*: Tables are partitioned based on an integer column.\\n\",1644906567.045219,1644909162.133439,U0290EYCA7Q\\n5e162f8d-4d19-4269-91b0-70b1814c56a1,U02TATJKLHG,,,So is this just specific to BQ?,1644906567.045219,1644909586.632469,U02TATJKLHG\\neb2fbfbc-c481-43c5-992d-346083417af0,U02UZ493J56,,,\"It worked, thanks!\",1644861631.604849,1644909815.914619,U02UZ493J56\\n7ea02579-59a9-4842-bbdd-22b13817febf,U02TATJKLHG,,,\"For me `SR_Flag` was stored as an `integer` columns, as Ankur says, and I could easily partition by SR_Flag by using the following syntax:\\n```partition by RANGE_BUCKET(SR_Flag, GENERATE_ARRAY(0, 45, 1))```\\nwhere 0 and 45 are the min and max values for `SR_Flag` . <@U01DFQ82AK1> would that not be a valid solution?\",1644906567.045219,1644910388.563889,U02UA0EEHA8\\n16953aa6-0dbd-4020-bd8b-fc3e709152f2,U02TATJKLHG,,,\"That is a valid solution, if we are sure that the future values of SR_FLAG are within 0-45\",1644906567.045219,1644911709.315119,U01DFQ82AK1\\nd85de4dc-af85-4d52-b2cd-c1d857983eb5,U02TNEJLC84,,,\"Hey; according to docs of this operator: `Note that the source path\\'s part before the wildcard will be removed; if it needs to be retained it should be appended to destination_object.`\\n\\nTherefore only `_yyyy-mm.parquet` part was retained.\",1644443541.970059,1644480777.168659,U02UKBMGJCR\\n2e32ae47-8684-4060-988f-b7c90929ab34,U02U3E6HVNC,,,\"The source tables are in the US, that\\'s what the error says, but I\\'m interested in your write location, can you check where your development schema is? Because the problem seems to be that you want to read in \"\"us-west1\"\" but write in other region\",1644432387.132949,1644481377.899999,U01B6TH1LRL\\n7539e04b-1ade-477d-8c30-6649e85ec2d6,U02TC8X43BN,,,\"hey <@U02TC8X43BN> as I said in the other thread, reading and writing location must not be matching.\\nWhen you say is in europe-west6 I assume you talk about the source data, what about you development schema?\",1644447441.422219,1644481487.282719,U01B6TH1LRL\\n4ed84ffe-d521-4e6c-bf85-5800a450fed6,U02U3E6HVNC,,,Most likely that schema is in the wrong location and you can simply drop it and create it in that same location of your source data,1644432387.132949,1644481526.798139,U01B6TH1LRL\\n877e101a-1367-4634-82a6-c325f6695758,U02VBG59VQ9,,,\"Please do not post your code directly on the channel, thanks!\",1644458029.042529,1644481543.270109,U02T941CTFY\\ne59224c0-e343-4611-ba09-6a96b2112eb5,U02TNEJLC84,,,One can always disable query caching in the query editor options,1644456177.655079,1644481645.990879,U02UKBMGJCR\\neff7c066-a503-4202-9164-fc390bcd100d,,1.0,,\"Just completed watching the solution video for week 2\\'s homework; I enjoyed the thought process and debugging process in all the coding videos, and also found it quite funny at the surprise when things work smoothly. Seems like fixing bugs is truly 80% of the work :joy:\",1644481830.422839,1644481830.422839,U02T941CTFY\\n71c4f95d-fdd6-48f7-b553-a8567ae8367e,U02TC8X43BN,,,<@U01B6TH1LRL> must or must not be matching?,1644447441.422219,1644482269.686579,U02UKBMGJCR\\n50dccaa4-4471-4f09-8e20-26769991ee03,U02TC8X43BN,,,\"must no be matching, hence the error.\\nRead location (i.e. trips_data_all.green_tripdata) and write location (i.e. dbt_victoria) must have the same location.\",1644447441.422219,1644482527.203719,U01B6TH1LRL\\n4b428b66-bb13-4030-89ca-0798d3421b7c,U02TC8X43BN,,,\"I am not sure what or where source data and development schema are. On bigquery when I check the details of my table is where I get the info it is in europe-west6. \\nNot sure where else there is a location information.\",1644447441.422219,1644482543.732509,U02TC8X43BN\\n02336fa1-9c8f-4230-a8fc-bbd04e4920e4,U02U3E6HVNC,,,\"Ok here it says europe-west6, where is the other location detail?\",1644432387.132949,1644482659.672449,U02TC8X43BN\\nf8f469ec-efe8-4bc8-8fac-07e4dd64701c,U02U3E6HVNC,,,\"Ah actually maybe not, it is the dataset detail, I am on my phone now, I will check later sorry\",1644432387.132949,1644482705.735949,U02TC8X43BN\\n110668a6-88f0-4cce-bdbf-4c74a958459b,U02T941CTFY,,,\"Completely agree. It\\'s always more interesting to look at the process of fixing bugs and finding out why those bugs occur than just to watch when everything works fine. In addition to understanding the tool of work, learn to think how to find and eliminate errors.\",1644481830.422839,1644483002.687809,U02RC9GPNG0\\ne3238247-f3b9-4e50-a2bf-03f320b3fdfc,U02TC8X43BN,,,\"Colloquially speaking, I got stupid that I had to re-check what the phrase \"\"must not\"\" means, which is ~prohibited so \"\"_reading and writing location must not be matching_\"\" means to me \"\"locations cannot match\"\" :slightly_smiling_face: nvmd, I do agree with your last comment and that\\'s what I\\'ve experienced as well\",1644447441.422219,1644484320.375309,U02UKBMGJCR\\n87a7b2eb-110c-409f-92fb-e8cf672c6383,U02U3E6HVNC,,,Ok so I have my first DBTmodel table on US,1644432387.132949,1644484969.824479,U02TC8X43BN\\na6257313-952c-44c7-bd5b-4d05bf85764b,U02TC8X43BN,,,\"If you go to the three dots next to your dataset (dbt_...) and click open you\\'ll get to see the location, that would be the write to location because it\\'s where it will create your models.\\nYou could even try to create a table in there as select * from trips_data_all.green_tripdata from bigquery and check the error, it should be the same.\",1644447441.422219,1644487106.008719,U01B6TH1LRL\\n7fc3a021-f35d-42e5-a978-c0775b06e2bc,U02U6DR551B,,,\"<@U02SEB4Q8TW> Hi Will, you need to set the start_date &amp; end_date in the default_args. And the schedule_interval too. Let say you set the schedule interval as @monthly, and start_date is 2019-01-01. Once u enable the DAG, it will start &amp; run the next datetime\",1643873524.132099,1644487662.116739,U02T697HNUD\\n5eb40c91-3030-4664-8273-62657a9e1c1b,,5.0,,\"Hello everyone! I have a strange problem with BigQuery. I had a problem running a task fhv_tripdata_2020-01 on airflow, as there were null strings, thus I decided to manually clean the data, load acquired parquet file into a docker image and run the task straight from load_to_gcs task. This step succeeds, but bigquery external table task runs for extremely long time and finally fails. When I tried to create an external file transfer job through gcloud console web interface, it failed, stating that project_id:trips_data_all.external_table is not allowed for this operation because it is currently a EXTERNAL. Could someone explain me what am I doing wrong? Everything except this specific month loaded perfectly from airflow ;(\",1644489638.932369,1644489638.932369,U02UECC4H6U\\n1b711855-2e41-443a-9371-86e4365d3c5f,U02UECC4H6U,,,Taken from week 2 HW github page,1644489638.932369,1644490649.239239,U02HB9KTERJ\\n38af51ae-c624-4b56-84f8-17a163841fdb,U02UECC4H6U,,,You don\\'t need fhv_tripdata_2020-*,1644489638.932369,1644490732.000169,U02HB9KTERJ\\nfc92d05d-b660-44f2-ae69-006c557f0e87,U02UECC4H6U,,,\"<@U02HB9KTERJ> well, thanks for your reply. Anyways, can you please share how should the bigquery dataset look like? Because I\\'m pretty sure something went wrong and it didn\\'t really import any tables for me :///\",1644489638.932369,1644490935.954189,U02UECC4H6U\\na2a647dd-b1f0-44dc-9062-aa1e43515d13,U02VBG59VQ9,,,\"<@U02VBG59VQ9> Also, we probably won\\'t need the `ehail_fee` column in the end. So a quick workaround for me was to skip this column in the last dag task. I simply added `EXCEPT (ehail_fee)` to the SQL query at line 67 like this `SELECT *  EXCEPT (ehail_fee) FROM {BIGQUERY_DATASET}.{colour}_{DATASET}_external_table;\"\"`\",1644456122.637809,1644491744.140799,U031T71PEL8\\n3a1f4ddf-9d11-4185-9c98-d2bfe7c59d13,U02CZA4HX99,,,I don\\'t know... do you delete the table before you starting to insert?,1644428185.308419,1644492953.162149,U01AXE0P5M3\\neab81a99-badb-47bb-b9f8-7b5898dc6520,,26.0,,\"To anyone who wants to setup dbt on Docker, I\\'ve created a quick guide that should help you setup dbt with Big Query as your data warehouse. You just need the official Dockerfile and the docker-compose.yaml I created for easier setup.\\n\\nI couldn\\'t find a straightforward way to setup dbt on Docker and had to google quite a few things (also because Docker is new to me) to get to the final setup. So thought of sharing this with anyone who wants to do the same.\\n\\nI took this route because I wanted to bolster my learning with Docker and create a setup that one might actually use in real-world if there is no dbt cloud available.\\n\\nLet me know if you try this and find anything in the guide that is hard to follow :D\\n\\n<https://github.com/ankurchavda/data-engineering-zoomcamp/blob/main/4_dbt/docker-setup.md>\",1644493645.074519,1644493645.074519,U02TATJKLHG\\nf1f267b2-dd42-4db5-9486-6fab0fb31bda,U02TATJKLHG,,,\"Also, you can use the same setup with Postgres by tweaking a couple of things.\",1644493645.074519,1644493707.945779,U02TATJKLHG\\n77fe3630-cd89-4727-995b-1604ffc99ef8,U02CZA4HX99,,,\"No, I didn\\'t\",1644428185.308419,1644496069.745199,U02CZA4HX99\\ne9d839ad-558f-4f20-8413-e12295fcf096,U02CZA4HX99,,,Maybe that\\'s the reason,1644428185.308419,1644497349.512719,U01AXE0P5M3\\n199cce1a-6cdc-4155-998d-9fa9dbd799a2,U02TATJKLHG,,,\"Hey, feel free to send a PR to the course repo. We can include it in the setup of the week4 if someone else wants to go that way.\",1644493645.074519,1644497530.390499,U01B6TH1LRL\\n3b8a4eda-07d8-4231-8ec4-cb776d7f325f,U02TATJKLHG,,,\"Hey Victoria,\\n\\nSure! Would be glad to do it. Will create a PR soon :D\",1644493645.074519,1644497867.702939,U02TATJKLHG\\n8098774c-4eb2-4d62-9091-6cccbe5f8dda,U02TATJKLHG,,,Thaaank you :raised_hands:,1644493645.074519,1644497958.365249,U01B6TH1LRL\\ne8f4e0c8-ab65-4655-90a4-e0fc2a71ffcb,,3.0,,\"Is there a way to make dbt-cloud store all its stuff in a subdirectory, not in a root of a github repo?\\nAnother way may be using sparse checkout, but it looks too complicated.\",1644498137.958639,1644498137.958639,U0301PJRDNV\\nb4545000-9fe8-47c4-bfdb-104aace8e1ea,U02T0CYNNP2,,,\"Not me, even the total count differs. Also getting different results in the data processed depending on the query itself.\",1644432298.405069,1644498306.508219,U02LQMEAREX\\n0c516844-985b-41d5-b57d-8d155271a465,U0301PJRDNV,,,\"there\\'s an option when you set up the project, under advanced. Let me find it to send you a screenshot\",1644498137.958639,1644500231.807219,U01B6TH1LRL\\n52427b6c-d461-4dd7-b5e3-e0e61b0e5636,U0301PJRDNV,,,This is the very first configuration you\\'ll see when setting up a new project. dbt cloud will create that folder in your repo and init the project there,1644498137.958639,1644500431.850979,U01B6TH1LRL\\n773996ea-9a0d-4767-a17b-328afc6db0d2,U0301PJRDNV,,,Thank you! It turned out I’d already specified it. But I was confused by GitHub root presented in a project.,1644498137.958639,1644500922.514289,U0301PJRDNV\\n557e750d-cfb6-4816-af86-90c2dad44a57,U02TNEJLC84,,,Oh wow. Thanks. I must have been looking at an old Stack Overflow post when I was trying to figure out how to do this.,1644456177.655079,1644502502.903159,U02TNEJLC84\\nf6d054ab-ce58-4190-af0b-5e71f8ef5edf,,4.0,,please anyone familiar with this error? upload to gcs dag was working last week but can get it to work now. same code,1644502594.037799,1644502594.037799,U02U5G0EKEH\\n78215058-3c46-42bc-9c5d-2f9e2847ceb9,U02U5G0EKEH,,,Your credentials aren\\'t correct somehow. Did you change the credentials (.json file) name? Did you put on the path they say?,1644502594.037799,1644502747.977379,U02CD7E30T0\\nafc889d3-bbb0-43f8-a972-4b1909d4e58c,U02U5G0EKEH,,,i did not change the credentials. have had it in same location it worked few days ago,1644502594.037799,1644502830.363769,U02U5G0EKEH\\n12ffe651-aeb1-4a67-97dd-51faf624564e,,1.0,,Hello. Is there a list somewhere with homeworks deadlines?,1644502967.042329,1644502967.042329,U02V24WAZRN\\nf753307d-2720-4916-8a9b-af782b025724,U02V24WAZRN,,,\"Each homework has the deadline in their md file, no list with all of them\",1644502967.042329,1644504019.751479,U01B6TH1LRL\\nd410b7b5-6a66-4703-8985-be04a781c1f1,U02U5G0EKEH,,,found this error when i checked further,1644502594.037799,1644505073.986979,U02U5G0EKEH\\n226fd26b-6ef5-4d18-bd3b-8874ed7aa65e,,2.0,,\"Hi, there. I had a late start and on the Week 1. :smile:\\nI\\'m having an issue with pgAdmin4, so am asking for your help!\\n\\nI made a connection between Postgres and pgAdmin4 by using `--network` stuff, but pgAdmin4 didn\\'t show a table(`yellow_taxi_data`).\\nI am able to find the table(`yellow_taxi_data`) via pgcli. What might be the reason for this?\\n\\nThank you in advance!\",1644505761.774029,1644505761.774029,U02DR2B6BC4\\n4db81409-223b-4505-a482-41f754bd9147,,4.0,,\"Will be require the `ehail_fee` column for the green taxi data?\\n\\nWas getting a datatype problem ingesting this data into BigQuery, and simplest solution is to just remove the column.\",1644507124.002539,1644507124.002539,U02U34YJ8C8\\n08e1d6c3-33f9-41b9-aca1-af36c7fac292,U02U34YJ8C8,,,\"<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1644429302569509> see this thread, there are different solutions. I personally chose to just dump csv instead of parquet in GCS for the green taxi, and this does not propagate the column conversion issue as when you convert to parquet\",1644507124.002539,1644511028.739329,U02UA0EEHA8\\n16dc5e32-4252-4c68-8177-1ec3543a16c7,U02DR2B6BC4,,,maybe try to refresh it? sometimes it helps,1644505761.774029,1644512284.598969,U01AXE0P5M3\\nbb806fca-eee4-4b66-84aa-f93fefb18a1b,,1.0,,\"Hi, I have a doubt. In the part where we execute the DAG, should we have previously created the bucket where it will be stored? because when executing the DAG it throws me that error that it can\\'t find the bucket\",1644512497.848359,1644512497.848359,U030PFH5CRX\\nbf52d55f-7d57-4cc9-aa1f-9f9675de49a4,U030PFH5CRX,,,The bucket should have been created when you ran terraform. Once you run terraform apply you will need to copy the bucket name into your docker-compose.yaml file.,1644512497.848359,1644512566.151629,U02TNEJLC84\\ncfeef77c-c9fa-4e48-aa8a-3acdd295c1af,U02TC8X43BN,,,\"So yeah my dbt thing is created in the US but otherwise my trips_data_all is in europe.\\nIs there a way to prevent writing in the US, was there a setting I missed?\",1644447441.422219,1644512695.983329,U02TC8X43BN\\nefb04db8-025d-43fe-8c6e-8ed1ad8af6a3,U02TC8X43BN,,,\"just drop the schema and recreate it in the same location, it\\'s one of the settings you define when creating it (if you do it via UI) but if you execute the create schema command looks like it uses a default one.\\nWorth adding to the FAQ document or maybe even readme for the setup. What do you think?\",1644447441.422219,1644513230.353339,U01B6TH1LRL\\na16def33-8de0-48cd-a802-987357b4e9c8,U02U3E6HVNC,,,Ok well I fixed by deleting the dataset in google bigquery and recreating but specifying the region I wanted.,1644432387.132949,1644513765.861009,U02TC8X43BN\\n1a0d3b32-8e56-445c-9179-f71d04a603d1,U02TC8X43BN,,,Ok well I fixed by deleting the dataset in google bigquery and recreating but specifying the region I wanted.,1644447441.422219,1644513774.702219,U02TC8X43BN\\n9e8b5ccf-e010-4466-8a0b-ef7bbe012353,U02TC8X43BN,,,*ah I did not see your message but that worked indeed,1644447441.422219,1644513793.333449,U02TC8X43BN\\n034e6344-9e97-488c-941e-77c48faa416a,,5.0,,\"This is related to week 4. When running fact_trips model, with test mode false, I am getting following error, `Bad int64 value: 1.95` . The error is cryptic, I am unable to find the row causing this issue.  This is with BigQuery. Any help?\",1644514706.213179,1644514706.213179,U02S9SSURMH\\n6d616736-bddf-4a0c-afce-8f5997d7ede2,U02U5G0EKEH,,,\"Fixed. thanks. (i had commented the authentication part of my airflow, hence the problem)\",1644502594.037799,1644514738.978969,U02U5G0EKEH\\n7b70ed67-8489-420b-bb07-9609f159d2a8,,,,*Data Talk Club: Data Engineering Course - Week 3 Summary:*,,1644516087.654199,U02S4A9TA5Q\\n21578a64-7857-48c4-93aa-3046aada0dc7,,2.0,,\"hello\\ni am trying to this query in bigquery but coming across an error at unknown option: format:\\n\\n```CREATE OR REPLACE TABLE `.trips_data_all.yellow_tripdata`\\nOPTIONS (\\n    format = \\'PARQUET\\',\\n    uris = [\\'<gs://dtc_data_lake_>&lt;&gt;/yellow/yellow_tripdata/2019/yellow_tripdata_2019*.parquet\\', \\'<gs://dtc_data_lake>&lt;&gt;yellow/yellow_tripdata/2020/yellow_tripdata_2020*.parquet\\']\\n);```\",1644516749.597819,1644516749.597819,U029DM0GQHJ\\nf1969f90-b2ac-4a97-b5a6-d54736c19b5a,U02S9SSURMH,,,Are you using big query or Postgres?,1644514706.213179,1644516924.481999,U01B6TH1LRL\\ne6d7a3b9-0560-47a1-9818-6d55a6dd7c48,U029DM0GQHJ,,,\"I noticed a few things\\n\\n• you dont\\' have project name\\n• `&lt;&gt;` in the bucket name \",1644516749.597819,1644517294.072829,U01AXE0P5M3\\nc357821f-f1ee-47c5-b0be-72b1b5e7c6f8,U02U3E6HVNC,,,\"Oh I see, I was checking trips_data_all but not dbt_zeryck. Doy. I will add this to the FAQ! Thanks Vic.\",1644432387.132949,1644517328.090269,U02U3E6HVNC\\n528421a4-6e8b-4873-a374-7d981997f310,,3.0,,\"Not a DE question, but about how the course is made. What software is used to record the facilitator’s screens when they are presenting material? It does not seem the be the same across everyone. I feel like Loom is used, but I’m just trying to confirm\",1644518472.082729,1644518472.082729,U02TEERF0DA\\ne67dde22-04d4-433e-85db-9491ff72b440,U02TEERF0DA,,,\"I use loom, yes\",1644518472.082729,1644519237.881709,U01AXE0P5M3\\n0660a5f0-8730-49f0-8aaa-f22af177137b,U02TEERF0DA,,,I know that Victoria uses OBS,1644518472.082729,1644519263.178469,U01AXE0P5M3\\n32e1fb25-7ccc-48d4-a934-88000fc0d6aa,,4.0,,\"Hello all. I had to change INPUT_FILETYPE from \"\"parquet\"\" to \"\"PARQUET\"\" on this DAG for it to work. Not sure if it was a version problem.\\xa0\\n\\nDAG in question: <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_3_data_warehouse/airflow/dags/gcs_to_bq_dag.py>\\n\\nAnd from the bigquery docs for sourceFormat: <https://cloud.google.com/bigquery/docs/reference/rest/v2/tables>\",1644520030.164979,1644520030.164979,U02TJ2U397D\\n4d0b7c07-7133-42df-a366-26211d62af1e,U02S9SSURMH,,,BigQuery,1644514706.213179,1644520111.527749,U02S9SSURMH\\n0683cee3-c152-4416-acd7-cbeac2c8a97d,U02S9SSURMH,,,\"found it :see_no_evil:\\nI\\'ll debug and get back to you soon\",1644514706.213179,1644520775.851369,U01B6TH1LRL\\nea1e104e-b951-4ad0-ba0a-941c950d890d,U02TJ2U397D,,,\"Yes, this has been mentioned in other posts as well as the video. Sejal had uploaded CSV files to her bucket and created the DAG accordingly.\",1644520030.164979,1644520885.973059,U02BVP1QTQF\\n8ac9f46a-a928-40de-9702-ec46a24eb071,U02TJ2U397D,,,My bad. Is it the 3.3.3 video?,1644520030.164979,1644521174.064979,U02TJ2U397D\\n0189f95e-a98e-41b6-b60f-753096dda61f,U02CZA4HX99,,,Thank you! I got it fixed,1644428185.308419,1644521239.023509,U02CZA4HX99\\nfd9feb99-8a46-46d2-9342-c881c0c8842e,U02U3E6HVNC,,,\"Oof called that too soon. Now when I try to run my model, it fails with the same error, and I see a new dataset \"\"dbt_szeryck\"\" automatically created in BQ--one with location US. I haven\\'t figured out why this is happening yet.\",1644432387.132949,1644521920.420749,U02U3E6HVNC\\n5eae4903-e1c5-45c1-a59a-456404ac8656,U02S9SSURMH,,,\"Looks like with the data I developed the project with ehail_fee was 0 or null and once I loaded the whole amount of both 2019 and 2020 start failing as well.\\nI just sent a commit to fix it and now it\\'s running everything fine. In the staging model change the casting to numeric instead of int\",1644514706.213179,1644522158.310469,U01B6TH1LRL\\n85ad28fd-1de1-4da6-b3a8-cc05667d5061,U02U3E6HVNC,,,is it the exact region in the US? I read some people had problems with that,1644432387.132949,1644522239.401329,U01B6TH1LRL\\nd02687a2-d630-419f-87a7-fd3f30c724f6,U02U3E6HVNC,,,Fixed it by deleting dbt_szeryck and creating a new schema with that name and the correct location. Apparently the name dbt_zeryck wasn\\'t good enough :sweat_smile:,1644432387.132949,1644523410.079959,U02U3E6HVNC\\n682a19f3-5b39-49c5-8560-d00b2c5315eb,,8.0,,\"Hi,\\nIn week 4, Does anyone got this error ,\\n`Runtime Error in model stg_green_trip_data (models/staging/stg_green_trip_data.sql)`\\n  `404 Not found: Dataset clear-rock-338918:nytaxi was not found in location US`\\n```I don\\'t know why location is US, my all setup  for GCP is in europe-west3 (Frankfurt)```\\n\",1644523445.824959,1644523445.824959,U0297ANJF6F\\n48411af5-53c8-4532-be5f-7e657e6018be,U0297ANJF6F,,,Can you check the location of the schema you have created for dbt to write the models to? (Something like I have dbt_victoria_mola),1644523445.824959,1644524120.062979,U01B6TH1LRL\\nd56719b2-a898-46a7-a6b2-5cbc4325eec4,U0297ANJF6F,,,\"You probably have it in the US, unlike the region of the trips_data_all and its tables. Just drop it and recreate it in the right region\",1644523445.824959,1644524169.724609,U01B6TH1LRL\\nb4738948-f675-4cfa-ad70-3075df2ee537,U02TJ2U397D,,,\"yes, she mentions it in passing somewhere in the video.\",1644520030.164979,1644525397.021769,U02BVP1QTQF\\nfc5c1b68-7acb-4390-ac6f-b457bcb8e7dc,U02TJ2U397D,,,Thanks for reporting. I\\'ll change the case.,1644520030.164979,1644526700.584229,U01DHB2HS3X\\n0222d174-7a30-47ae-9662-8c7672766cc5,,5.0,,\"Hello, please any clue for this error?\",1644527800.138399,1644527800.138399,U02U5G0EKEH\\n7bd6a44d-3f2a-480a-b4f9-5068ca191cb9,U02U5G0EKEH,,,\"Looks like you missed some rights when creating the service account, can you share what\\'s the setup?\",1644527800.138399,1644527995.426059,U01B6TH1LRL\\n62822b1f-2f48-4688-af4c-5283d3145f6f,U0297ANJF6F,,,\"I checked the schema created for dbt, Data location is US\",1644523445.824959,1644528149.298769,U0297ANJF6F\\n173abbed-9020-442f-9959-1c5610869092,U0297ANJF6F,,,But we are not mentioning the location in the dbt project explicitly,1644523445.824959,1644528201.223519,U0297ANJF6F\\n8efb4c74-4db7-4167-8fe9-5219cbd3b0f7,U0297ANJF6F,,,\"no, because you should be able to use any location you want. The problem is not the location itself but the fact that it\\'s reading from a location (trips_data_all) to write in another (dbt_victoria).\\nYou can specify the location in the schema.yml if you are running it locally, but the issue would persist as long as those locations do not match\",1644523445.824959,1644528529.728339,U01B6TH1LRL\\ne3958fbb-f794-4a45-a8f8-7d2f48c76aee,,,,\"Hi,\\nIn week 3, Does anyone got this error ,\\ndocker: Error response from daemon: mkdir C:\\\\Program Files\\\\Git\\\\serving_dir: Access is denied. While trying to run the tensor serving image\",,1644528765.169569,U02T0CYNNP2\\n8a08d35e-b0cc-4090-be4e-25aa0da6719c,U02U5G0EKEH,,,here is the setup,1644527800.138399,1644528789.190849,U02U5G0EKEH\\n05b171c0-9bd2-4bde-a705-8ec7ceb37b74,U0297ANJF6F,,,\"Yes, I understand location mismatch is happening(when I run the model in dbt cloud) dbt cloud i creating schema in location US and it expecting my dataset (trips_data_all) should be in location US but it is in europe-west3 (Frankfurt).\",1644523445.824959,1644528905.345739,U0297ANJF6F\\n1b25201f-7076-4ac0-af4a-2e883c4438f3,U02U5G0EKEH,,,solved. typo error,1644527800.138399,1644531626.589409,U02U5G0EKEH\\n39d3acb6-3f9f-43d8-9f61-be21a9f7c0cc,,10.0,,Did anyone else get error in partitioning query that the expression is wrong? I have tried different ways to cast `tpep_pickup_datetime` to timestamp but none worked. Error message in the first comment.,1644533816.628969,1644533816.628969,U02Q7JMT9P1\\nace91dcc-4f82-44a6-9577-76e09502e0a0,U02Q7JMT9P1,,,\"```google.api_core.exceptions.BadRequest: 400 PARTITION BY expression must be DATE(&lt;timestamp_column&gt;), DATE(&lt;datetime_column&gt;), DATETIME_TRUNC(&lt;datetime_column&gt;, DAY/HOUR/MONTH/YEAR), a DATE column, TIMESTAMP_TRUNC(&lt;timestamp_column&gt;, DAY/HOUR/MONTH/YEAR), DATE_TRUNC(&lt;date_column&gt;, MONTH/YEAR), or RANGE_BUCKET(&lt;int64_column&gt;, GENERATE_ARRAY(&lt;int64_value&gt;, &lt;int64_value&gt;[, &lt;int64_value&gt;]))\\n\\n(job ID: airflow_gcs_2_bq_dag_bq_ext_2_part_task_2022_02_09T00_00_00_00_00_f7e12e9e94801d0421a8528935852994)\\n\\n                                                                                                                               -----Query Job SQL Follows-----                                                                                                                                \\n\\n    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |\\n   1:CREATE OR REPLACE TABLE datatalks-de-zoomcamp.trips_data_all.yellow_tripdata_partitoned                                  PARTITION BY                                     DATETIME(tpep_pickup_datetime) AS                                  SELECT * FROM trips_data_all.external_table;\\n    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |```\\n`PARTITION BY  DATE(tpep_pickup_datetime)` gave the same result\",1644533816.628969,1644533853.173549,U02Q7JMT9P1\\n401af150-4f07-454e-b64f-54dbb18d63d4,U0297ANJF6F,,,\"Finally, It worked but I have to create new data-set  `all_trips_data`  location to the US. Because dbt is by default creating schema at US location.\\nI didn\\'t get any information about the, how to set   the location of the dataset from  the dbt-cloud schema.yml.\",1644523445.824959,1644535110.970639,U0297ANJF6F\\n010b34f6-a5dd-474c-a90c-6e741b3477ff,,6.0,,\"I\\'m trying to create a external table to green taxi data but I\\'m getting this error in Airflow and in Bigquery\\n\\n```Failed to create table: Invalid field name \"\"0\"\". Fields must contain only letters, numbers, and underscores, start with a letter or underscore, and be at most 300 characters long.```\",1644536483.340339,1644536483.340339,U02TC704A3F\\n6c72f017-1ff2-44d7-89a3-26885fabbe03,U02TC704A3F,,,\"I used <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_3_data_warehouse/extras/web_to_gcs.py|web_to_gsc.py>, I think pandas dataframe operation created a column \\'0\\' in the data file and exported with this to .parquet\",1644536483.340339,1644537351.458179,U02TC704A3F\\n180b0a47-92cc-46b1-8b87-3806c2cc2319,,10.0,,For Question 4 in Week 3 Homework I realize that a lot of people are not getting the exact answers to match up as far as estimated and actual and there has been a recommendation to just use the closest numbers that match. I\\'ve been struggling with this since yesterday because both estimated and actual are the same for me. After re-watching the videos and re-reading the homework a few times it occurred to me that *WE NEED TO IMPLEMENT A TABLE BASED ON OUR ANSWER TO QUESTION 3 AND THEN USE THAT TABLE TO DETERMINE THE ANSWER TO QUESTION 4.*  I don\\'t think the trickiness of this question was intended by the author of the questions. Just wanted to post this here in hopes of saving some fellow students some time and head scratching.,1644539007.881829,1644539007.881829,U02TNEJLC84\\n74d46a51-db8e-497c-8020-32bd9f9d149d,U02Q7JMT9P1,,,@llia I did it manually in bigquery,1644533816.628969,1644543714.079469,U02T8HEJ1AS\\n3B2FD2A1-5A3A-4BB0-BA7C-3666F09A57D9,U02DR2B6BC4,,,<@U01AXE0P5M3> Thank you for your reply! I’ll try to do it and update whether it works:blush:,1644505761.774029,1644546695.875529,U02DR2B6BC4\\n41e9ac91-0d67-47b5-a040-02355d5c7054,U02TNEJLC84,,,Even implementing that table (Part. and Clust.) doesn\\'t get the correct results!,1644539007.881829,1644547480.809589,U02HB9KTERJ\\nf5dc08dd-2ed4-42c1-b41d-c64308ec09d4,U02UECC4H6U,,,,1644489638.932369,1644547645.047259,U02HB9KTERJ\\n9eef5835-1f46-4171-adc3-43bca5e6ef32,U02UECC4H6U,,,the yellow/fhv_*external_table are the ones created with the first airflow task and the yellow/fhv_tripdata are the ones created after the second task.,1644489638.932369,1644547749.442279,U02HB9KTERJ\\n91C2CED8-F4BD-4B15-B25C-2DB642EB6037,U02TNEJLC84,,,\"You are right, just try \\n```CREATE OR REPLACE TABLE `taxi-rides-ny.nytaxi.fhv_partitioned_tripdata`\\nPARTITION BY DATE(dropoff_datetime)\\nCLUSTER BY dispatching_base_num AS (\\n  SELECT * FROM `taxi-rides-ny.nytaxi.fhv_tripdata`\\n);```\\n```SELECT count(*) FROM  `taxi-rides-ny.nytaxi.fhv_nonpartitioned_tripdata`\\nWHERE dropoff_datetime BETWEEN \\'2019-01-01\\' AND \\'2019-03-31\\' \\nAND dispatching_base_num IN (\\'B00987\\', \\'B02279\\', \\'B02060\\');```\",1644539007.881829,1644548427.107389,U01DFQ82AK1\\nb7a31e36-66e7-4449-9c3a-31e5b526dc9a,U02TNEJLC84,,,\"<@U01DFQ82AK1> Error running query\\nError while reading table: dezoomcamp22.trips_data_all.FVH_tripdata_external_table, error message: Parquet column \\'DOLocationID\\' has type BYTE_ARRAY which does not match the target cpp_type INT64.\\n-----when I use----\\nCREATE OR REPLACE TABLE `dezoomcamp22.trips_data_all.fhv_partitioned_tripdata`\\nPARTITION BY DATE(dropoff_datetime)\\nCLUSTER BY dispatching_base_num AS (\\n  SELECT * FROM `dezoomcamp22.trips_data_all.FVH_tripdata_external_table`\\n);\",1644539007.881829,1644550810.332509,U02T8HEJ1AS\\nBB694BB0-1EB4-457B-90EE-06A4603BAB4A,U02TNEJLC84,,,The idea of the question was to give practical experience in seeing advantage of partitioning and clustering ,1644539007.881829,1644554398.989179,U01DFQ82AK1\\n50912E62-B15E-4643-9E09-C09E5FA0F9B5,U02TNEJLC84,,,Can you check the column format for the external table you created? ,1644539007.881829,1644556451.946789,U01DFQ82AK1\\n6A651730-DA98-4F72-9BB4-C03D36EB81E6,U02TNEJLC84,,,\"If the column dropoff_datetime is not detected as date this query will not work. \\nIn that case you might want to hardcode the schema for the external table \",1644539007.881829,1644556513.633779,U01DFQ82AK1\\n0293f1c6-acc8-4b9f-a72e-e9ce211b3318,,2.0,,Learning in public. I\\'ve just noticed that there is quite a large variation here and a lot of marks in terms of total marks given. I\\'m just curious as to why some people only get 1 mark but others 7? Not that it\\'s a big deal just curious that\\'s all.,1644557285.502449,1644557285.502449,U02U5SW982W\\na706a220-24d4-4951-8e79-96f98ac64a20,U02U5SW982W,,,\"As far as I understand, it depends on the number of post links you added in the homework form. If you posted 7times with #dezoomcamp tag after you submitted the previous homework, you should add all the 7 links to the form to get 7 mark. If you add 3 links, you\\'ll get 3.\",1644557285.502449,1644558215.793319,U0313FMNNAW\\nd5d0f86f-b9e6-47ed-abab-6490d48f1a9f,U02U5SW982W,,,oooohhhhh. Thanks so much Horacio :raised_hands:,1644557285.502449,1644558318.651789,U02U5SW982W\\nf4eb4f6c-29d1-4663-9863-f7d959f3d328,,2.0,,BigQuery \\'best\\' tables: non-partitioned v\\'s partitioned v\\'s partitioned/clustered. I\\'m just trying to get my head around what is deemed best between these tables. If you have one table - say a partitioned table and it processes 30 MB when run but takes 0.6 sec and then you have a partitioned&amp;clustered table which only processes 27 MB but takes longer - say  1.5 sec then how do you make a choice? For the partitioned&amp;clustered table it would cost slightly less in terms of what is processed (which would be cheaper for a lot of queries) but it takes almost 3 times as long. How do you say what is \\'best\\' here? Do you go for $\\'s over speed (assuming that the 3 MB difference will add up if you\\'re doing a lot of queries) or speed over $\\'s (you do NOT have the patience to wait)?,1644558804.721179,1644558804.721179,U02U5SW982W\\n2793A303-2EA3-4B57-A8EA-0CBBEAAC272C,U02Q7JMT9P1,,,<@U02T8HEJ1AS> well I tried it as well but still got this error :laughing:,1644533816.628969,1644559075.871029,U02Q7JMT9P1\\n5F1A2243-4A24-437C-95FC-E3FC8DA128A5,U02U5SW982W,,,\"When it comes to tables with &lt; 1gb of data the performance improvement of partitioning or/and clustering is not significant (rather can impact negatively). \\nGenerally when you are using data warehouse you will always prefer cost improvement. \\nBut there are some cases when you want to optimize for query performance such as real-time dashboards backed by bigquery. \\nFor a 3mb read difference I will always prefer optimizing for query, but that will also depend on your future queries and filters. Assuming you are filtering by months partitioning might help few months down the line. \",1644558804.721179,1644561170.974249,U01DFQ82AK1\\nD771344E-E1F7-45FF-B818-F39A7CD090DC,U02Q7JMT9P1,,,Check the column type in your external table ,1644533816.628969,1644561271.719629,U01DFQ82AK1\\n62f5282d-3b7a-421e-a600-d3829d598a8d,U02TATJKLHG,,,\"Using ACT. No issues for me, as of now.\",1644993561.588799,1645002912.742009,U0290EYCA7Q\\ncc1df76a-51ac-40a4-b8e3-76d07b616125,U0301PJRDNV,,,\"Same problem, my count is not matching any option in the homework.\",1644922786.198149,1645003630.908589,U02S9SSURMH\\n0bb65b36-6250-4401-b73e-0ee75e67bf16,U02TATJKLHG,,,I see. I have the one from Hathway. Changed to Jio hotspot and works fine now.,1644993561.588799,1645003658.626189,U02TATJKLHG\\n559545ef-a251-4ef4-a9a8-0fafe8a61fc8,U0301PJRDNV,,,\"and i also realised that after every single time i did a  `dbt run` or `dbt build` , the counts from `fact_trips` table will change. no codes were modified, just running `dbt run`or `dbt build` only. hmm anyone experience same? i am running dbt on cloud, connecting with BQ.\",1644922786.198149,1645004219.784279,U02ULMHKBQT\\n92de73dd-5b11-4b77-8bca-2b2197041608,,4.0,,\"Error running dbt in docker\\n\\n```\\'dbt_utils\\' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with \"\"dbt deps\"\".```\",1645004802.322059,1645004802.322059,U02HB9KTERJ\\nc53eb14c-8aba-496b-adaa-b6b293d0fd77,,1.0,,Seems that dbt cloud has some infrastructure issues. Home page loads very slowly. Is it just my experience? Or somebody also has this issue?,1645007437.737939,1645007437.737939,U02T65GT78W\\nf8a1a53b-4258-4712-ac96-9d5dac7ff8f4,U02HB9KTERJ,,,you must run `dbt deps` to install the package,1645004802.322059,1645007912.783569,U01B6TH1LRL\\nf7ee93cf-69e6-483e-ae7f-11b4a65832bd,U02T65GT78W,,,\"Looks like they are running into some issues, I received a notification<https://status.getdbt.com/?utm_source=embed>\",1645007437.737939,1645007949.477919,U01B6TH1LRL\\n94df4629-4f64-497d-b837-b37539b0a2fd,U02NSF7RYP4,,,\"Can you share the schema.yml? Does it look like the one in the course repo? also, stupiod question, but did you save before running?\",1644993278.066569,1645008049.944869,U01B6TH1LRL\\n040124f7-f4e4-42cb-810b-c2f8847fe5f8,,2.0,,I am currently strengthening my knowledge in the prerequisite for this course. Can I take it at a later time?,1645008084.435089,1645008084.435089,U01PPFXTNR2\\nf630da2b-ad05-4746-94a3-52392f074158,U02TC704A3F,,,\"it is, I forgot to remove the wip when I added the form :woman-facepalming:\",1644972230.414909,1645008139.346399,U01B6TH1LRL\\n76b490dd-d9ca-436d-b9be-1a28c664bcc3,U02TJ69RKT5,,,\"Hey, can you check the compiled code from what you are trying to run and execute it directly form BQ?\",1644968876.461069,1645008176.489189,U01B6TH1LRL\\nff8617b1-9c3e-435a-9856-27cc33822c54,U02RSAE2M4P,,,\"<@U02RSAE2M4P> aside of the question that I think was answered clearly above, anything we can help to support you with dbt if you are interested in the topic?\",1644967397.653989,1645008235.628829,U01B6TH1LRL\\nd423b772-f40f-4612-ab27-34a3ec924eb7,U02NSF7RYP4,,,I have missed schema.yml in staging dir but had it in core dir. Placed in staging dir as well. it picked up the test.,1644993278.066569,1645008305.255509,U02NSF7RYP4\\n1322720c-12be-44e9-a033-35522ffb7ff9,U02NSF7RYP4,,,Thanks,1644993278.066569,1645008311.212219,U02NSF7RYP4\\n312ced7e-fe0d-4696-aad5-300da86e0096,U02NSF7RYP4,,,\"```10:43:50  Found 5 models, 11 tests, 0 snapshots, 0 analyses, 376 macros, 0 operations, 1 seed file, 2 sources, 0 exposures, 0 metrics\\n10:43:50  Concurrency: 4 threads (target=\\'default\\')\\n10:44:00  Finished running 11 tests in 10.23s.\\n10:44:00  Completed successfully\\n10:44:00  Done. PASS=11 WARN=0 ERROR=0 SKIP=0 TOTAL=11```\\n\",1644993278.066569,1645008344.994269,U02NSF7RYP4\\n496e3906-94e5-41f1-b52a-baae60ff2765,,13.0,,\"Anyone used the Python code to ingest the parquet files faster into GCP (without Airflow) ?\\nI think the script is incorrect but I am not sure because I uploaded manually but used part of the script to download.\",1645010628.987459,1645010628.987459,U02CD7E30T0\\n4a74fb78-066e-4633-a05d-b5ed12508795,U01PPFXTNR2,,,\"Yes, definitely\",1645008084.435089,1645010802.891079,U01AXE0P5M3\\n045a28a4-7c49-4e08-b1d6-8deb1770f399,U02CD7E30T0,,,Why do you think it\\'s incorrect?,1645010628.987459,1645010835.104459,U01AXE0P5M3\\n93da7272-5d74-4711-be4c-d1fa8ec05c4b,U02CD7E30T0,,,\"Because when downloading it created on extra row. Then when converting to Parquet it was only one column named 0 with all the columns.\\nMy intuition says that this is the problem:\\n`r = requests.get(request_url)`        \\n`pd.DataFrame(io.StringIO(r.text)).to_csv(file_name)`\\n\\nBecause we  are extracting a csv file and then we are converting to csv\",1645010628.987459,1645011131.337029,U02CD7E30T0\\n535186ba-bbce-4f1b-aa9c-ec0b10f09e0a,U02CD7E30T0,,,\"Since I have one latest version of pandas I just did\\n```df = pd.read_csv(request_url)```\\nand then\\n```df.to_parquet(file_name, engine=\\'pyarrow\\')```\\nAnd it work fine for me\",1645010628.987459,1645011236.855249,U02CD7E30T0\\n8cde2e2d-2648-48a8-a502-89fb412bf7cc,U02CD7E30T0,,,\"Maybe Sejal will answer this, but I think the intention was to save it to both parquet and csv so you can choose which one you want to upload\",1645010628.987459,1645013642.355369,U01AXE0P5M3\\nafaaa8e6-d011-4dbb-9ae4-057fe8f112c0,U02CD7E30T0,,,but yea I\\'d download the csv file directly as well skipping the pandas part,1645010628.987459,1645013686.310699,U01AXE0P5M3\\n7bb112e7-a892-45bd-8e83-c45caa1b550c,U02CD7E30T0,,,ah ok then.,1645010628.987459,1645013745.531449,U02CD7E30T0\\n08579d34-9474-4a71-a9cb-1d56daea9574,U02RSAE2M4P,,,\"Thanks <@U01B6TH1LRL>. I watched all videos for the classes already and implemented the solutions alongside with you in class. I understand the general picture and the final expectation, just can\\'t seem to grasp the idea of the concepts, like macros, seeds etc. I\\'m trying to follow the notes by Alvaro Novas and try a second implementation of the codes again.\\n\\nIf you have any articles I can read to help my understanding, pls let me know.\",1644967397.653989,1645015198.436239,U02RSAE2M4P\\n39af1445-cb06-4bb1-8c76-4f0e9b48314c,U02HB9KTERJ,,,\"Thanks. I had to place `packages.yml`  next to `dbt_project.yml` with the following contents to make `dbt deps` work.\\n\\n\\n```packages:\\n  - git: \"\"<https://github.com/dbt-labs/dbt-utils.git>\"\"\\n    revision: 0.8.0```\",1645004802.322059,1645015401.853469,U02HB9KTERJ\\n7d2e3a8c-f29d-46c3-9767-e92b07450ddf,U031TPVD3B4,,,\"I use dagster extensively, also in production. Not with gcs, but it doesn\\'t  matter where you run as long you use the Kubernetes deployment :slightly_smiling_face:\\n\\nIf you are interested in hands-on, I wrote a big part here about dagster: <https://www.sspaeti.com/blog/data-engineering-project-in-twenty-minutes/>.\\n\\nAnd as well here: <https://www.sspaeti.com/blog/business-intelligence-meets-data-engineering/#8220use-notebooks-to-open-up-the-data-silos8221>\\n\\nPlease let me know if you have any questions.\",1644079142.808109,1645015926.997119,U02LDQHTEJ0\\ndc945e39-1297-470f-a0e9-d1c1b6b60c83,U02CD7E30T0,,,\"For green_tripdata it is easier to create table from csv files (parquet needs special casting on some columns), that might be the reason for both csv and parquet uploads\",1645010628.987459,1645018705.587459,U02DD97G6D6\\n5f90f317-57b9-411a-b212-1a686f2acd73,U01PPFXTNR2,,,\"ok, Thanks for the response\",1645008084.435089,1645018877.524629,U01PPFXTNR2\\n669d7ea1-a7bb-411b-8fa3-e46b983b3004,,1.0,,Will questions be answered from time to time after the zoomcamp is over? If not I’ll just google stuff until I figure stuff out. I plan to follow along after work gets less busy.,1645019083.968999,1645019083.968999,U02QQEQGTV2\\n36b4c100-b8ef-4502-9835-cc78c39e74b2,U02QQEQGTV2,,,Yes some questions will be answered. But hopefully FAQ will help,1645019083.968999,1645020144.549609,U01AXE0P5M3\\n,USLACKBOT,5.0,tombstone,This message was deleted.,1645022002.218639,1645022002.218639,USLACKBOT\\n943e6c32-bb7b-46eb-8170-5ae0702c3636,U02CD7E30T0,,,\"<@U02CD7E30T0> I can confrim. I have used the code and got an error creating an external table: `Invalid field name \"\"0\"\". Fields must contain only letters, numbers, and underscores, start with a letter or underscore, and be at most 300 characters long.`\",1645010628.987459,1645022156.737829,U02Q7JMT9P1\\n3672acfe-3c12-4de7-a757-7ded37e0b41a,U02CD7E30T0,,,\"<@U02DD97G6D6> I don\\'t think that is the case. The problem is that we are converting to CSV a file that is already a CSV. So we only need to download the file, read as csv (read_csv from pandas) and then convert into parquet.\",1645010628.987459,1645022299.994599,U02CD7E30T0\\n5bdd3656-f3e3-4f75-b0ab-e375bdb6b211,U02CD7E30T0,,,Am I correct <@U01DHB2HS3X> ?,1645010628.987459,1645022329.759089,U02CD7E30T0\\n9733fb73-5d6b-4c6b-a4b3-f98ff0973fae,USLACKBOT,,,\"Also note I have tried already writing\\n\\n pgcli -h localhost -p 5432 -u root -d ny_taxi\",1645022002.218639,1645022525.153589,U0330DCCDN1\\n757443c3-5e0f-499b-97c8-435f81e28f3f,USLACKBOT,,,\"also note these recommendations on how to ask questions:\\n\\n<https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/asking-questions.md>\",1645022002.218639,1645022774.149249,U01AXE0P5M3\\nd2661288-409b-442d-ac84-fcc40c195de2,U02TA6MJZHR,,,\"it\\'s correclty asssigned in my .yaml file, still getting the same error\",1644884972.605279,1645022958.830729,U02TA6MJZHR\\nf8e17d96-ef57-4d49-a848-fc5e382af6af,U02TA6MJZHR,,,is there anyway to continue to week3 using the tranfer service?...noting that it does not migrate zone data and also doesn\\'t change them to parquet format,1644884972.605279,1645023013.018139,U02TA6MJZHR\\n4755446d-41a6-4c26-bb90-00c659350795,U02CD7E30T0,,,\"btw, the script also creates two empty files for 13th months\",1645010628.987459,1645023482.773959,U02Q7JMT9P1\\n144dc9c1-c7f5-41d4-89d7-a036cd989018,,2.0,,\"Hello, where I can find info about this data engineer zoomcamp??\",1645023664.593689,1645023664.593689,U02U7Q236UV\\n11ff04ff-c17f-4100-b3e1-40efb6db652a,USLACKBOT,,,Ok cool will follow the recommendations from now onwards. Could you help me with this error now ?,1645022002.218639,1645023751.676079,U0330DCCDN1\\n1ed77bdb-6ba0-48e3-8162-bd6b39d691b8,USLACKBOT,,,Apologies for ignoring recommendations.,1645022002.218639,1645024105.522389,U0330DCCDN1\\n3eea2775-f9e7-4ecf-bf98-394eaf676001,USLACKBOT,,,\"I\\'m not good with reading from pictures, so unfortunately I won\\'t be able to help. Maybe if it was text, I\\'d have better chances.\\n\\nAlso I assume you checked FAQ and didn\\'t find your problem there?\",1645022002.218639,1645024223.472209,U01AXE0P5M3\\nb5e78bb7-1963-486f-8cb9-da579f8ed5f8,U02TATJKLHG,,,\"I was able to do this.. After running the last (fact_trips), I simply ran the generate and then serve commands as you suggested.\",1644817851.455599,1645024377.666269,U02HB9KTERJ\\n5ecc0d7c-1392-4be6-8a03-992cfcd5d5d7,U02U7Q236UV,,,<https://github.com/DataTalksClub/data-engineering-zoomcamp>,1645023664.593689,1645024583.408159,U02R09ZR6FQ\\n2d2feac3-0967-478d-a194-7fd2385b73c0,U02U7Q236UV,,,Thanks Gustavo !!,1645023664.593689,1645024793.181659,U02U7Q236UV\\n12512e59-2217-4998-8537-2b38ea8356a9,,1.0,,\"Problem Statement :\\n\\n```I pulled the docker image of postgres and is ready to accept connections in local host but when I try to connect it with pgcli -h localhost -u root -d ny_taxi, then it asks for passoword and after writing password i.e root in my case and pressing enter it get stuck there and won\\'t show anything after that.```\",1645026598.827609,1645026598.827609,U0330DCCDN1\\n64dccd02-c726-4350-b744-274b266c5c21,,2.0,,\"Hi, I\\'m a little confused about dates, so the next week it\\'s going to be about  Apache Spark?\",1645028274.101279,1645028274.101279,U02CKHHRY6Q\\nc94c2149-247a-429c-b105-57a46548130a,U02CKHHRY6Q,,,\"Yes, next week is week 5\",1645028274.101279,1645029576.614379,U01B6TH1LRL\\nc4316a44-3a8c-47bf-89a2-1ea1cd22f2a6,U0330DCCDN1,,,<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1644423146232269?thread_ts=1644423146.232269&amp;cid=C01FABYF2RG|https://datatalks-club.slack.com/archives/C01FABYF2RG/p1644423146232269?thread_ts=1644423146.232269&amp;cid=C01FABYF2RG>,1645026598.827609,1645029965.268429,U0308MF3KUH\\n99402628-88d0-4d51-8d68-384963062d7f,U02CKHHRY6Q,,,\"Oh, so the course is almost over :astonished: . Alright, thanks.\",1645028274.101279,1645031811.303559,U02CKHHRY6Q\\n1faca7f0-a1a1-4840-bbb2-0d760f841ac8,,3.0,,Will week 5 videos be released sooner? or when will they be released? monday? just so it can help with planning schedule/timeline. Thanks!,1645033366.439989,1645033366.439989,U02T9550LTU\\n58edf553-1bf1-4c35-a4a6-b0930ff52f05,,32.0,,\"Hi All,\\nCan someone share me the rows count of fact trips table in dbt_name and production dataset. I could see a different rows count in Victoria\\'s 4.5.1 video.\",1645039735.829189,1645039735.829189,U02S9JS3D2R\\ne8f8c9ae-bac6-4397-a404-0659803377d1,U02T9550LTU,,,\"I\\'ve recorded 3 out of 7 videos already. Hopefully all will be released by monday, but I\\'m already uploading the finished ones\",1645033366.439989,1645040623.163689,U01AXE0P5M3\\n8cac0427-f033-48c7-b07b-8cf6ee7f05fd,U02T9550LTU,,,\"to answer your question - no, most likely they won\\'t be released sooner as most of them aren\\'t recorded yet\",1645033366.439989,1645040646.413059,U01AXE0P5M3\\necbd5c88-c6e6-4d2f-8394-ab00186f446e,U02S9JS3D2R,,,In fact_trips on production I have 61604282 rows,1645039735.829189,1645042991.358809,U032Q68SGHE\\n09d03888-700e-42af-b77e-ac0da51957e2,U0301PJRDNV,,,\"I also have the same problem. The count for the fact_trips table in Vic\\'s video is 61636717, however my count for the same table is 61,604,279. The count for my green_tripdata_external is 7778101 and for yellow_tripdata_external 109047518.  It would nice to compare the numbers with others as I have no clue what went wrong.\",1644922786.198149,1645043134.834399,U02UVKAAN2H\\n70c3d2d8-2e95-44c8-be16-06629dbbafb1,U02T9550LTU,,,Thanks! take your time,1645033366.439989,1645043842.927219,U02T9550LTU\\ndd97fb7b-b3e3-424c-927e-7044223bed4d,U02S9JS3D2R,,,\"My fact_trips count from production with no date filter is 61,604,277, so very different from Victoria\\'s count number. I don\\'t know what I did wrong.\",1645039735.829189,1645046164.751489,U02UVKAAN2H\\n4c1248fa-14df-499c-b354-c4a037b54b48,U0297ANJF6F,,,It is possible to specify which location to use in the project setting. (They are under account settings),1644523445.824959,1645046340.565569,U02QMH54HAN\\ne88fdec2-b15a-4b03-a576-bd852da5f788,U02S9JS3D2R,,,Thank you <@U032Q68SGHE> and <@U02UVKAAN2H>,1645039735.829189,1645049907.678039,U02S9JS3D2R\\n4f20b20a-40f6-4da4-b1aa-bd206081115e,,1.0,,\"hello, I am preparing the data for homework4 and I get confused:\\n• the `yellow_taxi_data` is 24 months (01.01.2019 - 12.12.2020). is this correct?\\n• for what period is the `green_taxi_data`? the same?\",1645051870.657919,1645051870.657919,U029JNJBPED\\n1680425a-9346-4b15-a95f-31e343201ba7,USLACKBOT,,,hey <@U0300EGP2EL> did solve this yet? I’m facing this too,1644401511.198979,1645055221.651109,U02SWHZKSDN\\nc05a58bf-38b2-4963-8359-777883788d44,U029JNJBPED,,,\"Hi Cristian\\n\\nYes, for classes you will need files for yellow and green taxi data from 2019-01-01 until 2020-12-12.\\n\\n~For Week 4 homework, you will need fhv_tripdata within the same dates, 2019-01-01 - 2020-12-12.~\\n\\nFor Week 4 homework you will need Green and Yellow Taxi, from the same dates, also will be needed fhv_tripdata only for 2019.\",1645051870.657919,1645058013.939619,U02TC704A3F\\n73400771-5eab-4be9-9027-5505d9d23455,U02S9JS3D2R,,,I don\\'t know why but my count is returning 194???,1645039735.829189,1645060287.003079,U02TC704A3F\\n5a853556-4d93-448b-b1a9-5fbf25ccfa22,U02S9JS3D2R,,,I will rebuild my production and set \\'is_test_run\\' to false,1645039735.829189,1645060795.862379,U02TC704A3F\\n85628350-e4db-4da0-94fc-1c382ba54dd4,,2.0,,\"Greetings, how would we go about modelling the new york taxi data following the star schema in dbt?  Thanks\",1645061263.390639,1645061263.390639,U02SEH4PPQB\\neec05eec-bff5-48da-937d-65743b0829f1,,9.0,,\"Hey folks, I’m wondering if my fact_fhv_trips model is correct. One of the months has about 100x more records than any of the other months. If this is correct, I have my answer for question 5 on the homework, but it just doesn’t look right. The primary key I defined in dbt is a combination of dispatching_base_num and pickup_datetime, in case that\\'s relevant.\",1645062231.005189,1645062231.005189,U02U3E6HVNC\\nb231a2d8-4648-4172-bded-da874cdffa74,,6.0,,\"Is it possible to work with FHV data as parquet? I\\'m facing a lot of troubles with data types. Already had two issues:\\n• Parquet column \\'pickup_datetime\\' has type BYTE_ARRAY which does not match the target cpp_type INT64.\\n• Parquet column \\'DOLocationID\\' has type DOUBLE which does not match the target cpp_type INT64. \\nI\\'ve seen some people here saying that the problem is fhv_tripdata_2020-01, this file seems to be giving everyone headaches.\",1645062644.184629,1645062644.184629,U02TC704A3F\\n4cb00bdf-4c4e-4a5a-b63f-edc26f848144,U02TC704A3F,,,\"We don\\'t need fhv 2020 data for the homework, so I just left it out.\",1645062644.184629,1645062781.234459,U02U3E6HVNC\\na110eed8-c351-49a0-aff6-64c9353a2221,U02TC704A3F,,,\"~But the first question is asking for \\'2019 and 2020 data only\\', are you sure that we don\\'t need 2020 data?~\\n\\nYeah is 2019 and 2020 for Green and Yellow\",1645062644.184629,1645062939.783819,U02TC704A3F\\nf0fbf77c-52cc-4983-ab88-0c68ce8237b6,U02TC704A3F,,,Oh nvm,1645062644.184629,1645062961.280069,U02TC704A3F\\n0a60aeae-7545-4f26-90af-fd3154060bf9,,3.0,,\"I have successfully run everything up to video #4.4.1 (using BigQuery). However when running the dbt_build job in production, I again get the error message related to `404 Not found: Dataset project:trips_data_all was not found in location US` (I had resolved this in earlier steps). What setting needs to be adjusted to address this now in production?\",1645063164.448789,1645063164.448789,U026040637Z\\nc3f1f6f5-b8b0-4a98-8321-eb38866aa5c4,U02S9JS3D2R,,,i have a number very close to <@U032Q68SGHE> too. nowhere close to any of the available options in question 1 though :smiling_face_with_tear:,1645039735.829189,1645063171.391239,U02ULMHKBQT\\naaeced07-8cdd-4086-803a-4b1ab248a527,U02TC704A3F,,,\"Thanks for the heads up <@U02U3E6HVNC>, I was loosing my mind already haha\",1645062644.184629,1645063311.291659,U02TC704A3F\\n9544f9d2-2245-4a22-894f-94589a6d575b,U02U3E6HVNC,,,\"i have the same result as well. seems like there’s one month where amount is exceptionally high. i was doubtful, but i went to check the initial table (before doing DBT on it) and results say the same.\",1645062231.005189,1645063395.050409,U02ULMHKBQT\\nd0019b3e-aa75-4346-bffc-9e190a691f87,U02S9JS3D2R,,,61604280 is the result for me,1645039735.829189,1645063472.004019,U02TC704A3F\\ne01e1122-9ddc-465a-a0f0-67d699ce7216,U026040637Z,,,\"Hi Aida\\n\\nDBT by default will try US.\\n\\nI fixed this going under projects/my-project/connection inside dbt cloud in account settings and inserting manually.\",1645063164.448789,1645063571.982439,U02TC704A3F\\n10b93615-51b2-416f-aec4-11ef62edae20,U026040637Z,,,\"i had this problem earlier when doing the production part. looks like it was because `production` dataset was created automatically via DBT build, and it’s not ideal.\\ntry going to your BigQuery console, delete the `production` dataset, and re-create the same dataset `production` manually on the console. Do state the correct  location under `Data Location` field this time. And re-try the build multiple times (sometimes DBT and BQ don’t sync in real time)\",1645063164.448789,1645063590.650899,U02ULMHKBQT\\nea8545e1-8a15-4d36-b437-b03e62722458,U02U3E6HVNC,,,\"Huh, my source table was the same at first (ingested using Airflow), but I re-ingested the data with Sejal\\'s Python script and got way more records in the other months... I\\'ll probably just go with it to get the homework done, but it seems fishy.\",1645062231.005189,1645063909.888939,U02U3E6HVNC\\na4075275-fd33-4648-9121-1fa5f8ca8b89,U02S9JS3D2R,,,61636502 for me in dev,1645039735.829189,1645064124.338269,U02T9550LTU\\n295df634-4506-4ac7-a028-a2124ad0e404,U02S9JS3D2R,,,I\\'m also having different numbers for all \\'count rows for this table after dbt\\'. My fhv count is 36041078 :eyes:,1645039735.829189,1645066348.125529,U02TC704A3F\\n0128445b-f258-44cc-85a4-d624dc3f029f,U02S9JS3D2R,,,\"Hmm, I have ~72 million rows in the `fact_trips` table. When I filter `WHERE\\xa0EXTRACT(YEAR\\xa0FROM\\xa0pickup_datetime)\\xa0IN\\xa0\\xa0(2019,\\xa02020)` I have ~61.6 million rows (61587378) so different from Vic\\'s number and different from any of the choices in the homework form :thinking_face::man-shrugging:\\n\\nInterestingly though, my proportion of yellow to green taxis is almost exactly one of the choices (0.01 percentage points away from rounding to the exact answer) and that matches the video!\",1645039735.829189,1645070640.288389,U02T9GHG20J\\n8c382b6e-4992-4151-b05c-45aef0ffb700,,17.0,,\"Week 4 - dbt_cloud_setup.md clarification. Hi All, I feel a bit silly asking this question because it seems like it should be completely obvious but I\\'m quite lost. I\\'ve followed along step-by-step and posted this to my <https://learningdataengineering540969211.wordpress.com/2022/02/17/week-4-setting-up-dbt-cloud-with-bigquery/|blog> (the detail is here in case more is needed) but I\\'ve got to the very last bit where it says \\'paste the deploy key provided by dbt cloud\\'. Have I missed something? The only key I know about up until now is the one that we downloaded from GCP at the very beginning - the JSON file. What on earth is this dbt cloud deploy key that we were provided with? Have I missed something? I\\'m also not sure whether I\\'ve done the \\'Add Github repository\\' in the way that was intended. Any help or clarification would be greatly appreciated.\",1645071297.194149,1645071297.194149,U02U5SW982W\\n056b6e1f-4e7b-467e-87e0-9105da0b5d09,U02TC704A3F,,,Join the club of \\'loosing minds\\' <@U02TC704A3F> - it\\'s a rather large crowd as far as I can tell :wink:,1645062644.184629,1645071418.475069,U02U5SW982W\\nA6F42AD4-A6D2-4F0E-80D4-CB99BB7D30BB,U02UNCG01PT,,,\"Hi <@U02UNCG01PT>, did you manage to find where the ny-rides.json was created? I want through the above videos but still can’t find it…\",1642943928.078000,1645071450.451969,U02V6TV3M2A\\n19cd7398-3c6d-4800-9959-5bf3404e29d4,U02U5SW982W,,,\"Very nice blog, <@U02U5SW982W>!\\n\\nDid you get to this part in DBT? I think after you import a repo, dbt will generate a key for you.\",1645071297.194149,1645071546.351699,U02T9GHG20J\\n7de35a7d-b5b6-4775-abc8-3c21dd1608c4,U02U5SW982W,,,\"Hi Michael,\",1645071297.194149,1645071603.439859,U02U5SW982W\\n7fb73e2b-4c0c-4e92-8eca-c0fc5c9c870e,U02U5SW982W,,,I was wondering what that was about. That page there that you show. I never got that in dbt. Once I clicked on Continue in Setup a Database Connection I got a blank page. Was it supposed to then give me this page or was I supposed to go looking for it somewhere in dbt?,1645071297.194149,1645071706.503859,U02U5SW982W\\nd6ed52d1-1204-47b0-a304-5e29ba1ceb89,U02U5SW982W,,,And thanks for your help <@U02T9GHG20J> :),1645071297.194149,1645071733.913229,U02U5SW982W\\n5caf1b30-cb8e-4f7f-a6f5-663475e12f3d,U02U5SW982W,,,\"Yes, if I remember correctly, this screen should have come up after you set up the BQ connection.\",1645071297.194149,1645071853.069669,U02T9GHG20J\\nd1886ff0-ed3e-4ec9-bb4a-eab1030028d7,U02U5SW982W,,,Aha... didn\\'t work for me. I\\'ve just updated my blog to show my big blank screen of death ... that\\'s why I was confused about that screen that you show. I thought I had to go to my own GitHub repo. Gosh not sure what to do now...,1645071297.194149,1645071977.562279,U02U5SW982W\\n0447373e-cc82-4c4a-a9fa-896daa819a4a,U02U5SW982W,,,\"yeah, check out this video around 1:30. That\\'s what it did for me  <https://docs.getdbt.com/tutorial/create-a-project-dbt-cloud>\",1645071297.194149,1645072042.191969,U02T9GHG20J\\n3b6a3e42-5735-4ff4-906e-b14c7e3648c9,U02U5SW982W,,,Awesome - thanks <@U02T9GHG20J> Love your work!,1645071297.194149,1645072122.149509,U02U5SW982W\\n776d610c-4ab5-4fe7-bf4e-8b17f8987f7f,U02U5SW982W,,,:pray: Thanks! good luck with dbt,1645071297.194149,1645072170.826809,U02T9GHG20J\\n913d3961-4cfd-40d2-b9b2-a8e1a8a194cd,U02U5SW982W,,,I\\'ll need it :),1645071297.194149,1645072688.339859,U02U5SW982W\\nda828819-75f7-4fa3-95af-7ec834adf685,U02TAA1LDQT,,,\"Hey Jacob, I got the same Issue. Did you solve this?\",1643207126.302300,1645074072.345809,U02CJGKV5NJ\\na39d6fe6-7f07-410a-a7c3-b35ff0757ec3,U02U5SW982W,,,All working now <@U02T9GHG20J>. I\\'ve now got extra detailed instructions on my <https://learningdataengineering540969211.wordpress.com/2022/02/17/week-4-setting-up-dbt-cloud-with-bigquery/|blog> for future forgetful me who 99.999% of the time needs it :wink:,1645071297.194149,1645074545.469429,U02U5SW982W\\n1de20544-85be-4895-83ab-6aa90c3e7eae,U02U5SW982W,,,\"Awesome. That\\'s great. I\\'m also bookmarking your blog, because your documentation great!\",1645071297.194149,1645074661.320289,U02T9GHG20J\\n597cb429-4b5f-4a6e-8414-c2e0fb27e6a0,U02TATJKLHG,,,I am still not able to figure out :confused:. Did you do something differently? I am sure there is a silly mistake in there somewhere,1644817851.455599,1645074743.859629,U02TATJKLHG\\nc3ac4b4f-fa1e-4c79-97ce-59343d738be0,U02U5SW982W,,,Thanks <@U02T9GHG20J> - make sure you reach out if I\\'ve missed something or got it wrong somewhere along the line :pray:,1645071297.194149,1645074879.622879,U02U5SW982W\\n2a392a06-501f-481e-b9c9-8a8079b2467f,U02TATJKLHG,,,\"Nopes. I ran the build etc. and then docs generate, then serve - nothing different.\",1644817851.455599,1645078216.571439,U02HB9KTERJ\\nc955a365-83e0-403a-9236-092b700fdf7a,U02U3E6HVNC,,,\"i was looking at the source file sizes for FHV (at <https://nyc-tlc.s3.amazonaws.com/trip+data/>) and looks like Jan 2019 file is over 1 GB, while the rest of the months are est. mere 90mb. i guess that explains the huge count difference? :dizzy_face:\",1645062231.005189,1645079130.762719,U02ULMHKBQT\\n3e8e8f9e-c8ce-46df-a170-2e62db7ab043,U02T65GT78W,,,\"<@U01B6TH1LRL> do you mind adding this information to the homework.md? I believe a lot of people (like me :grin:) are struggling unnecessary, because they stick to the template of the green, yellow tripdata models\",1644957970.026469,1645080339.952969,U02TYCYG25U\\n10692c9f-80f8-4c91-8b02-8f63a245e6e0,U02TC704A3F,,,I also had BYTE ARRAY problem in FHV-2019 data. Had to load raw csv files directly to GCS.,1645062644.184629,1645083747.570909,U02Q7JMT9P1\\n36aeb836-eb76-4356-b025-93ff4ac737e5,U02U6DR551B,,,Just specify the location in profiles.yml file,1645343764.071639,1645437444.256309,U0290EYCA7Q\\n28886e6c-4089-4d72-908e-b9f61eac4d2e,U032Q68SGHE,,,\"I am having problem with another column...\\n\"\"Error while reading table: uplifted-nuance-338810.nytaxy.green_tripdata, error message: Parquet column \\'RatecodeID\\' has type INT64 which does not match the target cpp_type DOUBLE.\"\"\\nThat Cast you are doing is at DBT?\",1644881953.727679,1645438891.698569,U02CD7E30T0\\na7cb3cd7-c3fc-49ed-8718-4b06d96d05ad,U02UBQJBYHZ,,,\"i\\'ve same problem, but i just avoid select that column in the end, this happen maybe because i\\'m uploaded yellow trips to bq with parquet file but green trips uploaded to bq with csv format\",1645380005.596629,1645439357.923309,U02T3F06EKT\\n02d30d17-b9a3-42ba-99ac-5806cbd15f38,,4.0,,\"Hello, I\\'m stuck at this error - Invalid syntax for this line `os.system(f\\'wget {url} -O {csv_name}\\').` getting this when i\\'m testing the script locally(1.2.4)\",1645443577.891309,1645443577.891309,U0343G3CH32\\nd2513275-23f6-426c-8460-c3c931f2e14c,U0343G3CH32,,,\"Also getting import error for the script. worked fine for the ipynb file, changed interpreter as well. no luck:(\\n\\n`File \"\"ingest-data.py\"\", line 2, in &lt;module&gt;`\\n    `import pandas as pd`\\n`ImportError: No module named pandas`\",1645443577.891309,1645444217.640619,U0343G3CH32\\n915ed56f-df83-4bff-827e-dc1cd9e7fdff,U0343G3CH32,,,\"Hello <@U0343G3CH32>. In the first error,  if you are on mac use `curl`  instead of `wget`  so that you have `os.system(f\\'curl {url} -O {csv_name}\\')`\\n\\nIn the second error, ensure you have `pandas`  installed by running `pip install pandas` from your terminal.\",1645443577.891309,1645446262.453469,U02SEH4PPQB\\ne7db04d5-7fcd-4e9a-9b9d-ef1fbce818e1,U02T9JQAX9N,,,I have changed it already.. yet it doesn\\'t still work,1645373750.829869,1645448304.475169,U02T9JQAX9N\\nb991fd0c-f2bc-4eaa-99ed-400f50140d99,U02UBQJBYHZ,,,Hi. You should use only CSV files for create green_tripdata and yellow_tripdata tables,1645380005.596629,1645449075.155019,U02QL1EG0LV\\n20902794-a910-475b-b653-7eaf86e3b550,U0343G3CH32,,,\"Hello <@U02SEH4PPQB> yup I should use curl. for the second error, I have pandas already installed... not sure why i\\'m getting it\",1645443577.891309,1645450409.145379,U0343G3CH32\\nccb6ddee-891b-4f0d-9aa0-2545cb1e9082,U0343G3CH32,,,\"Project is done individually. This week is week 5, I believe; project starts in week 7.\",1645429178.222289,1645453288.935989,U02T941CTFY\\n357208e9-b41d-45eb-9a08-7f6a46844a32,U02T9JQAX9N,,,\"Hi! , I had to use `date(date_trunc(pickup_datetime, month))`\",1645373750.829869,1645455564.674149,U02LQMEAREX\\n0ad23e1d-5a85-43fc-8a59-d50878cd9cc8,U02UBQJBYHZ,,,\"That\\'s good to know for the future, but I didn\\'t want to repeat the work I had already done (and the long wait times to load the data).\",1645380005.596629,1645456217.855319,U02UBQJBYHZ\\nff81fa78-c577-47aa-8aef-f7541f00c035,,1.0,,\"As I see, many of you have already discussed Q1 from week 4 in threads. I also wasn\\'t able to get one of the provided results in a straightforward way (guess, due to the data difference in cloud storage / BQ). But I tried to do it in a slightly different manner:\\n1. I executed two queries in my BQ - the first to calculate the total amount of records in the model `fact_trips` without filtering, and the second one with filtering for 2019 and 2020 data. Then I found the difference between these values.\\n2. After that I took the records count from the Victoria\\'s video (<https://www.youtube.com/watch?v=39nLTs74A3E&amp;list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&amp;index=43|DE Zoom camp 4.5.1>) and substracted from this number the difference I had got earlier in p.1 .\\nSo, the result I got is pretty close to the one of the options. But I\\'m still not sure, if this is a good approach. Mb someone else could try to reproduce it?\",1645457488.229129,1645457488.229129,U02DNSC6Z52\\n818ca4d2-05d8-44dd-944c-347d61ce3cb4,U0343G3CH32,,,Are you on anaconda? I\\'d recommend using it because it has all the required packages,1645443577.891309,1645458068.517549,U01AXE0P5M3\\n568b794b-f28a-499d-aac3-af3b16412d5e,U02U5SW982W,,,@sandy Just wanted to say thank you for putting this together. The introduction video was lacking and you have helped me immensely.,1645071297.194149,1645458126.597529,U02V1JC8KR6\\n21FD9E35-FF32-4EF9-AFD2-711DE4F4CC12,,,,Does week material have a missing episode?,,1645458531.017139,U02U07906Q0\\n41fd40d6-d2d2-4338-ae6c-058866769c72,,,,\"[Week 5] Hi! For those not using VSCode, and wondering how to forward ports, the following guide can help: <https://docs.anaconda.com/anaconda/user-guide/tasks/remote-jupyter-notebook/>\",,1645459431.568609,U02S2TZRBL7\\n6f0b4e7e-8830-4cbe-92b9-285a7913b6c9,,,,*Data Talk Club: Data Engineering Course - Week 4\\xa0Summary:*,,1645460999.136919,U02S4A9TA5Q\\n51da6da9-08ac-4482-a19d-f4ff45e7eb46,U02DNSC6Z52,,,\"We are skipping question 1, we haven\\'t defined whether we will ignore it entirely or if we will give one point to the 2 most voted questions so far\",1645457488.229129,1645464373.922759,U01B6TH1LRL\\n50248303-f2f6-4af9-8a71-83e665924387,,2.0,,For week 4 the homework instructions specify building tables for the fhv data. Is this covered in the videos and I just missed it or is it on us to figure out how to do this?,1645465317.301049,1645465317.301049,U02TNEJLC84\\n0b46dc51-3563-4c31-8bd0-4919aff9664c,U02TNEJLC84,,,\"That\\'s indeed the homewrok, you have to build those models :slightly_smiling_face:\",1645465317.301049,1645465717.755929,U01B6TH1LRL\\nc681ce21-4b4d-409e-ad82-9c56565436bc,U02TNEJLC84,,,Ah got it. Thank you!,1645465317.301049,1645465853.732889,U02TNEJLC84\\ndc13af02-47d0-4f7c-824b-09b7e23bd3d9,U02T9JQAX9N,,,\"Thanks man for replying.. This also did not work, but I found out that I had used `\\'month\\'` instead of `month`. When I corrected that, it finally worked.\",1645373750.829869,1645468419.010039,U02T9JQAX9N\\n4efabbd5-6fe4-4236-a7eb-3d467023135d,U02U5SW982W,,,\"Hmm, I wonder if my code for the join was wrong as well.\\n```from fhv_data\\ninner join dim_zones as pickup_zone\\non fhv_data.pickup_locationid = pickup_zone.locationid\\ninner join dim_zones as dropoff_zone\\non fhv_data.dropoff_locationid = dropoff_zone.locationid```\\nDo you mind sharing how you fixed yours? I can\\'t find out why my count is too low.\",1645415350.125939,1645469556.352189,U02U3E6HVNC\\nbea023ad-3722-4231-aabd-ee2463df28c5,U02U5SW982W,,,\"Ah nevermind, the issue was with my staging model.\",1645415350.125939,1645470119.164099,U02U3E6HVNC\\n8ab76d5c-8283-4f6d-be10-65f581840f94,,4.0,,\"Not sure if there\\'s a better place to put this, but there\\'s an error\\nin data-engineering-zoomcamp/week_5_batch_processing/setup/linux.md\\nThe path to JAVA_HOME should look like this, based on video 05 03:\\n```export JAVA_HOME=\"\"${HOME}/spark/jdk-11.0.2\"\"```\",1645473242.256019,1645473242.256019,U02U3E6HVNC\\n22c973a7-4292-4e69-b831-63d31426f896,U02U3E6HVNC,,,\"And SPARK_HOME has the same error, should look like:\\n```export SPARK_HOME=\"\"${HOME}/spark/spark-3.0.3-bin-hadoop3.2\"\"```\",1645473242.256019,1645473469.557979,U02U3E6HVNC\\n52bff655-6d91-4808-b5de-32c5ada0b943,,1.0,,\"New to docker, need your help. when i execute this command\\n```docker run -it \\\\\\n -e POSTGRES_USER=\"\"root\"\" \\\\\\n -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n -p 5432:5432 \\\\\\n postgres:13```\\ni get the below content.\",1645473546.719669,1645473546.719669,U02SSP7C4SD\\n26993a11-4566-45ab-879a-3544edf4c90d,U02U3E6HVNC,,,\"It\\'s not an error, both will work. But I can see that it could be confusing. I\\'ll fix it eventually, but it\\'ll be helpful if you could create a PR\",1645473242.256019,1645473572.638419,U01AXE0P5M3\\n88fa87bc-b555-4f59-adc1-e178bbf6e8d3,,,,\"however , a folder gets created as shown below and the folder is empty with out any postgress files.\",,1645473588.649019,U02SSP7C4SD\\n2cbe5146-97a4-4c85-be1e-793d53c7c0da,,12.0,,\"once postgress image is executed, should ny_taxi_postgres_data folder have postgress configuration data?\",1645473732.854749,1645473732.854749,U02SSP7C4SD\\n9a4e3aa8-43a0-40f6-9817-e5827f7ec953,U02TATJKLHG,,,\"quick update: mermaid works like a dream on github :star-struck: I’m using it for my notes for week 5; I’m still pretty slow but I’m quickly getting the hang of it.\\n\\n<https://github.com/ziritrion/dataeng-zoomcamp/blob/main/notes/5_batch_processing.md>\",1644919012.575939,1645475577.661149,U02BVP1QTQF\\n1def28a0-48a5-4485-8cae-ddd57d198958,U02U3E6HVNC,,,\"I\\'m pretty new to git, so I\\'m nervous of doing that haha. Could be good practice if you say it\\'s low-risk?\",1645473242.256019,1645476011.978829,U02U3E6HVNC\\n3c032331-745f-44b8-8959-cada4ca0b23d,U02U3E6HVNC,,,\"Yes! You won\\'t break anything, trust me =)\",1645473242.256019,1645476220.635659,U01AXE0P5M3\\n87b3996f-3d92-4432-b7f5-b48af3561eee,U02S9JS3D2R,,,Is there any solution? For question 1 my fact_trip has 61602987. Not equal to any of the options but closer to all of them :laughing:,1645039735.829189,1645477033.238479,U02QDKU4GTY\\n4012f25e-632d-4001-843c-17d1120e48e8,U02SSP7C4SD,,,\"Try go through the README.md file in 2_docker_sql folder of week_1, it contains the below guidelines on how to resolve this issue\\n\\n*#### Linux and MacOS*\\n\\n```bash\\ndocker run -it \\\\\\n\\xa0 -e POSTGRES_USER=\"\"root\"\" \\\\\\n\\xa0 -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n\\xa0 -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n\\xa0 -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n\\xa0 -p 5432:5432 \\\\\\n\\xa0 postgres:13\\n```\\n\\nIf you see that `ny_taxi_postgres_data` is empty after running\\nthe container, try these:\\n\\n* Deleting the folder and running Docker again (Docker will re-create the folder)\\n* Adjust the permissions of the folder by running `sudo chmod \\xa0a+rwx ny_taxi_postgres_data`\",1645473732.854749,1645477269.989869,U02V90BSU1Y\\n3595ed22-d937-4e06-83e0-999b7843b298,U02SSP7C4SD,,,\"when i run the above code, a folder gets created ny_taxi_postgres_data;c -is the folder name right?\",1645473732.854749,1645477513.181939,U02SSP7C4SD\\n98552090-35aa-4d77-afd2-0b112705d7c5,U02SSP7C4SD,,,\"check FAQ:\\n\\n<https://docs.google.com/document/d/19bnYs80DwuUimHM65UV3sylsCn2j1vziPOwzBwQrebw/edit#>\\n\\nspecifically, this may be helpful\\n\\n```$ docker volume create --name dtc_postgres_volume_local -d local\\n$ docker run -it\\\\\\n  -e POSTGRES_USER=\"\"root\"\" \\\\\\n  -e POSTGRES_PASSWORD=\"\"admin\"\" \\\\\\n  -e POSTGRES_DB=\"\"ny_taxi\"\" \\\\\\n  -v dtc_postgres_volume_local:/var/lib/postgresql/data\\n  -p 5432:5432 \\\\\\n  postgres:13```\",1645473732.854749,1645477757.170579,U01AXE0P5M3\\ncb90ab07-6c03-43f0-9d11-6bd9ec07314c,U02SSP7C4SD,,,you can also try adding an extra slash at the beginning of you `-v` part,1645473732.854749,1645477799.715289,U01AXE0P5M3\\n6544b2d4-45e7-4fc1-a563-11493616d21e,,3.0,,\"Can someone recommend a really nice Data Engineering book, that go along with this course?\\n\\nBtw.. thought I would share something I came across in the last week, which would save $$$$ and numerous trees. ACM members get access to O\\'Reilly books. Professional membership costs around $99 (kind of equal to 2 or 3 books cost). Price varies by country as well. \\n\\n<https://learning.acm.org/e-learning/oreilly|https://learning.acm.org/e-learning/oreilly>\\n<https://www.acm.org/membership/membership-options|https://www.acm.org/membership/membership-options>\",1645478349.390589,1645478349.390589,U02S83KSX3L\\ne0fbd038-638b-455e-a595-cdc821cb101e,U02SSP7C4SD,,,\"Hi Alex, i see database system is ready toa ccept connections, but when i go to the folder where its mounted , i donot see any configuration files in the var/lib/postgresql/data or the sub folders\",1645473732.854749,1645478794.591969,U02SSP7C4SD\\n619005b9-59dc-48d0-b6be-5ad4065acf05,U02SSP7C4SD,,,i donot see these files,1645473732.854749,1645478876.355329,U02SSP7C4SD\\n5cfc043f-2f91-4fcd-9e77-329025addd66,U02SSP7C4SD,,,\"Also when i try connecting to the postgress using pgcli. After i enter password, its not connecting me to the database.\",1645473732.854749,1645479250.618669,U02SSP7C4SD\\nf202155f-b905-4b34-ae94-f46fe82ae072,U02SSP7C4SD,,,,1645473732.854749,1645479255.089329,U02SSP7C4SD\\nc0e4302f-a487-4600-aa4b-703076380c3d,U02S83KSX3L,,,Hi <@U02S83KSX3L> - thanks for the tip. Good to know! One book that has been recommended for the course and that I\\'m currently reading is \\'<https://www.amazon.com.au/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321/ref=asc_df_1449373321/?tag=googleshopdsk-22&amp;linkCode=df0&amp;hvadid=341793124241&amp;hvpos=&amp;hvnetw=g&amp;hvrand=1012655294328299857&amp;hvpone=&amp;hvptwo=&amp;hvqmt=&amp;hvdev=c&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=9069291&amp;hvtargid=pla-432535594773&amp;psc=1|Designing Data-Intensive Applications>\\' by Martin Kleppmann. I\\'m finding it quite good - and I usually find these types of books a bit hard-going.,1645478349.390589,1645480470.687729,U02U5SW982W\\nbbd3f04a-8f50-4ae3-a99f-031a77ce97c8,U02U5SW982W,,,Hope you are all good <@U02U3E6HVNC>,1645415350.125939,1645480589.183179,U02U5SW982W\\n57700dcc-d2b5-4983-9482-41c3df059eec,,1.0,,\"Hi, I tried to create a dashboard in Tableau for the week 4 models. Here is an image and workbook for the dashboard. Sorry I could not upload it to tableau public for your direct access as the data was too large to create an extract but I\\'m working on a way around it. Thanks\",1645485417.180619,1645485417.180619,U02SEH4PPQB\\naec24506-e443-4f52-861e-b91812425811,,,,,,1645485439.144459,U02SEH4PPQB\\nac21dca2-b44e-489e-8671-07206535196d,U02SSP7C4SD,,,\"when executing the commands, a folder with name \"\"dtc_postgres_volume_local;c \"\" gets created, not sure why the last two charaters are coming from\",1645473732.854749,1645486369.759989,U02SSP7C4SD\\nf917e3dd-fba6-43cd-94df-ae48905a1c5e,U02SSP7C4SD,,,From docker on windows. It shouldn\\'t be there,1645473732.854749,1645487811.462159,U01AXE0P5M3\\n4ea4f407-5380-4ce5-a8bd-23722ef9ab28,U02SSP7C4SD,,,Please also check the next video about using jupyter instead of pgcli,1645473732.854749,1645487858.826249,U01AXE0P5M3\\nD1B71E95-499C-4DA4-8B69-5E56C5808F63,,3.0,,\"I am getting error while running this code \\n\\ndf \\\\\\n    .withColumn(\\'pickup_date\\', F.to_date(df.pickup_datetime)) \\\\\\n    .withColumn(\\'dropoff_date\\', F.to_date(df.dropoff_datetime)) \\\\\\n    .withColumn(\\'base_id\\', crazy_stuff_udf(df.dispatching_base_num)) \\\\\\n    .select(\\'base_id\\', \\'pickup_date\\', \\'dropoff_date\\', \\'PULocationID\\', \\'DOLocationID\\') \\\\\\n    .show()\\n\\nError message\\n\\nWARN TaskSetManager: Lost task 0.0 in stage 22.0 (TID 52, server.europe-west6-a.c.dtc-de.internal, executor driver): java.io.IOException: Cannot run program \"\"python\"\": error=2, No such file or directory\\n\\nPlease help\",1645488772.900959,1645488772.900959,U02TBCXNZ60\\n74d703bf-d1c4-4d47-abd4-8ce058bbadaf,,5.0,,\"I have an issue for dbt cloud. My IDE session always timed out every few minutes, is it my internet connection? Anyone encountered the same issue?\",1645488963.706489,1645488963.706489,U02T8ANTJGM\\ne226fdde-b592-452f-b8ce-a0a86fd4524e,U02T8ANTJGM,,,Hi <@U02T8ANTJGM> I\\'m experiencing that right now as we speak. It isn\\'t an uncommon problem for me...,1645488963.706489,1645489146.268339,U02U5SW982W\\na3d6ac4c-5309-469a-8854-e25898764825,U02T8ANTJGM,,,<@U02U5SW982W> did you use managed repository?,1645488963.706489,1645489355.278419,U02T8ANTJGM\\nbe00a864-f962-4ab0-9395-1aef0c0d44f1,U02T8ANTJGM,,,Hi <@U02T8ANTJGM> I used my own repo on GitHub,1645488963.706489,1645489824.623009,U02U5SW982W\\n18dbe18e-7cb1-42c7-a37c-26d7d3fbf2fc,U02T8ANTJGM,,,In my experience dbt just hangs pretty much whenever it feels like it. I\\'m not sure if that is because of my location (I\\'m in Australia) or if others experience that too...,1645488963.706489,1645490477.060209,U02U5SW982W\\ne7a59071-f739-40b0-9037-6ca90d775dd4,U02TBCXNZ60,,,Hi <@U02TBCXNZ60> exactly what are you working on - as in what week and what video and what specific task?,1645488772.900959,1645490534.199469,U02U5SW982W\\n408FF44A-5267-4599-8965-B448A698BAFF,U02TBCXNZ60,,,Hi Sandy. Week 5. Videos 05- 05 (Spark),1645488772.900959,1645490566.016039,U02TBCXNZ60\\nde7750d0-ac82-4acf-82e2-25a4962778ff,U030FNZC26L,,,This is beauty! <https://www.onemodel.app/>,1644697506.408849,1645493679.436709,U0290EYCA7Q\\nf8f712ac-fa76-4ed9-bf5a-edcec6b3fffb,,,,\"This is beauty. I can\\'t remember the number of times I have to fight with random tools to get this done.\\n<https://www.onemodel.app/>\",,1645494007.346419,U0290EYCA7Q\\n60e8d757-637d-40c6-837c-2ccca0236bc8,,7.0,,\"For anyone installing PySpark on Windows, I\\'m using Anaconda since ever and had zero problems installing and running PySpark.\\n\\nAnaconda is heavy but helps a lot with envs and packages :smiley:\",1645497064.778379,1645497064.778379,U02TC704A3F\\nc00c4d59-5c90-4067-8dcf-741821106699,U02TC704A3F,,,Same. winutils.exe is must though.,1645497064.778379,1645497282.681399,U0290EYCA7Q\\n10576d71-cdf4-4aab-82f4-aaca2361904c,U02TC704A3F,,,<@U02TC704A3F> how did you unpack the .sh file in linux vm?,1645497064.778379,1645498402.425819,U02T8HEJ1AS\\ncf8b174b-6961-4f5b-adc8-f1fdc471c077,U02TC704A3F,,,\"<@U02T8HEJ1AS>, I\\'m not using linux vm :grimacing:\",1645497064.778379,1645498475.043349,U02TC704A3F\\n2bd3c847-6c23-4ed5-aaf1-3780a479cbf4,U02TC704A3F,,,You can\\'t unpack .sh file. It\\'s the file format for linux script.,1645497064.778379,1645498729.685039,U0290EYCA7Q\\n410bf3dd-a46e-4c5b-b594-5d7ea6b6dc1a,U02U5L97S6T,,,\"<@U02U5SW982W> Hi Sandy, I read few articles in your blog and just wanted say, thank you for sharing your notes and the process! it helps me a lot.\",1645417366.901519,1645500383.841419,U0308865C0H\\nebd22583-30b7-4266-9b79-c0a8dd27569b,,,,Please i have this error,,1645501807.672169,U02UKLHDWMQ\\nbb6a5387-b338-42c3-9ee7-2bfd839cc857,,4.0,,\"```google.api_core.exceptions.BadRequest: 400 Error while reading table: trips_data_all.green_tripdata_external_table, error message: Parquet column \\'ehail_fee\\' has type DOUBLE which does not match the target cpp_type INT64.```\\n\",1645501822.553359,1645501822.553359,U02UKLHDWMQ\\n6646e271-ddf4-49b7-8684-bf67d97a4834,U02SSP7C4SD,,,\"Don\\'t worry about this issue. Just move on with the lesson and you\\'ll be fine. Alternatively, you can check out the FAQ for solutions pertaining to this\",1645473546.719669,1645502491.469269,U02T941CTFY\\n1ebc6394-768e-456a-be89-ebb780c1f2fc,,3.0,,\"Question on Week 5 (de zoomcamp 05 03):\\nWas VSCode merely used to perform port forwarding for Jupyter Notebook? I am guessing that in the past when we set up Airflow, that was not necessary because we defined the port in airflow-webserver. Trying to understand the process here, thanks!\\n\\nEdit: Or is it because Alex was working on a remote server? I\\'m using Windows on WSL, btw\",1645503128.545449,1645503128.545449,U02T941CTFY\\n54820f7b-d5e8-4cbc-88ac-9c3d504c08e4,U02T941CTFY,,,<@U02T941CTFY> from what i followed yes he is using remote server.,1645503128.545449,1645503494.923029,U02T8HEJ1AS\\ne5b40489-10a5-4b18-b872-2fe64b4aaf36,U02TC704A3F,,,<@U0290EYCA7Q> i am trying to install pyspark in my Gcp Vm and was stuck there. I have anaconda in the VM installed as .sh file,1645497064.778379,1645503630.110819,U02T8HEJ1AS\\n6788a6e5-e4fb-4e69-8d38-2496dbb1f2e0,,2.0,,Anyone know how i can remote-ssh my GCP vm with Vs code. tried few things and no luck. Thank you,1645503728.197339,1645503728.197339,U02T8HEJ1AS\\n73012dc5-5cd2-4965-b0a1-4b958f9e83da,U02S83KSX3L,,,\"Interesting! So instead of $499 per year, we can pay only $99 annually and get the benefits! :thinking_face:\",1645478349.390589,1645503788.420219,U02HB9KTERJ\\ne8fbf4b9-86fb-476c-a4a5-3deb77d0a694,U02UKLHDWMQ,,,<https://datatalks-club.slack.com/archives/C02V1Q9CL8K/p1644517912430799>,1645501822.553359,1645503794.317509,U02T3F06EKT\\ned32e286-372d-49b1-8f93-b7d18eab1dd9,U02T941CTFY,,,\"Yes and Yes.\\nSee this....\\n\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1645459431568609>\",1645503128.545449,1645504105.416979,U02HB9KTERJ\\n337215b1-f743-4ae8-8d8e-507341149afc,U02T8HEJ1AS,,,The video 1.4.1 has all the steps. Make sure you setup the config file in ~/.ssh.,1645503728.197339,1645504183.986559,U02HB9KTERJ\\nd5424171-7be0-461f-9cc6-d39a2c8e7956,U02T8HEJ1AS,,,<https://youtu.be/ae-CV2KfoN0?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb>,1645503728.197339,1645504557.634099,U0290EYCA7Q\\n5f2f3b91-32d8-4c55-91f8-3c8c66e948f8,U02TC704A3F,,,\"You need to run the sh file to install anaconda.\\n\\n`bash Anaconda3-2021.11-Linux-x86_64.sh`\",1645497064.778379,1645504618.924149,U0290EYCA7Q\\n88c3a138-3aa7-4155-9eea-56631c8e3156,U02U5L97S6T,,,Hi <@U0308865C0H> - thank you for that. I\\'m glad that it helps you. We are encouraged to do public learning in the course which is great and because of that I guess I just put notes here that I hope will be useful to me in the future. I make no claims that anything I say there is right though :slightly_smiling_face: . But yes it is a process. At the moment I\\'m busy investigating the magical fairies that make communication between dbt and BigQuery possible :wink: - I\\'ve put my progress so far in this blog <https://learningdataengineering540969211.wordpress.com/dbt-cloud-and-bigquery-an-effort-to-try-and-resolve-location-issues/|post> but I\\'m thinking it is going to be a rather protracted investigation. I hope you are travelling well with the course and do reach out in the future - particularly if you have any information on those magical fairies ...,1645417366.901519,1645505405.264949,U02U5SW982W\\n4b18b1b9-43e0-438f-a8f1-e3c20eddd33f,U02T941CTFY,,,Thanks!,1645503128.545449,1645506977.196759,U02T941CTFY\\n7d6b5d6a-5202-4a7d-999e-76d32ba5482c,,19.0,,\"Hi...While running dbt cloud, did any1 get 404 error... Location defaults to US. Anyone was able to resolve this error?\",1645507617.625979,1645507617.625979,U02QK4ZV4UX\\n1c45e65c-3fe8-420d-8a92-ade3a7b3dc66,U02QK4ZV4UX,,,<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1644432387132949>,1645507617.625979,1645508330.862789,U02T3F06EKT\\na9025770-761a-47ee-8989-07988e832fa6,U02U6DR551B,,,can you please share the link of the video on how to create external table,1643878397.499529,1645508358.287669,U02R9P66EQY\\n0de398e2-d1ab-4c9d-96dc-8535bf709edb,U02U6DR551B,,,<@U02TATJKLHG>,1643878397.499529,1645508378.572489,U02R9P66EQY\\ne7751cb8-ab2c-4b87-8a53-a0be05dd2332,,,,\"Please make sure to use `/spark/` in place of `/bin/`, if you created a separate directory named `spark` to install java and spark.\\n\\n```#export JAVA_HOME=\"\"${HOME}/bin/jdk-11.0.2\"\"\\nexport JAVA_HOME=\"\"${HOME}/spark/jdk-11.0.2\"\"\\n\\n\\n#export SPARK_HOME=\"\"${HOME}/bin/spark-3.0.3-bin-hadoop3.2\"\"\\n#export SPARK_HOME=\"\"${HOME}/spark/spark-3.0.3-bin-hadoop3.2\"\"```\",,1645509603.771149,U0290EYCA7Q\\nd6fb80e7-e353-4b1a-b8f8-c7424294b3e7,U02QK4ZV4UX,,,Hi <@U02QK4ZV4UX> I too ran into this error but the solutions that worked for others didn\\'t seem to work for me. It\\'s an issue that I\\'m following up now with dbt. I outlined the process I used to solve the problem. It is a workaround though. Depending on where you are based will - I think - impact how easy it is to solve your problem. I\\'m in Australia and had a project based there and it just doesn\\'t want to work. I had to set up a location in that defaulted to the US. If you want the detail here is a link to my <https://learningdataengineering540969211.wordpress.com/2022/02/19/week-4-de-zoomcamp-4-3-1-location-location-location-dbt-part-2/|post>.,1645507617.625979,1645511024.635109,U02U5SW982W\\nc3dd5988-23fa-447a-994e-84e14b0654e1,U02TC704A3F,,,Hi <@U02TC704A3F> - I found that Anaconda is excellent. But just too heavy for my set-up at the moment. Darn :disappointed: . But I am going okay on Linux with virtual environments set up. Seems to work... except when I forget all my commands ... which is like pretty much all the time.,1645497064.778379,1645511173.655329,U02U5SW982W\\nf3289618-46cc-4fce-a3c4-fd2c2e3eaa42,U02TBCXNZ60,,,Ah... you are well ahead of me <@U02TBCXNZ60> - hopefully I will get there soon :slightly_smiling_face:.,1645488772.900959,1645511250.865549,U02U5SW982W\\n704291c4-840b-4d62-8b65-b9c7051a1bee,U02QK4ZV4UX,,,Yes I read your blog <@U02U5SW982W>.,1645507617.625979,1645511334.854359,U02QK4ZV4UX\\n14d380fd-2085-445d-a1e6-232c4b74f3ba,U02QK4ZV4UX,,,I somehow seem to made it work. I changed the default location under profile credentials to the location BQ dataset is.,1645507617.625979,1645511390.017249,U02QK4ZV4UX\\n3d4e416f-91f6-4f1b-8379-93e1b4be5ac9,U02QK4ZV4UX,,,I think the reason for this error might be running with the example model which creates a dbt_&lt;username&gt; schema in the default US location,1645507617.625979,1645511449.644079,U02QK4ZV4UX\\n7206a837-196b-4919-bf60-20094bd59e75,U02QK4ZV4UX,,,if i created a dbt_&lt;username&gt; schema in the BQ location and it seems to work now.,1645507617.625979,1645511486.074029,U02QK4ZV4UX\\n4af6e0ea-f87c-462c-9c9e-ef9e8c6d8750,,4.0,,\"Not sure why, but I got a `no module named pyspark`  error.\\n\\nHad to add this for pyspark to work:\\n```pip install findspark\\nimport findspark\\nfindspark.init()```\",1645512707.679389,1645512707.679389,U02T941CTFY\\nf0848ac6-09a1-44a0-94c4-293cdc737643,U02T941CTFY,,,Did you add that zip file to PATH?,1645512707.679389,1645512818.933599,U0290EYCA7Q\\n76203629-310d-4981-95dc-6238d0134df3,,4.0,,\"Hi everyone! I have a question about dealing with missing data in data pipeline. I\\'m wondering whether I should fill missing values at the transformation stage of the data pipeline. The question is, to fill or not to fill? :thinking_face:\",1645514258.973169,1645514258.973169,U02TS7NHGPR\\n0dfd5cd0-9348-45c8-bdc1-ebaf1128ff7f,U02ECUGHWG4,,,Thanks for the apt response,1642317893.483700,1642330332.487200,U02ECUGHWG4\\n60a922fb-27ca-4b04-810c-9c275aa17122,,,,Hey everybody. Backend engineer from :flag-ie: but living and working in London. Looking forward to the course :),,1642332481.489700,U02U12BD0CE\\na2e7b54d-b79e-44ac-84c7-5a639dd17c5a,,2.0,,\"Hey guys, i wanna ask about this step, where we execute this step? is it on our local system or gcp environment?\",1642334790.491600,1642334790.491600,U02RA8F3LQY\\n73209bdf-1cfe-4c65-90a2-37269ebd311b,U02RA8F3LQY,,,\"Could be either, depending on your setup. Do you want to execute everything from your laptop?\",1642334790.491600,1642335334.491900,U01AXE0P5M3\\n9e090abf-1525-4bef-aed8-42a3c5b874f9,U02RA8F3LQY,,,i already did this step manually on my windows via environment variable settings. Just to make sure everything\\'s well,1642334790.491600,1642335568.492200,U02RA8F3LQY\\nafcd2cbf-163d-4e88-b106-9d63abcfac5c,,14.0,,\"SInce I have already spent all of my free credits on GCP, what solution should I use instead?\",1642337021.494100,1642337021.494100,U01E06WTHNG\\n1e8f878b-7bc2-45fb-a18c-816130bb4199,U01E06WTHNG,,,\"A new account, perhaps?\",1642337021.494100,1642337120.494200,U01AXE0P5M3\\nd5579ebf-faa9-4f42-83f8-3f4145a89f8c,U01E06WTHNG,,,\"Are we expecting to use up all the $300 in credits, during the zoomcamp? <@U01AXE0P5M3>\",1642337021.494100,1642337221.494500,U02QPBZ3P8D\\n63a78585-996a-4c54-91cf-cbb967ac1f29,U01E06WTHNG,,,\"I don\\'t think so. <@U01DFQ82AK1> how much will it cost, what do you think? Less than 100?\",1642337021.494100,1642337294.494800,U01AXE0P5M3\\n7aed21c5-8150-4854-b1be-bcf42bfabdfe,U01E06WTHNG,,,Or maybe <@U01B6TH1LRL> or <@U01DHB2HS3X> you have some ideas?,1642337021.494100,1642337571.495100,U01AXE0P5M3\\na9716c57-921a-40ca-955d-6edeb61d7676,U01E06WTHNG,,,\"So far, with multiple projects and experiments I\\'ve only spent EUR 35, this is with Storage and BigQuery. I guess &lt;100 should be a good buffer.\",1642337021.494100,1642338191.496200,U01DHB2HS3X\\n4183bb9e-c0e2-4889-b627-9716566f637f,U01E06WTHNG,,,\"Alternatively, these exercises should also be runnable/queryable without GCP, i.e. on a local environment using Postgres\",1642337021.494100,1642338263.496800,U01DHB2HS3X\\na39e6615-d169-4177-9c62-2db9ba5a4a64,U02T9JQAX9N,,,<https://www.youtube.com/watch?v=-6wlJW5ksFM> Zorin OS (Ubuntu),1642270989.467100,1642339452.497100,U02R4F43B0C\\n07496813-7722-4043-b84b-8e4afdf72076,U01E06WTHNG,,,\"<@U01DHB2HS3X>, I believe we should try the Postgresql path even if we manage to get a free tiered GCP account. Maybe some references to it during the Zoomcamp? I see some people having to get along without GCP. :wink:\",1642337021.494100,1642339497.497400,U02GVGA5F9Q\\n5df6f748-5d82-43b7-aeb7-8b55a3995308,,2.0,,Thanks <@U01AXE0P5M3> for the basic introduction on Docker. It was very insightful.:bulb:,1642339856.499600,1642339856.499600,U02QGA57GRY\\n7b48b78f-f08f-4fd8-bea6-b4452365b259,U01E06WTHNG,,,\"Hi <@U02GVGA5F9Q>! Yes, the course would cover both cloud-based as well as open-source examples. We\\'re still in the process of updating the material on the repo, but it\\'ll be available when the sessions are aired.\\n <@U01AXE0P5M3> anything to add here?\",1642337021.494100,1642339875.499700,U01DHB2HS3X\\n15ed9049-26ab-4542-8ed6-fa016f016ded,U02QGA57GRY,,,Oh it seems you already discovered the playlist? =),1642339856.499600,1642340774.000800,U01AXE0P5M3\\nb76ebeb4-7053-42fc-bfa4-95300c6e444b,U01E06WTHNG,,,Nope =),1642337021.494100,1642340803.001100,U01AXE0P5M3\\n3f5a94a7-a545-4c5c-8234-ce3c9fc2d7c4,U01E06WTHNG,,,\"The only thing we won\\'t be able to do locally is Big query, the rest should work fine\",1642337021.494100,1642340841.001300,U01AXE0P5M3\\n629ea71e-c3a1-4374-bd30-c57f51721d29,,,,\"I\\'m not sure how much sense it\\'ll make as a stand-alone video but you can also check the lesson about docker from <#C0288NJ5XSA|course-ml-zoomcamp> \\n\\n<https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/05-deployment/06-docker.md|https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/05-deployment/06-docker.md>\",,1642341087.002700,U01AXE0P5M3\\ncecf1e84-0e30-4781-ad96-3740dfe7b9eb,,,,\"Hey I\\'m Sudhir, Working as a BI Analyst. Great to meet you all.\",,1642344762.005600,U02U17FD36J\\n6d73fa7f-77ed-4e68-8442-4c852ea1a762,,,,\"Hi, I\\'m Paul, a .NET developer looking to learn more about data engineering.\",,1642346570.006700,U02U7UTSWD8\\n6427760a-3af7-4530-81df-161333d5fc95,,,,\"Hi, I am Karan, a Data Engineer, and a Freelancer. Glad to connect with everyone!\",,1642346736.007400,U02QME6N5EW\\n95345ce1-aae1-493f-8717-04c93efd568d,,,,\"Hii everyone, \\nI\\'m Furkan from Kosovo. It is greet to meet with everyone. I\\'m looking forward to learn more about data engineering. Nice to meet you all.\",,1642347344.009400,U02167JTT98\\nC9299D9C-A3C8-47B4-9EED-C7D0E07796B0,,,,\"Hi everyone, \\nI’m Nandita. Nice to meet everybody here and looking forward to the course :blush:\",,1642350504.013100,U02UAADSJ84\\n9f45356f-9748-4ded-ab7f-46dde0c83727,,,,\"Hi, I\\'m Gregory. I\\'m a CS student from Italy!\",,1642350577.014100,U02U546D36Z\\n2783f160-37ca-4d77-8495-e659cd31832e,,,,\"Hi, my name is Hans Kristian. I am currently working as a data engineer. Looking forward to the course. :)\",,1642351011.014900,U02U1BENXF0\\n9be44654-1b9b-49d4-9a5a-ec92c0fc7201,,,,\"Hello, I am Samuel. I am a network automation engineer looking forward to learning how to wrangle our data into something usable.\",,1642351594.016300,U02TSMSHU5V\\nf5cc631e-37cd-4e78-82e7-e2bf0ae2dfc4,,3.0,,Hello everyone! Any news about tomorrow?,1642352912.019700,1642352912.019700,U02RBKL48KE\\n84603B6E-CAAD-4EE9-A39A-E9F6C0CAF244,,,,\"Hello everyone, my name is Alexis and nice to virtually meet everyone. I am based on San Francisco, California and I am currently a Sr. Analyst in the Healthcare sector. I have a background in Data Science and look forward to learning more about Data Engineering as I plan to shift my career path to DE sometimes in the near future.\",,1642353095.022100,U02UXQ4J9LG\\nd236f0d6-b352-42dd-a450-e0add484db34,,,,Hello everybody. I\\'m Renny and I\\'m currently working as a data engineer. Looking forward to the course!,,1642353791.023900,U02QSRCSM8B\\n523a3459-86af-4f2a-91c6-5c0ecc37cd92,,,,Hello everyone! I am Serenay Ozalp and I am data science researcher. I am planning to shift my career to a data engineering,,1642355089.025300,U02TQUYTBJA\\ne4af4cc5-6229-4b6e-ba3a-a96047d0fb2a,,,,\"Hello everyone, I\\'m Arslane, I am a Junior Data Engineer based in Paris, France and I am looking forward learning new things trough the Zoom camp\",,1642355476.027000,U02U1DNBJ3G\\n0596068e-4d3d-4a25-b2ca-24c58a475119,U02RBKL48KE,,,I\\'ll send an email today. Tldr: tomorrow we\\'ll have the live event at 17:00 CET with all the details,1642352912.019700,1642355638.027200,U01AXE0P5M3\\n120469ae-ec6d-41d6-9e6d-5b266b45c6cd,,,,\"Hello everyone! I am Jishnu, currently working as Data/BI Analyst in Bangalore, India. I\\'m really interested in learning more about data engineering. Looking forward to the course!\",,1642357562.031100,U02ULP8PA0Z\\ndd6a0b69-b503-46c9-848f-8a9fbefb2e15,,3.0,,\"`Link to the call(s)`\\nHas the call link been sent around yet? I haven\\'t yet received anything despite registering. I will have to jump out of work tomorrow, so the earlier I can bookmark it the better :relaxed:\",1642359189.033200,1642359189.033200,U02RZHMDW5D\\n4b01e397-04cc-4bbb-b204-27b615b6aef5,U02RZHMDW5D,,,You can bookmark this one: <http://youtube.com/c/DataTalksClub|youtube.com/c/DataTalksClub>,1642359189.033200,1642360183.034100,U01AXE0P5M3\\ncc7d8c5e-7197-4d99-8f70-ffcc78a6aad4,U02RZHMDW5D,,,We\\'ll go live tomorrow there,1642359189.033200,1642360200.034500,U01AXE0P5M3\\n085654f6-01d7-44d5-9d97-0a0b96bccd88,,,,\"Hello everyone. I am Mikhail from St.Petersburg, currently work as product analyst/product manager. I am here to enrich my tech-stack with new skills\",,1642360349.036200,U02R0SQCCDP\\na14f12f7-2ec3-45c0-909f-8c880a7e4486,U02RZHMDW5D,,,\"Thank you, Alexey! :pray:\",1642359189.033200,1642360458.036400,U02RZHMDW5D\\n25a9f779-98fa-4ce4-b3ca-da0c23e34c31,U01E06WTHNG,,,<@U01AXE0P5M3> just a thought can those with depleted credits use the bigquery sandbox :thinking_face:,1642337021.494100,1642360634.036700,U02QMUQB3FE\\ndaa85c59-36e4-4254-9139-46bae749c951,U01E06WTHNG,,,I have no idea! <@U02QMUQB3FE> do you have an account with depleted credits and could maybe check it?,1642337021.494100,1642362766.038000,U01AXE0P5M3\\nBE6F0812-9DA2-4432-94B2-CAFF852FF728,,4.0,,\"Greetings, I’m Catherine and I’m from Texas! I’m leaving a job as a data analyst and starting as a Data Engineer later this month. Excited to learn &amp; share!! Any advice for getting started with GitHub??\",1642363148.041100,1642363148.041100,U02U88LUYMR\\n6e4a9634-f54f-42da-b077-811a666b2e30,,3.0,,\"Hi, everyone! I\\'m Svetlana from St.Petersburg! I plan to work as a data engineer. I signed up but haven\\'t received any email yet, is it Ok?\",1642363807.043600,1642363807.043600,U02U1HHK88N\\nd68353ef-7f04-4757-90aa-ab1d7091dc24,U02U1HHK88N,,,<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1641765666041400|https://datatalks-club.slack.com/archives/C01FABYF2RG/p1641765666041400>,1642363807.043600,1642363978.043700,U02Q87TQJEN\\nd098a2e9-8dc8-44f7-97e2-0cc754c889a2,U02U1HHK88N,,,Thanks a lot:slightly_smiling_face:,1642363807.043600,1642364185.045500,U02U1HHK88N\\n43a20420-6dec-4a46-b60b-90fa4d0db401,,,,\"Hello everyone from Morocco, I\\'m so exited for this course :wave:\",,1642364188.045700,U02SA55RT44\\n0fbfe40d-fd36-4d4c-8a8a-77a65e4c92ae,U02T9JQAX9N,,,I had this issue on Ubuntu (WSL) too. I had to run dockerd in a screen. After that I could run the hello-world example.,1642270989.467100,1642364918.046100,U02S51KRTNZ\\ne7a3a1b4-d0d6-495e-a618-99bdb207e945,,,,Hello everyone from Zurich. Happy to start a new journey with all of you. :smile:,,1642365097.047200,U02U89P8VDH\\nAE335483-9705-4357-A9A1-F525F56B3FB7,U02RBKL48KE,,,Is that 5 pm or 5am CET?,1642352912.019700,1642365647.048000,U02U85FB8TU\\n9f7c8fff-3703-4a34-9694-038f5b63a362,U01AXE0P5M3,,,Hi guys! Still can\\'t run docker on Win 10 (,1642186903.450800,1642366093.048300,U02C19H2UG3\\nbbd68bbe-4345-4ece-9ba9-81e7caa91f5b,U02U88LUYMR,,,Hello! Do you need suggestions for starting with github in general? or you mean the course\\'s repo?,1642363148.041100,1642366181.048800,U01AXE0P5M3\\n1530114e-deaf-4e4d-97ff-796fc493b2a2,U02U1HHK88N,,,I\\'ll be sending another email today,1642363807.043600,1642366199.049000,U01AXE0P5M3\\ncabb2c41-ee82-40c0-ae06-36831916c089,U02RBKL48KE,,,it\\'s 5 pm,1642352912.019700,1642366236.049700,U01AXE0P5M3\\ncd488db9-8ade-4835-a63a-49a2c5911823,U01AXE0P5M3,,,\"What\\'s your problem, Anna? Do you have windows 10 home?\",1642186903.450800,1642366440.051000,U01AXE0P5M3\\n952dbbb8-4ac8-4149-a1c0-4e45e0b844e3,U02T9JQAX9N,,,\"Did you install docker desktop for your windows? If you do that, you\\'ll have shared docker for windows and WSL - and you won\\'t need to start it\",1642270989.467100,1642366504.051300,U01AXE0P5M3\\n1C215210-CCEC-43B4-9FB3-E7C82542B094,U02U88LUYMR,,,\"In general. I haven’t used github before, so I’d appreciate some tips on what I’ll need to know to be successful in the course.\",1642363148.041100,1642366702.053200,U02U88LUYMR\\nfd0a529c-3372-4bc8-8b48-2d4eea0837c3,U02U88LUYMR,,,\"I don\\'t have good recommendations apart from just creating a repo and trying to commit something \\n\\nBy the way, I Googled \"\"github crash course\"\" and found quite a few tutorials. Maybe some of them will work for you\",1642363148.041100,1642366916.053500,U01AXE0P5M3\\n071eddbf-338f-4697-ab5d-6a7abed3507a,U02T9JQAX9N,,,got it. thanks. I did NOT install docker desktop for windows. I am totally new to docker so not clear what would be the preferred workflow. Is that what you\\'d recommend (the dual install)?,1642270989.467100,1642367797.054400,U02S51KRTNZ\\ncbcf008b-09ad-4f94-a5ef-7902cd86f182,U01E06WTHNG,,,\"<@U02QPBZ3P8D> you can create a new Gmail and create a new GCP.\\nIt would be great to see how these things runs on cloud.\\nGoogle also now add free $100. For non startup GCP newbies.\\nYou can take advantage of this during setup \\nWith your work email.\",1642337021.494100,1642367944.054900,U02RUUJ2TV5\\nc0b7bec5-ec69-46d3-a60e-191b8b532a6a,,,,\"I\\'ve just sent another email - <https://mailchi.mp/datatalks.club/dezoomcamp-1-1>\\n\\nTo remind you about the course kick-off, I\\'ll send another one 10 minutes before we go live.\\n\\nSee you soon!\",,1642367965.055500,U01AXE0P5M3\\nad186f12-2bd9-4b4c-8682-9c42b6128bb6,U02T9JQAX9N,,,\"For windows, I\\'d recommend installing docker desktop for windows - and that will automatically install docker for WSL as well\",1642270989.467100,1642368067.055900,U01AXE0P5M3\\n7c1dec0a-69b9-480b-8ede-3b827fb09070,,,,\"Hello Friends,\",,1642368508.057400,U02UAK8FQ3W\\nea91166e-7919-4d0d-8638-2bc3a10fd2e5,,,,\"I\\'m Kuldeep. I\\'m Data Engineer.\\nLet\\'s connect: kuldeeppal.tech\",,1642368536.058600,U02UAK8FQ3W\\n28A9256C-E24D-4E13-8997-642C2AFF1C6B,,2.0,,\"Got the email about tomorrow. One question, how long the webinar will take? \",1642368547.058700,1642368547.058700,U02Q7JMT9P1\\nd40559b2-b2ef-483f-8a9d-06782de23ba4,,,,<https://kuldeeppal.tech/>,,1642368553.058900,U02UAK8FQ3W\\nc943f3eb-662b-4edf-a64d-46dc415660da,,,,Hi everyone!,,1642368955.059600,U02UY2U862C\\nb4fc036d-4711-4015-8b8a-a9398a7d2d4d,,2.0,,hello can we use AWS instead of GCP and are they similar ?,1642369980.061600,1642369980.061600,U01MNATQ03T\\n3c716c97-e868-4ae6-b61c-ef9e9174c964,,1.0,,\"Hello everyone!\\nLet´s connect <https://www.linkedin.com/in/david-quinta/>\",1642369983.061800,1642369983.061800,U02TTN6GJJ3\\n9b7176ad-653c-4731-9f24-d2d7cea326a5,U02TTN6GJJ3,,,\"Hi David!\\nI\\'ve sent you connection request, let\\'s connect!\\nMy LinkedIn is <https://www.linkedin.com/in/senay-tesfamichael|https://www.linkedin.com/in/senay-tesfamichael> \\n:)\",1642369983.061800,1642370076.061900,U01UY819UG4\\n16b7e346-239f-4449-8e99-9377096c2bb3,,,,\"Hey everyone! I\\'m Petar, a CS student from the UK. Will be starting a DE graduate job at the end of this year.\",,1642371739.065400,U02UALMUNTE\\n3B20BD6C-283F-41AD-8ED9-17666C686F29,U02U88LUYMR,,,\"Thank you, I’ll give it a try!\",1642363148.041100,1642371815.066500,U02U88LUYMR\\n1fb80159-5e56-4b43-816b-a7841dde7d88,,,,\"Hi All Abi from AUS, Working as a Sys Analyst and looking to transition to DE.\",,1642371857.066800,U02UY1QTGHW\\n03452198-ae10-4066-afc6-4a39179d86e8,,1.0,,\"Hi everybody! I\\'m Marian from Bogotá, Colombia. I\\'m working as a Data Dcientist junior but also with some functions as a Data Engineer, that\\'s why I want to learn more about this discipline.\",1642371865.066900,1642371865.066900,U02SWST405V\\n914c0272-49b0-45d7-9f94-0ade84cc7b80,U02SWST405V,,,If someone want to connect this is my linkedin prof <https://www.linkedin.com/in/marianpachon/> :slightly_smiling_face:,1642371865.066900,1642371996.067200,U02SWST405V\\n0999bc35-da5b-4225-9537-8234f6c17b56,,11.0,,\"We already started putting some videos to the course\\'s playlist on YouTube - <https://www.youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb>\\n\\nYou can check them out if you\\'re impatient and can\\'t wait the course to start :slightly_smiling_face:\",1642377725.070900,1642377725.070900,U01AXE0P5M3\\n6e8688ed-205c-4a97-9e0e-c9ca5d23d809,,,,\"Hello everyone, my name is Isis and I am a front-end engineer from the sf bay area. Looking to transition into Data Engineering. Super excited for tomorrow’s course!  :blush:\",,1642378596.073600,U02U1R4NB0E\\nc44dba4c-8688-451d-a6bd-75e1c36b39ca,U01AXE0P5M3,,,Will all the videos for the Zoomcamp be posted by tomorrow or before each week\\'s sessions?,1642377725.070900,1642379238.074100,U02QPBZ3P8D\\n97ecd3cb-4765-4a34-b269-fab207b7b7d6,,,,\"Hi, everyone\",,1642380721.075200,U029ZH5KD38\\nBA9D6F44-E473-464D-8EED-DAA8D1685172,,,,\"Hello All, I am Amey from Chicago. I am an Analytics Consultant looking to move into Data Engineering space. Looking forward to this course and interacting with all of you wonderful peeps.\",,1642381428.079200,U02UM74ESE5\\n79f6046d-210c-4386-ac97-2ce375831942,,,,\"Hi all, this is Zhixin based in Melbourne. I am a junior research Data Scientist but mainly doing data engineering and web scraping. I\\'m currently learning to build up a data warehouse/lake from scratch.\",,1642381662.081600,U02U8HXSY6N\\n5ffeff8e-9b3f-42eb-8055-e5b3bab0f8ac,,1.0,,are we supposed to receive an email by now? i havent received anything,1642382223.082200,1642382223.082200,U0205L73QNS\\nd9d5948f-abba-4994-9f49-6cac51d4f797,,1.0,,when it starts,1642384179.083200,1642384179.083200,U02U8LNQG10\\n62661a55-8aeb-4d17-a29a-1f99a68dd269,,,,Hi everyone! My name is Andrea and I\\'m currently a Data Engineer looking to refine my current skillsets. Looking forward to working on this course and learning with everyone,,1642386945.085700,U02TU41GHFZ\\n58edfea4-348e-4a05-abc7-979b60bac56e,,,,First two videos are great!,,1642387493.086300,U02S51KRTNZ\\n19b23e4b-13e0-45c3-b73d-b668b8d3c170,,1.0,,\"Good to see all here before starting of the course, though not received the email but got the information from here that it would start today at 5 pm CET. Any pre-requisite /pre-read would help , <@U01AXE0P5M3>?\",1642388161.088400,1642388161.088400,U02U8QJLZL4\\nef6df669-8bf4-4572-81c3-b8d9bd18cc58,,,,Hi guys I\\'m Lynn and I am ready to learn about data engineer. Trying to become employable this year. Good luck,,1642388852.089400,U02KBC6KT1T\\n56E197CE-3385-4A6A-A647-2F65DB7AE1B4,U0205L73QNS,,,\"It all happens here, you don\\'t need any email :)\",1642382223.082200,1642390806.096700,U02UX664K5E\\n1c46558d-64bd-4594-a3d9-45c1a1a80531,,,,Hi all :wave::skin-tone-2:,,1642390841.097200,U02U8SBFWD8\\nc19ddb97-ce92-47c0-8e18-b7a386f718e9,,,,hi all,,1642392684.099200,U02SQQYTR7U\\n6f7fd1fe-724f-43c1-9184-932e41bb6fd9,,1.0,,\"this video will be push on youtube channel ??\\nor we can see the livestream ??\",1642392716.100000,1642392716.100000,U02SQQYTR7U\\n05457f72-9489-4ed8-b292-f76453960c22,,,,\"Hi, I\\'m Javier from Spain. I hope to learn a lot from all of you. This course seems amazing! :100:\",,1642393638.101500,U02UMJTBB6D\\n45897502-775b-4f4f-bfa5-f9afd6eac800,,,,\"Hello, I am Yerke! I am looking forward to start the amazing course! I think this course improves my skills in data engineering. Good luck to everyone!\",,1642393754.103100,U02R9627FLM\\n48d280d1-6939-48a8-bdb6-ac2a1ef8bdd0,,1.0,,\"Hi Alexey, can you please share a link for the live event today in this chat? <@U01AXE0P5M3>,\",1642393865.104400,1642393865.104400,U02R9627FLM\\n820a902a-2a7d-4a25-8c18-d489d4fe7804,,,,Hi everyone,,1642397993.107800,U02U67MRU3F\\nbe516280-01f9-4b85-a3fd-2018eb741050,U01E06WTHNG,,,\"It should be less than 100, but it will also depends on how much an individual will experiment with the data\",1642337021.494100,1642398407.108000,U01DFQ82AK1\\n958a3903-1fe8-45c3-8f48-2d64d00d85ec,,,,\"Hello everyone. My name is Roman, I am from Khabarovsk, Russia. Looking forward to the course! Good luck!\",,1642400865.111400,U02RYUWG4CQ\\n08397c50-d719-4aa7-968e-560902619ca9,,,,\"Hello all, this Durga from India, coming from reddit. Looking forward to it\",,1642401122.112300,U02UYTMD516\\n443cedff-74bf-4cf7-b71b-eae6f4d0cc61,,,,Hello! :wave::skin-tone-2: I\\'m İpek from Turkey. Can\\'t wait to get started. Good luck to everyone!,,1642401718.113600,U02U98Y1GLT\\n2fb84d8c-c549-495c-b421-124af3afb0bb,,,,1700pm CET == 1600pm GMT noted,,1642401718.113700,U02RTJPV6TZ\\n6efa577b-1bf1-40f7-b5b6-1c73f6cceb96,,,,\"Hi All, Pratik from India. Looking forward to this.\",,1642402617.114500,U021BQE904F\\nE9057C5C-7B36-425A-AA84-FA2EF05B6936,,,,\"Hi everyone, Sadia from San Jose. \",,1642402831.115500,U02U6DR551B\\nEE52D3A7-B0E5-4D56-874A-C66E5B0D7A65,,6.0,,Will there be a zoom class? Or just YouTube video?,1642402844.116000,1642402844.116000,U02U6DR551B\\n77a97663-1832-4460-aeca-ce41602e61f6,,,,\"Hello,\\nI\\'m Geoffrey, from Nigeria.\\nI\\'m looking forward to gaining this in demand skills :slightly_smiling_face:.\",,1642404517.118700,U02Q6HNS5ST\\n8ff94959-0065-49db-b9d6-380c41a97827,U01AXE0P5M3,,,Before each week session,1642377725.070900,1642404526.118800,U01AXE0P5M3\\nf3235659-267e-4d8a-b686-ff52451956bb,U02R9627FLM,,,<http://youtube.com/c/DataTalksClub|youtube.com/c/DataTalksClub>,1642393865.104400,1642404796.119400,U01AXE0P5M3\\n66e54da6-1671-44d1-bc93-2215b61d2708,U02U8LNQG10,,,Today 17:00 CET,1642384179.083200,1642404842.119900,U01AXE0P5M3\\n666155bf-7c22-4372-97a6-1aeb80c512aa,U02U8QJLZL4,,,\"Check this out \\n\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642367965055500|https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642367965055500>\",1642388161.088400,1642404937.120500,U01AXE0P5M3\\nf0819c02-edc2-4456-ae15-c634978a5516,U02SQQYTR7U,,,\"Both =)\\n\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642367965055500|https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642367965055500>\",1642392716.100000,1642405061.121300,U01AXE0P5M3\\neb451e1e-1019-448f-848c-931cb0f9b845,U02U6DR551B,,,For us it\\'ll be zoom. For you - youtube =),1642402844.116000,1642405125.122200,U01AXE0P5M3\\n0fa055c3-8701-4838-9b77-fb50078c4a2a,U02U6DR551B,,,We\\'ll be using zoom for streaming to YouTube,1642402844.116000,1642405141.122400,U01AXE0P5M3\\n5001bb54-9440-437c-9cff-4261f84ff301,U02U6DR551B,,,\"Only YouTube as I know\\n<https://youtu.be/INy5qVUH6u0|https://youtu.be/INy5qVUH6u0>\",1642402844.116000,1642405144.122600,U02QL1EG0LV\\n8748c110-3a95-46b0-b43b-5059cd651c30,U02U6DR551B,,,thanks <@U01AXE0P5M3> I clear right now,1642402844.116000,1642405663.123500,U02SQQYTR7U\\nc35b1e73-51e4-4ef6-bc0e-c96b6e770b25,U02U6DR551B,,,<https://www.youtube.com/c/DataTalksClub>,1642402844.116000,1642405692.123700,U01AXE0P5M3\\n4DDDE910-0744-4C6B-8589-939C9DE1E602,,,,Hi everyone ,,1642406243.124800,U02TBDVA604\\n6EDBD3BA-938B-4701-ADF8-6ADC3F2B6105,,,,Really looking forward to the data engineering course !!!!,,1642406257.125300,U02TBDVA604\\n342abf34-d5e7-46ed-b022-de5f1b2618fb,,7.0,,\"hi…Am running MacOS 12.1 Monterey. Am able to run the pgcli docker image but when i’m trying to login to pgcli in lecture 1.2.2 (7:30) but getting the following error in the new terminal could not connect to server: No such file or directory\\n```\\tIs the server running locally and accepting\\n\\tconnections on Unix domain socket \"\"/tmp/.s.PGSQL.5432\"\"?```\\n\",1644567924.375569,1644567924.375569,U02U43UTK8W\\nea308698-f1c7-4643-8e82-81034fec7eba,U02U43UTK8W,,,\"this is message i get when i run the docker container in the 1st terminal PostgreSQL init process complete; ready for start up.\\n\\n2022-02-11 08:20:35.477 UTC [1] LOG:\\xa0starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\\n2022-02-11 08:20:35.478 UTC [1] LOG:\\xa0listening on IPv4 address “0.0.0.0”, port 5432\\n2022-02-11 08:20:35.478 UTC [1] LOG:\\xa0listening on IPv6 address “::“, port 5432\\n2022-02-11 08:20:35.486 UTC [1] LOG:\\xa0listening on Unix socket “/var/run/postgresql/.s.PGSQL.5432”\\n2022-02-11 08:20:35.529 UTC [64] LOG:\\xa0database system was shut down at 2022-02-11 08:20:35 UTC\\n2022-02-11 08:20:35.569 UTC [1] LOG:\\xa0database system is ready to accept connections\",1644567924.375569,1644567955.182949,U02U43UTK8W\\n4495507d-48d4-4e64-a0d3-df51ff073f05,U02U43UTK8W,,,How do you do it? Can you try specifying the port explicitly?,1644567924.375569,1644568268.317209,U01AXE0P5M3\\n9207994d-68dc-463d-84b3-7571d9465431,U02V90BSU1Y,,,\"This has ben resolved!\\nI figured I had a wrong service account key.\\nI had to open a new gmail to claim the $300 free credit and forgot to update the new service account key.\\n\\nbelow is the proper response after I did \"\"terraform apply\"\"\\n\\ngoogle_bigquery_dataset.dataset: Creating...\\ngoogle_storage_bucket.data-lake-bucket: Creating...\\ngoogle_bigquery_dataset.dataset: Creation complete after 2s [id=projects/de-zoom-camp-339323/datasets/trips_data_all]\\ngoogle_storage_bucket.data-lake-bucket: Creation complete after 2s [id=dtc_data_lake_de-zoom-camp-339323]\",1644152686.645389,1644568858.658689,U02V90BSU1Y\\ncb1f9b0f-9844-4144-9648-0ca8b32b168b,,2.0,,\"[Week4] Does anyone got this error, when run test\",1644569607.924159,1644569607.924159,U02U8PPL9FU\\nd09db9c1-4f1d-417d-b452-458913406297,U02U8PPL9FU,,,can it be that you are missing the quote identifier setting in the project.yml? You can also find the compiled code under the target folder and find the query that\\'s trying to run to understand why is giving you an error,1644569607.924159,1644571845.394179,U01B6TH1LRL\\n8ba27650-bc25-4b78-8f97-1ffd5fb20e7b,U02U43UTK8W,,,\"```this is the command i used from the video docker run -it \\\\\\n\\t-e POSTGRES_USER=root \\\\ # username\\n\\t-e POSTGRES_PASSWORD=root \\\\ # password\\n\\t-e POSTGRES_DB=ny_taxi \\\\ # db name to be used in practice\\n\\t-v \"\"$(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data\"\" \\\\\\n\\t-p 5432:5432 \\\\\\n\\tpostgres:13```\\n\",1644567924.375569,1644571856.842679,U02U43UTK8W\\n50474282-be8b-4e41-82a8-65bf09d195a8,U02U8PPL9FU,,,thank you,1644569607.924159,1644572244.294109,U02U8PPL9FU\\n88481cd8-b9f5-405d-85d9-0b30d4f0f060,U02Q7JMT9P1,,,\"<@U01DFQ82AK1> That\\'s the problem. It is as expected of type `STRING`, it does not work though to cast the type on the flight in the clustered table definition query. I have tried to change schema with `CREATE OR REPLACE EXTERNAL TABLE` query, but it did work either as I got an \"\"Specifying a schema is disallowed for STORAGE_FORMAT_PARQUET\"\" error\",1644533816.628969,1644573205.119429,U02Q7JMT9P1\\na71cc9fa-4d85-477d-bd45-33b6211705d0,U02U43UTK8W,,,And how do you run pgcli?,1644567924.375569,1644573437.344879,U01AXE0P5M3\\neb9a00b7-ce56-4474-9c3c-4e05d37e5b06,,3.0,,I do not understand why bigquery\\'s location is in US.. I used europe-west6 when creating the biqquery project with terraform.. but now while trying to create an external table.. I\\'m having a location error. How do I solve it?,1644573775.837349,1644573775.837349,U02T9JQAX9N\\na31c9bea-bf30-434c-8892-130cef107991,U02Q7JMT9P1,,,\"Moreover, BigQuery supports string coercion into datetime: <https://cloud.google.com/bigquery/docs/reference/standard-sql/conversion_rules#coercion>\",1644533816.628969,1644573800.200259,U02Q7JMT9P1\\n33b29b2c-daa0-4e6c-901f-54ffac1bfd6c,U02U5SW982W,,,Thank you <@U01DFQ82AK1> :slightly_smiling_face:,1644558804.721179,1644573893.548129,U02U5SW982W\\n832328e7-046c-4237-b84c-98f01acd9d69,U02T9JQAX9N,,,\"it\\'s default location on your query editor, try compose new query and it should solve your problem.\",1644573775.837349,1644574651.454679,U02RA8F3LQY\\nac9483f7-5931-4586-b52c-48ce114c7684,U02U43UTK8W,,,just type pgcli on a new terminal as shown in the video,1644567924.375569,1644575810.290929,U02U43UTK8W\\n9AF57C46-4B7D-442C-B3AD-11AA618221B7,U02Q7JMT9P1,,,Try creating external table with schema ,1644533816.628969,1644575826.280429,U01DFQ82AK1\\n9d9e4d78-2d0b-4591-8d1a-5a5b259cbc19,U02Q7JMT9P1,,,\"Well, that\\'s just getting silly. I have modified the CTAS query that creates clustered table to cast `tpep_pickup_datetime` to datetime in SELECT statement. It did not show an error right away, but by running it I got an error\\n```Error while reading table: datatalks-de-zoomcamp.trips_data_all.external_table, error message: Parquet column \\'RatecodeID\\' has type DOUBLE which does not match the target cpp_type INT64.```\\nAfter forcing RatecodeID to INT64 in SELECT statement, I got the same error about passenger_count, then VendorID. Added casting for them as well... Did not work! I am randomly getting errors about those columns as the cast operations in the SELECT query are ignored.\",1644533816.628969,1644575915.363269,U02Q7JMT9P1\\n149bec7e-5b81-42ae-822d-82cf4970e976,U02Q7JMT9P1,,,\"<@U01DFQ82AK1> I did so, see above. BQ CREATE EXTERNAL TABLE statement does not support schema definition for parquet sources. I have to load all raw files in CSV - so, the second week was for nothing.\",1644533816.628969,1644576255.553379,U02Q7JMT9P1\\n4b32a69d-1fa7-4903-b3bd-43e786962861,,1.0,,\"Regarding the HW 3 (I am soooo late :disappointed: )\\nYou have this at question 5:\\n`What will be the best partitioning or clustering strategy when filtering on dispatching_base_num and SR_Flag`\\n`Review partitioning and clustering video. Clustering cannot be created on all data types.`\\nShouldn\\'t be \"\"Partition cannot be created on all data types\"\" ?\",1644576441.908639,1644576441.908639,U02CD7E30T0\\n0e208867-5289-4d13-b3e8-5244b7ef1262,U02U43UTK8W,,,Can you show the command you execute? I don\\'t remember how it is in the video,1644567924.375569,1644577513.819249,U01AXE0P5M3\\n8463e27a-770c-4a90-b591-185ff7b7674e,U02Q7JMT9P1,,,\"Okay, my struggles are obviously due to some bad rows in the data. I have tried querying the external table directly and had errors in the columns I have mentioned above. Any ideas on how I could work around them? I could add schema check in my DAGs from week 2 and skip bad rows.\",1644533816.628969,1644577723.766249,U02Q7JMT9P1\\nf1f43333-5851-48d1-bf1b-0d0fe6e6225a,,8.0,,\"Hi guys,\\nI have a question regarding new Macbooks and thought maybe you can help me here. I\\'m considering buying a new MacBook Pro (m1) and have heard it has some problems with some software. Does anyone use this laptop and have any problems? with tools that we used in this course and other tools that you use for your work, etc.\",1644577879.991049,1644577879.991049,U030FNZC26L\\n4c660d50-4146-4d98-9af7-1ff16d5f95e4,U030FNZC26L,,,I can say that lot of new joiners at my job have experienced issues with Python and dbt installations with M1 MacBook Pro.,1644577879.991049,1644580043.858869,U01B6TH1LRL\\na1a9bbf9-2c4d-4432-b231-f3078d806e07,U02TNEJLC84,,,\"<@U02HB9KTERJ> The results weren\\'t EXACT, but they sure were darn close for me. You might want to try partitioning and clustering on different columns and see what the results look like. The best partition/cluster solution for this **cough answer from question 3, should give something close.\",1644539007.881829,1644580454.606709,U02TNEJLC84\\nc851ae02-d2a6-4222-ace2-c96887162aad,U030FNZC26L,,,Docker images are architecture-dependent and most images are built for x86. I believe Docker for M1 Macs supports cross-building but I’m not sure about how it runs regular images.,1644577879.991049,1644581524.310199,U02BVP1QTQF\\ncb4d70e0-0b8c-40cc-b892-c36b0f953230,U030FNZC26L,,,\"thanks guys,\\nI\\'m also thinking about buying a used MacBook Pro with Intel or buying a machine for Ubuntu, like a ThinkPad x1 or sth.\\nDo you have any suggestions? I just want it for software development.\",1644577879.991049,1644581836.797479,U030FNZC26L\\n3EDCF160-45E4-4B7C-8E3A-D7414345D12C,U02TNEJLC84,,,Yes best would be to create a table (partitioned and clustered) which would be best for the query ,1644539007.881829,1644582381.130209,U01DFQ82AK1\\n5979f150-d764-4da3-9718-0f9bc5e4d6a6,U02T9JQAX9N,,,\"Hmm.. Okay, let me try that\",1644573775.837349,1644582555.946139,U02T9JQAX9N\\nfe2f3fc4-dd26-4324-9242-184a8ed4370c,U02V90BSU1Y,,,\"I have been able to resolve this.\\nThe challenge was that the below block of code was missing in my official docker-compose.yaml file\\nbuild:\\n\\xa0 \\xa0 context: .\\n\\xa0 \\xa0 dockerfile: ./Dockerfile\\n\\nit was commented as below\\n#build: .\\n\\nIt worked the moment this was resolved, I must have missed the part where Sejal implemented this\",1644140999.329219,1644582599.839609,U02V90BSU1Y\\n77957bb9-2ec9-4eff-9977-b443fc280fb3,U02T9JQAX9N,,,Thamks.. This worked,1644573775.837349,1644584663.375149,U02T9JQAX9N\\nb4ede300-c319-422b-a72d-322b04cef40b,U02U34YJ8C8,,,\"I chose to solve this one by explicitly defining the green taxi data shema (NOTE: import pyarrow as pa) table_schema *=* pa.schema(\\n    [\\n        (‘VendorID’,pa.string()),\\n        (‘lpep_pickup_datetime’,pa.timestamp(‘s’)),\\n        (‘lpep_dropoff_datetime’,pa.timestamp(‘s’)),\\n        (‘store_and_fwd_flag’,pa.string()),\\n        (‘RatecodeID’,pa.int64()),\\n        (‘PULocationID’,pa.int64()),\\n        (‘DOLocationID’,pa.int64()),\\n        (‘passenger_count’,pa.int64()),\\n        (‘trip_distance’,pa.float64()),\\n        (‘fare_amount’,pa.float64()),\\n        (‘extra’,pa.float64()),\\n        (‘mta_tax’,pa.float64()),\\n        (‘tip_amount’,pa.float64()),\\n        (‘tolls_amount’,pa.float64()),\\n        (‘ehail_fee’,pa.float64()),\\n        (‘improvement_surcharge’,pa.float64()),\\n        (‘total_amount’,pa.float64()),\\n        (‘payment_type’,pa.int64()),\\n        (‘trip_type’,pa.int64()),\\n        (‘congestion_surcharge’,pa.float64()),\\n    ]\\n)\",1644507124.002539,1644585144.137509,U030P1LNK5Z\\n1e9a6d1a-48b7-47d4-8b67-27e0aa78d5c7,U02U34YJ8C8,,,Then add a line in your format to parquet function:,1644507124.002539,1644585174.454819,U030P1LNK5Z\\n0646bdff-f4d8-4059-a440-17b145500fb8,U02U34YJ8C8,,,\"*def* format_to_parquet(*src_file*):\\n    *if* *not* src_file.endswith(‘.csv’):\\n        logging.error(“Can only accept source files in CSV format, for the moment”)\\n        *return*\\n    table *=* pv.read_csv(src_file)\\n    table *=* table.cast(table_schema)\\n    pq.write_table(table, src_file.replace(‘.csv’, ‘.parquet’))\",1644507124.002539,1644585176.637919,U030P1LNK5Z\\n0feba818-1091-4882-b3dd-265906fdad47,,2.0,,\"Hi All, want to ask is DE Zoomcamp 3.3.3 a necessary steps need to work on?\",1644586658.357369,1644586658.357369,U02T697HNUD\\n3b63c4fc-944c-4ed7-a661-6852cb2cdf0f,U030FNZC26L,,,\"Think Pad with Ubuntu is excellent for development. I use it at work. But if in addition to development you need other things, like editing an image or video or even have zoom calls, the software is less stable and might cause some headaches\",1644577879.991049,1644587544.845579,U01AXE0P5M3\\nd00e40a1-72f7-4cf8-8a78-2fd5725a77a8,U02TATJKLHG,,,\"I tried this but I have trouble with the profile.yml file.\\ndbt did not seem to recognise that I had a pre-existing profile.yml file and I had to recreate one through the CLI during init. Afterwards, I was not allowed to edit the profile.yml file.\\n\\nDid you have a similar experience?\",1644493645.074519,1644588924.743509,U02T941CTFY\\n8d8fbfaa-b6ec-43cd-a5c6-12521b769395,U02TATJKLHG,,,How do you open the profiles.yml that it doesn\\'t allow editing? Are you using windows?,1644493645.074519,1644589072.823159,U01B6TH1LRL\\n0f4697de-dffc-4b7a-b17e-11d355dc6b38,U02TEERF0DA,,,Thanks!,1644518472.082729,1644590105.570919,U02TEERF0DA\\nbaeb060e-8036-4dd8-ace4-d6a37440dd39,U030FNZC26L,,,\"Yeah, it seems like the ThinkPad is a good option. But its price is almost the same as the MacBook Pro :smile:\\nThanks for answers guys :slightly_smiling_face:\",1644577879.991049,1644590466.615219,U030FNZC26L\\nf27dbfab-ef52-45ed-a0c5-7badc4511c1f,,5.0,,\"Hii, I\\'m following the week 4, and it used green and yellow taxi data, I\\'ve already had the green data in my GCS but when I try to create new table this an error appear. It seems the schema not right so I want to manually change to Float64 (in my query) but I couldn\\'t query the external table neither manually change the schema, how to handle this\",1644591360.372469,1644591360.372469,U02TA7FL78A\\n13750ebe-9238-4db6-bd43-934a2cf37b95,U02TA7FL78A,,,\"I\\'m not sure I follow the error, you have this while running the dbt code? can you try numeric? That\\'s the datatype we use in that code\",1644591360.372469,1644592385.717049,U01B6TH1LRL\\n1713a300-d934-44eb-abfd-e70f77ce234d,U02TA7FL78A,,,\"See this thread.\\n\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1644429302569509>\",1644591360.372469,1644592618.237809,U0290EYCA7Q\\n781f0851-6465-4f81-8488-321e9de6ed4b,U02TA7FL78A,,,NaN in python is actually of float datatype. There are some NaN values in green taxi data for ehail_fee column.,1644591360.372469,1644592727.952249,U0290EYCA7Q\\n3E2DA8C5-FADB-48F3-9C2C-75F2D9317DD1,U02CD7E30T0,,,You are right. I am away from laptop. Will fix later ,1644576441.908639,1644594085.897829,U01DFQ82AK1\\n87ff82a0-4feb-4eac-85f5-d568a47a22a5,U02TA7FL78A,,,\"<@U0290EYCA7Q> thanks, I used your DDL for creating the table and solved the error, does it convert the None and NaN(float in python) to null in bigquery?\",1644591360.372469,1644594122.782059,U02TA7FL78A\\nBC29830E-B10E-49F6-A897-E3572E19DF53,U030FNZC26L,,,I\\'ve got an M1 and no real issues so far ,1644577879.991049,1644594675.212539,U02U34YJ8C8\\n073ae8c0-db04-4335-82ef-7feaa416030c,U030FNZC26L,,,good news :smile:,1644577879.991049,1644595579.893759,U030FNZC26L\\n42c58b92-a35a-44e8-bd37-b7d6113017ff,U02R09ZR6FQ,,,\"Hi <@U02SEB4Q8TW>, sorry to only answer now, I haven\\'t been here in slack the last few days. No, I have been doing everything locally, I didn\\'t set up VM so I really don\\'t know :sweat_smile:\",1643816806.173399,1644595981.260809,U02R09ZR6FQ\\n71b97305-df65-4aff-b2af-42b8474ef614,,,,\"hello\\nfor Question 2, anyone getting an answer from the options?\",,1644599625.965329,U029DM0GQHJ\\n683e60ff-8285-4444-924f-4893354fb538,,1.0,,\"Hi, people:grinning:  I have a question regarding command gcloud auth application-default login. I read in documentation  <https://cloud.google.com/sdk/gcloud/reference/auth/application-default/login?hl=zh-TW|https://cloud.google.com/sdk/gcloud/reference/auth/application-default/login?hl=zh-TW> that this command is used for a case \"\"if you want your local application to temporarily use your own user credentials for API access, run...\"\". I don\\'t get it. In week 1 we created key for a service account and set GOOGLE_APPLICATION_CREDENTIALS. Why then we use \"\"user credentials\"\"?\",1644600126.431209,1644600126.431209,U02TCDCDS9H\\nd88b81a8-18bf-4b58-b1bc-e678a69c815f,U02TA7FL78A,,,\"None is converted to null (int). NaN isn\\'t, because it is float.\",1644591360.372469,1644600775.052149,U0290EYCA7Q\\ne1a14f60-cf92-4497-b4df-bb47d8ccc6ec,,1.0,,\"I don\\'t have `wget` and I can\\'t install one at the moment. what\\'s the alternative method to use\\n`os.system(f\"\"wget {url} -O {csv_name}\"\")` when downloading the CSV. Thanks\\n\\ncc: <@U01AXE0P5M3>\",1644601411.927549,1644601411.927549,U02CZA4HX99\\na2672297-bc56-45e6-bb8a-aac9bbd5359f,,5.0,,\"Is it only me or the schema.yml on git will fait the test for accepted values because there is no quote=False afterwards?\\nIt is the case in the video but not in the file online.\",1644601609.415499,1644601609.415499,U02TC8X43BN\\n117fd619-6711-4ac2-a2ef-1bafa887dc2e,U02TC8X43BN,,,\"I set it in the pject I created from scratch and not the original code, I\\'ll update it\",1644601609.415499,1644602147.736189,U01B6TH1LRL\\nddd8b4de-d2e3-4fd7-a14b-06e5e1b13339,U02TC8X43BN,,,\"updated, it\\'s in the main repo now. Anyways, you can directly add it as I do in the video\",1644601609.415499,1644602258.008219,U01B6TH1LRL\\n816c2463-792a-482c-b5ef-f42d5ae41d94,U02TC8X43BN,,,\"I got confused because in the video you say something like the list of values is without a quotes since it is an integer in the database, but it took me a while to understand it also needed quote is false (which I think you don\\'t mention in the video, unless I am mistaken).\",1644601609.415499,1644602591.420379,U02TC8X43BN\\n75b114ab-14dc-458c-af2c-980a4e3c75a7,U02UNQNMH7B,,,\"Hi <@U02UNQNMH7B>\\n\\nYour idea of using `curl` saved a life:grinning:. Thank you!\",1643733210.051579,1644602600.253389,U02CZA4HX99\\n,,1.0,,`curl` did the magic. Thanks,1644602678.120899,1644602678.120899,U02CZA4HX99\\n71597ae8-e2f5-4c07-8c7b-b40e76d9aef8,U02TC8X43BN,,,\"I mention that I added `quote: false` specifically to avoid the error because bigquery will use \\'1\\' instead of 1 and fail.\\nAn interesting thing for you to check is to compile the code and look for it in the target folder with and without the quote.\",1644601609.415499,1644602777.282029,U01B6TH1LRL\\nbdffd609-35c6-412d-bc20-e4ffe2c59f80,U02CZA4HX99,,,Happy to hear you resolved it. Next time please reply in a thread,1644602678.120899,1644602785.445229,U01AXE0P5M3\\necec7c09-c380-406d-a2b0-2899fb8a6e22,U02TC8X43BN,,,\"Oh you actually did indeed and very explicitly.\\nI am sorry, I kept running into an error and rewinding the video at 8:26 but not earlier, there you mention quote but I thought you meant quote in this list.\\ni did test it without quote false and i kept getting the error cannot use IN for  string, as I was using the file from git, which I thought was properly formatted, I was really confused.\\nI am trying to follow along the video but since there was a big block of text added, I thought I will just copy the file.\\nI had a similar issue earlier when I copied dbt_project.yml and it took me a while to realize the seeds folder was different than by default.\",1644601609.415499,1644603374.299869,U02TC8X43BN\\nB8CD94BE-EEF5-4D3E-A962-E16744AA9A98,U02CZA4HX99,,,\"Please use curl as an alternative command \\n$ curl url &gt; output_file \",1644601411.927549,1644603627.522779,U02AGF1S0TY\\ne2ded9e4-52b3-4518-8f7f-df15dae25f61,,2.0,,\"how do i create a table in bigquery. I tried this and won\\'t work\\nCREATE OR REPLACE EXTERNAL TABLE `trips_data_all.FVH_tripdata_external_table`\\nOPTIONS (\\nCAST(dispatching_base_num as String) dispatching_base_num\\n,CAST(pickup_datetime as DATE) pickup_datetime\\n,CAST(dropoff_datetime as  DATE) dropoff_datetime\\n,CAST(PULocationID as INTEGER) PULocationID\\n,CAST(DOLocationID as INTEGER) DOLocationID\\n,CAST(SR_Flag as INTEGER) SR_Flag\\n  format = \\'parquet\\',\\n  uris = [\\'<gs://dtc_data_lake_dezoomcamp22/raw/fhv_tripdata_2019-*.parquet>\\']\\n);\",1644605172.029359,1644605172.029359,U02T8HEJ1AS\\n3ca4b4c3-2b2c-409a-b943-1667df56e09c,U02TC704A3F,,,\"Hey <@U02TC704A3F>, I made a few tweaks to the original file so that the request was properly parsed and then converted to DataFrame format. After uploading the files with the script I had no problem creating the external table on BigQuery. Here\\'s the script, hope it helps <https://github.com/sebastian2296/data-engineering-zoomcamp/blob/main/week_4_analytics_engineering/web_to_gcs.py>\",1644536483.340339,1644607592.623909,U02SUUT290F\\ne49ef7d9-9c4d-40f1-a7f1-a1482663e804,U0319KGEJ13,,,\"No, sadly enough wasn\\'t able to figure it out and also had some problems regarding loading in the files, 70% did work while others were crashing cause of missing fields.\\n\\nSo, I decided to swap to GCP instead of fighting the docker postgres battle :sweat_smile:\",1644433771.777729,1644610016.035059,U0319KGEJ13\\nfbe96148-95b4-4bac-81bf-f55f889c72ec,U0319KGEJ13,,,\"Not that that made it easier, since now with the github files, I\\'m not allowed to login into my airflow with \"\"airflow airflow\"\", even though the setup literally says the user already exists.\",1644433771.777729,1644610189.786609,U0319KGEJ13\\n5835e70f-350b-4df5-a4a0-e37b4e4a70f4,U0319KGEJ13,,,\"Hm, do you use the \"\"official\"\" compose file?\",1644433771.777729,1644611165.075799,U01AXE0P5M3\\n1c2166a4-b692-4a8a-b99b-7897d597a5f2,U0319KGEJ13,,,\"Yes, all I did was add my google creds + project id + bucket id\",1644433771.777729,1644611192.537269,U0319KGEJ13\\n4e903ed3-56b5-4a65-bb43-b9cae48d74a8,U0319KGEJ13,,,\"Might be just my windows bugging me, so currently setting up a vm in google instead.\",1644433771.777729,1644611210.592439,U0319KGEJ13\\neac4b9d8-b4e7-4fff-8cb0-eb3d480b106d,U02T697HNUD,,,\"If you want to continue using BigQuery for later parts of the course, it\\'s important. If you can\\'t get access to BigQuery, you\\'ll be using local Postgres instead and you can skip that video.\",1644586658.357369,1644614254.288449,U02U3E6HVNC\\ne2a8d670-23b8-40b6-8446-37d477a3ce1c,U02T8HEJ1AS,,,What error are you getting?,1644605172.029359,1644614408.939559,U02U3E6HVNC\\nd1a87f0b-82c1-4337-b245-fd14304695ac,U0319KGEJ13,,,\"Nope.. Also on the new VM I can\\'t seem to login with airflow airflow.\\n\\nRunning the following commands in week_2_data_ingestion/airflow:\\n`docker-compose build`\\n`docker-compose up airflow-init`\\n`docker-compose up`\\n\\nEverything is up and running, but the creds are still bugging me, just like on my windows :confused:\\n\\nCopied the .env_template into a new .env file and added my (same for in the actual `docker-compose.yaml`)\\n&gt; AIRFLOW_UID\\n&gt; GCP_PROJECT_ID\\n&gt; GCP_GCS_BUCKET\\nUnsure why I\\'m struggling so much getting my environments to run :confused:\",1644433771.777729,1644615712.527129,U0319KGEJ13\\nad6c026b-db59-43d2-bb7c-a83243e6618d,,,,\"[GCS to BQ DAG] Has anyone accidentally deleted their trips_data_all dataset from BQ, recreated it and successfully exported the data from GCS to BQ? I foolishly deleted my trips_data_all BQ dataset and now can\\'t get the DAG to work again... [Edit: solved — was another issue with syntax on exporting data from GCS, not sure why it was throwing a dataset not found error...]\",,1644617794.172459,U02SM3LKD2B\\nb1a47c6a-0a75-415a-a859-67f77f2b234a,U02TC704A3F,,,\"Hey <@U02SUUT290F>, thanks for that! I will try out this later.\\n\\nTrying to not get too far behind in week 4 :crossed_fingers:\",1644536483.340339,1644617853.974119,U02TC704A3F\\n4036dc88-8b7c-4284-b3c1-434a2846767e,U0319KGEJ13,,,\"Try without copying env template, I think it might be overriding some of the config variables\",1644433771.777729,1644618823.473279,U01AXE0P5M3\\n05f102b6-64ee-4a6f-bf3d-b200d2e36ad6,U0319KGEJ13,,,Just set the airflow uid there,1644433771.777729,1644618845.023409,U01AXE0P5M3\\n0b73a4a7-37b2-47c7-aa09-61e98c2dc21e,,7.0,,\"when creating a project in dbt cloud, I cant find where to put my oauth2 credentials. I can see *Bigquery Settings but no Oauth2.0 Settings as shown in the instructions file*\",1644630270.056449,1644630270.056449,U02V9V9NLJG\\n5b4afad0-430c-4b68-9b25-b8d9bf6e06e5,U02T697HNUD,,,Alright thanks for the info.,1644586658.357369,1644631766.908339,U02T697HNUD\\nfc0fa7ba-489e-4967-a572-8d697afdda2c,U02TATJKLHG,,,I am using Windows + WSL. I tried to edit it using notepad. Perhaps not the best way to go about it,1644493645.074519,1644632783.392629,U02T941CTFY\\n186009ef-62a8-47a9-919d-f4e302ecde19,,5.0,,\"[WEEK 4] Hi! In case someone wants to install `dbt` locally as a Python package in Anaconda, I documented my experience here: <https://github.com/canovasjm/deng-homework/blob/main/week_4/dbt_anaconda_setup.md>\\nThe official `dbt` docs explain how to do it using pip, but there is no help on how to do it in Anaconda environments\",1644635944.221919,1644635944.221919,U02S2TZRBL7\\n8383d9f8-acf3-4458-818a-6c2017153939,,18.0,,\"Has anyone else ran into issues with the final “dbt run” command, specifically with the fact_trips.sql model? All my other models seem to be running fine but i am getting the following error for the fact_trips model. Thanks!\\n\\n```03:19:54  4 of 4 START table model dbt_jpak.fact_trips.................................... [RUN]\\n03:19:55  4 of 4 ERROR creating table model dbt_jpak.fact_trips........................... [ERROR in 0.70s]\\nDatabase Error in model fact_trips (models/core/fact_trips.sql)\\n  Access Denied: BigQuery BigQuery: Permission denied while globbing file pattern.\\n  compiled SQL at target/run/taxi_rides_ny/models/core/fact_trips.sql```\",1644636171.927259,1644636171.927259,U02TBTX45LK\\n55c86a1d-6a41-460d-94a5-26859a6b96a0,U029DM0GQHJ,,,i forgot to add the keyword \\'EXTERNAL\\',1644516749.597819,1644645399.959659,U029DM0GQHJ\\n45772111-a8f5-4db7-bd35-d98987c6fd42,U02S9SSURMH,,,Thank you very much. Issue fixed,1644514706.213179,1644646059.891759,U02S9SSURMH\\n9b8700ff-895e-4dd4-bbe7-3280fd43a13f,,,,\"for homework3 question 4, do we have to query on partitioned table?\",,1644646906.622069,U02U6DR551B\\n63a8ceec-40b0-464f-996f-af231e086be6,,7.0,,\"i am confused about question 4, homework 3\\nWhat is the count, estimated and actual data processed for query which counts trip between 2019/01/01 and 2019/03/31 for dispatching_base_num B00987, B02060, B02279\\nwhich columns to use to determine trip count?\",1644648400.857029,1644648400.857029,U02U6DR551B\\ne79db2e5-1163-4bc7-a9d6-1a5e5fa04df9,U0319KGEJ13,,,\"I\\'ll try it in the afternoon, hopefully I can get it to work and finish W3 homework in time!\",1644433771.777729,1644648660.182469,U0319KGEJ13\\nb705dd8c-20b9-4c62-989c-53cbc22bd605,U02U6DR551B,,,I believe \\'count\\' refers to the number of rows. Hence count(*),1644648400.857029,1644649214.100539,U02T941CTFY\\n8413427d-45f3-4fc8-8fe4-4104f08bd0dd,U02U6DR551B,,,my count is not matching the ones in the options,1644648400.857029,1644649515.006799,U029DM0GQHJ\\nd940cd76-34a9-4f56-acdf-fe4c3745c451,,2.0,,\"hello\\neven after casting to date:\\n\\n```create or replace table `dtc-de-338618.trips_data_all.fhv_tripdata_all_partitioned`\\npartition by \\n    date(pickup_date_partition) as\\nselect \\n    dispatching_base_num, \\n    cast(\\n        concat(arr_pickup[SAFE_OFFSET(0)], \\'-\\', \\n            arr_pickup[SAFE_OFFSET(1)], \\'-\\', \\n            split(arr_pickup[SAFE_OFFSET(2)], \"\" \"\")[SAFE_OFFSET(0)]) \\n        as date) as pickup_date_partition,\\n    cast(\\n        concat(arr_dropoff[SAFE_OFFSET(0)], \\'-\\', \\n            arr_dropoff[SAFE_OFFSET(1)], \\'-\\', \\n            split(arr_dropoff[SAFE_OFFSET(2)], \"\" \"\")[SAFE_OFFSET(0)]) \\n        as date) as dropoff_date_partition,\\n    -- pulocationid_cast,\\n    -- dolocationid_cast,\\n    sr_flag_cast,\\n    Affiliated_base_number as affiliated_base_number \\nfrom( \\n    select \\n        dispatching_base_num, \\n        pickup_datetime,\\n        dropoff_datetime, \\n        cast(pickup_datetime as date),\\n        split(pickup_datetime, \\'-\\') as arr_pickup,\\n        split(dropoff_datetime, \\'-\\') as arr_dropoff,\\n        -- cast(PULocationID as INTEGER) as pulocationid_cast,\\n        -- cast(DOLocationID as INTEGER) as dolocationid_cast,\\n        cast(SR_Flag as INTEGER) as sr_flag_cast,\\n        Affiliated_base_number \\n    from `dtc-de-338618.trips_data_all.fhv_tripdata_all`\\n) x\\n\\nit would still complain: PARTITION BY expression must be DATE(&lt;timestamp_column&gt;), DATE(&lt;datetim```\",1644649584.455619,1644649584.455619,U029DM0GQHJ\\n0e9cd0e5-36ad-4718-86ef-58755ede4741,U02U6DR551B,,,\"Mine does not match exactly as well, I just picked the closest. There are only two choices for that, in fact ;)\",1644648400.857029,1644649679.285939,U02T941CTFY\\n53e9d059-2045-42f7-b025-7b25e79d27a2,U02U6DR551B,,,\"My count matches, but i think i havent created the optimised table\",1644648400.857029,1644649878.503549,U02U6DR551B\\n861691f5-baea-4c4a-99d8-39bc3461a4a3,U02U6DR551B,,,you cannot create partition on str column right?,1644648400.857029,1644649996.558789,U02U6DR551B\\nf7a0ac60-8370-4cee-8b9a-863e69015dab,U02U6DR551B,,,no you cant apparently,1644648400.857029,1644652378.496109,U029DM0GQHJ\\na38ac81e-83fe-4732-8688-3b1eadc82f6b,U02U6DR551B,,,\"it has to be one among date, datetime, timestamp etc\",1644648400.857029,1644652398.775669,U029DM0GQHJ\\n237d8895-5a0c-4986-8425-cd7b9f3c3788,,3.0,,\"Hi, every week I struggle with misunderstanding which steps should I go though to be ready to a homework. Are there only videos to extract these steps?\\nGreen taxi data is used in week3. Was there a request to ingest it earlier?\",1644310727.980389,1644310727.980389,U0301PJRDNV\\n0d762de9-9ac4-4b44-b029-7a45e08d0b62,U0301PJRDNV,,,\"I was also a little confused this week, but for week 3 you only need to ingest the FHV 2019 data into BigQuery.\\n\\nI think that it would be a great idea to add a couple of bullet points to the readmes for each week right at the beginning, mentioning what datasets are used as examples to follow the videos and which ones will be needed for the homework, because at the moment the naming conventions and examples don’t fully match among the teachers and it get confusing easily.\",1644310727.980389,1644313058.570469,U02BVP1QTQF\\n593ba2e8-5eda-4647-bd17-7c4ddced9572,,2.0,,\"Is database clustering a unique feature of big query? I googled \"\"partitioning and clustering\"\" and the only results I got were about gcp big query.\",1644315747.760699,1644315747.760699,U0308865C0H\\n9cdec193-239d-44bf-bc47-a86b45dcc7cc,U02BVP1QTQF,,,\"Hey <@U02SEB4Q8TW>, I updated my gist with a few additional notes, like how to create an instances from the command line with gcloud sdk and how to upload/download files. You may not need it but feel free to copy the changes to your gist if you like.\",1643661114.567919,1644316208.130959,U02BVP1QTQF\\n0ad8866f-19b2-4ec6-9b52-4dee937b98e9,U0301PJRDNV,,,\"Sorry for the confusion!\\n\\nHere\\'s a summary:\\n\\n• for week 3 videos you\\'ll need yellow taxi data\\n• for week 3 homework you\\'ll need FHV data\\n• for week 4 videos you\\'ll need yellow and green taxi data\\n• for week 4 homework - not sure, perhaps FHV as well\\nWe\\'ll give you a script for moving all this data to GCS, so it\\'s okay if you are a bit behind and didn\\'t have a chance to work on it during week 2. Also you can use transfer service which will move all the data very fast\",1644310727.980389,1644316386.157639,U01AXE0P5M3\\n5621158d-8fdf-4299-8758-8cff2159bfea,U0301PJRDNV,,,\"Jus confirming for week 4 homework the same FHV data you loaded for week3, I also added the exact datasets we\\'ll use as pre-requisites in week 4 readme\",1644310727.980389,1644317516.604679,U01B6TH1LRL\\n63f9f4c0-876e-47b1-b9d8-ceb21a77369c,U0308865C0H,,,\"As far as I understand, it is essentially the same as clustered indexes in relational DBs\",1644315747.760699,1644319730.067359,U02Q7JMT9P1\\n88b9a8f8-9b9e-4aee-ae32-baea297bfc30,,1.0,,\"Hi, do I understand from the livestream yesterday that week 4 will take two weeks before proceeding to week 5 of the course ?\",1644320904.622889,1644320904.622889,U02TMP4GJEM\\n1174fc9c-3717-46ed-9155-aed544114a91,,4.0,,\"To confirm I’ve not made a mistake, is 42,084,899 the number of records expected in the FHV BigQuery table?\\n\\nAlso, in the videos we were shown\\n how to create an external table, and then from there, create a native table, like so:\\n\\n```CREATE OR REPLACE EXTERNAL TABLE `project.trips_data_all.external_table`\\nOPTIONS (\\n  format = \\'parquet\\',\\n  uris = [\\'<gs://dtc_data_lake_silent-oasis-338916/raw/tripdata_2019-*.parquet|gs://dtc_data_lake_silent-oasis-338916/raw/tripdata_2019-*.parquet>\\']\\n);```\\n• create an external table from cloud storage\\n```CREATE OR REPLACE TABLE project.trips_data_all.native_table AS\\nSELECT * FROM project.trips_data_all.external_table;```\\n• create native table from external table\\nIs there a way to create a native table from cloud storage, skipping the external table creation step? I thought it could be done by removing the word EXTERNAL from the first query, but doesn’t appear to work.\",1644322188.876309,1644322188.876309,U02U34YJ8C8\\ndc67ffab-550a-488f-808f-f462358bfabf,U026040637Z,,,\"I\\'m on the account `dtc-de` (I checked that this is indeed the account I\\'m using on the Data Transfer, Data Storage, BigQuery, VM windows - shows up in the top left) with IAM permissions as: `BigQuery Admin, Storage Admin, Storage Object Admin, Viewer`. Is there an additional permission type I need?\",1644288053.633869,1644324415.168159,U026040637Z\\n1041d80c-a874-4392-ac4c-80f1bdbd8637,U02QMQWRMRS,,,It\\'s the same for me. You\\'re on the right path,1644265727.094329,1644325522.428169,U02T941CTFY\\ne19be7fa-aeec-4cd3-ba05-8ce3b321fd74,U02TMP4GJEM,,,\"Hey, every week is one week. But because  there were some delays with homework  we extended all deadlines a week, making every \"\"zoomcamp week\"\" two weeks in real life since we release the content until homework deadline\",1644320904.622889,1644325705.178609,U01B6TH1LRL\\ne2794d74-11a6-46be-bac0-19165d6f4cf1,U02U34YJ8C8,,,I can confirm that I have the same number of records,1644322188.876309,1644325727.723809,U02T941CTFY\\n546684ef-1b9c-426d-aac8-36e5efac6541,U02U34YJ8C8,,,Thanks Zach!,1644322188.876309,1644325878.525299,U02U34YJ8C8\\n0e1aca90-8195-4603-8677-e788d3442848,U02TJ69RKT5,,,\"<@U02UP3KN3SQ> I ran the command and apparently I only create the table for January. Thanks for helping me, I didn\\'t understand what was happening:man-facepalming::skin-tone-2:.\",1644287510.558709,1644327203.587989,U02TJ69RKT5\\n7759f81b-25e6-417d-9f9c-7607f10f614f,U02U34YJ8C8,,,Is the count without jan 2020 data?,1644322188.876309,1644327273.223809,U0290EYCA7Q\\nbf25cd0b-4a4c-4b14-80ea-2cc2fb790c7f,U02U34YJ8C8,,,No just 2019 I think,1644322188.876309,1644327894.498069,U02U34YJ8C8\\nd28ab021-6e11-4802-a14d-4ebfe6dbd111,,1.0,,\"For week 2,\\n\\nShouldn\\'t the airflow-worker part of the docker-compose have a kind of volume mapping or does it? If I run the `download_csv_task` and for some reason, my docker-compose network gets disconnected.. If I reconnect, I would have to redownload it again\",1644329108.776809,1644329108.776809,U02T9JQAX9N\\nb7a434cc-51ce-45ad-a0a0-84c97f60b450,U02SZARNXUG,,,Thank you everyone..I will try your option <@U02HB9KTERJ>. It frustrated me throughout the weekend. I couldn\\'t submit my assignment. I\\'m back to work now so I\\'ll try in out by weekend or one of the midnights before then:smiley:,1644136107.540759,1644329467.963109,U02SZARNXUG\\nf8b6784e-4029-4a83-8b42-e3ba8ab5af0b,U02T9JQAX9N,,,yes it\\'s a good idea to have this mapping,1644329108.776809,1644329470.895669,U01AXE0P5M3\\n101169a2-a1a6-4c2f-b2b9-04044e294791,U02HB9KTERJ,,,what happens to the data written to the data lake after i destroy?. Can i still access it when irun terraform apply later?,1643618720.680959,1644330004.742829,U02TA6MJZHR\\n1fec3761-71e9-4700-8331-0fa14ff45d43,,8.0,,\"[Off-topic] What do you guys think of this data engineering humble bundle? Is it worth it?\\n\\n<https://www.humblebundle.com/books/data-engineering-oreilly-books>\",1644331085.083539,1644331085.083539,U02BVP1QTQF\\n4e7d3ea1-1b0f-4166-a535-95234bc7159b,U02BVP1QTQF,,,\"If this isn’t the right channel for these kind of messages, please let me know and I’ll move the post. Thanks!\",1644331085.083539,1644331122.457459,U02BVP1QTQF\\nb2240d0e-0a6f-47cb-8d89-d2153489c65c,U0308865C0H,,,I don\\'t think it\\'s unique to BQ. You get partitioning and clustering capabilities with other such platforms and databases as well. It might just be Google pushing their own product in the search.,1644315747.760699,1644332413.152529,U02TATJKLHG\\ndb35a018-057c-406d-b191-b05c3bd6420b,U02BVP1QTQF,,,Never heard of humble bundle. But £14 for 15 O’Reilly books is really good! Even if some of them are more data science orientated!,1644331085.083539,1644332526.431159,U02U34YJ8C8\\n5d4dae8b-190a-4e3f-a05d-68311948d91c,U02BVP1QTQF,,,\"Wow that\\'s a lot of books and a cool resource. You can see how many books are you really going to need and read from these. And check the market prices for just those and see if the bundle is worth it.\\n\\nFrom books POV I guess Designing Data Intensive Applications and Data Warehouse Toolkit are the two that have been the most recommended in a lot of forums for Data Engineers to get started with or refer from time to time.\",1644331085.083539,1644332832.424019,U02TATJKLHG\\n49b6abc4-786b-4aae-8f3b-2ce1da340af9,U02BVP1QTQF,,,I\\'m wondering how R cookbook ended up there,1644331085.083539,1644333070.404829,U01AXE0P5M3\\n84ba02a0-106f-44da-bbe3-b2c2ade36e82,U02BVP1QTQF,,,That\\'s indeed a great offer. I had 2 of those books and spent more on those than the max suggested price to pay :sweat_smile:,1644331085.083539,1644334239.124279,U01B6TH1LRL\\nf234c0d4-1ef2-4d03-ad25-1f2f40c73856,U02CK7EJCKW,,,But GCP didn\\'t accept my debit visa card :expressionless:,1644250558.066349,1644334569.851319,U02CK7EJCKW\\n8334ad3d-eced-42b6-948e-8a6a07e041ef,,5.0,,\"For question 4, week 3, is anyone getting a different count from what’s given in the homework options?\",1644340207.544339,1644340207.544339,U02U34YJ8C8\\nbe34867c-7e58-428c-b750-9350525e1547,U02CK7EJCKW,,,Hmm. It does depend on country/bank etc.,1644250558.066349,1644340541.410189,U02HB9KTERJ\\n23bfa3e0-1b75-4cd1-9edf-965e67f52cd5,,2.0,,\"hello\\nfrom this video:\\naround 11:00, he says bigquery is not able to infer the number of rows and size of the table since the data is still residing in google cloud storage but then around  14:55, he mentions that the data is being transferred from the storage to bigquery..\\nSort of conflicting?!\\n\\n<https://youtu.be/jrHljAoD6nM?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb|DE Zoomcamp 3.1.1 - Data Warehouse and BigQuery>\",1644340691.746739,1644340691.746739,U029DM0GQHJ\\n3f5be657-f79f-4a12-9c56-9baf72e2c001,,10.0,,Hey all. How does everyone handle feeling like the course is overwhelming and stuck behind. I\\'m still stuck on week 1 due to a packed schedule and it feels as if I am drowning. Does anyone have any advice?,1644340781.202059,1644340781.202059,U02SXQ9L0FJ\\necbed1c9-55a9-4603-b4d5-ff67237cd893,U02SXQ9L0FJ,,,I\\'m slowly plodding as well. There\\'s no rush to finish though - I say just take your time even if it\\'s not at the same pace as everyone else. I\\'m still in week 2 :slightly_smiling_face:,1644340781.202059,1644340830.318619,U02A3AU35LL\\n39dbe1af-2206-411a-b78f-81c7434bc2a5,U02SXQ9L0FJ,,,\"are you stuck with a particular problem, Avery? Or just the amount of content that you need to go through it overwhelming?\",1644340781.202059,1644341050.801729,U01AXE0P5M3\\ndf656259-a748-4bba-bcbb-82d0c3aa13fb,U02SXQ9L0FJ,,,So far it is just the technical issues that would probably happen if I took on an engineering role. At this moment I don\\'t have enough time to sit down and go through all solutions like I would like too. It is rather annoying too me but not a problem due to the course,1644340781.202059,1644341185.664739,U02SXQ9L0FJ\\n5068716c-a6a1-41ee-b612-ac55863522b5,,8.0,,\"hello data folks, I\\'m trying to catch up cause i was busy for a while and i am at airflow and boy , that shit is confusing :grin: .\\nokay so scenario1, from ingesting data to postgres with airflow,i see that when docker-compose up parses the new dag directory it creates empty worksapce. this works . when i changed it back to dags , i found my previous matrix ,HOW?\\nwhen the container was rebuilt ,shouldn\\'t the dag history be deleted? since we are running them on containers and each container is different . What am  i missing ?\",1644341363.870689,1644341363.870689,U02S6KXPH8W\\ncfcd15b5-9b20-452e-bc55-27651758a5ea,U02S6KXPH8W,,,was just curiosity,1644341363.870689,1644341427.962519,U02S6KXPH8W\\n7712AA16-6071-4ED7-949D-F04201EAEDEB,U02U34YJ8C8,,,\"<@U02U34YJ8C8> have look at this \\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1644230952007529|https://datatalks-club.slack.com/archives/C01FABYF2RG/p1644230952007529>\",1644340207.544339,1644341541.176739,U02AGF1S0TY\\nfe84dc9c-532b-4fb2-a830-9793c201bf1f,U02SXQ9L0FJ,,,Also thank you <@U02A3AU35LL>,1644340781.202059,1644341715.571139,U02SXQ9L0FJ\\n7e1470ce-e224-4326-a4ab-9a8d8e0f4f62,U02TEERF0DA,,,\"I’m using a GCP VM, so yes it should work. If you cant SSH directly into the server from your local machine, then you might have issues with how the remote key was created or the credentials are not in the correct folder locally\",1642781895.135700,1644341770.816909,U02TEERF0DA\\neea4fec1-abc8-4779-835d-3529677cb5a0,U02SXQ9L0FJ,,,You\\'re welcome! There\\'s a lot of people with engineering backgrounds in the course. I\\'m using this course to understand the concepts of building data pipelines and getting more confident with containers. I have no expectation I will finish the course in time lol.,1644340781.202059,1644341852.697219,U02A3AU35LL\\n897ae558-b027-4316-8eb7-24d6db5818fc,U02U34YJ8C8,,,Perfect thanks. Is it just that Ankush didn’t cast the datetime as DATE?,1644340207.544339,1644342480.010329,U02U34YJ8C8\\neb871ab5-d896-4661-ac5c-d5b67fd00021,U02U34YJ8C8,,,\"I was also getting a different count, but I honestly didn’t bother with trying to get an exact count since all the possible answers used the same count number and my count wasn’t super far off. I did get different data estimates but after I clustered my table I got very similar results, so I’m assuming that the answer I picked is correct. We shall see when the solution is published :sweat_smile:\",1644340207.544339,1644342959.041879,U02BVP1QTQF\\ncafc3269-ab5a-4746-bf00-df312b9fedc6,U02U34YJ8C8,,,\"Lol yeah that’s what I was finding. From the comment above, looks like Ankush was using `WHERE dropoff_datetime BETWEEN \\'2019-01-01\\' AND \\'2019-03-31\\'`  The only difference here from what I was doing was not casting the date as date. Although still trying to work out why this actually changed the results…\",1644340207.544339,1644343427.222889,U02U34YJ8C8\\n1d98ac3e-5235-427f-b052-7fbb8cd4242f,U02S6KXPH8W,,,\"Just a hunch and anyone else reading this please feel free to correct me if I\\'m wrong. But what I think that is happening is that when you mount your local volumes, it is storing your dag history locally that you can access from your container. Similar, to week 1 where even though we stop the postgres container the db gets stored because of the mounted local volume.\",1644341363.870689,1644343718.297499,U01EKHDMRGT\\n72037ffe-5be0-49ec-8860-658bdb9d12c0,U02S6KXPH8W,,,ohhhh that would actually make sense.let me look into that,1644341363.870689,1644344539.449869,U02S6KXPH8W\\ne507c11c-c3dc-4929-8a1b-9fe36c82c167,U02HB9KTERJ,,,\"TF won\\'t let you destroy a non-empty bucket, so you\\'ll have to comment out the bucket config and destroy everything else.\\nAnyway, destroying things is just a guideline. For this course, as long as we don\\'t keep any running services on, there won\\'t be any harm\",1643618720.680959,1644344979.356309,U01DHB2HS3X\\n2c2a7d9f-d6f3-427b-80e0-6c6fa45b5907,U02BVP1QTQF,,,The Building Anonymization Pipeline book I didn\\'t find that useful. I bought another book by the same author called Practical Synthetic Data Generation: Balancing Privacy and the Broad Availability of Data and I found that quite interesting. Gives you ideas on how to build artificial datasets for real data that is hard to get access to,1644331085.083539,1644345672.843499,U02TMP4GJEM\\n16a90d65-2c21-4c1e-8ae0-d093fedfd0a9,U02UJGGM7K6,,,\"Oh, I had DATE(dropoff_datetime), now I see what was different. Thanks for the explanation Ankush.\",1644230952.007529,1644346343.578209,U02UVKAAN2H\\n56f6d4a1-3b1d-4440-a6ea-d822383d8a8e,U02U34YJ8C8,,,\"Hi all! Check this out, it might help:\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1644000821071479>\",1644340207.544339,1644346954.102659,U02S2TZRBL7\\n1D7F7A87-4D35-4BF5-BF8B-BE5B4483CD1F,U02BVP1QTQF,,,\"If only the time to read all those books was included in the price! By the time I come to read the last book, data pipelines will be running on fiber optic :joy: \",1644331085.083539,1644347317.705169,U02UX664K5E\\n366f1008-6145-47ed-876e-0dc8aacb4152,U02S6KXPH8W,,,\"i have been experimenting and you are absolutely right . thanks .\\nhere is an official doc if anybody wants to read on volumes in docker .\\n<https://docs.docker.com/storage/volumes/>\",1644341363.870689,1644349838.491069,U02S6KXPH8W\\n2494000c-7c35-48e3-b810-c3ee64f4c6ae,U02SXQ9L0FJ,,,I just was gonna try that <@U02BTB7H2Q3> I just have not had time to implement it. And I\\'m still at week 1 trying to get postgres to load but I\\'m having the password issues,1644033110.180109,1644352132.588959,U02SXQ9L0FJ\\n1AEB5CA7-D308-47EB-A3C3-023205FA0AD7,U02BVP1QTQF,,,I was always wondering how legal humble bundle is,1644331085.083539,1644358101.390139,U02Q7JMT9P1\\n03ab4b63-2d1f-4c28-8094-086769394fb4,,1.0,,\"hi all, I\\'m trying to ingest data to postgres with airflow from week 2, and I\\'m right at the end, but my ingest task isn\\'t working. I\\'m 99% sure i\\'ve set up all the variables correctly, but airflow keeps telling me that I\\'m attempting to pass in the port as \\'{5432}\\' AKA it can\\'t be parsed as an integer.\\n```  File \"\"/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/url.py\"\", line 71, in __init__\\n    self.port = int(port)\\nValueError: invalid literal for int() with base 10: \\'{5432}\\'```\\nCan anyone think of why this might be? it seems like I\\'ve improperly formatted an f-string somewhere but I can\\'t find that anywhere. My script and the DAG python file are functionally identical to Alexey\\'s and the env file reads\\n```AIRFLOW_UID=1001\\n\\nPG_HOST=pgdatabase-1\\nPG_USER=root\\nPG_PASSWORD=root\\nPG_PORT=5432\\nPG_DATABASE=ny_taxi```\",1644361206.575159,1644361206.575159,U02TVGE99QU\\n9181b1fa-08bc-497c-84be-fd3a23eedb36,,6.0,,[version control/git] Does anyone have advice on their scheme for git/version control for maintaining 1) the submission materials (files you generate/edit independently) and 2) the new course materials as they\\'re progressively uploaded to git?,1644362512.657099,1644362512.657099,U02SM3LKD2B\\n55ba8588-d940-4a6a-8429-bcf26a1ab73e,U02SM3LKD2B,,,\"I\\'m using a VM and had envisioned/understood the goal as:\\n-setting the course repo as an \\'upstream\\' branch\\n-making edits in my local, separate branch\\n-pushing my local repo to a remote owned by me on GH for submission materials\\n-pulling updates from the upstream course repo to use new course materials\\n\\nis this right? I\\'m sort of confused about how I might actually implement this, or if people have advice for how this should be done. Thanks for any thoughts anyone might have!\",1644362512.657099,1644362559.873609,U02SM3LKD2B\\n370aa784-5b6b-4b77-857e-38be4af06438,U02SM3LKD2B,,,\"I use a separate repo for my submissions and notes. I thought about cloning the course repo and adding my stuff on a separate branch but it seemed kinda messy, so I opted to create a separate repo which I organize the way I see fit.\\n\\nI’ve also forked the course repo in order to send PRs for my notes, but I don’t use the fork for anything else.\",1644362512.657099,1644363357.222299,U02BVP1QTQF\\n22725247-359b-4860-bb20-f7ae8ff6af59,U02TVGE99QU,,,\"Looks like it must be some quotes issue as it is taking it as string. Here is the snippet of reading env.\\n\\n`PG_PORT = os.getenv(\\'PG_PORT\\')`\\n\\n`ingest_task = PythonOperator(`\\n\\xa0 \\xa0 \\xa0 \\xa0 `task_id=\\'ingest\\',`\\n\\xa0 \\xa0 \\xa0 \\xa0 `python_callable=ingest_callable,`\\n\\xa0 \\xa0 \\xa0 \\xa0 `op_kwargs=dict(`\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 `user=PG_USER,`\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 `password=PG_PASSWORD,`\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 `host=PG_HOST,`\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 `port=PG_PORT,`\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 `db=PG_DATABASE,`\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 `table_name=tablename,`\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 `csv_file=outputfile`\\n\\xa0 \\xa0 \\xa0 \\xa0 `),`\\n\\xa0 \\xa0 `)`\\n\\n`engine = create_engine(f\\'postgresql://{user}:{password}@{host}:{port}/{db}\\')`\",1644361206.575159,1644363795.196689,U02S83KSX3L\\n116c4efe-9e0c-4aad-9fee-f0cfcffd8d30,,3.0,,\"Week3, Q5 says \"\"Clustering cannot be created on all data types.\"\"\\ndid you mean \"\"Partitioning cannot be created on all data types.\"\"\\n?\",1644364947.174169,1644364947.174169,U029JNJBPED\\nf7ce46ec-b4d9-4b6d-8879-25227d501ba6,U02SXQ9L0FJ,,,\"Yes. It is definitely lot of content, when we don\\'t know about GCP BigQuery, Airflow, etc and takes time to connect the dots. For me, it is definitely 8 - 10 hrs per week, going through the videos at normal speed (so that I can digest properly) and typing up commands myself (I can copy and paste, but prefer to type up myself for understanding).\\n\\nRe: Drowning... yeah - technology is changing so much that some new tool comes up every month that is better than previous one. I guess just focus on one course at time.\\n\\nAnother major tip, cut screen time somewhere else :-D\",1644340781.202059,1644365471.741569,U02S83KSX3L\\nba9b0ccc-6cd6-4c99-8f7c-b5d240c6ea34,,5.0,,\"Hello everyone,\\n\\nI\\'m in \"\"DE Zoomcamp 3.3.3 - Integrating BigQuery with Airflow\"\".\\n\\nWhen I try gcs_2_gcs_task, the filename is automatically changed from \"\"*yellow_tripdata_2019-01*.parquet\"\" in \"\"raw/\"\" to  \"\"*_2019-01*.parquet\"\" in \"\"yellow/\"\"\\n\\nThe \"\"yellow_tripdata\"\" part is automatically removed from the filename. Does anybody know what might have caused this occurence? Thanks!\",1644366977.276219,1644366977.276219,U02BRPZKV6J\\n1492be80-f7f1-474f-9a3c-348785534ff7,U02TC704A3F,,,\"Hey, big thanks <@U01DFQ82AK1>\\n\\nI will take a look into and see how it goes\",1644296434.560629,1644366989.152099,U02TC704A3F\\n0b4f15cd-9b35-4725-b92b-141c9338638a,U02BRPZKV6J,,,\"Add `f\\'{colour}/{colour}_{DATASET}\\'`  to your `destination_object`  parameter in the GCSToGCSOperator task.\\nEverything, which is left of the wildcard in the\\xa0`source_object`\\xa0parameter is omitted in the destination file name (e.g. yellow_tripdata). To keep this part of the file name, this part has to be added to the\\xa0`destination_object`\\xa0parameter again.\\n\\nMy code:\\n```move_files_gcs_task = GCSToGCSOperator(\\n            task_id=f\\'move_{colour}_{DATASET}_files_task\\',\\n            source_bucket=BUCKET,\\n            source_object=f\\'{INPUT_PART}/{colour}_{DATASET}*.{INPUT_FILETYPE}\\',\\n            destination_bucket=BUCKET,\\n            destination_object=f\\'{colour}/{colour}_{DATASET}\\',\\n            move_object=True\\n        )```\",1644366977.276219,1644367575.359679,U02UP3KN3SQ\\n3f53bacf-e1c0-4f4b-a404-ae56775a4a22,U02BRPZKV6J,,,\"Awesome, thanks <@U02UP3KN3SQ>!\",1644366977.276219,1644367722.963159,U02BRPZKV6J\\n473acdd5-7e39-42ef-b951-d11a438628e4,U02BRPZKV6J,,,\"I had the same thing happen, though I specified the wildcard like this\\n`source_object=f\\'{input}/{color}_*` .\\n\\nI found Google’s <https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/transfers/gcs_to_gcs/index.html#airflow.providers.google.cloud.transfers.gcs_to_gcs.GCSToGCSOperator|documentation> did a good job explaining why:\\n*destination_object*\\xa0(_<https://docs.python.org/3/library/stdtypes.html#str|str>_) -- The destination name of the object in the destination Google Cloud Storage bucket. (templated) If a wildcard is supplied in the source_object argument, this is the prefix that will be prepended to the final destination objects’ paths. Note that the source path’s part before the wildcard will be removed; if it needs to be retained it should be appended to destination_object. For example, with prefix\\xa0*`foo/*`*\\xa0and destination_object\\xa0*`blah/`*, the file\\xa0*`foo/baz`*\\xa0will be copied to\\xa0*`blah/baz`*; to retain the prefix write the destination_object as e.g.\\xa0*`blah/foo`*, in which case the copied file will be named\\xa0*`blah/foo/baz`*. The same thing applies to source objects inside source_objects.\",1644366977.276219,1644367957.758439,U02SQ1X29GE\\n62170b1c-db3d-4714-96cb-3028f5782af1,U02BRPZKV6J,,,Very detailed explanation. Thanks <@U02SQ1X29GE>!,1644366977.276219,1644368128.400589,U02BRPZKV6J\\nf10597d1-5968-4982-a3ee-aedaf32e7674,U02TC704A3F,,,Please let us all know how you make out with this!!,1644296434.560629,1644374593.562819,U031XK1GNA0\\nafa8cf1d-e7c7-4cb7-9cf4-d0c0c8edbd2e,U02SXQ9L0FJ,,,\"Hi Avery,\\n\\nThis might keep you motivated; Keep in mind that this a worthy investment of time. You might not become a master of these technologies after this course, but having a foundation really helps for future opportunities.\\n\\nKeeping up with the messages in slack is really  (really!) important. Many - if not all - of my queries (and certainly yours) were addressed by other people.  e.g. I found that in week 1, using `pgcli`  is not important; you can use `pgadmin` instead, In week 2, don\\'t use the official or the nofrills version, but the one mentioned in 2.3.4 etc.  Of course, you can take the lead and help others if you can.\\n\\nSetup a schedule for the lessons; this might help you keep up with other work.\\n\\nAlso, a dual-screen setup really helps - one for the video/readings and other for practice etc.\",1644340781.202059,1644375163.406209,U02HB9KTERJ\\n5c0dada8-031d-47a2-aa3b-7210f02a536c,U02S6KXPH8W,,,Even clearing the `logs` directory sometimes doesn\\'t help; I had to delete the images and re-create them using `docker-compose build` etc.,1644341363.870689,1644375255.172969,U02HB9KTERJ\\n648fdaf8-9bc3-49fe-93d8-e5f8a1a729e0,U029DM0GQHJ,,,\"The 1st one is an external table, where data resides in an external location like Google storage. In the 2nd one, a table is created in BQ and the data is transferred into the data warehouse. 2 different tables.\",1644340691.746739,1644376000.444539,U02QK4ZV4UX\\n10ac481d-47e2-4a7e-9c34-84b72e332040,U02S6KXPH8W,,,\"Airflow also maintains state files and logs. If you notice there is a delete button in the airflow UI. If you delete and refresh the page, the dag history will get deleted.\",1644341363.870689,1644376217.348269,U02QK4ZV4UX\\nd5f35876-bfa5-466d-ab5d-4416c56d3de9,U02SM3LKD2B,,,I also think having a separate repo is easier,1644362512.657099,1644389032.374419,U01AXE0P5M3\\n348f455b-8009-4414-85c3-b58db9923cc5,,3.0,,\"Will there be any difference if I use\\n```docker compose up```\\nand\\n```docker-compose up```\\nis it the difference in the versions or the entire command is itself different. Can I directly work with docker compose if my wsl is not throwing any error\",1644389115.865409,1644389115.865409,U0300EGP2EL\\n245ECDD4-2020-4928-819E-32FA1E1C7961,U0300EGP2EL,,,As far as I know they\\'re both the same thing.,1644389115.865409,1644389538.648029,U02BVP1QTQF\\n6B480400-8329-4246-862C-809AA442CD91,U029JNJBPED,,,\"I am away from laptop. But you are right, it should be partitioning\",1644364947.174169,1644392295.774869,U01DFQ82AK1\\nC0AD13D4-F668-41B2-BB70-8F274ADA15C2,U029DM0GQHJ,,,\"Maharajan is correct, does it make sense? \",1644340691.746739,1644392381.351209,U01DFQ82AK1\\nf945b1c6-e9e4-4c6a-a39a-22e36546f1cf,,2.0,,\"in week 2: the folllowing error is coming with command\\n```docker compose build\\nERROR [airflow_airflow-webserver internal] load metadata for <http://docker.io/apache/airflow:2.2.3|docker.io/apache/airflow:2.2.3>                                                                            11.8s\\n------\\n &gt; [airflow_airflow-webserver internal] load metadata for <http://docker.io/apache/airflow:2.2.3|docker.io/apache/airflow:2.2.3>:\\n------\\nfailed to solve: rpc error: code = Unknown desc = failed to solve with frontend dockerfile.v0: failed to create LLB definition: rpc error: code = Unknown desc = error getting credentials - err: exit status 1, out: ```\\nI am not understanding where I am going wrong\",1644392599.340299,1644392599.340299,U0300EGP2EL\\n38a86dad-85ed-408e-b0e5-2910133a3e62,U033VBQCGQK,,,\"Please read these guidelines for your next messages:\\n\\n<https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/asking-questions.md|https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/asking-questions.md>\\n<https://datatalks.club/slack/guidelines.html#taking-part-in-discussions|https://datatalks.club/slack/guidelines.html#taking-part-in-discussions>\",1646637342.400109,1646640265.105139,U01AXE0P5M3\\n89fd2615-f659-48e3-8164-e13c36827459,U033VBQCGQK,,,Alright thanks.,1646637342.400109,1646640370.091689,U033VBQCGQK\\ndea248f5-94d3-4547-b728-efe2a6502078,U033VBQCGQK,,,<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1646637681553229?thread_ts=1646637342.400109&amp;cid=C01FABYF2RG|https://datatalks-club.slack.com/archives/C01FABYF2RG/p1646637681553229?thread_ts=1646637342.400109&amp;cid=C01FABYF2RG>,1646637342.400109,1646640382.531009,U01AXE0P5M3\\n785de04a-f3f5-4283-b9dd-574b22b3c0b1,U02URV3EPA7,,,\"Maybe explanation with an example will help:\\n```WITH cte1 (select c1, c2 from table1),\\n     cte2 (select c1, c2 from table2)\\n     cte3 (select c1, c2 from table3)\\nSELECT c1, c2 FROM cte1 JOIN cte2 ON ... JOIN cte3 ON ... WHERE ...```\\nUsing something like is fine ^\\n\\n```WITH cte1 (select c1, c2 from table1),\\n     cte2 (select c1, c2 from table2),\\n     cte3 (SELECT c1, c2 FROM cte1 JOIN table3.....)\\nSELECT c1, c2 FROM cte1 JOIN cte2 ON ... JOIN cte3 ON ... WHERE ...```\\nBut something like this ^ is not, because in this case *cte1* is being used twice. This will result cte1 running twice.\\nIt is better to use `tmp` table for generation of cte1\",1646414115.669799,1646643828.384399,U01DFQ82AK1\\n05df4101-5528-4411-92ef-151a08a0450d,,1.0,,\"To visualise Kafka stream topology, check out this cool <https://zz85.github.io/kafka-streams-viz/|tool>\\nI generally find it helpful to communicate with externals or explaining internally :128px-apache_kafka-iconsvg:\",1646645019.544789,1646645019.544789,U01DFQ82AK1\\n0798d4db-6975-40e6-a95f-26e4955f05ff,U01DFQ82AK1,,,\"Java API StreamBuilder have a `describe` method that generates topology description\\n<https://jaceklaskowski.gitbooks.io/mastering-kafka-streams/content/kafka-streams-StreamsBuilder.html>\",1646645019.544789,1646645195.961329,U01DFQ82AK1\\n599c6da4-cf60-4cda-9d7c-21e67109660a,,3.0,,\"In the configuration of the producer part, does `acks` affect the writing of the message to the disk\",1646646485.528159,1646646485.528159,U02T9JQAX9N\\nd58d895b-3ac8-4109-98c0-7484cb8157fb,,,,\"Hello everyone, just a quick reminder that we have a FAQ file that you can a) read before asking here b) edit in case you have your question solved\\n:wink:\\n<https://docs.google.com/document/d/19bnYs80DwuUimHM65UV3sylsCn2j1vziPOwzBwQrebw/edit>.\",,1646648160.774219,U02CD7E30T0\\n91af9573-eed4-4600-8816-c5e3319cb024,U033VBQCGQK,,,\"I experienced this same thing yesterday, tried changing the port to 5431, and had an error\",1646637342.400109,1646649705.585959,U033PKGGFS7\\n217c2cd2-5a3e-4d35-bf56-2248f6289129,U033VBQCGQK,,,\"Have you found the solution tho? <@U033VBQCGQK> \\nMight go back at it later in the day\",1646637342.400109,1646649740.110939,U033PKGGFS7\\n47752fff-86d8-4e31-88ef-035f07e303ab,U02T9JQAX9N,,,\"yes, it will take more time to write the message if `acks=all`  vs `acks=0` . So your production is slower.\\nKeep in mind other config which impact speed such as `<http://max.in|max.in>.flight.requests.per.connection`\",1646646485.528159,1646650289.993659,U01DFQ82AK1\\neb123f30-2f54-4915-be66-bcddeb4ceff5,,11.0,,\"<@U01DFQ82AK1> In the kafka demo video, you said that the consumer would not consume what i had consumed before.. but here it seems it\\'s doing just that\",1646650600.991899,1646650600.991899,U02T9JQAX9N\\nd4b50dc9-851a-460c-ba44-dff3ef8eea4c,U02T9JQAX9N,,,Are you committing each message or using `auto.commit` ?,1646650600.991899,1646650654.955759,U01DFQ82AK1\\n7ac143ed-32a5-4d02-afef-9bf276dbf1b2,U02T9JQAX9N,,,On restart consumer will consume from the last committed offset,1646650600.991899,1646650689.266349,U01DFQ82AK1\\n5b7dcb2f-67d6-459a-a209-b99e0c1f6c8d,U02URV3EPA7,,,Now it is clear for me! Thank you <@U01DFQ82AK1>,1646414115.669799,1646651063.373859,U02URV3EPA7\\n5b5af07f-3f43-4e88-bdf8-221c0fde64db,U02T9JQAX9N,,,\"My `enable_auto_commit` is set to True, as it was done in the video. But I just discovered that I\\'ve been using onlu one partition all along.. I\\'m restarting the demo\",1646650600.991899,1646651395.228009,U02T9JQAX9N\\n39e17798-9349-4b36-ace0-589ccf3ae770,U02T9JQAX9N,,,\"Ok.. but imagine if I set `acks=0` where the producer wouldn\\'t wait at all, will the message in question still be written to the disk by the leader broker\",1646646485.528159,1646651524.034079,U02T9JQAX9N\\n7410dcf3-95d6-40d0-aebc-b64b672d55d0,U02T9JQAX9N,,,One partition is fine. Just set auto.commit to false. And commit after consuming each message.,1646650600.991899,1646651612.748159,U01DFQ82AK1\\n8a81e519-f0f7-41e1-a68c-eb4a2921018c,U02T9JQAX9N,,,\"You cannot guarantee that the message would be written. It will be (trying on) best efforts in that case.\\nSo your guarantee on delivery goes down to at-most once.\",1646646485.528159,1646651757.497099,U01DFQ82AK1\\ndfd040d1-abd7-4ace-98f2-28565f7ec8c2,U033VBQCGQK,,,<@U033PKGGFS7> Yes!,1646637342.400109,1646652333.765429,U033VBQCGQK\\nb108763c-9d0b-413e-a7f9-e363ad46a27c,U02T9JQAX9N,,,\"When I set `enable_auto_commit=False` , it still acts the same way, but when I stop running one, the other starts from the beginning..\\n\\nAnd it\\'s not asking me to manually commit anything\",1646650600.991899,1646652338.925759,U02T9JQAX9N\\naf6fb32b-2446-4ed5-900c-25a102a53320,U033VBQCGQK,,,How did you solve it?,1646637342.400109,1646652375.233599,U033PKGGFS7\\n82b554a2-a0cc-437a-955b-2aaff1270530,U02RSAE2M4P,,,<@U02RSAE2M4P> did you solve the issue?,1643937705.301049,1646652802.902909,U034Q6ZKR9C\\ne24807e1-43b9-43a0-9922-8783d45649a4,U033VBQCGQK,,,\"Well.. had to change the port from 5432:5432 to 5431:5432, see:\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1646583865839819?thread_ts=1646486970.082019&amp;cid=C01FABYF2RG|https://datatalks-club.slack.com/archives/C01FABYF2RG/p1646583865839819?thread_ts=1646486970.082019&amp;cid=C01FABYF2RG>\\n\\nThen ran:\\n`pgcli -h localhost -p 5431 -u root -d ny_taxi`\\n\\nDid all of this on vs code\\'s terminal.\\n\\nSent you a direct message, you may revert there if you still have a blocker.\",1646637342.400109,1646652943.938689,U033VBQCGQK\\n769f6941-fd4e-4faa-8d2e-cc4582ef1e76,U033VBQCGQK,,,I did that,1646637342.400109,1646653247.462199,U033PKGGFS7\\nc0b8b6fe-c1d1-4f5a-9d50-cefbf1f075fb,U02T9JQAX9N,,,\"after printing the message, commit the message. Try something like this\",1646650600.991899,1646655511.875889,U01DFQ82AK1\\n50c2e883-b568-40d4-91e7-917ff15116bc,U02T9JQAX9N,,,\"```consumer = KafkaConsumer(\\n    \\'demo_1\\',\\n     bootstrap_servers=[\\'localhost:9092\\'],\\n     auto_offset_reset=\\'earliest\\',\\n     enable_auto_commit=False,\\n     group_id=\\'consumer.group.id.demo.2\\',\\n     value_deserializer=lambda x: loads(x.decode(\\'utf-8\\')))\\n\\n\\nwhile(True):\\n print(\"\"inside while\"\")\\n for message in consumer:\\n  value = message.value\\n  print(value)\\n  consumer.commit()\\n sleep(1)```\\n\",1646650600.991899,1646655523.769329,U01DFQ82AK1\\n0f220cb6-657d-4285-8e8a-cd49cb813ebf,U02T9JQAX9N,,,\"&gt; ```consumer.commit()```\\ncommits the message (manually)\",1646650600.991899,1646655548.703929,U01DFQ82AK1\\n25a0b1f8-8aee-4a5a-aa3b-f35264557455,U02T9JQAX9N,,,\"Ok.. I did this.. but when I set up another topic and used 2 partitions this time.. I wasn\\'t able to run 2 consumers simultaneously. It was only when I stopped one that the other started reading messages. Although, I noticed they were printing different messages\",1646650600.991899,1646659375.191349,U02T9JQAX9N\\n37fda5e6-7f73-4be0-a2ba-26bc2946eb3b,,3.0,,\"Hi, what\\'s the deadline for Week 6\\'s homework? Not sure if I missed it, but I can\\'t find the date mentioned anywhere\",1646659583.280319,1646659583.280319,U03133B2HK8\\ne754587b-716f-4ef4-bea9-f0c832182593,U03133B2HK8,,,<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1646576289543219?thread_ts=1646574934.377499&amp;cid=C01FABYF2RG>,1646659583.280319,1646660443.755869,U02TGS5B4R1\\n190cd8c7-25cc-4426-90c9-cc6c1de331b6,U02T9JQAX9N,,,Just wait sometime. Kafka takes time to register new consumers.,1646650600.991899,1646661135.410299,U01DFQ82AK1\\n7c1641cb-092e-4f8c-b804-434ab65f3551,U02U3E6HVNC,,,\"Like it feels something is missing from 6.3.2, at 2:31 it starts with stream stream joins supposedly but then we, seemingly, go to ktables and at 6m:58 Ankush says we just saw an example of table join.\\n<https://blog.codecentric.de/en/2017/02/crossing-streams-joins-apache-kafka/>\",1646418691.258709,1646662410.783959,U02TC8X43BN\\n0dbe1032-0bdd-4a16-b338-e07bce1e420b,U03133B2HK8,,,\"Thanks, wow that\\'s quite a long while... Normally the deadline is within one week :joy:\",1646659583.280319,1646662612.510239,U03133B2HK8\\n92279745-4028-4b71-90ee-a35c6c4606ee,,5.0,,\"Hi all,\\nWhen I am using control center GUI, current cluster keeps going offline. Any idea on how to solve it?\",1646662814.216859,1646662814.216859,U02UZ493J56\\nc8f4443a-8724-42c9-9b77-dc896202d786,U02UZ493J56,,,\"check broker logs, looks like your broker is struggling. Can be related to memory\",1646662814.216859,1646663175.129029,U01DFQ82AK1\\na82ad4da-bfb6-45db-8f76-9e345307ebbd,U02T9JQAX9N,,,Ok.. Thanks,1646650600.991899,1646664043.092139,U02T9JQAX9N\\n4cdb8382-c0dd-40f3-91f7-8ec12f330ee9,,,,\"Hello All,\\n\\nFor *Week6*...\\n\\nIf you come across this error in Windows when running the producer of avro_examples folder...\\n\\n`ImportError: DLL load failed while importing cimpl: The specified module could not be found`\\n\\n... you may have to load `librdkafka-5d2e2910.dll` in the code. Add this before importing avro:\\n\\n`from ctypes import CDLL`\\n`CDLL(\"\"C:\\\\\\\\Users\\\\\\\\YOUR_USER_NAME\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\dtcde\\\\\\\\Lib\\\\\\\\site-packages\\\\\\\\confluent_kafka.libs\\\\librdkafka-5d2e2910.dll\"\")`\\n\\nIt seems that the error may occur depending on the OS and python version installed, in my case Windows, Anaconda and pip install of the requirements.txt for week6)\",,1646664227.845429,U02LQMEAREX\\nfa6f2d6d-4276-4031-9c3a-bce5f0b67ea0,U02UZ493J56,,,Where can I check them?,1646662814.216859,1646664873.852129,U02UZ493J56\\n3e1b72c2-4b19-4b74-b327-9527b523ee36,U02UZ493J56,,,\"And if it\\'s releated to memory, what can I do then?\",1646662814.216859,1646664897.719379,U02UZ493J56\\n1925B1FA-D677-4907-9758-BBED310FFFA6,U02UZ493J56,,,U can check “docker logs”,1646662814.216859,1646665977.380489,U01DFQ82AK1\\n1B35FD79-269A-4B71-8D46-CE0D2CE2B1DA,U02UZ493J56,,,You can increase memory in docker via docker console ,1646662814.216859,1646665999.264039,U01DFQ82AK1\\nef995cc8-92b9-4232-b858-5c59733b3272,U02RSAE2M4P,,,I think it had something to do with the address I provided for my volumes on airflow. But I don\\'t remember the details at the moment.,1643937705.301049,1646667629.519289,U02RSAE2M4P\\n03b0d1d0-fc99-422c-b6da-8d0568378af8,U02S2TZRBL7,,,<@U02S2TZRBL7> how did you do to run dbt init? same way you run pip install changing the path? I getting an error that looks like a problem with python version,1644635944.221919,1646670098.098279,U032Q68SGHE\\n88df7388-2199-4422-adcc-2cc359734c04,,2.0,,Hello guys does anyone know why my containers keep having the unhealthy status and thus preventing my airflow webpage to run? I have tried multiple times to stop and re run the containers in vain,1646673177.671119,1646673177.671119,U02T70K8T61\\ndebd489e-e1c4-41a7-9d67-36491775bb67,,,,\"```CONTAINER ID   IMAGE                  COMMAND                  CREATED          STATUS                          PORTS                              NAMES\\n44b91e24be34   apache/airflow:2.2.4   \"\"/usr/bin/dumb-init …\"\"   38 minutes ago   Up About a minute (unhealthy)   8080/tcp                           airflow-airflow\\n-scheduler-1\\ndb7695ebeef0   apache/airflow:2.2.4   \"\"/usr/bin/dumb-init …\"\"   38 minutes ago   Up About a minute (unhealthy)   0.0.0.0:8080-&gt;8080/tcp             airflow-airflow\\n-webserver-1\\nf8fada1b2085   apache/airflow:2.2.4   \"\"/usr/bin/dumb-init …\"\"   38 minutes ago   Up About a minute (unhealthy)   0.0.0.0:5555-&gt;5555/tcp, 8080/tcp   airflow-flower-\\n1\\nd43caa43b272   apache/airflow:2.2.4   \"\"/usr/bin/dumb-init …\"\"   38 minutes ago   Up About a minute (unhealthy)   8080/tcp                           airflow-airflow\\n-triggerer-1\\naf141aaf74d3   apache/airflow:2.2.4   \"\"/usr/bin/dumb-init …\"\"   38 minutes ago   Up About a minute (unhealthy)   8080/tcp                           airflow-airflow\\n-worker-1\\n75ed87084c5a   apache/airflow:2.2.4   \"\"/bin/bash -c \\'funct…\"\"   38 minutes ago   Up About a minute               8080/tcp ```\",,1646675706.765089,U02T70K8T61\\n46871459-13a8-4d31-a327-bb81092fc7b0,U02S2TZRBL7,,,\"Hi <@U032Q68SGHE>. I just run `dbt init` in the path where I wanted to initialize the dbt project. The terminal interactively asks you for a project name, connector, credentials paths, etc.\\nBefore running the `dbt init` command recall to activate the Anaconda environment where you installed the dbt package. You can do so by typing in terminal `conda activate &lt;env-name&gt;`\",1644635944.221919,1646676935.488669,U02S2TZRBL7\\n6de70c8e-75a1-42fa-958b-c9d2fd3ed9e6,U02S2TZRBL7,,,Thanks for your reply Juan. I tried to run it in another environment and works,1644635944.221919,1646678763.692329,U032Q68SGHE\\nc51e838b-b1d7-4b0c-8e8a-a30fb1ff8567,U02U3E6HVNC,,,\"Yes, I noticed that discrepancy. Either way, that part of the video was conceptual, and it\\'s really the implementation I\\'m struggling with. I thought perhaps I just don\\'t have enough of a background in object-oriented programming?\\n\\nAnyway, I\\'m gonna keep trying things until I get some output :sweat_smile: I\\'m sure as the week moves on, there will be more questions/help from other classmates!\",1646418691.258709,1646679018.124359,U02U3E6HVNC\\n2cf9eca8-6498-4658-90d9-55bd7552ed67,,2.0,,\"Did anyone face this issue with PGADMIN?\\n\\n```docker run -it -e PGADMIN_DEFAULT_EMAIL=\"\"<mailto:admin@admin.com|admin@admin.com>\"\" -e PGADMIN_DEFAULT_PASSWORD=\"\"root\"\" -p 8080:80 --network=pg-network --name pgadmin dpage/pgadmin4\\ndocker: Error response from daemon: error creating overlay mount to /var/lib/docker/overlay2/7488db3c9d199168367433b079f2445043bc6b35bf05ab5437fae82756207f07-init/merged: too many levels of symbolic links.```\",1646680391.638999,1646680391.638999,U0233FB1SB1\\naa15f7e7-8706-466e-9965-c7d05b77e4a0,U02T70K8T61,,,\"Please put the long output here instead of the channel. \\n\\nPerhaps you don\\'t have enough ram?\",1646673177.671119,1646682016.746809,U01AXE0P5M3\\n00d7d35a-1ce4-4445-abcf-b19e09a502aa,U0233FB1SB1,,,\"Have you tried googling this error? There are discussions on the internet, maybe you\\'ll find a solution there\",1646680391.638999,1646682889.666809,U01AXE0P5M3\\n3d03f5b0-5888-492e-90f0-643ffb4cd0dc,U03133B2HK8,,,That\\'s one week (from today),1646659583.280319,1646682934.196089,U01AXE0P5M3\\n387a31c4-8cfe-45a3-8142-032f5c3c79aa,,2.0,,\"Some of the questions from today\\'s stream about week 6:\\n\\nHow to use CLI commands for Kafka when the docker-compose up is running ? I mean rather than using Kafka control center I want to use CLI for Kafka\",1646686750.451479,1646686750.451479,U01AXE0P5M3\\n83f41531-f9aa-44a5-8348-8f371dd38ffa,,1.0,,How to keep a team of people in sync with the same python environment? Docker calling requirements.txt ?,1646686759.228509,1646686759.228509,U01AXE0P5M3\\n59229f45-8091-442c-a0b9-e60684e1104e,,1.0,,For week 7 we only submit the code only to github / gitlab for the data pipeline? or we build on image docker or docker compose for setup our data pipeline?,1646686776.060309,1646686776.060309,U01AXE0P5M3\\nb1a68aa0-c739-475b-ae96-468b6661dc61,U01AXE0P5M3,,,\"You probably mean a project? Yes, you\\'ll need to create a public git repo somewhere. Whether you\\'ll need to use docker or not will depend on the project and technologies you\\'ll decide to use\",1646686776.060309,1646686876.590129,U01AXE0P5M3\\n8c5039a4-7371-4b86-9063-ebaa74a67564,U01AXE0P5M3,,,\"Short answer: yes\\nLong: check <https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/05-deployment|this> (mostly videos 5.5 and 5.6)\",1646686759.228509,1646686917.171359,U01AXE0P5M3\\n5000e8f1-d347-4bd6-8135-f2387c62103e,U0233FB1SB1,,,Yea Alexey I did! I am trying to figure it out,1646680391.638999,1646687103.401669,U0233FB1SB1\\na8304658-e955-453a-941e-520d59ef5d5e,U02U3E6HVNC,,,\"Well I am personally loosing my mind with it. I tried some stuff and I don\\'t feel I am getting anywhere near close.\\nIn JVM it is kind of limpid the way to go at it, except I have no knowledge of java and already compiling java script is not something I am familiar with.\\n\\nWith faust I don\\'t get how we can take 2 streams together, you can only make an agent for a single topic (I think), so where can you pull the other stream from.\\n\\nAnd when I try to call the tables, I only get model errors.  Also unsure of what is the wrapper to be used for that.\\n\\nAlso for the join itself, it has to be on the same key but do we have to specify the key or the functions assume it can only be the key to join.\\n\\nMaybe it is possible to use ksqldb instead <https://docs.ksqldb.io/en/latest/developer-guide/joins/join-streams-and-tables/>\\n\\nMaybe it is something like in this page: <https://abhishekbose550.medium.com/basic-stream-processing-using-kafka-and-faust-7de07ed0ea77>\\n\\nWhere the first topic sends data to the second topic where I guess the join will be made somehow.\",1646418691.258709,1646687824.684039,U02TC8X43BN\\n995243c8-fe73-411a-ae0f-7fecf493e769,U02U3E6HVNC,,,\"Hmm yeah maybe KSQL is the way to go. I just remember Ankush mentioning that it isn\\'t used much in practice, so I assumed we weren\\'t supposed to use it.\\n\\nApparently you can combine two topics into one (see the first answer here: <https://github.com/robinhood/faust/issues/288>), but I wasn\\'t able to go anywhere from there. I\\'m gonna throw in the towel for today... I am getting pretty close to losing my mind too haha.\",1646418691.258709,1646690451.426909,U02U3E6HVNC\\n4c201a3e-b6a2-4546-bb2b-bbd0c03d79e1,,13.0,,Week 6 - ModuleNotFoundError: No module named \\'confluent_kafka\\'...,1646697104.015329,1646697104.015329,U02U5SW982W\\n9ce61df6-f941-41ee-9057-3c90a98a89c4,U02U5SW982W,,,Hi All - just wanting to know if my understanding of why I\\'m getting these errors is correct. I haven\\'t touched any of the code that is provided by Ankush on the repo and I\\'m getting all of these MoudelNotFoundErrors. Is this because we don\\'t have a Dockerfile set up here and it\\'s not reading the requirements.txt?,1646697104.015329,1646697182.017769,U02U5SW982W\\n531416c8-a199-452a-9c22-fb4091a9662e,U02U5SW982W,,,So it\\'s here in this Dockerfile (we looked at this way back) that we add our custom logic to start our application?,1646697104.015329,1646697282.738749,U02U5SW982W\\n4e0b2435-04b8-4f6b-a381-6bd484925143,U02U5SW982W,,,Don\\'t we have to install that first? I\\'m trying to figure out how right now. There\\'s a link in the readme.md but it just sends me to a web site that has bunches of options that I don\\'t understand.,1646697104.015329,1646700650.200419,U02UBQJBYHZ\\nfa2e414c-d0dc-478a-9008-615534e72583,U02U5SW982W,,,pip install confluent_kafka,1646697104.015329,1646701976.242059,U02T697HNUD\\nc61efef5-b912-4a6b-9509-8baf5f3b1277,U02U5SW982W,,,Thanks <@U02UBQJBYHZ> and <@U02T697HNUD> - I have done a pip3 install which is fine but I don\\'t want to have to do this on my local machine. I shouldn\\'t have to either - particularly when I\\'ve got all my requirements spelt out in a requirements.txt,1646697104.015329,1646702385.836619,U02U5SW982W\\n18a10f44-e558-4d1d-b00c-08e8e94e3afa,U02U5SW982W,,,That\\'s the point of requirements.txt after all isn\\'t it?,1646697104.015329,1646702416.589479,U02U5SW982W\\n4291c16e-a28d-4dce-aa23-e97f0545edbb,U02U5SW982W,,,\"So to get around this and to actually use our requirements.txt - which I assume is what it\\'s there for - don\\'t we have to create a Dockerfile? Where we can say \\'Hey, this is what you\\'re going to need\\'?\",1646697104.015329,1646702562.314089,U02U5SW982W\\nc1320c11-339e-490d-917a-96cfb48913ec,,9.0,,\"I cannot run the kafka demo. docker-compose up gives the error \"\"Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\"\" I don\\'t have a directory named unix that I know of.\",1646703018.926709,1646703018.926709,U02UBQJBYHZ\\n63dd723a-9cbf-4827-a402-e3bd7c9ea848,U02UBQJBYHZ,,,I installed all of the requirements.,1646703018.926709,1646703037.834569,U02UBQJBYHZ\\n28409093-9d1c-43ab-bf40-1c4b252cffcf,U02UBQJBYHZ,,,Can you try this? `systemctl start docker`,1646703018.926709,1646703312.736109,U02U5SW982W\\ne36e7221-8e2d-4578-baf5-eedc9c393605,U02UBQJBYHZ,,,As per this <https://stackoverflow.com/questions/44678725/cannot-connect-to-the-docker-daemon-at-unix-var-run-docker-sock-is-the-docker|post> on StackOverflow,1646703018.926709,1646703358.757019,U02U5SW982W\\n956fc2da-66a7-4b97-9f53-1fe978e8ecf5,U02UBQJBYHZ,,,Or it\\'s `sudo dockerd`,1646703018.926709,1646703390.179629,U02U5SW982W\\nbfe28adb-0979-45b3-b5d2-131b27443880,U02UBQJBYHZ,,,Apparently... although I haven\\'t had this problem myself. Not sure if that helps?,1646703018.926709,1646703439.589609,U02U5SW982W\\n62c8054a-ac1c-4fad-a610-49bec38aaa7c,U02U5SW982W,,,\"Hey <@U02U5SW982W>, you can use pip install -r requirements txt so that all packages are installed in your current environment. If you want to create an environment for this specific purpose you can use 1) python3 -m venv \"\"name of the environment\"\" 2) source \"\"name of the environment\"\"/bin/activate 3) pip install -r requirements.txt\",1646697104.015329,1646703863.520009,U02SUUT290F\\n9a4bc55b-e3e0-41b6-afad-3f53fa87185a,U02U5SW982W,,,\"Thanks <@U02SUUT290F> but do you mean that I do this in Dockerfile or I just do this at the CLI? I\\'m just a bit confused because isn\\'t the point of having Docker containers so that they are essentially sandboxed from everything else? I can, as you say, simply run pip at my CLI but won\\'t this make it available to my whole system and therefore interfere (probably not in my case) with other things I have installed on my system. That is, it won\\'t be sandboxed off? However, if I did it in a Dockerfile (as we did in earlier weeks) it should be shouldn\\'t it - sandboxed off from everything else? I\\'m probably sweating the small stuff here but I\\'m really just trying to search for understanding ... I can get it to work - that\\'s not an issue but I\\'m trying to get it to work within the whole ethos (or what I think the ethos) of Docker containers are.\",1646697104.015329,1646704422.697479,U02U5SW982W\\nb262ef81-6abd-4ef2-983c-e429f19d4b90,U02U5SW982W,,,I also know I can sandbox it off with a venv but what\\'s the go with Dockerfile - is it doing essentially the same thing without me having to do the venv?,1646697104.015329,1646704479.551879,U02U5SW982W\\n7c32f259-cf9b-45e6-88b9-00965c330c61,U02U5SW982W,,,\"Well, If you create a virtual enviroment you\\'re effectively creating a sandbox and none of the packages you install would be available for the rest of the system. I\\'m not an expert, but I think the main reason we used Docker containers was to build a custom environment for all of airflow services; we used Linux commands to assign users, to install Python packages, configure ports and to export multiple environment variables. In this case, we need an environment with a few python packages so, yes, we could build that with Docker, but that would be a bit of an overkill given that we could do the same with venv in a more simplified way.\",1646697104.015329,1646705639.275599,U02SUUT290F\\nfc2c3417-fa7a-44b5-a104-59f8376bcf22,U02U5SW982W,,,\"This could also be of help: <https://stackoverflow.com/questions/50974960/whats-the-difference-between-docker-and-python-virtualenv#:~:text=A%20Docker%20container%20encapsulates%20an,Alpine%2C%20even%20Windows%20Server%20Core.|What\\'s the difference between Docker and Python virtualenv? - Stack Overflow>\",1646697104.015329,1646705794.380609,U02SUUT290F\\n14433c46-04ad-49e1-a9aa-c7f03e792e42,U02U5SW982W,,,Thanks for the link <@U02SUUT290F> - I guess I\\'m just trying to reinforce what we\\'ve learnt so far with Docker - so that\\'s where I\\'m at. And you are right it - I am kind of turning myself inside out at the moment just getting this to work with a Dockerfile. That\\'s because there are multiple containers. Looks like I\\'m just trying to create work for myself :disappointed: I guess I would have just liked to get it to work with Dockerfile... if it\\'s too hard I\\'ll just move on though. Thanks for trying to help out.,1646697104.015329,1646706417.234499,U02U5SW982W\\n987db9cf-825e-44b4-a403-c8933058296d,,,,Hello everyone,,1646722256.551189,U02U67MRU3F\\n52387d61-303b-4799-b7d1-7e283087f322,,,,What\\'s the best way to start the channel from the beginning,,1646722272.782889,U02U67MRU3F\\nb1e6ac0b-2c3a-4e3d-8dd8-2aad0a30e4c3,,,,Seems this started a long time ago,,1646722297.764869,U02U67MRU3F\\n6a10355d-cd79-4f29-a5db-1d3c62a9cb80,,3.0,,<@U01AXE0P5M3> your inputs needed,1646722364.820419,1646722364.820419,U02U67MRU3F\\n925aa2bb-753c-4394-823d-9c2c31783468,U02U67MRU3F,,,\"Start with these two\\n\\n<https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/asking-questions.md|https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/asking-questions.md>\\n<https://datatalks.club/slack/guidelines.html#taking-part-in-discussions|https://datatalks.club/slack/guidelines.html#taking-part-in-discussions>\",1646722364.820419,1646723092.356629,U01AXE0P5M3\\ncee6e182-de2f-44a9-bd43-b44e8e40405d,U02U67MRU3F,,,\"After that, go to the github repository and follow the syllabus week by week\",1646722364.820419,1646723170.143349,U01AXE0P5M3\\nfd3249b7-2dd3-441a-a4e9-3173c19597c2,U02UBQJBYHZ,,,Are you doing this in wsl?,1646703018.926709,1646723233.733229,U01AXE0P5M3\\n896ae0bb-fbe7-457f-82cb-d72d76b0ec77,,7.0,,\"hi guys :wave:, how much time (average) in a week do you need for the course? i want to join but i don\\'t have much time to spend,  realistically a day per week so that\\'s about 10 hours tops\",1646723933.318809,1646723933.318809,U033QG69HRQ\\nb90a72b2-8518-421b-acdd-34688be1b261,U02U67MRU3F,,,Thanks,1646722364.820419,1646724878.229199,U02U67MRU3F\\n1c9fbc62-9632-4070-bbdb-5d12380505a6,,6.0,,\"hi i received this error. i have tried the absolute path method and also checked that the google_credentials json file is in the correct path. anyone has any suggestions what may gone wrong.\\n``` &gt; [airflow_flower 6/7] COPY google_credentials.json /opt/airflow/google_credentials.json:\\n------\\nfailed to solve: rpc error: code = Unknown desc = failed to compute cache key: \"\"/google_credentials.json\"\" not found: not found```\\n\",1644048656.985349,1644048656.985349,U02U5GQK25C\\n615d5197-df7b-4455-be34-b478a600b42f,U02T9550LTU,,,You need to change the execution date (so that it is before the end_date).,1644047251.049729,1644048686.704989,U02HB9KTERJ\\n605ba87a-7d43-4938-a0ad-f9e0979ea770,U02U5GQK25C,,,they should be in /.google/credentials/google_credentials.json,1644048656.985349,1644048735.599229,U01AXE0P5M3\\nd546a131-8a13-4314-a4ab-ed94d69ba3d7,U02U5GQK25C,,,Are you using the VM or local setup? In any case the `google_credentials.json` must be placed in ~/.google/credentials,1644048656.985349,1644048766.069549,U02HB9KTERJ\\na230a7a3-bee4-445f-ad72-1f3b6c3c38d6,U02U5GQK25C,,,\"this is the folder i placed in and also changed the path in volumes:\\n\\xa0 \\xa0 - ./dags:/opt/airflow/dags\\n\\xa0 \\xa0 - ./logs:/opt/airflow/logs\\n\\xa0 \\xa0 - ./plugins:/opt/airflow/plugins\\n\\xa0 \\xa0 - C:/Users/Khoo Ai Lin/.google/credentials/:/.google/credentials:ro\",1644048656.985349,1644049844.332839,U02U5GQK25C\\neee35eb0-baa7-466d-94db-70bb4cc3b209,U02U5GQK25C,,,\"Not sure, but it might be due to the \\'space\\' in your name, rather username! Can you put it in a different folder and map that one instead - just as a precaution? Otherwise, the syntax seems ok to me.\",1644048656.985349,1644050250.631389,U02HB9KTERJ\\n5f54e7e2-da04-417c-ac52-f8da7431aade,U02U5GQK25C,,,\"```COPY google_credentials.json /opt/airflow/google_credentials.json:```\\nalso remove this line from your dockerfile\",1644048656.985349,1644050550.887749,U01AXE0P5M3\\n80c6a9c1-5a22-4a9d-b4c7-dc02090810db,U02V90BSU1Y,,,\"I am using full version of airflow, sorry for late response\",1643965171.669549,1644052431.229599,U02V90BSU1Y\\n43de3c40-df61-4f45-8de2-d10d594d5707,U02U5GQK25C,,,it worked. thank you for the help.,1644048656.985349,1644052795.454629,U02U5GQK25C\\n0cb73524-ee8d-4591-96d9-20efde40fabc,,2.0,,\"Hello,\\nI have issues using remote ssh from my local vscode\\nI have installed the extension and selected the de-zoomcamp profile created but it is stuck here for quite some time.\\nPlease help\",1644052863.363059,1644052863.363059,U02V90BSU1Y\\n449259f5-5ed0-4f1e-8a4a-9ebfb6a2afd8,U02V90BSU1Y,,,\"So probably that is the problem. Your laptop can\\'t handle the highly demanding full version. \\nYou have 2 ways of providing the light version.\",1643965171.669549,1644053350.036929,U02CD7E30T0\\nc13136c5-2d8b-408c-8d69-5979cba8c597,U02V90BSU1Y,,,\"Every time you start and stop your VM instance, the ip is changing. After you start your VM, copy the new IP and update the .ssh/.config file. Click on linux, on the top dropdown menu that appears\",1644052863.363059,1644054713.304969,U0308MF3KUH\\na490db2d-06d2-4a8d-8864-ce8ef3c9f92e,U02T9550LTU,,,\"For me it worked by enabling the DAG and refreshing the page, didn\\'t need to trigger the DAG, but also set catchup to True as Olgas says\",1644047251.049729,1644054898.492629,U0308MF3KUH\\n8ff9f154-c507-46ff-98e7-16da40934ea2,U02SXQ9L0FJ,,,\"Yeah had the same issue with my windows/wsl environment. I moved on my linux environment. I think the VM cloud solution is the best, fastest and stable.\",1644033110.180109,1644054985.171449,U0308MF3KUH\\nf25a832c-9c4c-4ec4-be59-95d056b15ef1,,5.0,,\"Zombie Containers - yes you heard that right! As in containers that just won\\'t die: no $docker-compose down, no $docker kill, no $sudo killall, no \\'switch it on and off again\\'. I just spent way too long today trying to kill these guys. I finally ended up resolving it - and it wasn\\'t exactly elegant. I\\'m on Ubuntu 20.04 LTS and just wondered if anyone else has faced this problem? Or is it just me? In the Zombie Container Apocalypse? ;)\",1644056241.427939,1644056241.427939,U02U5SW982W\\n8e36fa57-ca76-4f9f-902b-740617179333,U02V90BSU1Y,,,\"Thank you.\\nI clicked on linux and it works fine now.\\n\\nI initially clicked on windows since I am on windows and nothing happened.\",1644052863.363059,1644061172.516419,U02V90BSU1Y\\nd9c0e3bb-69a7-4220-99b9-7fe358aa66d2,U026040637Z,,,\"the DAG was successful, yes, if that\\'s what you\\'re asking.\",1644028397.488689,1644066170.503669,U026040637Z\\nf2cb2b80-1cbf-4225-ad2e-c3bdee8e0701,U026040637Z,,,\"basically, you don\\'t need big query task on your week 2. You only need to upload the parquet files into cloud storage and then in week 3 we create the external table from big query\",1644028397.488689,1644066518.591939,U02RA8F3LQY\\n508b6084-a659-4fec-b1ed-9b061b72df4f,U02U5SW982W,,,Mind sharing the solution? Maybe it was better than mine; I had to manually stop (using `docker stop`) and later remove images (using `docker rmi`) if needed.,1644056241.427939,1644068947.277879,U02HB9KTERJ\\nb474b34d-d57f-4bdb-b3c3-91ec2141b021,U02U5SW982W,,,If images are not connected to a container you can prune them. `docker image prune -a` <https://docs.docker.com/engine/reference/commandline/image_prune/>,1644056241.427939,1644071117.187429,U0319KGEJ13\\n4807363c-fef7-438c-9848-4447e99902f0,U02R09ZR6FQ,,,\"docker-compose is the one that gives me problems, but only when running docker-compose.yaml of week 1, it works fine when I apply to docker-compose.yaml of week 2\\nBut yeah I\\'ll try to understand what might not be working\",1644019141.832489,1644072880.367519,U02R09ZR6FQ\\n34c15d50-869b-4782-ba2d-f875e72ceff3,U02T9550LTU,,,\"for me it worked when i removed “end_date”: datetime(2021, 1, 1),\",1644047251.049729,1644074078.654619,U02ULGHNT33\\n52ef79c1-a013-4d6c-b662-3c95f33f82ea,,3.0,,\"Hi, <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/a22c0e654a48662c95682eeda92f5d2ac367bca6/week_2_data_ingestion/airflow/dags/data_ingestion_gcs_dag.py#L95|in our Airflow DAG>, I notice that some of the parameters used in the BQ operator were not mentioned in the <https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/bigquery/index.html#airflow.providers.google.cloud.operators.bigquery.BigQueryCreateExternalTableOperator|documentation>, such as `projectId`, or named differently in our DAG `sourceFormat` vs  that in the documentation `source_format`.\\n\\nIn general, I wonder how would we know how to specify the parameters for our Airflow operators for other use cases, or understand what is the default behaviour of the operator, such as how what happens if the table already exist in BQ?\",1644075937.391289,1644075937.391289,U02UDRBE3RS\\n1a6a4748-e030-49de-b97c-7c7cecd22f9e,U02UDRBE3RS,,,\"```    bigquery_external_table_task = BigQueryCreateExternalTableOperator(\\n        task_id=\"\"bigquery_external_table_task\"\",\\n        table_resource={\\n            \"\"tableReference\"\": {\\n                \"\"projectId\"\": PROJECT_ID,\\n                \"\"datasetId\"\": BIGQUERY_DATASET,\\n                \"\"tableId\"\": \"\"external_table\"\",\\n            },\\n            \"\"externalDataConfiguration\"\": {\\n                \"\"sourceFormat\"\": \"\"PARQUET\"\",\\n                \"\"sourceUris\"\": [f\"\"gs://{BUCKET}/raw/{parquet_file}\"\"],\\n            },\\n        },\\n    )```\\n\",1644075937.391289,1644076305.482819,U02UDRBE3RS\\nd74e84c0-c9f4-4da0-a768-d7233be94a3f,U02UDRBE3RS,,,The source code is the ultimate doc =) sometimes you\\'ll have to resort to it (not sure if it was the case here though),1644075937.391289,1644076795.157259,U01AXE0P5M3\\n1e6dfda8-da63-4d2f-83df-34e8297adabc,U02UDRBE3RS,,,\"I think you are right  Alexey, I was able to find this link in the source code which eventually provide links to help me find the parameters used here.\\n\\nThank you!\\n\\n```:param table_resource: Table resource as described in documentation:\\n        <https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#Table>```\",1644075937.391289,1644077816.411459,U02UDRBE3RS\\n1310445a-7c54-4bbe-9b7e-6ce5e90d702d,U02Q7JMT9P1,,,\"<@U02Q7JMT9P1> Getting airflow running was taking too much time, so I am trying to get data over using transfer services (tho getting access denied … why???)  … so am now going to try with Terraform or just running the script directly ….  Would love to use (and learn from) your code if you can share it?\",1643977195.789939,1644078324.773199,U02U5DPET47\\nd6d14fe5-f411-4f94-bddc-7d0557810ca3,U02Q7JMT9P1,,,\"<@U02U5DPET47> You can check the code in my repo <https://github.com/IliaSinev/de_zoomcamp/tree/main/week_02/airflow/dags_new> Although, it won\\'t help you if you don\\'t have Airflow running...\",1643977195.789939,1644078951.680829,U02Q7JMT9P1\\nf8e39925-2f26-43f9-9b66-f9c7621d40ce,,3.0,,\"Hey, has anyone here tried to use Dagster instead of Airflow ingest data to gcs?\",1644079142.808109,1644079142.808109,U031TPVD3B4\\nbb68030f-ed83-4e1f-aff0-a8fc80ea4dab,U02Q7JMT9P1,,,<@U02Q7JMT9P1> Thank you!,1643977195.789939,1644079772.788879,U02U5DPET47\\nd992f30b-85b1-4a46-a8b2-ef427c6c4099,U02U5DPET47,,,<@U02UNB4G739> That would be much appreciated!,1643937023.081149,1644080044.926129,U02U5DPET47\\n2fb3910f-2c09-4f3d-8e9a-2db363d91b14,U02Q7JMT9P1,,,\"<@U02Q7JMT9P1> Yes indeed, won’t help me now (had misunderstood what the adaptation was), but still glad to see it to learn …. and perhaps will try again with airflow in a future week &amp; then will try using it.  Thanks again for sharing!\",1643977195.789939,1644080425.016779,U02U5DPET47\\ndab70168-40ea-4b7f-9649-f54b381a5e1f,,,,Is it possible to launch confluent from a Virtual machine like airflow or does it require a special configuration. Somehow I cannot connect to the control center on <http://localhost:9081/> despite all my services running.,,1644082784.256839,U02TC8X43BN\\n6c413d39-21ba-41ca-932b-239cb8167808,U031TPVD3B4,,,would be interesting to see!,1644079142.808109,1644082843.184099,U01AXE0P5M3\\n15480a96-bf4a-49be-962a-37e9675ec3f5,,3.0,,\"Hi, there! In the google form of HW3, there isn\\'t a row for the link of code and feedback :crying_cat_face:. Is it correct?\",1644084893.400949,1644084893.400949,U02R2PU9NLD\\n19f1ac36-b06a-45c2-898f-9cbc3c80beef,U031TPVD3B4,,,I\\'m attempting to set it up- but I\\'m new to this DE &amp; workflow process so I\\'m kind of fumbling around. If I can get it I\\'ll certainly share it here.,1644079142.808109,1644085007.615449,U031TPVD3B4\\n668cd8b0-b7ec-4d90-89d9-f1aeeea532cc,U01EKHDMRGT,,,Thanks for the detailed responses <@U02CD7E30T0> <@U02U5DPET47> <@U02TATJKLHG>!!,1643987049.413629,1644085328.401489,U01EKHDMRGT\\n25f9fd6f-9a9e-4ce7-803d-b7c9f6fd2b49,U02SZARNXUG,,,\"just read this message.\\n\\nDoes this cover things like:\\nCIDR\\nPrivate/Public Subnet\\nRouter\",1643228405.409200,1644085805.044069,U02TT8VADCL\\ne6753644-c90e-4e9c-8f2c-fe3ae532605d,,4.0,,\"`python ingest_data.py \\\\`\\n    `--user = root \\\\`\\n    `--password = root \\\\`\\n    `--host = localhost \\\\`\\n    `--port = 5432 \\\\`\\n    `--db = ny_taxi \\\\`\\n    `--table_name = yellow_taxi_trips \\\\`\\n    `--url=\"\"<https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>\"\"`\\n\\n*usage: ingest_data.py [-h] [--user USER] [--password PASSWORD] [--host HOST] [--port PORT] [--db DB] [--table_name TABLE_NAME] [--url URL]*\\n*ingest_data.py: error: unrecognized arguments: root root localhost 5432 ny_taxi yellow_taxi_trips*\",1644087250.752949,1644087250.752949,U02C1H2P3HQ\\n2ebf641b-8bb0-49e9-9639-73f36741a30d,U02C1H2P3HQ,,,\"Try it again without the \"\"=\"\"\\nSo:\\n```python ingest_data.py \\\\\\n    --user root \\\\\\n    --password root \\\\\\n    --host localhost \\\\\\n    --port 5432 \\\\\\n    --db ny_taxi \\\\\\n    --table_name yellow_taxi_trips \\\\\\n    --url \"\"<https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>\"\"```\",1644087250.752949,1644087665.732549,U0319KGEJ13\\naa2f4332-0f3c-4fcc-8b50-be2a4a7083d8,U02R2PU9NLD,,,\"It\\'s not, we\\'ll fix it\",1644084893.400949,1644087702.341669,U01AXE0P5M3\\ncadcd35d-5e30-40d8-8caa-99d01adefc8d,U02C1H2P3HQ,,,Or without spaces around =,1644087250.752949,1644087740.813509,U01AXE0P5M3\\n0414fac5-54cf-44fe-9d52-7538b419d8d0,,14.0,,\"good morning - I\\'m a little behind, but I\\'m at the point of video 1.2.3 and I\\'m trying to link the pgadmin to postgres instance via the network link. I\\'m getting a login error. I\\'ve altered my password in postgres but still getting the same error. I\\'ll share my code in the thread\",1644088632.177789,1644088632.177789,U02TJ1V22EM\\na576c7ff-93ed-451c-a064-2a5c9dc30acf,U02TJ1V22EM,,,\"```docker run -it \\\\\\n  -e POSTGRES_USER=\"\"jpurrutia\"\" \\\\\\n  -e POSTGRES_PASSWORD=\"\"root\"\" \\\\\\n  -e POSTGRES_DB=\"\"postgres\"\" \\\\\\n  -v $(pwd)/2_docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n  -p 5432:5432 \\\\\\n  --network=pg-network \\\\\\n  --name pg-database \\\\\\n  postgres:13```\",1644088632.177789,1644088667.338129,U02TJ1V22EM\\nae1e3cbb-6b97-4ae9-b1aa-1148f0f07b1f,U02TJ1V22EM,,,In the picture of the original post I\\'m showing the terminal were I\\'m getting a role doesnt exist (even though it shoes in pgcli) and in the pgadmin I\\'m getting password auth failure,1644088632.177789,1644088745.471469,U02TJ1V22EM\\nc2be7ede-d7f6-4c2c-8e2c-f44f243f1622,U02TJ1V22EM,,,\"password is \"\"root\"\" and you  use another in which character are more than 4\",1644088632.177789,1644088829.829629,U02C1H2P3HQ\\n21fd55d9-532a-4e15-9088-b2307ba01479,U02TJ1V22EM,,,the passwords are the same - I used root in the above code block to not show my actual password,1644088632.177789,1644088902.483269,U02TJ1V22EM\\nc67068f2-d672-455d-90c2-f15dce671517,U02TJ1V22EM,,,Add user root instead of jpurrutia.,1644088632.177789,1644088926.981069,U02QNCUUHEY\\n65ca65ad-d683-49d7-aa7d-3ee4641fb72a,U02TJ1V22EM,,,I am not sure but database must have user root.,1644088632.177789,1644088991.224079,U02QNCUUHEY\\n46b699c0-0acb-4959-b46d-60cb78faaa7b,U02TJ1V22EM,,,so I had an existing postgres on my machine before the course and I dont have root on that - I have to use jpurrutia (i think),1644088632.177789,1644089003.996809,U02TJ1V22EM\\n68688cfe-1972-4355-97ea-72b292b34cd8,U02TJ1V22EM,,,Don\\'t I have to match with what I defined in the docker run command?,1644088632.177789,1644089042.630079,U02TJ1V22EM\\ne6215a6f-7944-4837-9689-6fd4f582cc68,U02TJ1V22EM,,,now you run postgress in docker container so don\\'t worry about username and password you chose what you want but when you connect be sure write the the same credential which you define early,1644088632.177789,1644089178.611379,U02C1H2P3HQ\\n4af6db32-d694-4388-8737-ef0434485799,U02TJ1V22EM,,,ok so I restarted the process of spinning up pg-database and pgadmin - I was having some issues upstream in populating the docker postgres db so I may have to go back to that,1644088632.177789,1644089703.396679,U02TJ1V22EM\\n3b2c6d9f-1629-4415-8ae7-dc2f26a90424,U02TJ1V22EM,,,I think I found the solution here - I had a running instance of postgres through `brew services start postgresql`  so I ran `brew services stop postgresql` I think this cause some port conflict issues I was having,1644088632.177789,1644090542.933129,U02TJ1V22EM\\nf456d20f-4326-41b7-a01e-9bb2905a4a91,U02TJ1V22EM,,,thanks for the help all,1644088632.177789,1644090876.347809,U02TJ1V22EM\\na349661e-c2af-4314-85d5-d17f66fd2020,U02T9550LTU,,,\"<@U02HB9KTERJ> how were you able to do that? unfortunately seetting catchup=True didn\\'t work for me. I thought that would have fixed it but it didn\\'t... When i trigger it, it also always starts at today\\'s date and never pull data from 2019 which is what i want\\n\\n here\\'s my code\\n\\n```default_args = {\\n    \"\"owner\"\": \"\"airflow\"\",\\n    \"\"start_date\"\": datetime(2019, 1, 1),\\n    \"\"execution_date\"\": datetime(2020, 12, 31),\\n    \"\"depends_on_past\"\": False,\\n    \"\"retries\"\": 1,\\n}\\n\\n# NOTE: DAG declaration - using a Context Manager (an implicit way)\\nwith DAG(\\n    dag_id=\"\"data_ingestion_gcs_dag\"\",\\n    schedule_interval=\"\"@monthly\"\",\\n    start_date=datetime(2019, 1, 1),\\n    end_date=datetime(2021, 1, 1),\\n    default_args=default_args,\\n    catchup=True,\\n    max_active_runs=3,\\n    tags=[\\'dtc-de\\'],\\n) as dag:```\",1644047251.049729,1644091287.748559,U02T9550LTU\\n1c1e1118-d632-4bea-acf8-0ea908bf7042,U02T9550LTU,,,\"There is also the option to connect to the scheduler container with \"\" docker exec -it &lt;container id&gt; bash \"\"\\nAnd then use \"\"airflow dags backfill &lt;dag id&gt; --start date 2019-01-01 -e 2020-01-01\"\"\\nMaybe not the most right solution but share it here if thia might help you\",1644047251.049729,1644091551.950049,U0308MF3KUH\\ne38cff99-b5aa-4715-b6e0-111e4b58a5b7,,2.0,,\"Hi everyone, I have a rather trivial question: When I try to run a ingest_data.py locally, I receive this error “No such file or directory: ‘output.csv’“. Did I miss a step when we created this file or what am I missing here? Thanks a lot! BTW: I really appreciate the amount of work behind this :heart:\",1644092337.159259,1644092337.159259,U02UE339SP6\\n25237000-a7a6-4ec9-b5ee-45be09390b54,U02TJ1V22EM,,,All resolved?,1644088632.177789,1644093668.494849,U01AXE0P5M3\\ncdb6645c-d3d8-4ae4-8470-a9b00cc9c6c4,U02UE339SP6,,,Maybe wget didn\\'t work? What do you see in logs?,1644092337.159259,1644093758.079959,U01AXE0P5M3\\nd938bb6e-0c8b-4ed9-940b-f4440fbb0647,U02U5SW982W,,,\"Hi <@U02HB9KTERJ> and <@U0319KGEJ13> I outlined what I did on my <https://learningdataengineering540969211.wordpress.com/|blog> in my notes/walkthrough for Alexey\\'s tutorial. Short story is that my issue had to do with the apparmour service. As I said I\\'m not sure if this \\'solution\\' is any better, it just worked in my given set-up (what OS you\\'re on etc. etc.). Given what you both describe if you can get away with this then \\'happy days\\' as they say :).\",1644056241.427939,1644094782.951339,U02U5SW982W\\ne2d20618-1629-48db-a393-be8889490de1,U02E30U011U,,,\"I don’t know how, but I fixed :sweat_smile:. I created new project  and reinstalled gcloud\",1643578051.958769,1644096026.753309,U02E30U011U\\n724412ed-995b-4b07-80cf-396273bcdf49,U02U5SW982W,,,\"FWIW, I had the same issue on MacOS and used the following launchctl commands:\\n\\n`&gt; launchctl list | grep docker`\\xa0\\n`&gt; launchctl remove\\xa0[for everything listed by the previous command]` -\\n- e.g. for me\\n&gt;  `launchctl remove <http://application.com|application.com>.docker.docker.14062548.14062843`\\n&gt;  `launchctl remove com.docker.helper`\\n..probably could be piped …\",1644056241.427939,1644096585.663049,U02U5DPET47\\n0581dd97-777c-489e-bb6d-ba78ff1388d2,,5.0,,Week 3 is done but I\\'m looking forward for the homework solution :ok_hand:,1644097257.301339,1644097257.301339,U02TC704A3F\\n94af1c7c-e46d-4e64-971f-341e355aa0b6,U02TC704A3F,,,\"<@U02TC704A3F> Looking forward to catching up with you :slightly_smiling_face:\\n\\nSince you have done it, can you help me clarify exactly which datasets we need?  (am using google transfer jobs to get things to BigQuery, ran out of time for airflow debugging…). Per <@U01AXE0P5M3> HW week 2 says that for week 3 we need\\n• “For the lessons, we’ll need the Yellow taxi dataset. For the homework of week 3, we’ll need FHV Data (for-hire vehicles, for 2019 only).” \\nWondering if we need ALL the yellow taxi datasets for the lessons, or 2019 only?\",1644097257.301339,1644101079.889859,U02U5DPET47\\n28c48278-acad-4fca-91e9-137c2754d3bc,U02TBTX45LK,,,\"ahhh thank you so much! ive ran into a few compatibility issues with the M1 macbook. If anyone else was having problems, instead of running “-t tensorflow-serving” use “emacski/tensorflow-serving”\",1643993990.864529,1644101264.999749,U02TBTX45LK\\n6d9f7c77-43e2-468f-a9be-e880ec798297,,3.0,,\"I want to created the  permission for my service account with I can\\'t find any of the resource selected by Sejal (Storage admin, storage object admin and bigquery admin)\\n\\nWhat am I doing wrong?\\nI also noticed there are some service accounts automatically created.\",1644103450.874419,1644103450.874419,U02V90BSU1Y\\n0a27d3ab-74f8-4c61-b88b-0569e92a3db6,,5.0,,\"My airflow is not loading. \\n\"\"This site can\\'t be reached \"\"\",1644110549.437789,1644110549.437789,U02UKLHDWMQ\\na76e62aa-1ba1-4e84-8958-e98b4aba51bf,U02TC704A3F,,,\"<@U02U5DPET47> The questions in week 3 hw only pertain to FHV data from 2019.  If you transfer just that data, you should be fine: <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_3_data_warehouse/homework.md|Week 3 HW>\",1644097257.301339,1644113037.230899,U02U8CB58G3\\n4293d254-6f01-47ac-a9ed-759b1c747010,U030FNZC26L,,,\"Hey, I\\'ve created a gist with a few findings: <https://gist.github.com/nervuzz/d1afe81116cbfa3c834634ebce7f11c5>\\n<@U01DHB2HS3X> and maybe <@U01AXE0P5M3> would like to see it as well\",1643646196.837139,1644116251.641089,U02UKBMGJCR\\n2306c161-879d-419c-b242-3c146cfbd7af,U02V90BSU1Y,,,<@U02V90BSU1Y> i think you have to do it under IAM not under the service account,1644103450.874419,1644116941.477459,U02T8HEJ1AS\\n6fc02c9c-ac83-4d94-8d58-a0590f8166be,U02UKLHDWMQ,,,<@U02UKLHDWMQ> is docker running?,1644110549.437789,1644116976.110719,U02T8HEJ1AS\\n0fda85f2-06d1-4b88-bb05-e547de6de246,U02T9550LTU,,,\"I have catchup=True, airflow ran the first one starting in 2019-01-01 but then failed on the next one\",1644047251.049729,1644117418.613419,U030P1LNK5Z\\n4af029be-a4c7-4c4b-a5d6-9a7672553e9b,U02T9550LTU,,,I think it was confused. The one that failed was manual and all of the 2019 ones were scheduled and succeeded. I had to delete runs of dag after changing it to Catchup=True,1644047251.049729,1644117962.597729,U030P1LNK5Z\\n2f4c6bb2-cf24-49dc-9765-e27ff350d89e,U02T9550LTU,,,\"Try changing the date from the calendar/date you see above the tree diagram. Also, I had to refresh the view. It was a manual, repetitive \\'push\\'; I think there must be a better solution.\",1644047251.049729,1644118579.980179,U02HB9KTERJ\\nbe63d3ec-57de-4614-9159-fd8e1f0b6176,U02UE339SP6,,,\"I had this issue. For me, manually downloading the csv to local directory and then running the python script helped!\",1644092337.159259,1644123278.361269,U030PS2SXQV\\n99a8a7a4-1686-4e97-825e-0d5f03f2eb81,U02TJ1V22EM,,,alexey - currently with the postgres and docker set up im constantly running into issues that I think are due to my pre-existing environment configuration but I\\'m figuring it out as I go :slightly_smiling_face: thanks,1644088632.177789,1644128540.423279,U02TJ1V22EM\\n58988745-4c8d-4e07-bae3-62bf31217272,U030FNZC26L,,,\"Wow that\\'s a lot! Can you please create a PR and mention this link in <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_2_data_ingestion/airflow/2_setup_nofrills.md|https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_2_data_ingestion/airflow/2_setup_nofrills.md>\\n\\n?\",1643646196.837139,1644130231.418959,U01AXE0P5M3\\nca52fbca-5359-4bf0-8df0-311f5ea623c8,U02C1H2P3HQ,,,<@U0319KGEJ13> Man its work but how much time it take ? in my system it take ~90 sec to make a 1 chunk..,1644087250.752949,1644131068.904379,U02C1H2P3HQ\\n627a11ee-63cc-485c-90f1-510cd1ac4bd8,U02C1H2P3HQ,,,Depends how much CPU and RAM your docker image can take. But I think it takes 40-60 secs per 100k chunk for me. You can use `docker stats [container]` to see the usage,1644087250.752949,1644132683.028719,U0319KGEJ13\\nd3388f11-4703-4d96-8dcd-e0055d6449c3,U02UKLHDWMQ,,,And did you give more than 4gb of ram to your docker image? Check your docker dashboard settings. (The docker application on your OS) I think Sejal had a video about it in week 2. Explaining how you can check it.,1644110549.437789,1644132773.718559,U0319KGEJ13\\n0e630fc9-7e83-4c95-9f8a-89778251fe1b,,5.0,,\"I completely execute the \"\"ingest_data.py\"\" and successfuly connect my pgadmin but there is no any table ?:face_with_rolling_eyes:\",1644133063.305329,1644133063.305329,U02C1H2P3HQ\\n3800a1f9-584b-4d50-bb4c-90eb36058f22,U02C1H2P3HQ,,,are you sure? check on the table tab below,1644133063.305329,1644133193.930699,U02RA8F3LQY\\nd617394e-c096-4b3e-b4d1-84f97613c669,U02C1H2P3HQ,,,Yeah I refresh 3 to 4 time but not able to see table,1644133063.305329,1644133576.140379,U02C1H2P3HQ\\nbdd255eb-20f8-40de-ae11-3d2e6d271c55,U02UKLHDWMQ,,,Did you try localhost:8080?,1644110549.437789,1644134016.699739,U02QNCUUHEY\\n00a1bbf4-1d93-4d0f-8fef-67e4f1a32da2,U02TBTX45LK,,,\"First make sure you connected the service account you created for that purpose, and second go to gcp and check that accounts rights. Should look like the ones explained in the setup\",1644636171.927259,1644744729.792149,U01B6TH1LRL\\nd9d6f80d-2e89-40a1-a63f-bb067d764c3e,U030FNZC26L,,,\"Hmm well I use figma since I\\'m a bit familiar with the tool, but that might help you. You can also look at <http://draw.io|draw.io> which is a bit more simpler\",1644697506.408849,1644746439.928659,U01EKHDMRGT\\n408e07f2-1369-4fcd-9551-a57aacd95659,U02DD97G6D6,,,\"I think deleting folders inside week 4 and initialising the project once more should fix it. Can you show me where you specified the subdirectory?\\nI\\'m thinking maybe dbt is looking the project in some other folder?\",1644704990.517419,1644750707.690679,U01B6TH1LRL\\n0d081021-fb39-44b3-90f6-23d478698516,U02V9V9NLJG,,,Can\\'t you do it using the json file instead?,1644630270.056449,1644751378.435009,U01B6TH1LRL\\n8b9b0ef3-b10e-460c-83e1-39da0b32e8cc,U030FNZC26L,,,\"The explanation to the problem dbt is solving is in the *what is dbt* video.\\nIt allows us to perform data transformations while enforcing the software development practices (deployment, version control, encapsulation and modularity).\\nI think there\\'s a misunderstanding with what dbt does, it\\'s not an alternative to ML tools and although one could use spark and not dbt, it\\'s also not a replacement to spark, in fact one could connect dbt to spark and push the code there. It would also be covered during next week why to use spark.\\n\\nCould we simply write the transformations using SQL directly in BQ? yes, absolutely. dbt is not providing something that we weren\\'t able to do, just enforcing better practices, therefore making our codebase cleaner and easier to maintain. Some of the benefits that I\\'ve seen that improved in my way of working:\\n• You must develop and run your code in a sandbox environment.\\n• You run the same code in both sandbox and production, no need to be changing databases names, etc. Something very error prone. \\n• It\\'s easy to schedule and it manages dependencies for you, no need to check what to run first in a 50 steps schedule\\n• You must use version control in order to deploy, something that in my experience before was optional and in many cases the repo didn\\'t reflect the reality\\n• It enforces and auto generates documentation with the auto generated dag \\nI linked the official pages in the repo, I encourage you to read there to get more clarification on what dbt does exactly. Lastly, not sure how advanced you are, but while working on the project many of those pieces will come in place hopefully.\",1644737526.039989,1644751926.653139,U01B6TH1LRL\\n2d36adbc-2fd7-4aad-8bc9-0c3a1554d09f,U030FNZC26L,,,thanks for the explanation :slightly_smiling_face:,1644737526.039989,1644753946.210719,U030FNZC26L\\nDF82C3B5-E896-4306-BA25-33D85BAFF0EE,,3.0,,\"This question is confusing.\\n\\n*What is the count of records in the model fact_trips after running all models with the test run variable disabled and filtering for 2019 and 2020 data only (pickup datetime)*. \\n\\nWe are to use just 2019 fhv data and there is no pickup date time for year 2020 in the 2019 data. Although There are records for drop off date time 2020 in the 2019 record. \\n\\nCan you please clarify this. \",1644754959.668629,1644754959.668629,U02TBCXNZ60\\n4bca7899-c46a-4aa2-ab38-908423675c61,U030FNZC26L,,,\"If we compare spark vs dbt, what they do is indeed similar, but there are a few important distinction\\n\\n• DBT delegates the execution to a data warehouse, and Spark needs a cluster\\n• DBT is SQL-only while Spark can do much more\\nThe first one is especially important because BigQuery and many other warehouses are \"\"serverless\"\" - you don\\'t need to manage a cluster. This makes operational support much much simpler.\\n\\nSo I\\'d say if you can express your data pipeline as a bunch of SQL queries, do it - BigQuery is much better at this than Spark\\n\\nAnother thing you can think about is why do we need DBT if we can create a DAG in Airflow with SQL queries. Airflow is a generic workflow engine and it can do a lot of things, but DBT is focused on SQL only and brings a lot of extra functionality that\\'s not easy to have in Airflow\\n\\n(disclaimer - I haven\\'t really used DBT in practice)\",1644737526.039989,1644755036.142969,U01AXE0P5M3\\ne5be2c6a-d4f7-434e-8ec9-0d3c6e37c505,U02TBTX45LK,,,\"Hi am facing same issue too, I have checked that\\n1. Profile --&gt; Credentials --&gt; Client Email is correct\\n2. For the Service Account in Google IAM, the necessary permissions are granted\",1644636171.927259,1644756420.295359,U02UDRBE3RS\\n6731851a-c03f-4845-9ebb-e87de0533c11,U02TBTX45LK,,,\"Is there another configuration in dbt I should be checking?\\n\\nI am of the impression that if I am connected to the wrong service account, I won’t be able to run other models at all, but I am able to do so except for `fact_trips`\",1644636171.927259,1644756533.969239,U02UDRBE3RS\\nfb61f507-d068-4222-b0bf-f535bbc64860,U030FNZC26L,,,\"I didn\\'t get the first point. Can\\'t we connect Spark to BQ? I think there is a connector: <https://cloud.google.com/dataproc/docs/tutorials/bigquery-connector-spark-example>\\n\\n`&gt; BigQuery is much better at this than Spark`\\nYou mean dbt+BQ &gt; pyspark+BQ? I\\'m not sure if I got it correctly. Should we compare spark vs BQ or spark+BQ vs dbt+BQ.\\n\\n(I haven\\'t used none of them before)\",1644737526.039989,1644758184.748719,U030FNZC26L\\nf03bb183-7537-492c-9124-3aefa9bdf445,U030FNZC26L,,,You can. But spark pulls data from there and the processing happens in spark,1644737526.039989,1644758609.887779,U01AXE0P5M3\\n7854d5eb-5554-4375-ad33-830e00421475,U030FNZC26L,,,Clear now. Thanks :slightly_smiling_face:,1644737526.039989,1644759206.841789,U030FNZC26L\\n1d31d7c7-5cd6-435f-acc5-d04487eef9ce,,2.0,,\"leader board is above my understanding to follow, i have used the given website to generate a number.....i have failed to understand how to link the generated number to my email.\",1644759293.285319,1644759293.285319,U02RTJPV6TZ\\nd5d018be-7c9c-4076-ba3d-055832331552,U030FNZC26L,,,\"So yes, here I first was comparing spark and BQ as something that\\'s doing the computation work. \\n\\nBut then you need another thing to orchestrate all these SQL queries, that\\'s why I was comparing dbt and airflow\",1644737526.039989,1644759544.310299,U01AXE0P5M3\\n6aef37ff-8698-4be4-98b0-ef63d9241c0f,,2.0,,\"Hello! I also got a question about week 4 homework, question 3:\\nWhat is the count of records in the model stg_fhv_tripdata after running all models with the test run variable disabled (:false) ?\\nCreate a staging model for the fhv data for 2019 keeping only records with entries for *affiliated_base_number*. Run it via the CLI without limits (is_test_run: false).\\n\\nMy question is: the *affiliated_base_number* column doesn\\'t exist by default in *<https://nyc-tlc.s3.amazonaws.com/trip+data/fhv_tripdata_2019-06.csv|For-Hire Vehicle Trip Records>*, perhaps the author meant *dispatching_base_num* or *affiliated_base_number* should be cast somehow?\",1644759646.413779,1644759646.413779,U02UJGGM7K6\\n1e7b58f1-19ac-4379-9abf-7d3952ad6ed8,U02RTJPV6TZ,,,\"Here\\'s an example:\\n```from hashlib import sha1\\n\\ndef compute_hash(email):\\n   return sha1(email.lower().encode(\\'utf-8\\')).hexdigest()\\nmy_email_hashed = compute_hash(\\'<mailto:my_email@gmail.com|my_email@gmail.com>\\')\\nprint(my_email_hashed )```\\nthen switch to the leaderboard, press Ctrl+F and search by generated hash code\",1644759293.285319,1644759815.021619,U02UJGGM7K6\\nAB2A95A2-1F2E-47CE-821F-CC6499182277,U02TBCXNZ60,,,<@U01B6TH1LRL> please help here ,1644754959.668629,1644759902.394669,U02TBCXNZ60\\n90e7feab-7633-4adb-a4fb-814877966ec9,U02RTJPV6TZ,,,\"awesomeness <@U02UJGGM7K6> thanx for the help, i have learnt something new today:blush:\",1644759293.285319,1644760097.055299,U02RTJPV6TZ\\n65531bb6-d82d-4920-afae-69f03fbb0024,U02DD97G6D6,,,\"Hi again, this is also something I have did couple of times already\",1644704990.517419,1644760785.120789,U02DD97G6D6\\n037cf6c4-c39c-4378-ac8f-7e8896230af5,U02DD97G6D6,,,And my settings looks like below image,1644704990.517419,1644760805.412619,U02DD97G6D6\\n9d985be3-d9d9-4125-b223-6dcf97db75de,U02DD97G6D6,,,,1644704990.517419,1644760849.859959,U02DD97G6D6\\ne97443f0-1564-4152-b161-236239c42599,U02ULMHKBQT,,,search in this channel. you need to pull a different verson of tf serving compiled for arm,1644737424.801719,1644760956.872639,U01AXE0P5M3\\n133cad8d-d4d4-4046-82fe-09a6e54277e5,U02TNEJLC84,,,\"I figured that we\\'d have to use the partitioning/clustering techniques, but I can\\'t get the same results. They\\'re coherently slightly less and approximated...\",1644539007.881829,1644761215.001699,U02GVGA5F9Q\\n9f1091f4-9e5d-4371-ba45-3db8e1dd291a,U02UJGGM7K6,,,\"wondering the same, I guess it is the dispatching base num column\",1644759646.413779,1644761937.563139,U02UA0EEHA8\\n1662e65a-8981-4049-ab03-89ee9ea71036,,,,\"Hey guys!\\nBased on some materials from lesson 1, I’ve summarised some basic operations for Docker and Docker Compose :whale::whale:\\nFeel free to check it out to refresh your memory! And any feedbacks are welcome! :raised_hands:\\n\\n<https://riven314.github.io/alexlauwh314/de-zoomcamp/software%20engineering/2022/02/13/DockerSurvivalGuide.html>\",,1644765792.706859,U02R65WFT16\\n93836388-e40f-4e57-bf50-81b107a57fad,U02TBCXNZ60,,,\"Fact trips is an union of green and yellow data, it doesn\\'t include fhv, and it has a pickup date field. This question is targeted to use what you build while following the videos\",1644754959.668629,1644769025.502859,U01B6TH1LRL\\nA7678631-D8E7-4362-BFFC-2004649942F3,U02TBCXNZ60,,,\"Oh, Got it. That would require us to download both green and yellow taxi for the assignment.\\n\\nI thought we don\\'t need to use that. Thank you  \",1644754959.668629,1644771051.249409,U02TBCXNZ60\\neabf4a0e-b5c7-4656-9117-943788c38dca,,13.0,,\"Hey, i know its been mentioned in a few threads already but i’m still having problems with the location. Its saying the table is not in the europe-west6 region.\",1644772102.799279,1644772102.799279,U02TAA1LDQT\\n765599f7-10bc-4d4a-9031-ddd784e59ca6,U02TAA1LDQT,,,I don’t know where i’m going wrong because the data appears to be in the correct region.,1644772102.799279,1644772217.773929,U02TAA1LDQT\\n6085b15a-a368-4289-8faa-d158e3c0fe24,U02TAA1LDQT,,,I have almost the same problem: table was found in location US,1644772102.799279,1644772536.663559,U02ULGHNT33\\n4b07e84d-f5d2-4ca4-9b0b-c788adc8c80d,U02TAA1LDQT,,,\"I’m just getting started with dbt but I noticed that when you create a new dataset in BigQuery, if you don’t specify the location, by default it will be put in the US.\\n\\nI noticed this when creating empty dev and prod datasets in BigQuery for dbt.\",1644772102.799279,1644773142.260239,U02BVP1QTQF\\n7f27a74b-9005-4ba0-95aa-cfa3043af662,U02TAA1LDQT,,,I already noticed that so i went back and changed it to europe-west6 but still having the same problem.,1644772102.799279,1644773747.217729,U02TAA1LDQT\\n4b53559d-90ab-4d37-85b6-b318162c3947,U02TAA1LDQT,,,,1644772102.799279,1644774290.464169,U02TAA1LDQT\\nbf342287-1766-4f15-9ca1-7d6cfeef2424,U02TAA1LDQT,,,I did the same and have the same issue now..,1644772102.799279,1644774821.910369,U02ULGHNT33\\n44182876-7957-48ee-8868-e89417d95153,U02DD97G6D6,,,\"Just to inform if someone having similiar issue  I was having seems like the dbt bug. I cleanup the projects and recreated everything but always ended up with same green button for initializing the project, even it is already initialized. I have followed the same steps for different repository, everything worked perfectly fine as <@U01B6TH1LRL> showed in the videos. fyi\",1644704990.517419,1644777772.487189,U02DD97G6D6\\nbcc85ef8-d421-4fd2-9c2e-961047855654,U02TNEJLC84,,,\"<@U02T8HEJ1AS> I had the same error. That\\'s probably because you have 2020 data for FHV as well. There\\'s a couple of corrupted strings in a `2020-01` file.\\nIf that\\'s your case, I suggest just deleting 2020 data, and creating FHV table based on on 2019 data only.\",1644539007.881829,1644779408.182879,U01JCHNT84U\\n24c682a8-bb75-46e8-92d3-e7335d39d0d9,U02TAA1LDQT,,,\"I had the same problem before and following steps helped me .\\n1. Check the location information in your trips_all_data schema. (The location should be same for all of your schemas)\\n2. I deleted all other schemas that I have created and re-create `dbt-&lt;first-name-initial&gt;&lt;lastname&gt;` , `production`  and `staging`  schemas by specifying the location exactly the same that you have find in the source schema (as your trips_data_all)\",1644772102.799279,1644779989.290659,U02DD97G6D6\\nf0cd42f1-3324-4e79-bd41-5430527792df,U02TAA1LDQT,,,\"As seen below, I have 4 schemas (trips_all_data, staging, production and dbt-iertuerk).\\nThe trips_all_data is the source schema and rest are the destinations.\\nTo dbt to work with them, all has to be on the same region or allow multi region (this is what I understand)\",1644772102.799279,1644780226.893749,U02DD97G6D6\\n67dbe1ff-4238-4b87-b553-3d1b4ed7b240,U030FNZC26L,,,Lucid chart,1644697506.408849,1644780890.731349,U02TF1JFK3J\\nfc596a40-8c24-44c9-84f4-fe31ecbeddd9,U02TAA1LDQT,,,I have all of the schemas on the same region europe-west6,1644772102.799279,1644781671.623099,U02TAA1LDQT\\n9bf691f3-3199-416f-b3da-df6401c4eb1b,U02TAA1LDQT,,,You must recreate manually the table with the right location after that you delete the table and run the script throw dbt,1644772102.799279,1644782403.364199,U02DBNR22GN\\n3add5aef-65a6-4eea-ad39-8b04e578d9ad,U02TAA1LDQT,,,\"Sorry, i don’t really understand what you mean. Which table?\",1644772102.799279,1644782750.457109,U02TAA1LDQT\\n0287f89a-889c-4d65-b73b-748d84ef5787,,,,Anyone getting this error in data studio? I see the table schema but cannot explore data and visualize data. It connected one time but then again I see this error.,,1644784505.747069,U030FNZC26L\\n7511fbd7-7ea6-4225-9b6d-de979928a106,U02U5G0EKEH,,,\"Hi, i have the same issue can you share the solution thank u\",1644527800.138399,1644785119.580369,U02DBNR22GN\\ndce877ec-9394-4d9c-9a2f-5057da2c2b5d,U02TC8X43BN,,,\"I was looking at documentation for the\\n```BigQueryCreateExternalTableOperator```\\nand was wondering if I could do some type casting there to help eliminate the issue with the ehail_fee field when doing\\n```UNION ALL```\\nduring the creation of fact_trips.\\n\\nIs it possible to type cast ehail_fee to numeric upon creating the external table, using the schema section found in the code below?\\n```    create_external_table = BigQueryCreateExternalTableOperator(\\n        task_id=\"\"create_external_table\"\",\\n        table_resource={\\n            \"\"tableReference\"\": {\\n                \"\"projectId\"\": PROJECT_ID,\\n                \"\"datasetId\"\": DATASET_NAME,\\n                \"\"tableId\"\": \"\"external_table\"\",\\n            },\\n            \"\"schema\"\": {\\n                \"\"fields\"\": [\\n                    {\"\"name\"\": \"\"ehail_fee\"\", \"\"type\"\": \"\"NUMERIC\"\"},\\n                ]\\n            },\\n    )```\",1644429302.569509,1644785180.312459,U02U8CB58G3\\n0695e272-d3dd-4056-b9af-4d44fb1250c9,U02TAA1LDQT,,,You need to create for exemple stg_green_tripdata manually in big query with the right location after that run the script throw dbt also keep the location in dbt empty without any location,1644772102.799279,1644785394.158899,U02DBNR22GN\\n17a2a5d6-ef45-40af-9dfb-8ef606cddcf8,,6.0,,\"Has anyone else ran into issues with the final “dbt run” command, specifically with the fact_trips.sql model? All my other models seem to be running fine but i am getting the following error for the fact_trips model. Thanks!\\n\\n 03:19:54  4 of 4 START table model dbt_jpak.fact_trips.................................... [RUN]\\n 03:19:55  4 of 4 ERROR creating table model dbt_jpak.fact_trips........................... [ERROR in 0.70s]\\n Database Error in model fact_trips (models/core/fact_trips.sql)\\n   Access Denied: BigQuery BigQuery: Permission denied while globbing file pattern.\\n   compiled SQL at target/run/taxi_rides_ny/models/core/fact_trips.sql\",1644785599.998209,1644785599.998209,U02DBNR22GN\\nd35c6b5e-9909-4a1b-bbe5-9058a461662e,U02TAA1LDQT,,,I figured out what the problem was. I had a trailing space after “green_tripdata” in my BQ table. That was several wasted hours haha,1644772102.799279,1644789577.132389,U02TAA1LDQT\\nbcc668c3-20db-4383-bbc0-0cce90204c95,U02TBTX45LK,,,\"The same here. All models run correct, except `fact_trips` I will do some research\",1644636171.927259,1644789752.460719,U02LQMEAREX\\n26acdba5-058d-4bc9-9bbf-b2465352e1e1,,1.0,,\"Hi everyone. I set up a trial for us of the remote meeting service called Gather Town. I used it at a remote conference recently and thought it was great for meeting people and having more \"\"in person\"\" like chats. Let\\'s give it a try. I\\'ll be working in here for the next couple of hours so if you are interested, come and say hi!\\n\\nOn this free version, we can have up to 25 people in our remote office. Let\\'s see how it goes.\\n\\n<https://gather.town/invite?token=4BmmarMW1FzDWadfLfBHXp-6Q4zWwQoX>\",1644790203.902869,1644790203.902869,U02T9GHG20J\\n90975720-3944-4b98-910b-a786d6371ccc,U02TBTX45LK,,,\"After adding the Viewer role to the service account in use, it worked. I do not know why... Maybe this has something to do with the fact that I am using external tables in BigQuery?\",1644636171.927259,1644790413.416629,U02LQMEAREX\\n74e1c42c-3b0f-4cfe-bc6a-d0530162e9f8,U02DBNR22GN,,,\"Hi!, I found the same problem. After adding the Viewer role to the service account in use, It worked for me.\",1644785599.998209,1644790488.506329,U02LQMEAREX\\nc84ec5f4-439a-440a-aa39-41646a8c4651,U02QS4BD1NF,,,\"<@U02DD97G6D6> thanks, though did you run into this error when triggering the DAG with default arguments for start and end date -\\xa0`The execution date is 2021-02-12T00:00:00 but this is after the task\\'s end date 2021-01-01T16:20:30.363268.`\",1644286226.784259,1644791811.806589,U02SEB4Q8TW\\nd5ed11c2-09ed-48b1-8a92-20a99373ba65,,10.0,,\"Hello, i am getting `Access Denied: BigQuery BigQuery: Permission denied while globbing file pattern.`  error when tried to dbt run new model for homework. When I checked the compiled files and tried it in BQ directly, compiled version works without issue. What can be issue here any ideas, suggestions ?\",1644792118.370219,1644792118.370219,U02DD97G6D6\\nac074c40-0454-46a6-9f68-7c85198890ce,U02V9V9NLJG,,,im not sure what you mean. Which json file? how do i use that to set up oauth?,1644630270.056449,1644792463.062509,U02V9V9NLJG\\naecab029-7a2a-4d46-9a18-0979515efefd,U02QS4BD1NF,,,Yes I think thats is normal behaviour with end_date specified,1644286226.784259,1644792573.485619,U02DD97G6D6\\n6d56eaa5-b991-42de-a7c7-6fa1d38e7fc0,U02QS4BD1NF,,,Toggle-on the dag should be enough for starting the dag runs when start date is already passed,1644286226.784259,1644792643.573729,U02DD97G6D6\\na83ee739-b5a2-442d-b376-173a2f31a790,U030FNZC26L,,,<https://diagrams.mingrammer.com/docs/getting-started/installation|https://diagrams.mingrammer.com/docs/getting-started/installation>,1644697506.408849,1644794675.830609,U0290EYCA7Q\\n56ac5291-46fd-432a-afb5-120217af1f25,U02TBTX45LK,,,\"This worked for me after I re-uploaded my `Service Account JSON` file in dbt cloud. I added `Bigquery Viewer` and `Viewer` roles though.\\n\\n&gt; After adding the Viewer role to the service account in use, it worked\",1644636171.927259,1644797267.259109,U02UDRBE3RS\\n6bb67773-862c-4bd8-b777-b2465d54ad4a,U02T8HEJ1AS,,,Try removing the CAST functions inside the options list..I think that\\'s the problem,1644605172.029359,1644797959.067179,U02ULP8PA0Z\\nb513486c-ef81-4b4d-b013-d646aa8e6af5,U02ULMHKBQT,,,sorry must have missed this  <https://datatalks-club.slack.com/archives/C01FABYF2RG/p1644101264999749?thread_ts=1643993990.864529&amp;cid=C01FABYF2RG|thread> out. will give that a try! thank you!,1644737424.801719,1644805782.887669,U02ULMHKBQT\\ncbc79d73-fac6-4dad-b4cb-86d2220578a7,,2.0,,Hi everyone! :slightly_smiling_face: I\\'m curious to know if there would be any difference if the files in week 3 were `parquet` instead of `csv`? Any adjustments or considerations to be made for `parquet` files in BigQuery?,1644811740.957389,1644811740.957389,U02UX664K5E\\n646431e4-c91e-4210-8add-2bf017fb0a9b,U030FNZC26L,,,\"Thanks <@U0290EYCA7Q>, this is amazing :slightly_smiling_face:\",1644697506.408849,1644811799.825299,U030FNZC26L\\nACEB4AD7-51F9-42F3-B393-37063E7F9D25,U02UX664K5E,,,Check the schema when you are loading data from parquet to external table. Few messages showed that the schema for datetime is not detected. But I did not try it myself ,1644811740.957389,1644813231.026339,U01DFQ82AK1\\n7252c1dd-b33f-4cf6-8639-3144d56ea254,U02T9GHG20J,,,\"<@U02TNEJLC84> and I had a good chat today in the gather town \"\"metaverse.\"\" I hope to meet more people in there later!\",1644790203.902869,1644813261.718689,U02T9GHG20J\\n809b06f7-a6bb-43ba-9695-897df9bc9250,,7.0,,\"Hi All, I am trying to serve the dbt docs from Docker but it says the site is not reachable every time. More in the thread.\",1644817851.455599,1644817851.455599,U02TATJKLHG\\n2921749d-415b-416c-a21c-8fbc543b8edf,U02TATJKLHG,,,\"I am using `host` network mode so I don\\'t need to forward the port. Hence, I haven\\'t mapped port using `-p`. Even then, I tried it earlier but it\\'s not working.\\n\\nAs per the message below, the docs are being served at the port `8080` but I am not able to see anything when I open it in the browser.\\n```docker compose run \\\\\\n&gt;   --workdir=\"\"//usr/app/dbt/taxi_rides_ny\"\" \\\\\\n&gt;   dbt-de-zoomcamp \\\\\\n&gt;   docs serve\\n05:47:31  Running with dbt=1.0.1\\n05:47:31  Serving docs at 0.0.0.0:8080\\n05:47:31  To access from your browser, navigate to:  <http://localhost:8080>\\n05:47:31\\n05:47:31\\n05:47:31  Press Ctrl+C to exit.```\",1644817851.455599,1644818056.314559,U02TATJKLHG\\n0a455924-2b88-49a6-95da-cc25c2e732a4,U02TCDCDS9H,,,\"It\\'s an alternative way of logging in. Instead I\\'d use\\n\\n```gcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS```\\nLike here <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/windows.md>\",1644600126.431209,1644821272.811149,U01AXE0P5M3\\nE5914851-6FD6-41DC-AA4F-CB6C8B768092,,,,Hi everyone. I’ve been going over the course and everything has been amazing and very applicable! I’m curious to hear about folks experiences on how data engineering (tools processes for example Hadoop is not that commonly used anymore) have changed over time and predictions of where it is headed. I know it’s still a relatively new discipline and all. ,,1644821892.189319,U02UX5MHB40\\ne5e9bec6-d7d4-474c-8c24-1e7e21792352,,,,\"Hey guys i\\'m running into this same error again specifically for current_date() and @yearly schedule... any ideas to debug this one? :joy::joy::joy:\\n\\nIn the meanwhile, i\\'ll be spending the day working on week 4, can\\'t believe its been a month already. Thanks to everyone for this community, the knowledge sharing is awesome learned a ton - it\\'s truly a \"\"Data Talks Club\"\" and checking slack has been awesome for debugging :raised_hands:\",,1644824195.058629,U02T9550LTU\\ndc2c6dd2-ff7a-40d4-8b50-3923716109ab,,9.0,,\"Does anyone have any suggestions on how to visualize data from bigquery in real-time? I have a problem with connecting data studio to BQ and also it is not real-time (I think the refresh rate is minimum 15 mins). Basically, I have a lot of data sources which each of them send data 10 times a day and I want to update a chart constantly for each one of them.\",1644824603.281529,1644824603.281529,U030FNZC26L\\n6485d711-880a-4780-8331-e320a2e7e3b6,,2.0,,\"i am also getting another error when I would push new changes to my github repo (dbt cloud):\\n```git push\\nfatal: could not read Username for \\'<https://github.com>\\': No such device or address```\",1645259786.208399,1645259786.208399,U029DM0GQHJ\\ndef86abd-0061-401f-b681-b80f53eaf47b,U02RH0V5K33,,,Check if the ingest_script.py file is inside the docker image and container you are running.,1645243694.371979,1645261318.617639,U0308MF3KUH\\n3b19e678-bb54-4707-ba9c-d3077675cf62,,9.0,,\"Is it too early for a week 5 question? I hope not. I have correctly set up java/hadoop/spark on my windows machine, but I have issue when it comes to pyspark, namely I seem not have a PYTHONPATH among the environmental variables. I browsed for help already, but didn\\'t come across a solution that helps me: any idea?\",1645261933.512719,1645261933.512719,U02UA0EEHA8\\n331d97ea-fb01-4afd-aa48-0e058073d8ea,U02UA0EEHA8,,,<https://stackoverflow.com/questions/859594/cant-find-my-pythonpath> the closest thing to a solution I found,1645261933.512719,1645261972.724609,U02UA0EEHA8\\necf97b5d-26b9-45db-a3af-081ce25e4d06,U02UA0EEHA8,,,\"and yes, I ran these commands:\\n```export PYTHONPATH=\"\"${SPARK_HOME}/python/:$PYTHONPATH\"\"\\nexport PYTHONPATH=\"\"${SPARK_HOME}/python/lib/py4j-0.10.9-src.zip:$PYTHONPATH\"\"```\\n\",1645261933.512719,1645262260.141209,U02UA0EEHA8\\naee4783c-9dbc-44b6-a05b-c5f201c847e0,U029DM0GQHJ,,,\"You can try setting up an ssh key pair with your github account, this might help. You can wait for more answers though, maybe someone has a better idea\",1645259786.208399,1645262261.117979,U0308MF3KUH\\ndd974735-7a32-4472-9ca3-c47848c2c779,U02UA0EEHA8,,,more details (basically python looks at the wrong file because of what stated above),1645261933.512719,1645262599.796179,U02UA0EEHA8\\n8ca46fc4-3a70-4c60-b17e-03c1d902f005,U02UA0EEHA8,,,<https://stackoverflow.com/questions/3701646/how-to-add-to-the-pythonpath-in-windows-so-it-finds-my-modules-packages|https://stackoverflow.com/questions/3701646/how-to-add-to-the-pythonpath-in-windows-so-it-finds-my-modules-packages>  export is working on Linux environments,1645261933.512719,1645262607.805739,U0308MF3KUH\\n5c301577-29ce-4590-bb38-ed27b380f34b,U02UA0EEHA8,,,It seems jupyter in vs code doesn\\'t pick these env variables. Does it work if you run jupyter in the same terminal where you set these variables?,1645261933.512719,1645262896.784119,U01AXE0P5M3\\n2e04b579-d6e7-4261-86bb-caa7657e58dd,U02UA0EEHA8,,,\"You can also modify Python path in Python. It doesn\\'t look clean, but should solve your problem \\n\\n<https://stackoverflow.com/questions/31291608/effect-of-using-sys-path-insert0-path-and-sys-pathappend-when-loading-modul|https://stackoverflow.com/questions/31291608/effect-of-using-sys-path-insert0-path-and-sys-pathappend-when-loading-modul>\",1645261933.512719,1645263013.429729,U01AXE0P5M3\\nb41363d0-55e5-46bd-8ba0-57f2f78ac635,U02UA0EEHA8,,,\"Also if you previously installed pyspark with pip, I\\'d recommend removing it\",1645261933.512719,1645263202.775869,U01AXE0P5M3\\n66fbfdff-ec44-4c52-beca-c759843b2931,U02UA0EEHA8,,,\"I did install it with pip, I will try that\",1645261933.512719,1645263243.663849,U02UA0EEHA8\\nd51dc115-de39-4e25-9313-941e9a8957b8,U02UA0EEHA8,,,\"I turned out the issue was somehow exactly related to the pip installation of pyspark, as Alex said. An update of this package was the key to solve, now all works:\\n```Collecting py4j==0.10.9\\n  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\\n     ---------------------------------------- 198.6/198.6 KB 2.0 MB/s eta 0:00:00\\nInstalling collected packages: py4j\\n  Attempting uninstall: py4j\\n    Found existing installation: py4j 0.10.9.3\\n    Uninstalling py4j-0.10.9.3:\\n      Successfully uninstalled py4j-0.10.9.3\\nSuccessfully installed py4j-0.10.9```\\nthanks <@U0308MF3KUH> as well for your help :slightly_smiling_face:\",1645261933.512719,1645263624.031719,U02UA0EEHA8\\ne6a7f38a-baed-4a88-9b5d-1125d51ce31a,U02UA0EEHA8,,,same issue here,1645171121.759679,1645265207.909399,U02T0CYNNP2\\n6837d5b8-9606-47e5-a0a3-98ec1d10dc48,U029DM0GQHJ,,,\"Hi <@U029DM0GQHJ>, Ensure you create and work on a branch as opposed to working on the main repo..\",1645259786.208399,1645266035.492449,U02SEH4PPQB\\na9b68fcb-c32d-49ff-8f03-072ff8e6b885,U02CD7E30T0,,,\"&gt; (...) so \"\"working at scale\"\" means implementing basic common sense in your SQL queries so that you\\'re not going over the 5GB memory limit on any given node\\nWhat does it mean? What is this \"\"basic common sense\"\" - just a filtering on some columns to reduce data loaded?\",1645175852.583289,1645266228.734359,U02UKBMGJCR\\n99c8356e-7a66-4a54-bd68-d8920baebe22,U031HNNSW3A,,,\"<@U031HNNSW3A>\\n• First close your billing account using *GCP*. <https://cloud.google.com/billing/docs/how-to/manage-billing-account#close_a_billing_account>\\n• Then remove your card information from <http://pay.google.com|pay.google.com> etc. \\n• Meditate for a few hours\\n• Open another email account and use the same card\",1645210766.400019,1645275313.892199,U02HB9KTERJ\\nf93d1c64-6d33-4c0a-889f-b691262ad52a,,3.0,,\"hi\\ni keep getting this error:\\n\\n`Error while reading table: dtc-de-338618.trips_data_all.green_tripdata_all, error message: Parquet column \\'VendorID\\' has type INT64 which does not match the target cpp_type DOUBLE.`\\n\\ni tried casting it to `numeric`, `bignumeric` &amp; `float` but to no luck\\n\\nit happens when i use VendorID in a where clause:\\n`where VendorID is not null`\\n\\nor when i try to perform a cast operation, otherwise it runs without giving me a hard time\",1645279317.124999,1645279317.124999,U029DM0GQHJ\\ne0f9327c-7b88-421d-a5a7-d4be28931aa5,U0301PJRDNV,,,I also have this problem now. Any reason why this is so?,1644922786.198149,1645280013.587729,U02RSAE2M4P\\n222e1d65-cbbe-407b-87b2-622f080b8121,U02TNEJLC84,,,<@U01AXE0P5M3> Wow awesome! PR submitted! Keeping an eye on it to see if my wording/placement need to be adjusted.,1645198942.177789,1645287457.658699,U02TNEJLC84\\n16F41BC9-F90C-454A-9608-E04B704D9BBD,U02U34YJ8C8,,,Thanks all! Fingers crossed I don\\'t have any major issues! Going to go through it all this evening ,1645213874.310119,1645288181.921529,U02U34YJ8C8\\nd518c2fc-a98e-4c06-925c-9d7443024938,U02U3E6HVNC,,,\"I think I may have found the issue.\\n\\nLooks like for dbt cloud, when location is defined in \\'Account Settings -&gt; Project -&gt; BigQuery Setting\\', dbt does not pick up schema from schema.yml file, but instead using one defined in your profile. However, if you clean out location (which unfortunately defaults to US), it will use the schema variable. Seems like bug to me.\\n\\nFix: The way to change the default schema for dbt is click on your profile is by clicking your profile at top right, go to credential and change dataset there. (like this screenshot from Vic - <https://datatalks-club.slack.com/files/U01B6TH1LRL/F032N5L6F7Y/image.png>)\\n\\nSo, seems like bug is when location is defined, it is picking up schema from your dataset value defined in profile and not from project schema.yml. I don\\'t this it justifies as feature.\",1644432387.132949,1645288193.855329,U02S83KSX3L\\nf944f9e3-f4cb-453b-ba85-f30dc7ad733f,,,thread_broadcast,\"Can anyone confirm whether these row counts (`select count(*)`) are correct:\\n\\n• `yellow_tripdata`: `109_047_518`\\n• `green_tripdata`: `7_778_101`\\n• `taxi_zone_lookup`: `265`\\nBecause my count for `fact_trips` is `61_446_541` for 2019/2020 (which is incorrect I believe)\",1645039735.829189,1645288227.037859,U02TGS5B4R1\\nd91e8f56-16dc-463a-b0da-a93dde9de766,U02TJ69RKT5,,,\"Hi Santiago, were you able to find a fix for this issue? I am stuck on the same issue. Can\\'t really find any documentation regarding this error/issue.\",1645242815.271579,1645289268.802179,U02UM74ESE5\\n75772296-e053-4aab-83c3-f827926aff3d,,6.0,,\"Hi folks! Sorry for the delay... I am think this question is about a topic from week 2 :man-facepalming: :\\nI try to load a particulary library (`pytrends`) in an Airflow enviroment. But when run Airflow I have this error message:\\n```Broken DAG: [/opt/airflow/dags/data_ingestion_gcs_trends.py] Traceback (most recent call last):\\n  File \"\"&lt;frozen importlib._bootstrap&gt;\"\", line 219, in _call_with_frames_removed\\n  File \"\"/opt/airflow/dags/data_ingestion_gcs_trends.py\"\", line 16, in &lt;module&gt;\\n    from pytrends.request import TrendReq\\nModuleNotFoundError: No module named \\'pytrends\\'```\\nBut `requirements.txt` contain pytrends. I suspect the docker-compose.yaml dont linked OK with the Dockerfile file.\\n\\n<https://github.com/cacrespo/data-engineering-zoomcamp/tree/main/project/gt_monitor|Here> are my files\\nCan somebody help me please?? Thanks\",1645291417.326719,1645291417.326719,U02URV3EPA7\\n0a713159-a0d9-4cbb-aff3-4545d69921d0,U02URV3EPA7,,,Maybe try re running docker build?,1645291417.326719,1645291632.361879,U01AXE0P5M3\\nbb1621db-5948-4708-b908-4aab15209636,U02URV3EPA7,,,\"Yes, I tried... :-1:\\nThe files looks well?? Right?\",1645291417.326719,1645292032.375689,U02URV3EPA7\\na24ec038-25ef-404e-b316-44e6262e9158,,,,\"anyone else have this issue? at the docker-compose build command, its stuck at pip install requirements for an hour, and it keeps showing this\\n```pip is looking at multiple versions of google-cloud-automl to determine which version is compatible with other requirements. This could take a while.```\\nedit: adding `RUN python -m pip install --upgrade pip` after copy requirements.txt works now\",,1645292516.657689,U02SWHZKSDN\\n81673d9b-51ac-472f-af83-9e5986602c5d,U02URV3EPA7,,,Yes they do... not sure what\\'s the issue,1645291417.326719,1645293118.920589,U01AXE0P5M3\\ne7fa4de8-2390-420c-87e3-28ce22c0c0fd,U02URV3EPA7,,,\"Ok. Thanks...\\nI will try again later  :simple_smile:\",1645291417.326719,1645293222.740449,U02URV3EPA7\\n1343dd03-4ea9-4c1b-9a57-a9a6756ff7a5,U02TJ69RKT5,,,I didn\\'t find anything about it in the documentation or StackOverflow. I don\\'t know what to do :smiling_face_with_tear:,1645242815.271579,1645293736.724859,U02TJ69RKT5\\n4bf86a96-3809-475e-9208-775063647ec6,U02S9JS3D2R,,,My query output for question 1 is 61630138… :harold:,1645039735.829189,1645297727.821239,U02BVP1QTQF\\n41419330-540c-4d24-aad6-83d85f2ab6bf,U029DM0GQHJ,,,Hi! I guess you need to check your source tables in BigQuery. Seems like the `VendorID` column in one of your tables has a `FLOAT` type instead of `INTEGER` one.,1645279317.124999,1645300174.620569,U02DNSC6Z52\\nebf463c2-b2ef-4dfc-814e-981ff199fc93,U02S9JS3D2R,,,I have the same counts for the 3 input tables,1645039735.829189,1645300968.489619,U029JNJBPED\\n319a9383-0ba6-402c-bc35-4d6fd394c0b6,U02S9JS3D2R,,,fact_trips  61539457,1645039735.829189,1645301540.962769,U029JNJBPED\\na29cd562-cda7-4b93-af38-e2bf7f929190,U02U34YJ8C8,,,Let us know how you go with it <@U02U34YJ8C8> - I\\'m still struggling along. Getting ready for another big day of it :confused:,1645213874.310119,1645303073.444119,U02U5SW982W\\n89a9a275-18a8-4edb-aa6e-8098601c02a5,U02TJ69RKT5,,,\"I just created a new project in dbt cloud and redid everything and got to run the seed file. Unfortunately, couldn\\'t find any other solution\",1645242815.271579,1645303797.523069,U02UM74ESE5\\nd4228962-495f-4219-a860-aa5ef26c33b6,U02S9JS3D2R,,,\"• fact_trips - 61,604,281\\n• taxi_zone_lookup - 265\\n• stg_green_tripdata - 6,304,783\\n• stg_yellow_tripdata - 56,100,630\\n• green_external_table - 7,778,101\\n• yellow_external_table - 109,047,518\\n:man-shrugging::thinking_face:\",1645039735.829189,1645304145.831899,U02TNEJLC84\\n8e105fe3-7086-43cc-913e-6442792e897f,U02V90BSU1Y,,,\"Thank you <@U02TATJKLHG>\\n\\nturns out I do not have that code in my  dbt_project.yml file\\nI have added it but now I have the below new error\\n\\n21:32:31  11 of 13 ERROR creating table model trips_data_all.fact_trips................... [ERROR in 4.42s]\\n21:32:31  12 of 13 SKIP relation <http://trips_data_all.dm|trips_data_all.dm>_monthly_zone_revenue................... [SKIP]\\n21:32:31  13 of 13 SKIP test not_null_dm_monthly_zone_revenue_revenue_monthly_total_amount [SKIP]\\n21:32:31\\n21:32:31  Finished running 3 view models, 7 tests, 3 table models in 16.94s.\\n21:32:31\\n21:32:31  Completed with 1 error and 0 warnings:\\n21:32:31\\n21:32:31  Database Error in model fact_trips (models/core/fact_trips.sql)\\n21:32:31    Error while reading table: de-zoom-camp-339323.trips_data_all.green_trip_data, error message: Parquet column \\'ehail_fee\\' has type DOUBLE which does not match the target cpp_type INT64.\\n21:32:31    compiled SQL at target/run/taxi_rides_ny/models/core/fact_trips.sql\\n21:32:31\\n21:32:31  Done. PASS=10 WARN=0 ERROR=1 SKIP=2 TOTAL=13\",1645202465.420209,1645306685.665929,U02V90BSU1Y\\n2565ac18-8de1-4db9-8fe7-4263aaaacddb,,12.0,,I\\'m facing this issue while trying to do `dbt run` for the `fact_trips.sql` file. I do not understand why it\\'s doing that,1645310364.645229,1645310364.645229,U02T9JQAX9N\\nddef2f9c-a4e1-42b4-a2fc-e840c8c1cd17,U02TNEJLC84,,,\"Hi Michael, thanks for putting the video together, its very informative. Although, I am still getting\",1645198942.177789,1645310376.687929,U02UM74ESE5\\n42a9c5f8-3c26-4cde-8ba0-04b3d420010f,U02TNEJLC84,,,\"```2022-02-19T22:32:01.478091Z: 22:32:01  1 of 1 ERROR creating table model dbt_amey_kokane.fact_trips.................... [ERROR in 0.84s]\\n2022-02-19T22:32:01.478210Z: 22:32:01  Finished running node model.taxi_rides_ny.fact_trips\\nDatabase Error in model fact_trips (models/core/fact_trips.sql)\\n  Access Denied: BigQuery BigQuery: Permission denied while globbing file pattern.```\",1645198942.177789,1645310390.299449,U02UM74ESE5\\n8722c34b-2173-4c19-9013-b678d1889799,U02T9JQAX9N,,,I used `BigQuery Admin` permission while setting up,1645310364.645229,1645310422.354509,U02T9JQAX9N\\n5d69256a-acc4-4a3b-aa04-f40ae2775be4,U02TNEJLC84,,,But when I run the compiled code in BQ it worked :astonished:,1645198942.177789,1645310454.930069,U02UM74ESE5\\nc8e2af05-7d7b-4333-9e88-78bac2bd1d43,U02TNEJLC84,,,dbt is driving me crazeeyyyy....,1645198942.177789,1645311005.368989,U02UM74ESE5\\n0340d309-faaa-4741-bf3c-392f55ebe3e9,U02S9JS3D2R,,,\"even the values from video seems different:\\nfact_trips:\",1645039735.829189,1645311174.078139,U029JNJBPED\\n68760eb5-ea44-4cce-b214-ef7fa4678dd5,U02S9JS3D2R,,,\"also from video, green and yellow tripdata count is different:\",1645039735.829189,1645311250.032079,U029JNJBPED\\n45d7bcad-60b9-4341-b009-2a11d026f5b7,U02S9JS3D2R,,,FYI <@U01AXE0P5M3>,1645039735.829189,1645311292.369089,U029JNJBPED\\nff2ef07d-0d42-4b97-9ad0-1fcfb8e3ebeb,,,thread_broadcast,\"I was able to resolve the issues by going through the FAQ.\\nThat document is detailed and helpful\",1645202465.420209,1645312193.548729,U02V90BSU1Y\\necb3adf8-398b-42c2-a3d0-d6ec6e436fc9,U02T9JQAX9N,,,\"hello <@U02T9JQAX9N>\\nhave you tried the below solution from FAQ?\\n\\nI did not experience this issue though\\n\\n\"\"\\n*When executing dbt run after fact_trips.sql has been created, the task failed with error:*\\xa0\\n\\n“Access Denied: BigQuery BigQuery: Permission denied while globbing file pattern.”\\n\\nFixed by adding the Viewer role to the service account in use in BigQuery.\\n\"\"\",1645310364.645229,1645312475.833539,U02V90BSU1Y\\n06cc4412-13a3-4719-856f-3af6176b885c,U02URV3EPA7,,,\"You can also try this, in the docker_compose.yaml _PIP_ADDITIONAL_REQUIREMENTS <https://stackoverflow.com/questions/67851351/cannot-install-additional-requirements-to-apache-airflow/68607370#68607370|https://stackoverflow.com/questions/67851351/cannot-install-additional-requirements-to-apache-airflow/68607370#68607370>\",1645291417.326719,1645315177.021529,U0308MF3KUH\\nA02CD66E-0F59-4D19-81B3-44C93F4537F4,U02BVP1QTQF,,,:sweat_smile: I just did the exact same. Glad I found this comment so I could fix it,1645188792.791639,1645315901.705599,U02U34YJ8C8\\n4259835c-fb4e-45ff-8e4c-df8a31974081,U02U5SW982W,,,I don\\'t know how I could have done this without reading your blog. Thanks so much!,1645071297.194149,1645318542.648829,U02UBQJBYHZ\\n3cfc58fc-84ab-482d-9f83-910d33c98320,U02U5SW982W,,,Assuming I did it right of course. :slightly_smiling_face:,1645071297.194149,1645318554.713509,U02UBQJBYHZ\\nbcca332f-ef6f-486e-875a-04c5fe06002f,U02U3E6HVNC,,,\"In other words, please ensure that you have created the schema in BigQuery first, as there will be an issue if you allow DBT to attempt to create it. You will need to set the optional location for the DBT credentials, and in my case I had to re upload the JSON to re input the private key as I was editing it afterwards.\",1644432387.132949,1645319010.573889,U02SEB4Q8TW\\nF936C77F-FC53-4BB5-9B2A-7CE5E17CD296,U030FNZC26L,,,<@U030FNZC26L> Did you follow a particular guide? I\\'m looking to deploy superset on GCP later as well.,1644902599.749989,1645319468.743989,U02UX664K5E\\n1808286b-3372-4514-8017-0b815625aada,U02HB9KTERJ,,,\"Thanks Max, I am using the web IDE with GitHub integration and also had to use your code to install DBT utils\",1645004802.322059,1645320913.525009,U02SEB4Q8TW\\n6a538456-e422-4937-8b47-d4ad2d48ede9,U02T9JQAX9N,,,make sure on schema.yml table name and database name is same in bigquery,1645310364.645229,1645323770.551359,U02SQQYTR7U\\na7759478-73c2-4514-970b-ba8a599a2ef1,U02S9JS3D2R,,,\"Me too still confused about  count the data `fact_trips`\\nI already rebuild the csv and upload to bigquery but still my count data is not available on choice\",1645039735.829189,1645324070.409229,U02SQQYTR7U\\n356f4305-0749-459c-a917-76530bcd076a,U02U43UTK8W,,,this was a typo… i figured it out.. .thanks!,1644567924.375569,1645328179.989949,U02U43UTK8W\\n1ccfcb25-af3d-4ca9-9964-a20e2d30fb6d,U02HB9KTERJ,,,Great Will. I am using Ankur\\'s \\'hack\\' to run dbt in docker with BigQuery,1645004802.322059,1645333597.739299,U02HB9KTERJ\\nc926c9e2-d29f-45ce-8d55-60645d4fdc0c,U02S9JS3D2R,,,<@U02T3F06EKT>,1645039735.829189,1645336795.687249,U02SQQYTR7U\\nb5b3e828-ce67-493a-8814-2022db0ecb9a,U02S9JS3D2R,,,cek di sini mas,1645039735.829189,1645336799.285439,U02SQQYTR7U\\n7c3725ee-b7fc-46de-9444-c4e5881d0b53,U02S9JS3D2R,,,<@U02T3F06EKT>,1645039735.829189,1645336803.553779,U02SQQYTR7U\\n152364e6-e9d9-4fda-beac-b8a34a9a1352,,5.0,,Homework 4 - Any chance of a slight extension? I keep running into walls with dbt. I\\'ve feel like I\\'ve been chasing my tail for the last 3 days.. not sure if it\\'s just me...,1645338532.693739,1645338532.693739,U02U5SW982W\\n45b99cbc-3286-48da-ba0e-5ce25a654b5b,,3.0,,\"is cloud dbt server down/error? i can\\'t execute dbt seed\\n```an error occurred in the dbt server. Please contact support if this issue persists.\\nUnexpected empty response.```\",1645340086.281699,1645340086.281699,U02RA8F3LQY\\n07304acf-3bf2-48f3-9bd6-274eb282a063,U02RA8F3LQY,,,I\\'ve been using it fine <@U02RA8F3LQY>,1645340086.281699,1645340499.299279,U02U5SW982W\\n2772e56b-3eb7-4f24-9f79-be98c0b2e2c5,U02RA8F3LQY,,,Did you try just leaving it for a bit and then refreshing? Sometimes I get weird errors.. but then they self-resolve,1645340086.281699,1645340526.371999,U02U5SW982W\\nf7759e88-5b59-48a2-9cf8-03334e8a25f0,U029DM0GQHJ,,,yes it is,1645279317.124999,1645340889.651399,U029DM0GQHJ\\nf1f136e5-0709-4773-b17b-47e11215c487,U029DM0GQHJ,,,looking for a workaround,1645279317.124999,1645340902.948459,U029DM0GQHJ\\n54700f72-4351-42ed-b724-f406e5b92c3e,,6.0,,\"Cannot read and write in different locations: source: europe-west6, destination: US\\nI am stuck at this error. I created my bigquery dataset again in US, but still getting this error\",1645343764.071639,1645343764.071639,U02U6DR551B\\n8131e7cd-336a-4a78-a67e-1d2e5441b67e,,,,\"Good morning everyone, I\\'m Bahij Youssef a systems engineer based in the UK. Really looking forward to the course :slightly_smiling_face:\",,1641801802.114200,U02SUH9N1FH\\n1294888a-8a25-46dd-822e-db5b70756c97,,,,Hello everyone from the UK !!! Happy to be here and I am super excited to learn and improve my data-engineering skills with this course with this course,,1641802500.116200,U02JV8F2DCK\\nf1ad8514-9487-419a-8d1d-580fdfa4ee43,,,,\"Hello from Bangalore, India !!\",,1641802682.116900,U02TA6PCN3V\\n6c427b73-4138-455a-9091-bc4a20d7c4b8,,,,Hello everyone !! Very much looking forward in getting an expert perspective in data engineering and imbibing the same in my work !,,1641802799.118400,U02TBS3A02Y\\n3735eeaa-ed79-43a6-81b9-5a4aaf237510,,,,\"Hello everyone, I am Tiisetso, a Data Engineer from Cape Town, South Africa. I am looking for forward to learn more about Data Engineering and improve my skills repertoire!\",,1641803097.120300,U02TA54RTU2\\nf65319c9-b715-4204-91d8-8d16dd4244af,,,,\"Hello form Lahore, excited to learn with you guys!\",,1641803124.120700,U02RU6NFCQ0\\n10a0dedb-74b9-43d2-ba1b-4106d2951c2c,U01AXE0P5M3,,,\"hi <@U01AXE0P5M3>, when we need GCP account ? because free tier GCP account only 3 months\",1641566368.013200,1641803733.122300,U02B5AM1ZT4\\na2c2e11c-3430-4173-a5be-443c95ad705d,U01AXE0P5M3,,,\"3 months is enough, you can register now\",1641566368.013200,1641803781.122500,U01AXE0P5M3\\n087e7c0a-3dc6-45f2-aca2-16bdae37fc0a,U02QNEVDSMA,,,\"Also, Victoria &amp; Ankush. Vic\\'s latest commits on DBT are looking incredible!\",1641768851.058400,1641803801.122800,U01DHB2HS3X\\nC9F77AF5-A7C3-41D5-8EA0-9B437FA700EE,,,,\"Hello from New Delhi, India … Excited and looking forward \",,1641803829.123700,U02TN145GE5\\n48116596-6ad1-414d-acb1-2b1eb93cd8cf,U02QNEVDSMA,,,Yes indeed! And I\\'m mostly just merging PRs and not doing any other work =),1641768851.058400,1641803837.123800,U01AXE0P5M3\\n529627ff-b1d8-4316-ab71-89cad35d35f9,,,,Hello all from France !,,1641804875.125400,U02TA9ZP9KL\\n802729cd-57be-42f5-a73d-4a3ab37315c4,,,,Hello everyone from Turkey. Looking forward to participate,,1641805038.126100,U02U01NF23S\\n3dff6ebc-e6d3-40b3-8b12-bc73ccf632ea,,,,\"Hi all from Amsterdam! Reyn here, BI analist looking to learn more on data engineering, looking very much forward to starting next week!\",,1641805599.127300,U02T3LP87GE\\n8fef0a35-f274-4b5b-99ca-68a36316c659,,,,\"Hello from London! Currently working in legal technology, looking forward to apply the learnings of this course on my day to day!\",,1641805950.128300,U02C9UF5HKR\\n79eeff94-7c66-417d-be67-fbf736dae6c1,,,,\"Hi everyone, I am from Poland :slightly_smiling_face: nice to meet you all!\",,1641806186.129100,U02TCMEDTUL\\n58eb7d1e-86ca-4e56-a17d-70b930f6e938,,,,Hi Everyone from Madrid!,,1641806597.130700,U02T7FN7KBP\\n0d07d4b4-4bb5-487e-975f-82d173f71967,U02SFFC9FPC,,,\"Dark Matter Research, Black Hole Microstates, String Theory, and Particle Physics(Quantum Gravitation) are the fields of research i am currently within.\",1641275845.206400,1641806637.130800,U02SFFC9FPC\\nec4aae7a-60ad-4448-bbc4-ad2335d57734,U02SFFC9FPC,,,Yourself?,1641275845.206400,1641806646.131000,U02SFFC9FPC\\n9BED74F9-F5E9-46D1-8056-9C704507A082,,,,\"Hey all, I am from Cyprus working in Amsterdam. Cant wait to expand my data engineering knowledge and share concepts and projects with all of you :)\",,1641807404.133000,U02U07906Q0\\n3d87cea8-9641-44c8-96f8-dfa40237ed7b,,,,Hi everyone from France:flag-mf:,,1641808833.136100,U024BBM48NN\\nba9d82f6-9f2f-4357-a2bd-47170ee8cc1a,,,,\"Hi, I am Vaibhav from India. Would love to connect with you on <https://www.linkedin.com/in/vaibhav-gupta-98b5671b0/|LinkedIn>.\",,1641809526.137000,U02SH7ZL536\\na09af930-604d-428e-9458-a9071e59dc19,,,,Hi everyone I am Sampath from India,,1641809550.137400,U02T2TX1GS2\\neb126aca-6219-40b7-9952-68f8d45e8bc3,,,,Hi it\\'s Ziad from :flag-dz:,,1641809896.138300,U02TCSGRBAQ\\n9186da25-989a-434a-a6de-419b5a0a1990,,2.0,,Hi! I\\'m Alejandro from Argentina :flag-ar:,1641810834.142400,1641810834.142400,U02SW4N130F\\n12c805bf-e51c-434f-bcf3-dd54e0e84f83,U01AXE0P5M3,,,ok thx,1641566368.013200,1641811091.142700,U02B5AM1ZT4\\n178d1d5e-3744-4650-aab2-e6e28380ffcb,,,,\"Hi everyone, Simanga from South Africa\",,1641811911.143900,U02U0H195ME\\n27b24a0f-aee5-4044-aaa0-3960e2700439,,,,\"Hi everyone, Taieb from France\",,1641812363.144500,U02DBNR22GN\\n8d0302a6-4c71-4f8f-9074-5fcdb8805cdc,,,,\"Hi, I\\'m Hadi from Indonesia\",,1641812715.145700,U02TAR5QE1Y\\nc2e4ecb0-61fb-4176-9c8f-982ac31c9e6a,,,,\"Hi everyone! I’m Alex, from Portugal\",,1641812725.145900,U02U0K2V1PS\\na7c91dee-acd5-40f2-908f-df4b90aeb6fd,,,,Hi everyone! I\\'m Ruslan from Moldova :slightly_smiling_face: :flag-md:,,1641812812.146700,U02N774Q4B1\\n20f7aea4-4638-4422-b832-c9581cf846ce,,,,\"Hi everyone, I am Eva from Germany\",,1641814061.147600,U02T81RNJFP\\n86cef616-458a-4893-90c8-31d788c3a961,,,,\"Hi everyone, I\\'m Ifeanyichukwu from Nigeria.\",,1641814559.148400,U02TD8DP6UC\\n2d82d602-1fea-412d-98f3-7a7305a42e31,,,,\"Hey everyone, I\\'m Francis from Kenya\",,1641814642.148900,U02EXACMVAA\\nb5a62e7e-027f-492b-b1ce-249a5aae6d83,,,,\"Hi, everyone! i\\'m Andrew from Russia:wave:\",,1641814851.149600,U02T0CZ25U2\\n3dfe11de-87c5-478f-98be-9f3c0a3fd977,U02SW4N130F,,,Bienvenido coterraneo :flag-ar:,1641810834.142400,1641814887.149700,U01B6TH1LRL\\n14dc5e01-1715-43bb-adbd-2524675be50d,,1.0,,\"Hi, everyone! Which SO is more convenient for the course: Linux or Windows?  From my experience I know that Docker runs better on Linux...\\nThanks in advance.\",1641815369.154000,1641815369.154000,U02SW090PFZ\\nf1c9147d-abb8-47c2-a261-7d0642cdfde8,,,,Hi everyone~ I\\'m Kat from South Korea : D,,1641815453.154600,U02RRSRTDD1\\n46dc62f8-a3d7-4e2a-a372-44d0abe3c0d5,,,,\"Hello everyone, I\\'m Vincenzo from Italy :)\",,1641815486.155000,U02BH0Y1GT1\\nc9e35f4d-7bc0-46be-8c8d-6c2e5e521e6c,,,,\"Hello Folks, this is Cesar from Colombia\",,1641815516.155900,U01QPFCUXFZ\\n5b0f1890-d639-494e-95d9-48c33c4a2d41,,,,\"Hi everyone, I\\'m Jaikumar from India.\",,1641815526.156500,U02TP679CRX\\n7ae0d414-94a7-4402-970e-116bd0c7441c,,,,Hi everyone. My name is Emin Can from Turkey.,,1641816846.158200,U02TB5KFP0A\\ndba49794-1e46-4c67-aaf3-b072b846cb37,,,,\"Hi everyone,  I am Sandra from Paris (France)\",,1641816876.159100,U02TB5NK4FL\\nef04240d-f616-4683-a7e6-51ffe3206623,,,,Hi! I\\'m Timur from Russia. Nice to meet you all!,,1641818015.159900,U02DNSC6Z52\\nbe54a4d9-87d0-4897-af0d-b1a741261b57,U02SW090PFZ,,,\"Both should be fine. These days docker works quite well on Windows. So if using Linux is way out of your comfort zone, go with Windows\",1641815369.154000,1641818943.161200,U01AXE0P5M3\\nf83ddeda-e601-47ca-9fcd-223f9a715c5a,,,,\"Hello, I am Mariusz from Poland, nice to meet you all! :wave:\",,1641819601.163100,U02TPUDQK7T\\nf1ce72cf-6f6e-41e7-a37f-9e842c650fe5,,2.0,,\"Hello everyone, I\\'m Bani from Indonesia\",1641819619.163600,1641819619.163600,U02T8ANTJGM\\n9c75b457-3746-4d7e-bf4a-0bf396330d49,,,,\"Hi all, I’m Kan from Thailand. Very nice to meet you!\",,1641820157.164300,U01EZ2KNAQ0\\nc8a34873-38e6-4ed4-b230-3e9a9520d75f,,,,\"Hi all, I’m Eesa from London!\",,1641820256.165200,U02SWPWGGPR\\n21eb9098-6cc0-413a-8c16-55c98e4fa2f7,,,,Hey Everyone! I\\'m Murali from India. Glad to be here,,1641820686.166700,U02T213S7RC\\nea7b8a15-7939-4ea7-a858-7e46ed8c9ec6,,,,\"Hi, everyone, I am Eyvaz from Azerbaijan. Extremely happy to join\",,1641820981.167700,U02SWN3SJ95\\n017d81b7-7a9f-403c-86d6-8d41ee737ef6,,,,<https://www.linkedin.com/in/eyvaz-najafli-43407618b/>,,1641821053.169200,U02SWN3SJ95\\n7dd3ba4d-cc8c-466e-bce2-734827eaee65,,1.0,,\"Hi All, Will we receive the zoomcamp link on email. If yes, by when will we receive them?\\n<@U01AXE0P5M3>\",1641821128.169500,1641821128.169500,U02R8V9MF6E\\n6036c174-7976-48d6-94bc-13bf34b80653,,,,\"Hello All, I am Abhishek Sawant from India.\",,1641821537.170400,U02SWSTT11D\\ne52853cb-6230-41a3-8c14-b25e36070e25,,,,\"Hello All, i am Langat from Nairobi\",,1641821792.170900,U02TDNSU9TN\\nc39dc376-2d14-4249-952b-59e2599fb4ae,,,,\"Hi, everyone! Max from Russia\",,1641821908.171400,U02TBGHQ6SW\\n8da88ac1-2ec4-45cf-918c-98838990daeb,,,,Hallo Everyone ! I am Favour from Nigeria. Thank you for this opportunity !,,1641822380.172900,U02SJDCSGSF\\nFC09DADB-5D1F-41E7-8D0C-4FB9F7354EFB,,2.0,,Hey <@U01AXE0P5M3>. Can we do data engineering just by using python? What is the difference between apache spark and hadoop? I’m really confused.,1641822464.174100,1641822464.174100,U02QHQU3LF7\\n92746863-d47f-4d82-9009-102f498234e2,U02QHQU3LF7,,,\"Apache Spark runs on top of existing Hadoop cluster to access Hadoop Distributed File System.\\n\\nSpark is a distributed processing system which is used for bigdata workloads.\\nHadoop uses the MapReduce to process data, while Spark uses resilient distributed datasets (RDD\\'s).\",1641822464.174100,1641822774.174500,U02QGA57GRY\\na9070a28-bd21-4920-8226-027fc77bde3c,,,,Hi there! Here Matías from Chile! :flag-cl:,,1641822836.175100,U02TQ0Q1BCZ\\n413582ed-eef1-4d82-9738-7190967e46b1,,,,Hi All! I am Justin from the United States. I am really excited for this class!,,1641826517.177900,U02TBTX45LK\\n817f84b9-ae26-4b89-ad12-d677bac8a9e8,,,,Hi all! I\\'m Anand from India,,1641827563.179500,U02SXAD9RAT\\n19dc878d-583c-4680-88d3-0c872e8de983,,,,\"Hello everyone, I am Emmanuel from Nigeria.\",,1641827583.179800,U02T59ZAVE2\\n93546d12-e96a-40d3-b66f-739e6ef9c550,,,,\"Hello everyone, I am Souvik from India. I have just recently started my journey of learning in this field of Big Data. It will be of great help to me if anyone can guide me on how to gain work experience in the form of internships and projects as a Data Engineer.\",,1641828132.182400,U02T96HEARK\\ne861c11a-a581-42e4-850e-cac53530da07,U02R8V9MF6E,,,I\\'ll send another email soon with details. And also write here,1641821128.169500,1641828523.184200,U01AXE0P5M3\\n398E8E56-802E-461A-AFB5-48C78EA2674E,,2.0,,Hello I am Mathew from Nigeria!! Excited to be here! I know class will be on Mondays at 17:00 CET. Does anyone by chance know the duration of class ? Thank you in advance ,1641828527.184500,1641828527.184500,U02TMQELP8R\\n75e752d9-9ebe-413d-a371-38c4bebe0f42,U02TMQELP8R,,,\"Hi Mathew, the course lasts for 6 weeks and for the estimate duration of each session on Monday, have a look at their Github repo: <https://github.com/DataTalksClub/data-engineering-zoomcamp>\",1641828527.184500,1641828650.184600,U02QPBZ3P8D\\n1ab84a93-cce5-4081-88ac-52413d5d9fad,U02TMQELP8R,,,You will have the entire Syllabus and the estimated duration for each session,1641828527.184500,1641828688.185000,U02QPBZ3P8D\\n5be7e789-1e8b-4ebb-b93d-f7099482c8b6,U02S51KRTNZ,,,Thanks <@U01DFQ82AK1> and <@U01B6TH1LRL> for the hard work on this. Should be super valuable to all of us,1641768630.056700,1641829591.185400,U02S51KRTNZ\\n3362a1dc-9f61-41fe-a431-1e182629c178,,,,Hi All! thanks for this zoomcamp.,,1641834122.188700,U02T64KSX2S\\nf274e5e2-d303-463d-ab64-fcaf8aae447c,,,,Hii Folks. I am Aybüke from Turkey,,1641836714.190700,U02T6CGSCKY\\n79f836e0-6fae-49c7-a374-770ceac687de,,,,\"Hi everyone, greetings from Guatemala!\",,1641836861.191200,U02TCUQFCG2\\n90187469-b7c1-4e00-af17-586009b241eb,,,,LA - California ....,,1641838254.192400,U02QDMQ969L\\n25a73823-3120-477c-ac18-5641751e2ad4,,,,\"Hi from Saint-Petersburg, Russia\",,1641838427.193100,U02SXLK1QKZ\\n241bf7c6-a1c8-40d4-bf89-fecaf201203c,,,,\"Hi all, I\\'m Wolfgang from Germany.\",,1641840328.195000,U02TDAWNASW\\n1A1DEFE1-0D1D-45F2-A6DD-CD969995E99B,,,,\"Hey everyone, erik from Oregon, USA. \",,1641840366.195600,U02T96W4GKV\\n98248fd9-95e7-4855-9034-0f73b5358006,,,,\"Hi, I\\'m Alexander from Kazakhstan:flag-kz:, currently living and working as Data Engineer in Estonia:flag-ee:\",,1641840416.195900,U02E63F2S4V\\nbec4d36f-9642-44a6-a931-457a9a5bc397,,,,\"Hi guys, here from Chile\",,1641841308.196600,U02TEKD2UKE\\nD68F1A42-AF4A-489B-BF1E-303F9DFE9EFC,,,,\":wave:. I\\'m JB from New York City, US\",,1641843361.198500,U02SZ3X8K0F\\ncdd0f493-effc-4601-8bb0-5b4b0da6f4fe,,,,Hi Renee from Vegas! Nice to meet you all.,,1641843567.199000,U02U3F301FA\\nd5cb310b-e318-4731-b69d-a1c3432c5372,,,,\"Hello everyone, my name is Golden from Nigeria and I\\'m very happy to be here.\",,1641846205.200300,U02SF9XHSLC\\n6D37B835-E655-4E56-B858-9C117AE06E8C,,,,\"Heyia, I am Pasha from Russia :ru:. Good luck everyone!\",,1641848122.201900,U02TE4T7S74\\n4ac56117-766c-45e4-82e1-0d1a4223ad14,,1.0,,Hey! I\\'m Lucas from Argentina,1641848504.202600,1641848504.202600,U02T2JGQ8UE\\nef27b048-beec-4534-b28a-a23fb60daeab,,,,Excited to Join! looking forward to the Zoomcamp.,,1641853886.205300,U02TD4Q3CPM\\na69f8e6d-95e0-4978-b79c-e3529c29c6af,U02T2JGQ8UE,,,Bienvenido :flag-ar: :mate_drink:,1641848504.202600,1641855396.205800,U01B6TH1LRL\\n379bb23d-7fbb-4365-b8d0-5d5c340c69d8,,,,\"Hi! I’m Salman from Toronto, Canada :flag-ca:\",,1641857918.210400,U02TT81U75F\\nf2aee249-024f-488a-804a-3c1360a9808a,U01AXE0P5M3,,,i dont have a credit card/ debit card so i cant create a gcp account. Is there any other alternative?,1641566368.013200,1641859027.211500,U02RU9CMSRM\\nb8b34c52-616b-444a-b14b-9d7fda0d0ec3,,,,\"Hi, I\\'m Nick from Maine, USA\",,1641860136.214000,U02TB19TM7C\\n574F4B0D-04B7-45D6-9F9B-FB0B5F1F8287,,,,Hi everyone. I’m Danny from Peru :flag-pe:. Nice to meet you!,,1641862360.215200,U02TF2DTGH0\\n88b9b97b-7052-4626-a959-3fe4ab0a76f7,,,,\"Hi everyone, I am Shanan from Egypt\",,1641864184.216300,U02TTPVDCC9\\ncc627c03-21a6-40d2-9895-a1a790feaddb,,,,Hello Everyone.  Thanks <@U01AXE0P5M3> for this course.,,1641864700.217900,U02EHEM7BP1\\n386df187-5c0e-48e8-b8a6-bc329167d4d5,,,,\"Hi everyone, I\\'m Phat from Vietnam\",,1641864863.218400,U02TTQC61FB\\nc2f69a1d-8d9d-4df6-a399-3beb5d3c085e,,,,\"Hello All, I am from DC area. Excited to start this journey with you all\",,1641867316.220600,U02T8HEJ1AS\\n09c4f1f6-71cb-4afd-a9fe-559c51dcc260,,,,Hi everyone I\\'m AI Researcher from Thailand Nice to meet all you guys ^^,,1641871368.221100,U02TU38C5MX\\n5C5B842A-C1AD-4FEB-9D9E-4DEDB8E67A04,,,,\"Hi All, I am Sreelakshmi from NYC,USA\",,1641872200.222200,U02TBH02GQ4\\n3da16d71-bef6-4156-80cf-b4b05738d8af,U02T8ANTJGM,,,semoga bisa sharing - sharing nanti mas hehe,1641819619.163600,1641874943.223600,U02SQQYTR7U\\n19225557-c1fa-4d90-8cc1-4cfba3eaedea,,,,Hey everyone I am a new grad programmer analyst from :flag-in:. I am really excited to be a part of this grp.,,1641876632.225400,U02THT8U1GC\\nf819c3fb-ef9d-4e4a-b41e-696f848cf8df,,,,\"Hi everyone, I\\'m Software Engineer from Thailand.\\nCall me Non.\",,1641877448.227300,U02TFLGBWTD\\nc86a3b2d-3430-4b60-b097-83d5cbbf31d6,,,,Hi everyone. I am Hande from İstanbul,,1641878119.227900,U02TFN2LWAF\\nca187244-0b44-4556-98b2-543503142eaa,,,,\"Hi all, Im Andi from Indonesia :wave:\",,1641881662.229000,U02RQEQP0R2\\ne91c97cc-2480-45a3-a411-acadb61a0008,U02QHQU3LF7,,,Yes you can,1641822464.174100,1641882248.230900,U01AXE0P5M3\\na5ec9e9c-5390-4900-bd66-95952c28cd62,U01AXE0P5M3,,,\"Yes, it should be possible to use a local postgres instance instead of big query\",1641566368.013200,1641882485.234200,U01AXE0P5M3\\n87c920aa-0e23-4238-9ba7-d52256d05477,,,,\"Hi everyone, I\\'m Bayu from Indonesia\",,1641882547.235300,U02TDA937GU\\n1dcdc958-b845-420a-8253-d1181cf86851,,1.0,,Hello from thailand. Is it live only? I\\'m in SEA time zone.,1641883740.238200,1641883740.238200,U02TFHNMV7V\\n43a4ee58-781a-4b83-856a-cd14ee313ee6,,,,Hello Everyone :wave:. This is Gaurav from India.,,1641884052.239600,U02TFUUEXAN\\n974d0906-de7a-4179-87b3-88d629ee4fe9,,2.0,,\"Morning everyone, when does the course starts?\",1641884730.240300,1641884730.240300,U02TBKCEMAQ\\nf8bcad81-75e9-4e98-905b-335eaccaf6c2,U02TBKCEMAQ,,,Morning! On Monday,1641884730.240300,1641885126.240400,U01AXE0P5M3\\ncab9659f-29ed-4aeb-9797-448a8a3dcdcc,U02TFHNMV7V,,,\"No, everything will be recorded\",1641883740.238200,1641885147.240700,U01AXE0P5M3\\n857285d9-8ec8-4ac7-897b-c9dc7deb9e10,U02TBKCEMAQ,,,Lovely. Thanks! :pray: :blush:,1641884730.240300,1641885212.241100,U02TBKCEMAQ\\n3beb7b90-d8b0-4eb7-a6ca-3246920007a4,,,,Hi everyone! I\\'m Tatiana from Madrid,,1641885472.242100,U02R6MXRXFE\\na7031329-582c-495e-89c9-0e7bca6e63b6,,,,Hello from Mexico! :taco::flag-sn:,,1641885519.242800,U02T9JK011Q\\n334b909d-caba-4ac3-88a4-1879537af7e2,,,,Hello all from Russia ),,1641886034.243500,U02SR5MAA48\\nb159463b-e09a-477b-b015-12cf36e2af8b,,,,Hi everyone! Looking forward to the course and learning from y\\'all :wave:,,1641886181.244200,U02T941CTFY\\n822e9157-8a87-4ead-893d-ef0178962a44,,,,Hello from Kazakhstan!,,1641886634.244900,U02T9CQPXNJ\\n7cfa817b-c09a-44dc-8baa-6f5f5eb8faf1,U02U6DR551B,,,<@U02U5SW982W> Your email accounts are visible on one of the screenshots. JFYI,1645343764.071639,1645517364.410539,U0290EYCA7Q\\nb34d3966-75f1-4046-bd88-3faf03a51228,U02T941CTFY,,,And why do you need findspark?,1645512707.679389,1645519101.515289,U01AXE0P5M3\\nadcea418-d6cd-4938-a6b3-04aa69953c46,U02TS7NHGPR,,,I would not fill it and leave this decision to downstream tasks,1645514258.973169,1645519145.942199,U01AXE0P5M3\\n191670ab-9f38-443a-abd2-bbba92d6613c,U02TS7NHGPR,,,But it really depends on what you do,1645514258.973169,1645519169.315269,U01AXE0P5M3\\nde009277-78af-4f23-8264-c3e0e5757a2d,U02UKLHDWMQ,,,But this will not create the table and partition in bigquary,1645501822.553359,1645519591.201989,U02UKLHDWMQ\\n3005ca11-cd66-4b09-908b-cf3f03ddc238,U02TS7NHGPR,,,\"As a data engineer, I will always leave the missing values in upstream.\\nMissing data can be treated differently, null is very different from average or 0.\",1645514258.973169,1645520533.134739,U01DFQ82AK1\\nee92202d-21c9-4078-9a24-d48e0caf43f8,U02T8ANTJGM,,,\"I didn\\'t experiences that much, only after having open the tab inactive for a day and trying to get back. Could be the connection.\\nI know they had an outage last week, but other than that I don\\'t know.\",1645488963.706489,1645522074.265499,U01B6TH1LRL\\n375b3b46-133a-4095-a5b0-253b9913984d,U02QK4ZV4UX,,,The problem is that you are trying to read from a location (trips_data_all) but write into a different one (dbt_name) and BQ doesn\\'t allow multiregion queries. Just drop and recrete the schemas you\\'ll use with the same location as trips_data_all from BQ console directly.,1645507617.625979,1645522210.031969,U01B6TH1LRL\\n0237ea4b-289f-44f6-8e38-06465a3fc1cc,U02TS7NHGPR,,,Thanks <@U01AXE0P5M3> and <@U01DFQ82AK1> for answering my question!,1645514258.973169,1645522302.116119,U02TS7NHGPR\\nce996365-7d12-44b9-8991-434943d428f8,,5.0,,\"Hi all. pgcli is requesting for password for my user account, I have supplied the password I use to login to my laptop but I still get password authentication error. I\\'m in week 1. Can someone help? I have tried leaving it blank, same error\",1645526853.296059,1645526853.296059,U034BND3P2M\\n6ff2f4dc-4821-4d49-b64b-bab77b748c1d,U02T941CTFY,,,You should add it in the PYTHONPATH as shown in the video. And double check the installation path,1645512707.679389,1645526925.710649,U0308MF3KUH\\n1d016eba-56fe-41d4-be27-45e98b65bbdf,U02QK4ZV4UX,,,Did just that <@U01B6TH1LRL>,1645507617.625979,1645527047.165339,U02QK4ZV4UX\\n20906114-f283-4c9e-8338-b1ab22da3655,U02QK4ZV4UX,,,\"I think at the start when dbt profile is created and the Location is empty, it creates the dbt_name in us by default. And BQ doesn\\'t allow multi region queries. If the location is specified as the one your trips data is everything should work fine.\",1645507617.625979,1645527134.802119,U02QK4ZV4UX\\na45d1a3a-d026-4cff-8103-829d968edc02,U02QK4ZV4UX,,,Location should be specified in the dbt profile before you dbt run.,1645507617.625979,1645527182.518179,U02QK4ZV4UX\\n21b58000-1a77-4327-b520-2c5f8297f5c7,,3.0,,<@U01B6TH1LRL> you created three different datasets in the videos.. `dbt_&lt;name&gt;` was used for development and you used `production` dataset for the production environment. What was the use for the `staging` dataset?,1645527335.440119,1645527335.440119,U02T9JQAX9N\\n196228a1-9074-4b18-85a3-33bac500efe0,U02TATJKLHG,,,\"Hi <@U02TATJKLHG>. I have followed the instructions and it worked but I can\\'t create a new folder or file. It says\\n```Error: EACCES: permission denied, mkdir \\'/home/bani/dtc-de/dbt-bq/dbt/ny_taxi_bani/models/staging\\'```\",1644493645.074519,1645529533.156559,U02T8ANTJGM\\n612bd736-5428-42c6-b139-85513c09de44,U034BND3P2M,,,This is sorted now.,1645526853.296059,1645530061.070609,U034BND3P2M\\na99a4c52-b29a-4159-94da-c51f42389e3a,U02T9JQAX9N,,,\"Staging, as the name suggests, is like an intermediate between the raw datasets and the fact and dim tables, which are the finished product, so to speak. You\\'ll notice that the dataset in staging are materialised as views and not tables.\",1645527335.440119,1645531394.042869,U02SZARNXUG\\na6282caa-733e-4580-bd40-08fe9f7713cc,U02TATJKLHG,,,\"<@U02T8ANTJGM> A simple hack would be to open the folder using root (right-click and open as root - I am using nemo/cinnamon). Or from the terminal, `sudo gedit/mkdir` etc.\",1644493645.074519,1645532151.004959,U02HB9KTERJ\\n51398db0-b346-4cc4-ab20-918c419ad745,U02UKLHDWMQ,,,\"it has problem with green parquet file, so u just try to run the DAG again\",1645501822.553359,1645533698.086659,U02T3F06EKT\\n3330675f-93f6-4ed7-bb40-e8f4f5d1fb10,,3.0,,{Week5] Is spark resource-intensive? Won\\'t a desktop pc suffice for these files?,1645534158.245849,1645534158.245849,U02HB9KTERJ\\n3b819c21-8260-452c-b836-a01bb65b4cf1,U02T9JQAX9N,,,\"Thanks for replying.. I was referring to the staging dataset created in BIgQuery, not the staging folder created in dbt\",1645527335.440119,1645535204.365249,U02T9JQAX9N\\n1fac98ae-2515-46ba-acc9-f01d213ebe80,U02T9JQAX9N,,,\"I didn\\'t use it for the project, you just need to create production and dbt_name + trips_data_all that you had already\",1645527335.440119,1645536876.156909,U01B6TH1LRL\\nCDA9086F-E41B-4874-94C4-17E30A67C36C,U02UKLHDWMQ,,,You can comment out ehail field since it is not used for any kind of analysis for now. But if you need a better solution check week 5 office hours video from yday in which Alexey mentioned it would be better if we cast ehail column in airflow dag step while bringing it into gcp ,1645501822.553359,1645537412.656949,U02UM74ESE5\\n146bc408-5085-4629-9b69-13427185c4f8,U02HB9KTERJ,,,\"I guess partitioning will take some time,  but you can easily switch to colab for week 5\",1645534158.245849,1645541127.040979,U02UJGGM7K6\\n43873466-ae0f-4462-8dc3-ba711853f8fb,,4.0,,\"*Java*:tea:  *or Scala*:red_envelope:*?*\\nI\\'m wondering what would be the best bet for Data Engineer\\'s career path. In job postings, especially senior roles this preference is not strongly specified most of the time,  assuming if a person knows any of them he/she will be a match for this job.\\nWhich one will gain more popularity and presence in the coming years?\\nPlease share your thoughts down below  :simple_smile:\",1645542413.218429,1645542413.218429,U02UJGGM7K6\\nd4ed008a-6c70-4109-8291-d21259ab336d,U02HB9KTERJ,,,Yea the stuff we do isn\\'t that resource intensive. You indeed can skip the reporting part,1645534158.245849,1645543201.669359,U01AXE0P5M3\\nc525f79d-8dc2-4cc3-ab07-b66c2e5dee94,U02QK4ZV4UX,,,\"Hi all.\\n\\nFor those who are still running into issues!\\n\\n In DBT cloud you can actually specify the location using the following steps:\\n\\n1. Go to your profile page (top right drop-down --&gt; profile)\\n2.  Then go to under *Credentials --&gt; Analytics* (you may have customized this name)\\n3. Click on *Bigquery &gt;*\\n4. Hit *Edit*\\n5. Update your location, you may need to re-upload your service account JSON to re-fetch your private key\\nOnce done your IDE will ask you to reset (if you have it open in another tab). Now any queries should have the correct location associated with it.\",1645507617.625979,1645543230.618049,U03456UBF18\\n86929472-2d70-4ef4-9495-35c2b56382b9,U02UJGGM7K6,,,\"I personally can\\'t stand Scala. They wanted to do a better Java but ended up with so many features that even with C++ you\\'re less likely to shot yourself in the foot\\n\\nThat said, for spark you probably use a very small subset of language features and with good code review and keeping away the FP zealots, it can be not only bearable, but even sometimes enjoyable\",1645542413.218429,1645543386.682469,U01AXE0P5M3\\n30c1fb43-8b26-49b2-a458-6a0bb9b7cdc1,U02UJGGM7K6,,,\"Java is verbose but predictable and you spend less time figuring out what this FP smart ass wrote - everything is straightforward \\n\\nSorry for ranting, I just had to use Scala at some point of my career and I can\\'t say I enjoyed it\",1645542413.218429,1645543526.260029,U01AXE0P5M3\\nfa527d89-d226-4b82-823b-89b7ac3f2a77,U02QK4ZV4UX,,,\"I was able to get this to work with `northamerica-northeast1`. If you run into issues after this, make sure you check where your BQ tables are located. When using a specific location (non-multi-region) you can only process data where it is located\",1645507617.625979,1645543532.993539,U03456UBF18\\nba26012e-40ef-4891-9e5a-09c82a533439,U02QK4ZV4UX,,,\"lmk if you have questions, happy to help :slightly_smiling_face:!\",1645507617.625979,1645543551.427469,U03456UBF18\\n1ff0e799-f3cc-4c88-85c2-673101b8ada4,,4.0,,\"Anyone else only get 6 parquet files when following along with video 5.3.1? Re-ran three times now, but keep getting only 6 files and not 24. I made sure to run re-partition on the data frame. :thinking_face:\\n```df = spark.read \\\\\\n    .option(\"\"header\"\",\"\"true\"\") \\\\\\n    .schema(schema) \\\\\\n    .csv(\\'fhvhv_tripdata_2021-01.csv\\')\\n\\ndf.repartition(24)\\n\\ndf.write.parquet(\\'fhvhv/2021/01/\\')```\\n\",1645544316.294169,1645544316.294169,U02TNEJLC84\\n7186d61f-e656-40c7-a424-db072194f7d8,U02TNEJLC84,,,Perhaps it\\'s not worth spending time on that. I\\'m not sure I completely understand how exactly spark partitions works sometimes,1645544316.294169,1645544606.681749,U01AXE0P5M3\\nf922a7fd-44e5-42e7-9b91-09202da4388b,U02TNEJLC84,,,Oh but in your case you need to assign df to df.repartition,1645544316.294169,1645544657.488209,U01AXE0P5M3\\nda27d923-39c6-4345-9e84-91282e71c077,U02TNEJLC84,,,\"Because spark operations are lazy, you need to assign the result of repartititoning back to df\",1645544316.294169,1645544720.949449,U01AXE0P5M3\\nbc18cba7-8a3b-4db2-84ea-98cd6c742f0a,U02TNEJLC84,,,\"Didn\\'t take much time <@U01AXE0P5M3> :man-facepalming: Yup, needed to assign it to a new dataframe Shift + Tab on the reparation function showed \"\"RETURNS\"\" Found that after I ran df.rdd.getNumPartitions() and  saw that it was still 6. Thank you as always!\",1645544316.294169,1645544768.199839,U02TNEJLC84\\n6587edd9-b892-4938-b0dd-fcd3c4439dfd,U034BND3P2M,,,Please how did you fix it?,1645526853.296059,1645544887.222779,U03441WKFPC\\nc1b5ad4a-c8d2-4bfa-a8bf-dffb33a53ed8,U034BND3P2M,,,Currently experiencing same,1645526853.296059,1645544906.630639,U03441WKFPC\\n95d46c77-ad80-47f0-9dd7-8bc538420351,U034BND3P2M,,,<@U034BND3P2M>,1645526853.296059,1645544951.372579,U03441WKFPC\\n43c885db-a5fb-4922-a838-da9ed6327bae,,2.0,,\"In question 2 of week 5, is the size of the result the total size of the folder where we write the pq files? or something else?\",1645546455.270829,1645546455.270829,U02UA0EEHA8\\nd0020fb9-da56-4a48-89a1-adea93320c0a,U02UA0EEHA8,,,\"Yes, total size of the folder. I\\'ll update the description to make it clearer\",1645546455.270829,1645547542.032089,U01AXE0P5M3\\n4163429a-60f3-46b6-bfbb-87298bfac367,U02UA0EEHA8,,,Thanks :slightly_smiling_face:,1645546455.270829,1645547726.207079,U02UA0EEHA8\\n807B7C91-CDEF-4E3E-8BF9-EBC3360588BA,U02SEH4PPQB,,,The best way to change it into physical table while creating an extract if possible ,1645485417.180619,1645550925.399449,U03440149B3\\n69E6F8E9-2585-4D2D-B2A4-566C5CA7BCCC,U02U34YJ8C8,,,\"Thanks <@U02U5SW982W>. I got through it all okay, with a few minor issues thankfully. Are you still struggling?\",1645213874.310119,1645553202.730789,U02U34YJ8C8\\n09DB754B-34EB-49BF-AF62-F6F55108D308,,5.0,,Hi Everyone….my first day getting into the Zoomcamp. Please i need some help. After following the steps for installation on MacOs…When trying to execute spark-shell…i get an error message… ‘unable to locate a Java Runtime’… please how do I proceed?,1645553447.039289,1645553447.039289,U034HSVUBTK\\n5f4c6e05-99f0-479c-871f-5e95e8651f80,U034HSVUBTK,,,Did you follow the steps in <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_5_batch_processing/setup/macos.md> ?,1645553447.039289,1645553854.021779,U02BVP1QTQF\\n4C6734CD-9B41-47B4-96D3-60453E9DBB51,U034HSVUBTK,,,\"Hey. Yes, I did. Down to a Tee. It’s the last portion that’s tripping me up. Do you recommend i start the installation from the beginning?\",1645553447.039289,1645554000.230899,U034HSVUBTK\\n44c0a6c5-2632-4b8e-a1a8-3810b1f8bcee,U034HSVUBTK,,,You can also try the approach from Linux way but download the macos binary for JDK instead of Linux on the first step,1645553447.039289,1645555021.885149,U01AXE0P5M3\\nA43102FC-FBB6-4D81-B7B8-E7F84D815E23,U034HSVUBTK,,,<@U01AXE0P5M3> Okay…will try this. Thanks ,1645553447.039289,1645555319.222789,U034HSVUBTK\\nfaad2c27-73a9-41ac-a510-00b84cfa73c3,U02QK4ZV4UX,,,\"Hi <@U02QK4ZV4UX>, I think <@U03456UBF18> fix will work for most? But no go for me here - and after 3 hrs trying to chase the dbt Cloud fairies with dbt helpdesk I\\'m not sure I\\'m going to find a fix in dbt Cloud. I *think* it really is very much location dependent. dbt Cloud guys are saying that it\\'s an issue with BigQuery and how they set things up. I\\'ve posted the detail about this <https://learningdataengineering540969211.wordpress.com/dbt-cloud-and-bigquery-an-effort-to-try-and-resolve-location-issues/|here> but - long story short - I think it might turn into one of those \\'it\\'s not my problem it\\'s your\\'s\\' type of deal. I just did a simple work around setting things to the US and it works but it would be just nice to know why there\\'s a problem when you\\'ve got a dataset based in Australia...\",1645507617.625979,1645559333.124839,U02U5SW982W\\n81bcc47c-4350-4cba-ba7f-cac9fa8ff408,U02U6DR551B,,,Thanks for the heads up <@U0290EYCA7Q> - and no unfortunately that does not work - not as simple as that.,1645343764.071639,1645559564.553319,U02U5SW982W\\nce4e06b5-c96b-466c-92b1-9bd7c0b6ee5a,U02U34YJ8C8,,,\"Hi <@U02U34YJ8C8> good to hear. All is well here - I did end up getting through it. However, I need to start getting stuck into week 5. Glad to hear you only had a few minor issues. Have you started with week 5?\",1645213874.310119,1645559897.042569,U02U5SW982W\\n5e258d1d-c1b9-4638-858b-29a867822502,,14.0,,dbt Cloud alternatives?,1645560296.440029,1645560296.440029,U02U5SW982W\\ne54de7a2-76ff-46e4-a20b-831c79d3a2d5,U02U5SW982W,,,\"HI All, am just curious to hear what alternatives there might be to dbt Cloud that people have experience with. dbt Cloud seems great but wondering about others \\'lived experience\\' I guess. Any suggestions greatly welcome...\",1645560296.440029,1645560388.076969,U02U5SW982W\\n53cc429d-0122-42b3-b224-16fb7d1657e2,U02QK4ZV4UX,,,\"<@U02U5SW982W> this is indeed a problem with BQ settings, if you were trying to execute the same query (create table us_schema.table from select aus_schema.table) it would fail. It\\'s also extremely frustrating how US has so many regions and they all qualify as different regions, therefore the issue again. The same goes with EU and even Germany has like 3 regions.\\nI admit that the settings of the location, and by default using US are not super clear in dbt cloud and I expect this to be improved, but it\\'s an underlying issue of the BQ configuration.\\nI\\'ve been using dbt with postgress, redshift and Snowflake and never had such issue.\\nI had the same issue at the very beggining of setting up this project and once I created my schemas in the same locations as I had trips_data_all, as I suggested in threads above, it worked just fine for me.\",1645507617.625979,1645561578.137259,U01B6TH1LRL\\nbe0e4ddb-9929-433c-bc45-41b1693e064e,U02U5SW982W,,,\"You can always use dbt locally with an IDE of your choice and limit dbt cloud for production only. From your messages I take your main issues are around the IDE.\\nThis would be alternative B videos, there I use postgres, but you can connect it to BQ as well (or any other database)\\nOnce you deploy, after merge the PR you will be able to run that code in production from the cloud.\\nIf you don\\'t want to use the cloud at all, there was a link that we shared in the office hours last week where I explained about using airflow. But there are many more in the official documentation.\",1645560296.440029,1645561872.014499,U01B6TH1LRL\\nad8d40bf-31fb-405f-91d9-a67aee429342,,4.0,,\"[WEEK 4] An alternative way of uploading green and yellow data to GCS by specifying the schema using pyarrow. Essentially, I made a few tweaks to `web_to_gcs.py` and reused some of the previous weeks code so that 1) URLs are properly parsed and 2) Variables have the right type. Hope it is of some help <https://github.com/sebastian2296/data-engineering-zoomcamp/blob/main/week_4_analytics_engineering/web_to_gcs.py>\",1645562995.282099,1645562995.282099,U02SUUT290F\\n18812049-7d10-40b3-8107-d21dba3c3afb,U02U5SW982W,,,Excellent - thanks <@U01B6TH1LRL>,1645560296.440029,1645563749.114919,U02U5SW982W\\n6d4362b2-85d9-4661-ac67-dcbd8126e7ec,U02U5SW982W,,,I also just checked the slides again for this week and DataForm is also here. So I assume that this is an alternative to dbt Cloud? It does similar things?,1645560296.440029,1645564809.554179,U02U5SW982W\\n2402522d-806c-4e6b-b5c4-cfbb50bd2798,U02U5SW982W,,,\"Yes, it allows you to use SQL to build your transformations. But I\\'m not sure if it\\'s open source, plus it\\'s heavily oriented to be used with BQ while dbt is more \"\"database agnostic\"\"\",1645560296.440029,1645565305.471089,U01B6TH1LRL\\n642a0881-e601-4491-86e8-d906a75b8065,U02U5SW982W,,,(dataform was acquired by Google),1645560296.440029,1645565314.512909,U01B6TH1LRL\\n9ac25a3b-db2e-4a28-92d4-919a286761dd,U02U5SW982W,,,\"I wasn’t comfortable working on the dbt Cloud IDE because I wasn’t comfortable managing branches after commiting just to fix small things, so what I ended up doing was editing the files locally, commit and push the changes and then pull the changes to dbt Cloud and run everything there. The downside is that I got a lot more commits for every small fix in the files.\\n\\nI still need to test how dbt Core works with BigQuery.\",1645560296.440029,1645565499.357959,U02BVP1QTQF\\nbfceeea5-721b-480e-92c9-e78b6c07ec2c,U02T9JQAX9N,,,\"Did you guys manage to resolve it? I can reproduce the error, fact_trips passes with `view` in the config macro, results in the above error for the `table` argument. No difference between `dbt run` and `dbt build` here, neither it\\'s an issue of regions. What permissions are we talking about exactly?\",1645310364.645229,1645567020.816329,U02CGKRHC9E\\nb8aa2994-7d55-46e8-a457-66d765f9b508,U02U5SW982W,,,<@U01B6TH1LRL> - yes I was wondering about something that was more Open Source but really have no idea what\\'s out there... <@U02BVP1QTQF> being uncomfortable seems to be my standard operating procedure lately :wink: . I\\'m feeling like I\\'m on a bullet train in this course - it is a veritable \\'tour-de-force\\'. So much here to come back to and try to get a better handle on. But it is week 5 already and I have to get back on that bullet train :slightly_smiling_face: . Great to hear your experience of it though - please keep sharing...,1645560296.440029,1645572535.719329,U02U5SW982W\\n0d33502f-3aff-44ea-b24f-6693def139d0,U02SUUT290F,,,\"I got this error when i ran the script\\n\\ngoogle.api_core.exceptions.Forbidden: 403 POST <https://storage.googleapis.com/upload/storage/v1/b/dtc_data_lake_bright-task-339417/o?uploadType=resumable>: {\\n  \"\"error\"\": {\\n    \"\"code\"\": 403,\\n    \"\"message\"\": \"\"Provided scope(s) are not authorized\"\",\\n    \"\"errors\"\": [\\n      {\\n        \"\"message\"\": \"\"Provided scope(s) are not authorized\"\",\\n        \"\"domain\"\": \"\"global\"\",\\n        \"\"reason\"\": \"\"forbidden\"\"\\n      }\\n    ]\\n  }\\n}\\n: (\\'Request failed with status code\\', 403, \\'Expected one of\\', &lt;HTTPStatus.OK: 200&gt;, &lt;HTTPStatus.CREATED: 201&gt;)\",1645562995.282099,1645578463.926399,U02UKLHDWMQ\\n4b5267ad-d4c7-44b7-82bd-189c2c79cc11,U034HSVUBTK,,,\"Can you try this?\\n\\nbrew tap adoptopenjdk/openjdk\\nbrew install --cask adoptopenjdk11\\n\\nThis should install Java under /Library/Java/JavaVirtualMachines/adoptopenjdk-11.jdk/Contents/Home\",1645553447.039289,1645579101.233839,U0290EYCA7Q\\n4b7642b0-c39b-4895-bcb3-392dbf60975f,U02U5SW982W,,,\"Try Holistic.\\n<https://www.holistics.io/|https://www.holistics.io/>\",1645560296.440029,1645579446.845539,U0290EYCA7Q\\n68bb9c1a-2972-4c28-b396-9005584abf2f,U02SUUT290F,,,\"Hey <@U02UKLHDWMQ>, check error 403 on this list: <https://cloud.google.com/storage/docs/xml-api/reference-status#standardcodes>\\n\\nLooks like your code may be missing a way for GCloud to access your credentials file. <https://pastebin.com/fw26zASi|Check my code snippet here to see how I access my local credentials> - place that just after the BUCKET variable definition in your code. Let me know if that works for you.\",1645562995.282099,1645582889.356039,U02TXLWFG86\\n187bb981-9eb1-4bf8-94a5-bcc6f67539b4,,,,\"Hi everyone, in dbt cloud, after connecting to a GitHub profile, how do we preview/compile a specific folder (e.g. taxi_rides_ny in week_4_analytics_engineering)?\\nWhen I click on the \"\"initialize your project\"\" button, it does not seem to activate the taxi_rides_ny folder for dbt cloud compilation.\",,1645583901.139929,U02BRPZKV6J\\na96640bb-07bb-467a-93e2-166df8256f51,U02SUUT290F,,,Thanks it worked. Can i use the dag to create table and partition in BigQuery. Maybe i will just remove the first part.,1645562995.282099,1645586365.161999,U02UKLHDWMQ\\nFDEB39BF-DBD8-49C1-9708-F483F99495F4,,13.0,,\"Anyone investigated why we all got different answers for Q1 Week 4?\\n\\nWould be good to know what caused this. \",1645587287.934269,1645587287.934269,U02U34YJ8C8\\n47e52b8f-d3a3-403d-8b3e-aa01a2e11820,U030FNZC26L,,,thanks <@U0290EYCA7Q>. It\\'s really nice :slightly_smiling_face:,1644697506.408849,1645588381.891159,U030FNZC26L\\nC819A717-8618-4D5A-84B2-6C1C21B54B2B,U02U34YJ8C8,,,\"I am also investigating why the count is not matching. \\nIs there error in the data or the query.\",1645587287.934269,1645588620.675119,U02U6DR551B\\n9e7e1540-cd58-4b9e-b4b9-6b803b571f48,U02S83KSX3L,,,I searched for additional video tutorials on YouTube in addition to the product tutorials. Some of them are very good and concise.,1645478349.390589,1645595429.180069,U02UUU3PUDA\\n4318cdca-423e-4a61-baad-9a788beed600,U02U34YJ8C8,,,\"HI <@U02U34YJ8C8>, <@U02U6DR551B> and all. Wasn\\'t there a stage when we were trying to feed the data into BigQuery that we were told there would be some problems with the files(and there were)  and it was up to us how we dealt with it? I can\\'t remember exactly off the top of my head but some of us were doing parquet and some were doing csv with some cleaning. I thought there were files that simply weren\\'t loading - was it January? Possibly this might have been cause for variation?\",1645587287.934269,1645599837.653969,U02U5SW982W\\n065f1123-f4c3-411f-a84b-4d32e2eab20e,U02U5SW982W,,,Excellent - thanks for the tip <@U0290EYCA7Q>,1645560296.440029,1645599882.899659,U02U5SW982W\\n0132d1c9-675d-4e0b-b67d-bf6993b37eb6,,,,\"I found an interesting insight. We are partitioning on \"\"vendorid,\\xa0lpep_pickup_datetime\"\" columns and they contain one valid Pickup location ID and one invalid. This is the reason that we are getting inconsistent counts in facts trips table. Example case can be seen by executing the query below:\\n`select\\xa0VendorID,lpep_pickup_datetime,PULocationID\\xa0from\\xa0(`\\n`select\\xa0*\\xa0from\\xa0\\xa0`zoomcamp-de-339322.trips_data_all.green_trips_external_table``\\n`where\\xa0VendorID=2\\xa0and\\xa0lpep_pickup_datetime\\xa0=\\'2020-12-01\\xa012:38:05\\xa0UTC\\')h`\\n265 and 264 refers to borough \\'Unknown\\' in dim zones table.\\nI am sure the same issue exists with drop off location ID and in external yellow trips table.\\n\\n\\nI think this will be the accurate query for the facts trips table.\\n`select\\xa0count(*)\\xa0from\\xa0(`\\n`select\\xa0\\xa0distinct\\xa0VendorID,lpep_pickup_datetime\\xa0from\\xa0(`\\n`select\\xa0*\\xa0from\\xa0\\xa0`zoomcamp-de-339322.trips_data_all.green_trips_external_table``\\n`where\\xa0extract(year\\xa0from\\xa0lpep_pickup_datetime)=2019\\xa0or\\xa0extract(year\\xa0from\\xa0lpep_pickup_datetime)=2020`\\n`and\\xa0PULocationID\\xa0not\\xa0in\\xa0(265,264)\\xa0and\\xa0dolocationid\\xa0not\\xa0in\\xa0(265,264)\\xa0and\\xa0VendorID\\xa0is\\xa0not\\xa0null)a`\\n`UNION\\xa0ALL`\\xa0\\n`select\\xa0distinct\\xa0vendorid,\\xa0tpep_pickup_datetime\\xa0from\\xa0(`\\n`select\\xa0*\\xa0from\\xa0\\xa0`zoomcamp-de-339322.trips_data_all.yellow_trips_external_table``\\n`where\\xa0extract(year\\xa0from\\xa0tpep_pickup_datetime)=2019\\xa0or\\xa0extract(year\\xa0from\\xa0tpep_pickup_datetime)=2020`\\n`and\\xa0PULocationID\\xa0not\\xa0in\\xa0(265,264)\\xa0and\\xa0dolocationid\\xa0not\\xa0in\\xa0(265,264)\\xa0and\\xa0VendorID\\xa0is\\xa0not\\xa0null)b)c`\\n\\n\\n\\n<@U01AXE0P5M3> \\nI think question 1 should be removed from the homework as none of the options are correct. Let me know what you think!\",,1645601254.310759,U02U6DR551B\\n5f8a82ab-e9fb-4872-aac5-7de3491481d5,,,,,,1645601267.070939,U02U6DR551B\\ndb768810-7502-469c-b961-636754f7c16e,U02U5SW982W,,,\"Since there is Pyspark, if you already use python doesn\\'t mean you should go with Dask\",1645937133.963009,1645954390.460529,U0308MF3KUH\\n27d46912-99d2-4f18-ba5c-f97c947b4a05,,8.0,,\"Hi. I have a warn when I run df.write.parquet. Then I have 24 files in my folder and _SUCCESS file, but I have smaller size of my fles then Alexey. Is this a problem or not?\",1645954905.103999,1645954905.103999,U02QL1EG0LV\\nc0431efe-8fb6-44a2-a3a7-f0cef8726b47,U02QL1EG0LV,,,\"```22/02/27 12:15:13 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 95,00% for 8 writers\\n22/02/27 12:15:13 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 84,44% for 9 writers\\n22/02/27 12:15:13 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 76,00% for 10 writers\\n22/02/27 12:15:13 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 69,09% for 11 writers\\n22/02/27 12:15:13 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 63,33% for 12 writers\\n22/02/27 12:15:14 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 69,09% for 11 writers\\n22/02/27 12:15:14 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 76,00% for 10 writers\\n22/02/27 12:15:14 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 84,44% for 9 writers\\n22/02/27 12:15:14 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 95,00% for 8 writers\\n22/02/27 12:15:14 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 84,44% for 9 writers\\n22/02/27 12:15:14 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 76,00% for 10 writers\\n22/02/27 12:15:14 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 69,09% for 11 writers\\n22/02/27 12:15:14 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 76,00% for 10 writers\\n22/02/27 12:15:14 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 84,44% for 9 writers\\n22/02/27 12:15:14 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 95,00% for 8 writers\\n22/02/27 12:15:14 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 84,44% for 9 writers\\n22/02/27 12:15:14 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 76,00% for 10 writers\\n22/02/27 12:15:14 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 69,09% for 11 writers\\n22/02/27 12:15:14 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 63,33% for 12 writers\\n22/02/27 12:15:14 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 69,09% for 11 writers\\n22/02/27 12:15:14 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 76,00% for 10 writers\\n22/02/27 12:15:14 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 69,09% for 11 writers\\n22/02/27 12:15:14 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 63,33% for 12 writers\\n22/02/27 12:15:14 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 69,09% for 11 writers\\n22/02/27 12:15:14 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 76,00% for 10 writers\\n22/02/27 12:15:14 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 84,44% for 9 writers\\n22/02/27 12:15:14 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 76,00% for 10 writers\\n22/02/27 12:15:14 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 69,09% for 11 writers\\n22/02/27 12:15:14 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 63,33% for 12 writers\\n22/02/27 12:15:15 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 69,09% for 11 writers\\n22/02/27 12:15:15 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 76,00% for 10 writers\\n22/02/27 12:15:15 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 84,44% for 9 writers\\n22/02/27 12:15:15 WARN MemoryManager: Total allocation exceeds 95,00% (1 020 054 720 bytes) of heap memory\\nScaling row group sizes to 95,00% for 8 writers```\\n\",1645954905.103999,1645954950.332689,U02QL1EG0LV\\n16dd07cc-422d-4402-bfb6-ca9bd62ca3a4,,4.0,,\"Hi :wave:\\nI tried running notebooks from Jupyter lab but Spark Master UI doesn’t open (the page froze in a loading state).\\nDoes anyone know to fix it or has tried doing the same?\",1645967412.188089,1645967412.188089,U02R6QGM3R6\\n7932bb7c-92df-4379-84ce-dbc17b047741,U02UM74ESE5,,,\"Hi <@U02UM74ESE5>, could you create a pull request that add the steps you mentioned in the setup/macos.md file. That would be permanent and helpful for people taking course later as well\",1645917095.457459,1645967947.498879,U02DD97G6D6\\n3c335a33-b912-4c13-bb99-096665e418b2,U02QL1EG0LV,,,are you sure it\\'s the same files? maybe it\\'s not high-volume FHV?,1645954905.103999,1645971062.924259,U01AXE0P5M3\\n43d9d63d-1ffe-4b78-be9b-2eef96c751b2,U02R6QGM3R6,,,\"does it work from usual jupyter? it should start a master UI the moment you create spark session with local master, regardless of whether you start it from jupyter, spark-shell, ipython or jupyter lab\",1645967412.188089,1645971114.419349,U01AXE0P5M3\\nbd0a4ddd-fbdd-4a4a-8b11-b409f03b3a6a,U02RSAE2M4P,,,do the authentication step in week 1,1645126441.317339,1645971429.348269,U02TA6MJZHR\\n55bf0d66-754f-4b82-b225-386ba1e63f09,U02UM74ESE5,,,yes please create a PR,1645917095.457459,1645971651.708709,U01AXE0P5M3\\n24c2d399-1a08-427a-9977-1569d7e381a3,,,,\"Hi, everyone! I am starting now from week 1 (I was busy catching up with python as a student over at Frauenloop. And I have a big hello to add in here from me and from Shruti Srivastava!\",,1645974221.920929,U02CCSM8WSY\\nf16e041e-a789-4f75-8eb9-33f4d150fee4,U02QL1EG0LV,,,Yes. I have the same number of rows :slightly_smiling_face:. But I also have this warn. And that didn\\'t stop me from doing my homework,1645954905.103999,1645976556.461149,U02QL1EG0LV\\n664c46d7-a9eb-4e51-8e02-21fd36970298,U02U3E6HVNC,,,I found my query result as an option in the form,1645828558.420299,1645981136.867519,U0308MF3KUH\\n561d625a-b290-4b0c-8f48-f40dbe263bf5,U02QL1EG0LV,,,And you use the same spark version? And you use the same schema?,1645954905.103999,1645983821.916609,U01AXE0P5M3\\neb96fa78-bfcd-4655-be12-5d4c08a31490,U02QL1EG0LV,,,It shouldn\\'t be a problem but I\\'m a bit worried because one of the homework questions is to check the size of the folder with results,1645954905.103999,1645983868.891269,U01AXE0P5M3\\n80d78510-3f40-45bb-be95-b2b383b4350c,,1.0,,Hello! Has anybody started HW5? It is clear that the suggested file (fhvhv feb 2021) is more than the maximum size suggested in any answer. The same for the records count in the third question. Is it ok or the HW5 is not yet finalized?,1645984012.657129,1645984012.657129,U02T65GT78W\\nd4869ffb-964c-4398-ae8a-aa0b569e7e77,U02R6QGM3R6,,,\"I thought the same thing, but it just didn\\'t work with lab for some reason, but did work with notebook.\",1645967412.188089,1645984348.479679,U02R6QGM3R6\\nb4cb04cb-8e83-4337-beca-0a5b200ea1e1,U02T65GT78W,,,\"Ah, I see. It is not mentioned that file size should be shown after writing to parquet, and I guess that record count is also for specific date. Understood when combined with homework.md file\",1645984012.657129,1645984723.427249,U02T65GT78W\\n53B32A44-2EFF-4E48-B83C-B75A262F499D,U02UM74ESE5,,,\"I have never done a PR before :sweat_smile:, but will give it a try!!\",1645917095.457459,1645987092.118639,U02UM74ESE5\\n9b4403eb-032f-48c4-a2bf-ea86e1786376,U02U5SW982W,,,\"I came here today to ask the same thing. Does anyone have experience choosing between pyspark and dask? I read the same page <@U02U5SW982W> quoted from the other day and was surprised to see the last point saying\\n&gt; If you are looking to manage a terabyte or less of tabular CSV or JSON data, then you should forget both Spark and Dask and use <https://www.postgresql.org/|Postgres> or <https://www.mongodb.org/|MongoDB>.\\nWould the instructors agree with that? We are using much less than that in our examples. I understand it\\'s for demonstration purposes, but I\\'m trying to get an idea of when to apply each of these technologies (SQL vs dbt vs spark/dask) in real world applications.\",1645937133.963009,1645989643.521049,U02T9GHG20J\\n29b48fb0-b6ad-4fc3-b51a-aeecc2780315,U02QL1EG0LV,,,\"I reproduced all the commands from the <https://www.youtube.com/watch?v=r_Sf6fCB40c&amp;list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&amp;index=50|video> on Windows and also got a slightly different results: 8.9 Mb for each file and 212 Mb in total. I guess, it\\'s ok.\",1645954905.103999,1645994380.559479,U02DNSC6Z52\\n84c7ae69-4b77-45d1-8abd-f9adea79bcdc,,5.0,,\"Hi Getting below error will dockerizing the ingestion script    .  can someone help.\\n```(base) Javeed@de-javeedzoomcamp:~/de/w1/1_docker_sql$ docker run -it --network=pg-network taxi_ingest:v001 --user=root --password=root --host=pg-database --port=5432 --db=ny_taxi --table_name=yellow_taxi_data --url=${URL}\\n--2022-02-27 21:03:02--  <https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv>\\nResolving <http://s3.amazonaws.com|s3.amazonaws.com> (<http://s3.amazonaws.com|s3.amazonaws.com>)... 52.217.129.72\\nConnecting to <http://s3.amazonaws.com|s3.amazonaws.com> (<http://s3.amazonaws.com|s3.amazonaws.com>)|52.217.129.72|:443... connected.\\nHTTP request sent, awaiting response... 200 OK\\nLength: 125981363 (120M) [text/csv]\\nSaving to: \\'output.csv\\'\\n\\noutput.csv                                                          100%[==================================================================================================================================================================&gt;] 120.14M  33.3MB/s    in 4.2s\\n\\n2022-02-27 21:03:07 (28.7 MB/s) - \\'output.csv\\' saved [125981363/125981363]\\n\\nTraceback (most recent call last):\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py\"\", line 3250, in _wrap_pool_connect\\n    return fn()\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py\"\", line 310, in connect\\n    return _ConnectionFairy._checkout(self)\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py\"\", line 868, in _checkout\\n    fairy = _ConnectionRecord.checkout(pool)\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py\"\", line 476, in checkout\\n    rec = pool._do_get()\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/impl.py\"\", line 146, in _do_get\\n    self._dec_overflow()\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py\"\", line 70, in __exit__\\n    compat.raise_(\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py\"\", line 207, in raise_\\n    raise exception\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/impl.py\"\", line 143, in _do_get\\n    return self._create_connection()\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py\"\", line 256, in _create_connection\\n    return _ConnectionRecord(self)\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py\"\", line 371, in __init__\\n    self.__connect()\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py\"\", line 666, in __connect\\n    pool.logger.debug(\"\"Error on connect(): %s\"\", e)\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py\"\", line 70, in __exit__\\n    compat.raise_(\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/util/compat.py\"\", line 207, in raise_\\n    raise exception\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/pool/base.py\"\", line 661, in __connect\\n    self.dbapi_connection = connection = pool._invoke_creator(self)\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/create.py\"\", line 590, in connect\\n    return dialect.connect(*cargs, **cparams)\\n  File \"\"/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/default.py\"\", line 597, in connect\\n    return self.dbapi.connect(*cargs, **cparams)\\n  File \"\"/usr/local/lib/python3.9/site-packages/psycopg2/__init__.py\"\", line 122, in connect\\n    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)\\npsycopg2.OperationalError: FATAL:  could not open file \"\"base/16384/2601\"\": Permission denied```\",1645996032.402719,1645996032.402719,U033WHDKP0T\\ne430bbf6-c92c-4f75-b6fa-e577cda4b467,,4.0,,\"Hello all, my jupyter notebook is not functioning properly on the gcp vm, any other person with this same issue.\",1645996093.443159,1645996093.443159,U02T0CYNNP2\\n7deedef4-37f8-41c0-97eb-e60c96d676bc,U02T0CYNNP2,,,can you share more details?,1645996093.443159,1645997605.421809,U01AXE0P5M3\\n5472eae2-8b50-4bb1-8bf3-70aec340831f,U033WHDKP0T,,,\"I did a quick search in google and it seems the database is corrupted. maybe you can start the database container again, deleting previously created volume?\",1645996032.402719,1645997740.305659,U01AXE0P5M3\\n4b52b0d2-085e-447c-81a2-0e538e564489,U033WHDKP0T,,,\"For your next posts, please put logs inside a thread\",1645996032.402719,1645997760.050249,U01AXE0P5M3\\n542371a0-cad4-4ce2-9617-de43a46e6457,,3.0,,\"Hello, while following week6 video 6.3.1-Kafka Streams, when running the stream.py file, im getting the following error. Has anyone encountered this before?\\n`python3 streams/stream.py`      \\n`Traceback (most recent call last):`\\n  `File \"\"streams/stream.py\"\", line 6, in &lt;module&gt;`\\n    `topic = app.topic(\"\"datatalkclub.yellow_taxi_ride.json\"\", value_type=TaxiRide)`\\n  `File \"\"/Users/marcelanis/code/data-engineering-zoomcamp/week-6/week6_venv/lib/python3.7/site-packages/faust/app/base.py\"\", line 781, in topic`\\n    `return self.conf.Topic(`\\n  `File \"\"/Users/marcelanis/code/data-engineering-zoomcamp/week-6/week6_venv/lib/python3.7/site-packages/faust/app/base.py\"\", line 1796, in conf`\\n    `self._configure()`\\n  `File \"\"/Users/marcelanis/code/data-engineering-zoomcamp/week-6/week6_venv/lib/python3.7/site-packages/faust/app/base.py\"\", line 1740, in _configure`\\n    `conf = self._load_settings(silent=silent)`\\n  `File \"\"/Users/marcelanis/code/data-engineering-zoomcamp/week-6/week6_venv/lib/python3.7/site-packages/faust/app/base.py\"\", line 1752, in _load_settings`\\n    `return self.Settings(appid, **self._prepare_compat_settings(conf))`\\n  `File \"\"/Users/marcelanis/code/data-engineering-zoomcamp/week-6/week6_venv/lib/python3.7/site-packages/faust/types/settings.py\"\", line 768, in __init__`\\n    `self.Worker = cast(Type[_WorkerT], Worker or WORKER_TYPE)`\\n  `File \"\"/Users/marcelanis/code/data-engineering-zoomcamp/week-6/week6_venv/lib/python3.7/site-packages/faust/types/settings.py\"\", line 796, in __setattr__`\\n    `object.__setattr__(self, key, value)`\\n  `File \"\"/Users/marcelanis/code/data-engineering-zoomcamp/week-6/week6_venv/lib/python3.7/site-packages/faust/types/settings.py\"\", line 1204, in Worker`\\n    `self._Worker = symbol_by_name(Worker)`\\n  `File \"\"/Users/marcelanis/code/data-engineering-zoomcamp/week-6/week6_venv/lib/python3.7/site-packages/mode/utils/imports.py\"\", line 269, in symbol_by_name`\\n    `**kwargs)`\\n  `File \"\"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/__init__.py\"\", line 127, in import_module`\\n    `return _bootstrap._gcd_import(name[level:], package, level)`\\n  `File \"\"&lt;frozen importlib._bootstrap&gt;\"\", line 1006, in _gcd_import`\\n  `File \"\"&lt;frozen importlib._bootstrap&gt;\"\", line 983, in _find_and_load`\\n  `File \"\"&lt;frozen importlib._bootstrap&gt;\"\", line 967, in _find_and_load_unlocked`\\n  `File \"\"&lt;frozen importlib._bootstrap&gt;\"\", line 677, in _load_unlocked`\\n  `File \"\"&lt;frozen importlib._bootstrap_external&gt;\"\", line 728, in exec_module`\\n  `File \"\"&lt;frozen importlib._bootstrap&gt;\"\", line 219, in _call_with_frames_removed`\\n  `File \"\"/Users/marcelanis/code/data-engineering-zoomcamp/week-6/week6_venv/lib/python3.7/site-packages/faust/worker.py\"\", line 19, in &lt;module&gt;`\\n    `from aiokafka.structs import TopicPartition`\\n  `File \"\"/Users/marcelanis/code/data-engineering-zoomcamp/week-6/week6_venv/lib/python3.7/site-packages/aiokafka/__init__.py\"\", line 4, in &lt;module&gt;`\\n    `from .client import AIOKafkaClient`\\n  `File \"\"/Users/marcelanis/code/data-engineering-zoomcamp/week-6/week6_venv/lib/python3.7/site-packages/aiokafka/client.py\"\", line 10, in &lt;module&gt;`\\n    `import aiokafka.errors as Errors`\\n  `File \"\"/Users/marcelanis/code/data-engineering-zoomcamp/week-6/week6_venv/lib/python3.7/site-packages/aiokafka/errors.py\"\", line 4, in &lt;module&gt;`\\n    `from kafka.errors import (`\\n`ImportError: cannot import name \\'ConnectionError\\' from \\'kafka.errors\\' (/Users/marcelanis/code/data-engineering-zoomcamp/week-6/week6_venv/lib/python3.7/site-packages/kafka/errors.py)`\",1645999629.113869,1645999629.113869,U02V9V9NLJG\\nd7800c07-f9fa-4393-acf1-f7956c4cfc36,U02V9V9NLJG,,,please put the log in the thread and use ``` for code blocks,1645999629.113869,1645999693.752309,U01AXE0P5M3\\n59068fc8-74af-4171-bf80-d1452d2d6969,U033WHDKP0T,,,\"thanks alexey . that worked.\\nWhat if this happens in a real project are there tools to recover the corrupt database.\",1645996032.402719,1646005945.810329,U033WHDKP0T\\n6d4b2132-f5b3-44f5-9be1-4ad136e6d29d,,3.0,,has anyone figured out how to read from GCP data lake instead of downloading all the taxi data again?,1646008578.136059,1646008578.136059,U02UAHJHJ20\\nff22bc5c-4ed5-457c-85ad-9dfe4cb901a2,,,thread_broadcast,\"nevermind I figured it out. There’s a few extra steps to go into reading from GCS with PySpark\\n\\n1.)  *IMPORTANT:* Download the Cloud Storage connector for Hadoop here: <https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#clusters>\\nAs the name implies, this `.jar` file is what essentially connects PySpark with your GCS\\n\\n2.) Move the `.jar` file to your Spark file directory. I installed Spark using homebrew on my MacOS machine and I had to create a `/jars` directory under `\"\"/opt/homebrew/Cellar/apache-spark/3.2.1/` (where my spark dir is located)\\n\\n3.) In your Python script, there are a few extra classes you’ll have to import:\\n```import pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.conf import SparkConf\\nfrom pyspark.context import SparkContext```\\n4.) You must set up your configurations before building your SparkSession. Here’s my code snippet:\\n```conf = SparkConf() \\\\\\n    .setMaster(\\'local[*]\\') \\\\\\n    .setAppName(\\'test\\') \\\\\\n    .set(\"\"spark.jars\"\", \"\"/opt/homebrew/Cellar/apache-spark/3.2.1/jars/gcs-connector-hadoop3-latest.jar\"\") \\\\\\n    .set(\"\"spark.hadoop.google.cloud.auth.service.account.enable\"\", \"\"true\"\") \\\\\\n    .set(\"\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\"\", \"\"path/to/google_credentials.json\"\")\\n\\nsc = SparkContext(conf=conf)\\n\\nsc._jsc.hadoopConfiguration().set(\"\"fs.AbstractFileSystem.gs.impl\"\",  \"\"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\"\")\\nsc._jsc.hadoopConfiguration().set(\"\"fs.gs.impl\"\", \"\"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\"\")\\nsc._jsc.hadoopConfiguration().set(\"\"fs.gs.auth.service.account.json.keyfile\"\", \"\"path/to/google_credentials.json\"\")\\nsc._jsc.hadoopConfiguration().set(\"\"fs.gs.auth.service.account.enable\"\", \"\"true\"\")```\\n5.) Once you run that, build your `SparkSession` with the new parameters we’d just instantiated in the previous step:\\n```spark = SparkSession.builder \\\\\\n    .config(conf=sc.getConf()) \\\\\\n    .getOrCreate()```\\n6.) Finally, you’re able to read your files straight from GCS!\\n```df_green = spark.read.parquet(\"\"gs://{BUCKET}/green/202*/\"\")```\",1646008578.136059,1646013709.648279,U02UAHJHJ20\\n60ac7a7d-6605-4405-a95b-f2293641c54d,,18.0,,\"None of my answers for Q2/Q3 match no matter what I do, compress, not compress, csv, parquet. Close but not the same. Do I have the right file? hvfhv_tripdata_2021-02.csv? I\\'m just downloading it from the browser to my computer, not using wget.\",1646014250.535359,1646014250.535359,U02UBQJBYHZ\\n7cb547db-9e66-4302-a03b-14d903136869,U02UBQJBYHZ,,,So for some reason I wasn\\'t getting the 24 partitions. I ran it again assigning df = df.partition(24) and got 24 partitions. The size of the folder still does not match any of the answers.,1646014250.535359,1646015485.464459,U02UBQJBYHZ\\n4d8fb645-a391-4059-a3a4-cf4339f2baf2,U02UAHJHJ20,,,Good job <@U02UAHJHJ20>. Thanks for forwarding this on.,1646008578.136059,1646018621.047909,U02U5SW982W\\na0e2afad-37ef-4fe2-b6f8-407bba968063,U02UBQJBYHZ,,,Hi <@U02UBQJBYHZ> and you\\'re reading this from the Spark Master UI?,1646014250.535359,1646019219.974689,U02U5SW982W\\n5ad347be-e739-4314-af48-3900042d11c9,U02UBQJBYHZ,,,Have you tried using wget? As a bash command in the Jupyter Notebook? Just to see if there is any difference.,1646014250.535359,1646019283.836819,U02U5SW982W\\nc5f879d0-6fc1-48fa-a3dc-7eed1fb69b82,U02U5SW982W,,,That\\'s where I\\'m at <@U02T9GHG20J>. I\\'m using Dask at the moment because I don\\'t have a huge set-up to deal with. I still have to deal with pretty large sizes and of course pandas is not going to cut it for that. Dask helps me solve that problem - it\\'s quick and lightweight and does what I need it to do right now. I\\'m just wondering - are we using Spark because it\\'s like a full production thing (the ecosystem for it is very well developed) and it has more functionality than Dask for these types of situations? Situations that we aren\\'t quite fully aware of at this point in time? This is my stab in the dark for an answer ...,1645937133.963009,1646019938.415859,U02U5SW982W\\nfdc6cfc0-d909-4cad-9ea3-9ef38d0ee95e,U02UBQJBYHZ,,,Hi <@U02UBQJBYHZ> I have gone to the NY Taxi data site and downloaded directly from the site just to check. My download here was (when it finally came through - which takes ages for me) was 733.38 MB when I look at this file \\'fhvhv_tripdata_2021-02.csv\\' visually through my Ubuntu GUI for files and folders. When I ask about the same information from my terminal it says 700 MB. But of course what you are really looking for here is the output size of the parquet files from the job you do when writing this csv file to parquet. Look at your Spark Master UI and it should give you a much smaller sized file.,1646014250.535359,1646020388.598079,U02U5SW982W\\nba855cb1-d637-4fb4-8755-4547081480ec,U02UBQJBYHZ,,,The Master UI should look like this (the one I show here is a different job). This is what you should be looking at to get the size of the folder for all the parquet files (note that in Stage 25 there is an input of 425.3 MiB but then in Stage 27 there is an output of 14.4 MiB). It is this Output that you should be looking at to answer this question I believe:,1646014250.535359,1646020558.475729,U02U5SW982W\\n9fa34381-6eff-4c19-93f6-8b755e88ecd2,U02UBQJBYHZ,,,\"<@U02UBQJBYHZ> <@U02U5SW982W> I was going to say i have different size of the partitioned files after doing `ls -lh` on my parquet directory and showed to be around 430MB?? But looking at the output in the Master UI does show something different that’s a little bit closer to one of the answers for #2 for me. Still not the same exact answer but close enough\\n\\nFor question #3, I do get the same answer even though my answer for question #2 is off. Did you try to download the hvfhs file locally using wget, and are reading directly from the csv file for your queries?\",1646014250.535359,1646022815.689709,U02UAHJHJ20\\n5c099b71-93e3-457f-96c1-ff2a6d1ec31f,U02UBQJBYHZ,,,Hi <@U02UAHJHJ20> and <@U02UBQJBYHZ> - this is interesting. Mine shows exactly #2. Would there be any difference due to the OS we are using? I\\'m using Ubuntu 20.04LTS which I believe <@U01AXE0P5M3> is also using. Would this be an explanation?,1646014250.535359,1646022975.069419,U02U5SW982W\\na2224b42-2035-4d94-8350-8aeeff2195fa,U02UBQJBYHZ,,,And you and <@U02UBQJBYHZ> are using MacOS?,1646014250.535359,1646022998.373889,U02U5SW982W\\n7a5509ff-4160-41f6-8189-f3d3d9fd4e5c,U02UBQJBYHZ,,,:woman-shrugging:,1646014250.535359,1646023011.119909,U02U5SW982W\\na1e90059-dbbd-402c-9309-f0813d042932,U02UBQJBYHZ,,,<@U02U5SW982W> That really is strange. My Macbook is also equipped with an M1 arm processor so I wonder if that changes anything with the partitioning as well,1646014250.535359,1646023158.744679,U02UAHJHJ20\\n3ada490d-e25d-4827-a0cc-34c0274ed5be,U02UBQJBYHZ,,,<@U02UAHJHJ20> - well mine is not equipped with a heck of a lot :joy: (<https://www.seeedstudio.com/ODYSSEY-X86J4105800-p-4445.html|this> is what I\\'m using) - a little Mini PC but only with Ubuntu installed.,1646014250.535359,1646023621.613769,U02U5SW982W\\n411f2c0b-df4a-4917-bda7-29ce6933c6d6,U02UBQJBYHZ,,,How does an an ARM Cortex-M0+ MCU compare to M1? Not very favourably I imagine ;),1646014250.535359,1646023675.702089,U02U5SW982W\\ndb487abf-b9a8-4ce6-9c45-e4caf2cf4022,U02UBQJBYHZ,,,\"<@U02U5SW982W> That’s so cool! Did you build your arduino for your own personal server? I have a Raspberry Pi 3b+ that i used for an old project in college and it does save me some costs instead of setting up a VM using AWS or GCP. But yes, Apple’s new silicon chips are dominating the CPU market right now in terms of performance haha\",1646014250.535359,1646023951.017049,U02UAHJHJ20\\nd280822d-d239-43fc-bdbb-e24e0bcf4a55,U02UBQJBYHZ,,,<@U02UAHJHJ20> - I do yes. And just about everything else. You cannot argue with the price - that\\'s for sure. My logic (and I use that term loosely) was that I\\'d like to spend my money on paying for stuff in the cloud when my data got too big. But I\\'ve expanded it to include more storage and I really haven\\'t had to worry about it - yet. Although I know that day is coming. Hmmm... maybe you are right. Maybe that is the difference. Would it really compress them differently - if they were more performant I wonder? The processors? I know very little of these things ...,1646014250.535359,1646024186.155239,U02U5SW982W\\n7e6e56f3-f255-4dc7-85c3-a88f84c4dedd,,3.0,,Week 5 - no RDDs video?,1646028228.463959,1646028228.463959,U02U5SW982W\\n91a1b20a-3df7-44d6-8056-62534d3da6f3,U02U5SW982W,,,\"Hi, just checking that there is no video (the last in the week 5) that covers RDD\\'s?\",1646028228.463959,1646028261.165219,U02U5SW982W\\nec6408f0-c509-4a01-8921-539ba5654188,U02T0CYNNP2,,,\"The remote vm is constantly disconnecting and connecting on the vscode, so its actually interrupting the jupyter note, so in most cases the jupyter notebook  gets interrupted during operation and i will have to close and restart again. Maybe its network issue\",1645996093.443159,1646029425.730959,U02T0CYNNP2\\n2f2fa3dd-f8b4-4645-8ac1-6be9b53ce202,U02TATJKLHG,,,\"<@U02TATJKLHG> <@U01AXE0P5M3> inferschema reads the entire dataset to get the datatype of columns. You can verify this from the DAGs. Even the time taken to run the job is higher as it needs to read the whole dataset. By reading the csv using predefined schema, it took just 0.23s when compared 30~40s. When it comes to huge data, this may make a huge performance boost.\",1645807526.556439,1646030023.310609,U02QK4ZV4UX\\n2400e5a3-c5c3-4349-8525-da781c19994d,U02TATJKLHG,,,\"basically reading csv using defined schema, is lazy executed. reads the data only when the data is required.\",1645807526.556439,1646030070.942519,U02QK4ZV4UX\\n76ab89cf-9faa-4860-a1e2-56883bf94b3d,U02TATJKLHG,,,\"Hi Maharajan, totally agree with you. Explicitly passing the schema is better than inferring the schema. The point however was to use the `inferschema` option in the same way as Alexey used to just get a rough schema for your dataset and then updating and using it to read the entire dataset. Just a matter of convenience since you don\\'t need to use pandas explicitly.\",1645807526.556439,1646030389.601309,U02TATJKLHG\\n56487145-b4a5-4457-8a07-661e536da8e6,U02T0CYNNP2,,,\"You can start the notebook in a screen. \\n\\n```screen -R notebook\\n```\\nTo start a screen. Ctrl+A then D for detaching it\",1645996093.443159,1646030531.675599,U01AXE0P5M3\\nc272e49a-7ebe-4831-9673-849e8b4ae55f,U02UBQJBYHZ,,,For question two select the closest answer,1646014250.535359,1646030699.132939,U01AXE0P5M3\\n05fcdf75-1a36-489f-a743-432c49ac8e58,U02UBQJBYHZ,,,\"For 3 as well, but here something feels off... you shouldn\\'t be getting a different number of rows\",1646014250.535359,1646030764.708169,U01AXE0P5M3\\n1ad6b55c-4343-4f76-ad3a-ab3ffe98f967,U033WHDKP0T,,,You have backups =),1645996032.402719,1646030879.544979,U01AXE0P5M3\\nd9f531bc-8404-4059-9df8-b2913ff32bbd,U033WHDKP0T,,,\"I don\\'t know, probably there are tools, yes\",1645996032.402719,1646030896.083019,U01AXE0P5M3\\nc54f0e7a-f6fb-43ed-a1c8-61020295c714,U02U5SW982W,,,\"Yes, no videos so far. Check <#C02V1Q9CL8K|announcements-course-data-engineering>\",1646028228.463959,1646030997.395959,U01AXE0P5M3\\n60ef5f75-cb03-4a4f-8a99-3b0dcb26b421,U02UAHJHJ20,,,\"Thank you! Can you please make a PR with this information? For example, create a file \"\"google_cloud.md\"\" with this info and link it from README.md of week 5?\",1646008578.136059,1646031066.798659,U01AXE0P5M3\\n4845d344-7407-43f1-964d-4f102baf5729,U02V9V9NLJG,,,\"Use\\n```python3 streams/streams.py worker```\",1645999629.113869,1646031272.157079,U01DFQ82AK1\\n2af83ee7-7dde-4888-a552-d411bbb9b6b3,U02V9V9NLJG,,,also please make sure that all packages in requirement.txt are installed,1645999629.113869,1646031333.626289,U01DFQ82AK1\\ned7184b2-0d27-44e0-8086-672504c148ed,U02U5SW982W,,,Okay <@U01AXE0P5M3> thank you. No problems.,1646028228.463959,1646031557.068389,U02U5SW982W\\nfb238442-5fb0-43fb-ac2c-69d150bf85e8,U02T0CYNNP2,,,Thanks,1645996093.443159,1646031928.929719,U02T0CYNNP2\\n306e5c0f-c5b7-4cea-a5bd-83ee4965a478,U02TBTX45LK,,,\"Hey, check the permisions granted to your service account\",1644636171.927259,1644657088.854779,U01B6TH1LRL\\nf536ca40-f7c8-4579-b205-b007068a00a1,U02V9V9NLJG,,,Can you shared a screenshot?,1644630270.056449,1644657197.887769,U01B6TH1LRL\\nb2b29c63-45fd-4577-89bd-119bd0578fe5,U02TATJKLHG,,,\"sounds like a windows admin issue to me, can you try creating the profiles.yml outside and then replace it?\",1644493645.074519,1644657270.110269,U01B6TH1LRL\\n652D77D7-C38C-4E28-A11C-FC6B827D0B45,U029DM0GQHJ,,,That rather a complex casting. Why not simply CAST ( &lt;column&gt; as datetime) ?,1644649584.455619,1644658004.685919,U02Q7JMT9P1\\nc7a000ec-d147-4adc-ab56-84fa0385c53c,U029DM0GQHJ,,,thanks <@U02Q7JMT9P1>,1644649584.455619,1644664419.560559,U029DM0GQHJ\\n9f02ced9-339b-437d-b799-aebb8f3855e6,,,,\"hi\\ncan partitioning or clustering be done on a column with null values?\",,1644674547.392359,U029DM0GQHJ\\n79f94617-6489-45d5-ba26-c8bd141fd23c,U030FNZC26L,,,\"I\\'ve been using an M1 pro, and also no issues so far\",1644577879.991049,1644677927.618699,U01EKHDMRGT\\nB78D9FB1-1A03-4086-9098-6E5849C75607,,1.0,,\"For week 4 homework, Do we need to create a single external table (on big query) for all the 2019 fhv records and just stage the table in dbt or should we stage the tables one by one for each month? \",1644679474.634469,1644679474.634469,U02TBCXNZ60\\n765f887d-21f4-4bf6-824e-b4a823ac15ff,,2.0,,\"Hi, every one\\nI just started studying data engineering, I want to know what is the difference between data science, data engineering and ML engineer ? :face_with_rolling_eyes:\",1644679565.574349,1644679565.574349,U02C6UVB08N\\n1ac51631-3aca-4977-8ca9-8b1aa28070a1,U02C6UVB08N,,,<https://datatalks.club/blog/data-roles.html>,1644679565.574349,1644679996.188229,U02UMV78PL0\\nd59023b7-6fee-4ed9-b2ed-4862b76d9ad1,U02S2TZRBL7,,,\"Thanks <@U02S2TZRBL7>\\nas a suggestion: if you could add to help people get the required location of pip\\n`which pip`\",1644635944.221919,1644684436.812589,U029DM0GQHJ\\n218caf4d-562a-4dec-b190-9db40647a9d6,,1.0,,\"I don\\'t know if it is because I had to ingest the `green_tripdata`  as csv into cloud storage, whreas the `yellow_tripdata`  are parquet files, but when I dbt run after defining `fact_trips.sql`  in week 4, I get an error in the union all due to different data types of the column `store_and_fwd_flag` between yellow and green staging tables. If you have the same issue, `cast(store_and_fwd_flag\\xa0as\\xa0string)\\xa0as\\xa0store_and_fwd_flag`  in the `stg_green_tripdata.sql` file solves everything\",1644687683.126519,1644687683.126519,U02UA0EEHA8\\nE1C23C88-DDA3-4E79-9ED5-6D1E577FD1EB,,1.0,,\"Hello guys,\\nCan someone tell me how to catch up, I just started this course\",1644689676.291689,1644689676.291689,U02CED6PDHR\\n2fdb0b0a-57de-4486-8643-d2c0e82fb28f,U0319KGEJ13,,,\"Was able to make it work after reducing the amount of .env\\'s, thanks for the tip :slightly_smiling_face:\",1644433771.777729,1644691532.174859,U0319KGEJ13\\n23b7b68f-3eea-4240-a00a-d566d7c8338a,U02UA0EEHA8,,,\"I can\\'t reproduce the error because I used the same format for both, but I\\'ll check the code and could incorporate it for others in the same situation as you.\",1644687683.126519,1644696321.698079,U01B6TH1LRL\\n334dce7d-cc62-41e0-8708-4a7d2095e2fd,U02TBCXNZ60,,,\"Hey, one single table for fhv in general, that you had to do already for week 3 homework and the create a stg_ model like we do with green and yellow data\",1644679474.634469,1644696359.086599,U01B6TH1LRL\\n8d47413b-9f87-4365-9f71-ee62a748557e,U02UY1QTGHW,,,@Abi Did you manage eventually?,1642916703.026200,1644697403.534929,U02T70K8T61\\nd4c16e52-9eb5-41ec-84d3-536199ad47e7,,9.0,,\"Hi all,\\nDo you guys know a tool for drawing cool data pipelines?\",1644697506.408849,1644697506.408849,U030FNZC26L\\n4d04a1e2-0bdb-4156-aaa6-1d693a535b21,U02CED6PDHR,,,\"Hey, check the youtube list and the repo -&gt; <https://www.youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb>\",1644689676.291689,1644697713.745059,U01B6TH1LRL\\n7182a507-42c0-46af-b5ca-d82a1dd51f94,U030FNZC26L,,,whiteboard! :slightly_smiling_face:,1644697506.408849,1644699291.213109,U01AXE0P5M3\\na1296382-3b12-4c0d-9e62-89c1c2882f79,,2.0,,\"Hello I am having issues with dbt cloud, I cant change the files from the web ui.  Has anyone experienced sth similar?\\nAnd as you see in the screenshot , branch is set to read_only not sure what is happening tbh :disappointed:\",1644699539.410049,1644699539.410049,U02DD97G6D6\\n76e89a71-ded2-4f27-ba10-3b1948e052b7,U02V9V9NLJG,,,This is what i see when I select a project,1644630270.056449,1644700151.277559,U02V9V9NLJG\\n044e99be-6ec9-4bf5-b144-2f6a94171200,U02V9V9NLJG,,,\"If i click edit, this is what i see\",1644630270.056449,1644700176.933299,U02V9V9NLJG\\n4cbfc61c-e74d-401f-a57a-1057dfc8732c,U02DD97G6D6,,,\"It is working as expected if I checkout to other branch, but does not allow me to work on the master branch\",1644699539.410049,1644700214.061559,U02DD97G6D6\\n632a25b3-9a24-42e8-a06f-e281e9b99c18,U02V9V9NLJG,,,\"From project page, if I click configure a connection, and select big query, i see this. The below two screenshots are part of the same screen. I can see Bigquery settings but cannot see any oauth 2.0 settings.\",1644630270.056449,1644700323.878449,U02V9V9NLJG\\n9139a221-5360-4028-9fb9-cf73252643a1,U02U6DR551B,,,\"<@U02T697HNUD> thanks, I understand that now.  I am still not sure how to execute the DAG successfully without a backfill.  When I set the default arguments for start and end date, I run into the error - `The execution date is 2021-02-12T00:00:00 but this is after the task\\'s end date 2021-01-01T16:20:30.363268.`\",1643873524.132099,1644700973.166269,U02SEB4Q8TW\\ne6ae9a71-3079-4eb6-96e6-37ebc0380a89,U02TBTX45LK,,,\"ahah yup, looks like i mixed up accounts when setting up. Thank you so much for your quick response!\",1644636171.927259,1644701422.151499,U02TBTX45LK\\n30bb7e2a-fca6-441b-887b-1be6cfa482c1,,4.0,,\"For setting up dbt at the bottom do we just leave the Development Credentials as they auto populated? The dbt_cloud_setup.md doesn\\'t really say what these are or if we need to do anything. It just states \"\"Scroll down to the end of the page and set up your development credentials\"\" :thinking_face:\",1644704463.895909,1644704463.895909,U02TNEJLC84\\n0AB15321-C372-4A88-BD8C-D7E62CC62E8A,U02C6UVB08N,,,This helped me a lot. Thank you so much <@U02UMV78PL0> ,1644679565.574349,1644704958.372069,U02C6UVB08N\\n24bf4c7d-ad25-4711-b12a-97de4114ccf6,,19.0,,What can be the reason for not seeing the lineage in the dbt cloud UI?  I cant see the lineage for sql file .,1644704990.517419,1644704990.517419,U02DD97G6D6\\n27a420df-7e81-4a58-ba3d-f850315b156b,U02DD97G6D6,,,\"Were you able to run the project before from the cloud? Looks like the project is not initialised, I can see that the green button says \"\"initialize your project\"\"\",1644704990.517419,1644705121.880959,U01B6TH1LRL\\na5558a3f-10f7-417d-bfad-e96eda3ac2f9,U02DD97G6D6,,,\"I explain this in one of the last videos I think, it will only allow you to do the first commit directly to master/main to initialise your project, but after that, to ensure you work always on a development (sandbox) environment and later deploy it will treat your master/main branch as read only.\",1644699539.410049,1644705202.382289,U01B6TH1LRL\\n85117748-e093-4245-ab34-6df93cb86dc0,U02TNEJLC84,,,\"Only change the dataset if you created it with another name in bigquery, otherwise it should be fine.\",1644704463.895909,1644705282.898919,U01B6TH1LRL\\n0b2cfb30-0dc9-4f17-ae72-8ee65c14bcae,U02U3E6HVNC,,,In my case both the `dbt_emasakhwe` and `trips_data_all` schemas are in the same location of `europe-west6` but I\\'m still getting the same error `trips_data_all was not found in location US` when i run dbt. Any help <@U01B6TH1LRL> <@U02TC8X43BN> <@U02U3E6HVNC> Thanks,1644432387.132949,1644705552.459369,U02SEH4PPQB\\nc9394b7d-0fec-4279-b9db-45be33647df5,U02TNEJLC84,,,\"<@U01B6TH1LRL> So it should be the name of the dataset with the taxi ride data? Most likely \"\"trips_data_all,\"\" or \"\"taxi_ride_data\"\"....whatever it is called when you log into BigQuery. Correct?\",1644704463.895909,1644705650.512649,U02TNEJLC84\\nbda2e171-5cbe-4c18-9add-b8f95ca9f317,U02TNEJLC84,,,Oh wait the dataset or the target name? Should the target name be the name of our dataset in Big Query?,1644704463.895909,1644705861.196569,U02TNEJLC84\\n01269798-01c9-4000-a112-24a3243d2530,U02DD97G6D6,,,\"I have initialized in the cloud actually and all these folders created afterwards, but it didnt worked out in the master branch and I needed to create start working on different branch, And as you see, I have specified subfolder for creating the dbt project\",1644704990.517419,1644706238.424229,U02DD97G6D6\\n4c530086-0194-4719-8d72-3b8530908cf6,U02TNEJLC84,,,\"This is the schema where you\\'ll create your models in the development environment, It\\'s what in the videos I show as dbt_victoria_mola.\\ntrips_data_all is the schema where you would pick the data from, your source data.\\nTarget leave as default, only change the dataset name in case you want to use something different to the one provided.\",1644704463.895909,1644706626.174289,U01B6TH1LRL\\na49a3862-f79b-4f45-916f-d3881c07656f,U02U3E6HVNC,,,\"Can you check that that\\'s indeed your development schema that you defined in dbt cloud? Maybe it\\'s using other one, hence the confusion.\\nFrom dbt cloud o to your profile in the top right, and then credentials -&gt; project (I took a screenshot of mine). Check if there it also says  dbt_emasakhwe\",1644432387.132949,1644706793.467319,U01B6TH1LRL\\n213678bb-4269-45f5-a15b-5f1c6602e673,U02DD97G6D6,,,\"what happens if you go to main branch? you still can\\'t see the dag? do you still see the \"\"initialize the project\"\" from the main branch as well?\",1644704990.517419,1644706871.942879,U01B6TH1LRL\\n91275467-da89-42fa-a129-37b14821b0fe,U02DD97G6D6,,,\"As I had issue with the main branch, I renamed it to master. And in the master branch I have complete different dbt which is lets say empty. And it is read-only and I am still seeing the yellow initialize your project button.\",1644704990.517419,1644707550.028999,U02DD97G6D6\\nd5294e80-0abb-404b-a59b-5dd960c8664f,U02DD97G6D6,,,\"I guess you did already the first commit and after that dbt sets the main/master branch to read only. Then I\\'d say go back to your dev branch (or a new one) and initialize the project again, I assume it would create again some of the folders, you can delete them or move them around.\",1644704990.517419,1644707910.841119,U01B6TH1LRL\\n2dfda9db-d714-4462-9d65-17646ac3d383,,3.0,,Anyone has a problem running the \\'dbt run --select stg_green_tripdata\\'. I keep getting a location problem. details in the thread.,1644707956.803599,1644707956.803599,U02RSAE2M4P\\n5f9fb740-c703-45f5-b1ff-d8731373bd3e,U02RSAE2M4P,,,Is this somewhat related to the service account linked to dbt?,1644707956.803599,1644708158.068589,U02RSAE2M4P\\n47fbcf8b-bc0c-4d61-b8ab-152fc1b3c8e5,U02DD97G6D6,,,\"Hmm, I tried to create a new fresh branch from my master branch but initialize your project thing still persist\",1644704990.517419,1644708689.308059,U02DD97G6D6\\na7f67839-bbdf-428b-a6f5-29d6005c1a14,U02DD97G6D6,,,,1644704990.517419,1644708710.417629,U02DD97G6D6\\n29fe578f-b358-4e99-8ac8-2a22046689dc,U02RSAE2M4P,,,\"There are several threads above around the same issue, you probably have your development schema (something like dbt_samson) located in a region different to the trips_data_all_schema.\\nFrom bigquery, drop your development schema and create it again in the same region as trips_data_all.\\nBig query doesn\\'t allow multiregion queries and what\\'s happening under the hood in that run is that it wants to create a table in a location with a select from another location.\",1644707956.803599,1644708733.753419,U01B6TH1LRL\\n0c2c7848-db05-49ed-afdc-c013ae0ac984,U02DD97G6D6,,,should i completely delete the dbt project and recreate it ? I dont know?,1644704990.517419,1644708746.798229,U02DD97G6D6\\nd9b27b1a-c3f6-427b-a45f-76a9f2c69143,U02DD97G6D6,,,\"That\\'s weird, you initialize the project from that green button and it stays the same? I guess you can delete the week 4 and initialize once more.\\nOnce thing before that, I see you have logs outside and inside week4/ny_taxi_rides. I take that\\'s from initialising for a second time the project, but can you check that there\\'s no project.yml duplicated as well?\",1644704990.517419,1644708876.179219,U01B6TH1LRL\\nfba2437a-54d3-4e54-a970-5ffc94b12947,U02DD97G6D6,,,\"Other thing I can think of, did you specify a subdirectory in the repo when setting up the project in dbt? It\\'s an option under advanced, when connecting to the repo\",1644704990.517419,1644708911.554519,U01B6TH1LRL\\nf8f153a2-1e81-4280-994c-9943f787860a,U02DD97G6D6,,,Yes I specified it with the  advanced option already option,1644704990.517419,1644708942.020139,U02DD97G6D6\\n5435bb36-134b-4867-beff-730be8af9b66,U02DD97G6D6,,,\"What you are suggesting right now,\\n1, On main/master branch delete the week4 folder and recreate everything (or just delete the ny_taxi_rides subfolder for dbt)\\n2, Then by the advanced options setting re-initialize the project\",1644704990.517419,1644709052.187319,U02DD97G6D6\\n427474b4-0616-45f7-a138-11392d4cf7ee,U02RSAE2M4P,,,Thanks <@U01B6TH1LRL> works perfectly.,1644707956.803599,1644709673.150569,U02RSAE2M4P\\n2f27f75e-a3c9-4734-8f54-0c04484a458c,U02DD97G6D6,,,\"It didnt worked out again, I have deleted the ny_taxi_rides folder inside the week4 and also deleted all the branches I have created and  re-initialize the dbt project on master branch, committed and pushed the folders. Still seeing initialize button\",1644704990.517419,1644709745.879539,U02DD97G6D6\\nc049b47f-6f51-4e15-9beb-b67c11efee74,U02DD97G6D6,,,I have also tried to delete the project from the dbt and re-create it by specifying github and deploy-keys and bigquery access key again from scratch but still seeing same green button and could not really see the lineages,1644704990.517419,1644710983.910899,U02DD97G6D6\\n6b85e458-a679-443a-8ccd-e77f7bc27e49,U02U6DR551B,,,\"<@U02SEB4Q8TW> I guess you set the end date too short. If you want run the DAG once, you can set the schedule interval there.\",1643873524.132099,1644718919.346829,U02T697HNUD\\nd0c8400f-b39c-48d7-bbf5-8b0b045f4598,U030FNZC26L,,,:smile: I\\'m looking for sth fancy to show stuff to client. Sth to draw like this:,1644697506.408849,1644725019.446749,U030FNZC26L\\n2efbd025-a652-48a7-91d5-ec1f575a0d6a,U030FNZC26L,,,,1644697506.408849,1644725038.567099,U030FNZC26L\\n0cfb9873-41bb-4b8b-908e-2a181a36539c,U02TBTX45LK,,,\"I’m having this issue too, but I’m not really sure what I’m looking for in the permissions section of gcp for my dbt service account.  What could I have done wrong in the setup process?\",1644636171.927259,1644737388.282279,U02U8CB58G3\\n9f35c212-eddc-4491-b536-9a6d25872ef0,,4.0,,\"hi all, it’s regarding Week 3 - 3.3.2 - BigQuery Machine Learning Deployment\\n\\ni was trying to run the docker tensorflow/serving image, however looks like this image is not compatible with Mac M1 system. I understand that tensorflow still have troubles working on Mac M1 chips. In that case, is there any work around for this step? Eg. any other serving image i could use? thanks! FYI - error in thread.\",1644737424.801719,1644737424.801719,U02ULMHKBQT\\n3a5bd4eb-697b-4a47-aa0e-6fd0e813d80b,U02ULMHKBQT,,,\"```docker pull --platform=linux/amd64 tensorflow/serving\\n\\ndocker run -p 8501:8501 --mount type=bind,source=pwd/serving_dir/tip_model,target=/models/tip_model -e MODEL_NAME=tip_model --platform linux/amd64 -t tensorflow/serving &amp;```\",1644737424.801719,1644737450.969859,U02ULMHKBQT\\nd8e2e6f6-a4a4-4b2e-a581-e2d4a58aa675,U02ULMHKBQT,,,\"Error below:\\n```[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/generated_message_reflection.cc:2345] CHECK failed: file != nullptr:\\nterminate called after throwing an instance of \\'google::protobuf::FatalException\\'\\n  what():  CHECK failed: file != nullptr:\\nqemu: uncaught target signal 6 (Aborted) - core dumped\\n/usr/bin/tf_serving_entrypoint.sh: line 3:     9 Aborted                 tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"\"$@\"\"\\n\\n[1]  + 4069 exit 134   docker run --platform linux/amd64 -p 8501:8501 --mount  -e  -t```\",1644737424.801719,1644737508.496549,U02ULMHKBQT\\n17625d43-4d8b-4525-bfcb-fbe7dc98794d,,7.0,,\"Hi guys,\\nRegarding using dbt, I\\'m not convinced yet why we have to use it (learning and adding one more tool to the pipeline to do transformation using SQL which we have in BQ already)?\\nI watched the videos and got more confused. One reason could be that I\\'m not comfortable with SQL and prefer python. So when we have data in BQ, what are the other options that we can use instead of dbt? Any ML tools? spark?\\nit would be great if we could have a short video that explains what we want to do, and what is the problem that we want to solve, to make it clear why we want to use dbt or other tools in other weeks.\",1644737526.039989,1644737526.039989,U030FNZC26L\\nd5a5351f-2e24-485f-a1d5-e65f8a35330d,,2.0,,\"Hello everyone, I am trying to give some \"\"boost\"\" to our <https://docs.google.com/document/d/19bnYs80DwuUimHM65UV3sylsCn2j1vziPOwzBwQrebw/edit|FAQ> in order to help everyone.\\nSo far we only have FAQ for week 1.\\nLet\\'s try to improve it. If each user inserts 1 (!!) Question and Answer we will improve it a lot.\\n:rocket:\",1645605971.227189,1645605971.227189,U02CD7E30T0\\nb39f2eca-667c-4c2e-bd0d-b080fafd8e02,U02CD7E30T0,,,Already had some extra FAQs. Feel free to add more Questions and Answers ;),1645605971.227189,1645608326.443859,U02CD7E30T0\\n7aec77e7-a753-45ce-8225-e5e86fed8bd3,U02SSP7C4SD,,,\"I faced the \"\";c\"\" problem too. I was using git bash and everything was working fine until then. I finally run the command from Windows command.exe and worked.\\n```docker run -it -e POSTGRES_USER=\"\"root\"\" \\xa0-e POSTGRES_PASSWORD=\"\"root\"\" \\xa0 -e POSTGRES_DB=\"\"ny_taxi\"\" \\xa0-v /D/L/DataTalks/data_engineering_zoomcamp/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data -p 5432:5432 \\xa0 postgres:13```\\nIf you fix it with this solution, let me know and I will add it to the FAQ.\",1645473732.854749,1645608642.297619,U02LQMEAREX\\n55766024-bed6-48cb-a976-9a96464c20c7,U032Q68SGHE,,,\"Hello, my casting is done in airflow ingestion dag.\",1644881953.727679,1645610290.447279,U02LQMEAREX\\n188509a0-4ba4-48e0-9b9e-3a30ef21e3dd,U02QK4ZV4UX,,,\"<@U02U5SW982W> This problem is same for me, I had tried each and every possible way to changing the different locations for the GCP cloud storage then Bigquery dataset location in the same region, creating new git repo, starting project from start, even creating new dbt project. But at the end when I try to create a materialized table this error() is always show up but with materialized view everything works. I had checked BigQuery roles everything is correct.(Most interesting thing is the queries which fails in dbt, I run them in BigQuery editor , queries work there perfectly)\\nIt\\'s been more then one week, i\\'m just stuck with this error global partitioning error.\",1645507617.625979,1645610920.902529,U0297ANJF6F\\n4a922f16-7d76-4986-ab5a-13778c4af32a,U02U34YJ8C8,,,\"<@U02U5SW982W> Was this not about the fhv data? January 2020, which is not needed for the homework, right?\",1645587287.934269,1645611315.511459,U02CGKRHC9E\\n066e529c-860c-4a53-975c-e2928a70f17d,,6.0,,\"Can we submit the Homework 4 after deadline.\\nBecause i\\'m getting this `Permission denied while globbing file pattern` error from last two weeks and I have tried all the solution from the other people here but nothing is working. I have debugged every thing, moved the dateset in Bigquery and google bucket data at same locations but nothing changed for me.\\n\\nI was trying last option to install the dbt locally and run the models but again there is error when try to run the  `docker compose build`  docker doesn\\'t find dbt-core repo.\\n `Running command git clone --filter=blob:none --quiet <https://github.com/dbt-labs/> /tmp/pip-install-gipbrn3z/dbt-core_b434f816da844f8f9a299bc3501a28a2`\\n  `remote: Not Found`\\n  `fatal: repository \\'<https://github.com/dbt-labs/>\\' not found`\\n  `error: subprocess-exited-with-error`\",1645612737.147509,1645612737.147509,U0297ANJF6F\\nad01c231-7f43-4cce-bca5-4c0ca1cefbf4,U0297ANJF6F,,,\"Check this channel, there\\'s a video with the solution to your problem\",1645612737.147509,1645612781.799189,U01AXE0P5M3\\n7f6fd5d4-7d2b-402a-bf06-57e541f76bd7,U0297ANJF6F,,,Or it didn\\'t work for you as well?,1645612737.147509,1645612798.364389,U01AXE0P5M3\\nb5ba900e-f859-461b-accd-57206f613d23,U02UJGGM7K6,,,Scala has a big learning curve comparatively. Unless you are not interested into exploring spark source code I would not do it. But I did actually in the beginning :grinning:,1645542413.218429,1645613217.431269,U02NSF7RYP4\\n3d235e12-7d91-418a-84c0-bcfb82d94820,U02UJGGM7K6,,,I would prefer java 8 over scala. Scala maven integration is really pain otherwise sbt would be needed.,1645542413.218429,1645613441.792329,U02NSF7RYP4\\ncdd282d3-16b0-41a8-8da3-2e746eb7f43e,U02HB9KTERJ,,,In the longer run you would be running on cluster for production thats where spark would excel. It just not about these files. Idea is to get ourselves ready for big data.,1645534158.245849,1645613661.369619,U02NSF7RYP4\\n430b97d8-bfc5-46be-83a4-27001ab3499b,U0297ANJF6F,,,I didn\\'t worked for me,1645612737.147509,1645618117.245999,U0297ANJF6F\\n36af4e8d-1ef5-4a32-808d-9b796857feb4,U0297ANJF6F,,,\"Hey Syed, are you using external tables? You could create a table directly in BQ from that and use as your source. \\nRegarding locally, are you using the image from the course repo? You can also install locally without docker btw with brew install or pip install\",1645612737.147509,1645618386.176649,U01B6TH1LRL\\na2375167-26fc-4928-960f-498b0a50c26e,U02QK4ZV4UX,,,\"Syed can you share the query? It could be that you\\'re just running the select, so limited to one region, in BQ. But the problem is when it creates the table or view, and that becomes multiregion\",1645507617.625979,1645618468.782999,U01B6TH1LRL\\ndcc51d59-20f2-48a4-9c48-4575d424810a,U02QK4ZV4UX,,,\"It\\'s also not about the storage but in bq schemas and tables, or are you using an external table?\",1645507617.625979,1645618507.563669,U01B6TH1LRL\\nac7adf37-2a22-4e30-be12-3eb16b2af32c,U0297ANJF6F,,,\"Yes , i\\'m using external table.\\nI will try to create from the bigquery.\\nYes, I\\'m using the exact same images from the repo.\",1645612737.147509,1645619237.693959,U0297ANJF6F\\ncf821e04-f06f-429c-9d1a-5afcd7da390a,U0297ANJF6F,,,\"ohh, thanks God , finally it worked. All my tests are passed and all models are created.\\nthanks <@U01B6TH1LRL>\",1645612737.147509,1645623945.008499,U0297ANJF6F\\ncee7c9a4-9f6d-4521-9bf6-7bc4590506f3,,3.0,,\"<@U01AXE0P5M3> can we extend untill today 23:59 UTC? I just handled this dbt error, and it seems this HW will take some time :sweat:\",1645627494.126859,1645627494.126859,U02U5L97S6T\\nb6222764-661d-408c-923b-5c78c2f7599c,U02SUUT290F,,,\"Yes, you can use an Airflow DAG to orchestrate those functions in BigQuery. I haven\\'t done that for this zoomcamp, but I\\'m planning to incorporate it into my personal project soon. Airflow provides *operators* - built-in ways to orchestrate a huge number of third-party platforms. <https://airflow.apache.org/docs/apache-airflow-providers/operators-and-hooks-ref/google.html|You can check out the list of Airflow operators for GCP here>, and <https://github.com/apache/airflow/tree/main/airflow/providers/google/cloud/example_dags/|scope some sample scripts for Google here>.\",1645562995.282099,1645629017.788399,U02TXLWFG86\\n792a1c05-38b9-442c-a9a9-e20388c28c1a,U02U5L97S6T,,,Do you think two hours will make a difference?,1645627494.126859,1645629285.140849,U01AXE0P5M3\\n0969d179-9dfb-47df-929f-9fe6d4b545c0,U02CD7E30T0,,,\"Luis, this is a great idea - thanks for taking leadership on this and reminding us of the importance of documentation.\",1645605971.227189,1645629492.833089,U02TXLWFG86\\ncafe4d19-97e2-4a05-b9b9-23994daa44fd,U02TATJKLHG,,,This looks really neat man. Will try my hands too on this.,1644919012.575939,1645630579.109699,U02TATJKLHG\\ned5d1be2-aeed-43d1-bb16-f9b4ebd237c6,U02U5L97S6T,,,I hope so :slightly_smiling_face:,1645627494.126859,1645632039.628679,U02U5L97S6T\\nea68b268-1ffc-4c7f-bc5c-9c1762f877cc,,3.0,,\"Hi Everyone, had an error when running \"\"dbt run\"\"\\n```Database Error\\n  Could not deserialize key data. The data may be in an incorrect format or it may be encrypted with an unsupported algorithm. Code: 10003```\\nThis error occured after I changed the BigQuery Location to \"\"europe-west6\"\" in the dbt Account Setting page.\\nHope that someone could help me! Thanks!\",1645634338.661579,1645634338.661579,U02BRPZKV6J\\n28dfc3fd-4368-46d7-979e-193399b65b3d,,1.0,,\"Hi. If I have openjdk 11.0.13, should I update it to openjdk 11.0.2? And how can I do this?\",1645635650.066359,1645635650.066359,U02QL1EG0LV\\n3352a5bc-2650-42b6-8f1c-9d7619234a44,U02U5L97S6T,,,\"My memory isn\\'t always reliable, I sometimes forget to do things in time\",1645627494.126859,1645635665.997359,U01AXE0P5M3\\n03fea30f-a53e-43b6-b8e2-0660e7bb3f02,U02QL1EG0LV,,,It should be fine,1645635650.066359,1645635742.890139,U01AXE0P5M3\\n4a1b8ec0-e588-4215-9f60-8ce47c80bea5,,1.0,,\"I have some ideas why we get different results for Question 1 HW4\\n\\nAFAIU the order in which the query will return records is not guaranteed.\\nIn the model `stg_yellow_tripdata.sql` we patrition records by `(vendorid, tpep_pickup_datetime)` and select only the first of such records. It is not necessarily the same record for each query run because of the different order.\\nSo, the `pickup_locationid` field of this record may contain a different value for different query runs.\\n\\nIn the model `fact_trips.sql` we join these record and records from `dim_zones`\\n`from trips_unioned`\\n`inner join dim_zones as pickup_zone`\\n    `on trips_unioned.pickup_locationid = pickup_zone.locationid`\\n`inner join dim_zones as dropoff_zone`\\n    `on trips_unioned.dropoff_locationid = dropoff_zone.locationid`\\nIf `pickup_locationid` or `dropoff_locationid` = 264 or 265, the corresponding record will not get into the `fact_trips` table, since there are no such values in the `dim_zones` table. And with any other value, it will.\\n\\nThus, a record with a unique key `(vendorid, tpep_pickup_datetime)` may or may not be included in the resulting table, depending on which of the duplicates with this key was the first and which value ended up in the `pickup_locationid` or `dropoff_locationid` field.\\n\\nMaybe that\\'s why we get a different number of records.\\nIn the development environment I have 61634570 records, in the production environment - 61634951. Same code, same data - different result.\\n\\nAll these are just my thoughts. I haven\\'t checked it and I haven\\'t figured out how to check it yet. :)\",1645637372.659909,1645637372.659909,U02U85H3W10\\n0ecb172a-2916-44c4-990e-57bd98dd2e0c,,5.0,,\"I am trying to create a directory but it won\\'t let me do it\\nUser1@DESKTOP-PD6UM8A MINGW64 /\\n$ mkdir .ssh\\nmkdir: cannot create directory ‘.ssh’: Permission denied\",1645642561.385659,1645642561.385659,U02VBG59VQ9\\n6cab6c31-86c7-44b7-adca-6eb905ac1742,U02VBG59VQ9,,,You should do it in your home directory,1645642561.385659,1645642593.345449,U01AXE0P5M3\\nd925cadf-19bf-4b34-aade-0197f1578edb,U02VBG59VQ9,,,<@U01AXE0P5M3> oh okay thank you,1645642561.385659,1645642895.377879,U02VBG59VQ9\\n3bb838d3-5c5a-4c83-a5dc-fce5d2113c1f,U02VBG59VQ9,,,<@U01AXE0P5M3> do i create that in my VM or local.  In video 1.4.1 and 5.2.1 i see that you created it in your local? i just setup my VM yesterday,1645642561.385659,1645643311.603809,U02VBG59VQ9\\n1d0c721e-dda1-4605-8e5a-ce112ea34169,U02VBG59VQ9,,,Local. But it seems you\\'re trying to do it in the root folder (/). Should be your home (~),1645642561.385659,1645643385.874909,U01AXE0P5M3\\n50e1fdf8-5742-41f4-b993-5ecb272a163c,U02VBG59VQ9,,,you are right thank you,1645642561.385659,1645643837.082099,U02VBG59VQ9\\n07d16f34-fd4b-4291-92a8-cf73da27aaec,U02U34YJ8C8,,,Hi <@U02CGKRHC9E> - it could have been but I thought it was green taxi data. Did we do this one on our own?,1645587287.934269,1645645007.261229,U02U5SW982W\\n3e741d68-5abc-49a7-a66f-991a64b638da,,6.0,,FAQs - an idea to boost the FAQs,1645645044.333119,1645645044.333119,U02U5SW982W\\n5967a6a0-5181-496f-a976-8c6236064367,U02U5SW982W,,,Further to <@U02CD7E30T0> I\\'m imagining if this was part of the homework there would be a great boost in the FAQs page... thoughts?,1645645044.333119,1645645134.756399,U02U5SW982W\\nc2cb4cf1-600e-4747-9a9f-8cc8feee475d,U034BND3P2M,,,\"Sorry I\\'m seeing this now but I searched in the channel and got this solution\\n\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642693557370600?thread_ts=1642615746.189300&amp;cid=C01FABYF2RG>\",1645526853.296059,1645645811.324459,U034BND3P2M\\neda5d046-78fd-4d4e-baf7-710131cfce9c,,9.0,,\"hello all. This is in my VM\\n(base) girmaysisay@dezoomcampsisvm:~$ which python\\n/home/girmaysisay/anaconda3/bin/python\\n(base) girmaysisay@dezoomcampsisvm:~$ python\\nPython 3.9.7 (default, Sep 16 2021, 13:09:58)\\n[GCC 7.5.0] :: Anaconda, Inc. on linux\\nType \"\"help\"\", \"\"copyright\"\", \"\"credits\"\" or \"\"license\"\" for more information.\\n&gt;&gt;&gt; import pandas as pd\\n&gt;&gt;&gt; pd._version_\\nTraceback (most recent call last):\\n  File \"\"&lt;stdin&gt;\"\", line 1, in &lt;module&gt;\\n  File \"\"/home/girmaysisay/anaconda3/lib/python3.9/site-packages/pandas/__init__.py\"\", line 244, in __getattr__\\n    raise AttributeError(f\"\"module \\'pandas\\' has no attribute \\'{name}\\'\"\")\\nAttributeError: module \\'pandas\\' has no attribute \\'_version_\\'\",1645646519.418039,1645646519.418039,U02VBG59VQ9\\nc288ff2b-b131-4028-b4b2-3d31ba3b37d0,U02VBG59VQ9,,,Doesn\\'t it have to be two underscores there in version rather than one?,1645646519.418039,1645646768.912569,U02U5SW982W\\n7a45638f-776c-45ec-bd7f-c8dac1958f01,U02VBG59VQ9,,,`pd.__version__` rather than `pd._version_`,1645646519.418039,1645646799.465759,U02U5SW982W\\n97d4f823-63d6-4193-ac32-1f002f62c349,U02VBG59VQ9,,,you are awesome i follow your blog too,1645646519.418039,1645646901.453239,U02VBG59VQ9\\n0202c2da-d81d-403c-b4c6-6bb61b9e9d61,U02VBG59VQ9,,,you are killing it <@U02U5SW982W>,1645646519.418039,1645646920.940389,U02VBG59VQ9\\nc4915aba-93ed-45f7-8353-6daf40cf116e,U02VBG59VQ9,,,More like killing myself <@U02VBG59VQ9> :wink:,1645646519.418039,1645646998.081099,U02U5SW982W\\n2c7fa23d-f35e-48fe-a656-ea0cccab3642,U02VBG59VQ9,,,I make silly mistakes like that all the time <@U02VBG59VQ9>. Sometimes I even surprise myself ...,1645646519.418039,1645647065.226619,U02U5SW982W\\n531f5454-b849-4d41-ba3c-3c0ddb43946f,U02VBG59VQ9,,,<@U02U5SW982W> for us. doingit for the team,1645646519.418039,1645647122.601169,U02VBG59VQ9\\n608d7e62-2560-443e-bc50-0ae3deec2b8c,U02VBG59VQ9,,,I just wish I knew how to use WordPress better for my blog. It\\'s all over the shop and I\\'d like to organise it better. Oh well I\\'m still learning I guess...,1645646519.418039,1645647487.600549,U02U5SW982W\\n5546caa7-028e-4738-9f6c-40bb1ff4bd3e,U02VBG59VQ9,,,And i see you are a morning person too:thumbsup:,1645646519.418039,1645648631.478859,U02VBG59VQ9\\nd98d5cb6-6d6a-487a-b82e-8787ee8ec3af,U02U34YJ8C8,,,\"<@U02U5SW982W> Both green taxi and fhv we did on our own, I didn\\'t face any problems with green taxi 19/20. Fhv for 2019 run smoothly as well. Apparently there is an issue with the 2020 data as mentioned in the <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_2_data_ingestion/homework.md|note to Q2> . But I never touched that.\",1645587287.934269,1645648880.728439,U02CGKRHC9E\\n531c7200-88b3-432b-8c37-e8f54e537c78,U02U85H3W10,,,\"Hey, I tried to explain something similar in the homework solution. The deduplication should have had an order by to avoid this behaviour. I expect a discrepancy of up to almost 7k just for just green trip data :crying_cat_face:\",1645637372.659909,1645650616.240479,U01B6TH1LRL\\n7ba54771-933b-4adc-a958-b105a9e340ea,U02BRPZKV6J,,,\"Hey Erwin, I haven\\'t seen this one before and a quick google search doesn\\'t seem to shed any light. All results point to Python, which leads me to suspect this is a dbt issue :confused:\\nDo you still experience it? What about executing the compiled code directly from BQ?\",1645634338.661579,1645651211.118349,U01B6TH1LRL\\ndf613da2-7045-4e50-ac65-d9c8584f9d71,U02U5SW982W,,,\"Great idea !!!!! One point to \"\"question and answer\"\" you had in the week. <@U01AXE0P5M3> what do you think?\",1645645044.333119,1645651458.436249,U02CD7E30T0\\n9debf10a-d2f4-49c9-b9a7-e3aecca33e13,U02QK4ZV4UX,,,\"Hi <@U01B6TH1LRL> thanks for sharing your experience - I\\'m just kind of glad that I\\'m not alone. <@U0297ANJF6F> - the only way I could get around it is - as I explained in my blog post - and as <@U01B6TH1LRL> explains is an option, was to default everything to US. I do see the irony that pretty much the only time in my life I don\\'t accept the defaults is when everything falls apart :face_with_rolling_eyes:<@U0297ANJF6F> did you try the workaround and default everything to the US? Don\\'t worry I feel your pain...\",1645507617.625979,1645652845.600749,U02U5SW982W\\n41ac28d9-8da3-4231-9ca8-0ec3adb68ab6,U02U34YJ8C8,,,HI <@U02CGKRHC9E> - from memory I did face problems with this. And I\\'m wondering if we all did something different here could that be a reason for the variation?,1645587287.934269,1645652964.320619,U02U5SW982W\\n665b59fb-174e-4c1e-bd1b-af8e96b6c279,,4.0,,\"Hi everyone\\n\\nI had no issues installing Spark in my computer but I was curious to see if I could use something already inside GCP for direct access to my Cloud Storage.\\n\\nThat\\'s why I tried Dataproc and created a little guide to help anyone.\\n\\n<https://github.com/Leo200467/de-zoomcamp-2022/blob/main/de-zoomcamp-week-5-batch-processing/pyspark_in_dataproc_cluster.md>\\n\\nIt\\'s a short guide and I will work more on it but hopefully this can be useful for someone.\",1645659834.521759,1645659834.521759,U02TC704A3F\\nbe37096c-f539-498c-9569-3c102238cac5,,,,\"guys please i need help. I have been on this error since yesterday\\nError while reading table: trips_data_all.green_tripdata_external_table, error message: Parquet column \\'VendorID\\' has type BYTE_ARRAY which does not match the target cpp_type INT64.\",,1645660222.019149,U02UKLHDWMQ\\n797325a6-8976-4038-b157-67b9d726909a,,6.0,,\"with is\\nError while reading table: trips_data_all.green_tripdata_external_table, error message: Parquet column \\'ehail_fee\\' has type DOUBLE which does not match the target cpp_type INT64.\",1645660295.275939,1645660295.275939,U02UKLHDWMQ\\n0dc6e506-8bbd-4d58-a4d2-508d0912cc81,U02UKLHDWMQ,,,Exactly where are you up to <@U02UKLHDWMQ>? The problem is what you are trying to read is not matching. I think others simply casted this type to get it to work...,1645660295.275939,1645661076.642009,U02U5SW982W\\n6e21b5dd-5669-403c-ae0c-c6d6f852bf29,U02UKLHDWMQ,,,week 3. at what point should i use the CAST function to change the type. Cos when i checked the schema it shows that the external table of the particular column is int64,1645660295.275939,1645662140.809069,U02UKLHDWMQ\\n4761b187-d1ad-4fa7-83d9-df26cc03eea9,U02UKLHDWMQ,,,Sorry <@U02UKLHDWMQ> and what particular video - and even the exact section? It\\'s been ages since I\\'ve looked at this and I\\'m having trouble remembering. You\\'re doing Airflow right? I think this is talked about in the FAQ\\'s for the course ... from memory...,1645660295.275939,1645663952.768349,U02U5SW982W\\nf4af6156-a8f1-43ca-b7c0-a0f5250ff778,U02UKLHDWMQ,,,See the <https://docs.google.com/document/d/19bnYs80DwuUimHM65UV3sylsCn2j1vziPOwzBwQrebw/edit#heading=h.u3mfeppzfpyy|FAQ>\\'s week 4 - you mean week 4 right? That exact error is dealt with in the FAQ\\'s for this week. Does that sound right to you?,1645660295.275939,1645664071.865309,U02U5SW982W\\nf7f59003-9aba-4ed8-b693-58d02965006f,U02UKLHDWMQ,,,Now working thanks <@U02U5SW982W>,1645660295.275939,1645664910.436479,U02UKLHDWMQ\\n5cef1c81-b1ae-46ef-9833-90b60836f90e,U02UKLHDWMQ,,,Awesome :raised_hands:,1645660295.275939,1645664937.794899,U02U5SW982W\\n143b0180-719e-4296-bb3c-c1911fd22ad9,U02UEE4MBEG,,,Hi <@U02UEE4MBEG> where you able to resolve this?,1642847257.296400,1645680879.789069,U032TP2AS2Z\\n02bba7cc-c6ee-499a-8d69-eefccd013776,U02U5SW982W,,,Thanks <@U02V1JC8KR6> and <@U02UBQJBYHZ> - sorry didn\\'t mean to be rude. Only just saw these messages. I\\'m glad it helped. I\\'m still just running around like a mad chook :slightly_smiling_face:.,1645071297.194149,1645685399.423429,U02U5SW982W\\n398ccaa9-c9df-4081-b78c-0b1aafe0f062,U01AXE0P5M3,,,\"Hi, you can follow the following commands to create a new topic, `50cbe9e796c4` is my `confluentinc/cp-server:5.4.0` container id\\n```docker exec -it 50cbe9e796c4 bash\\ncd /usr/bin\\n/kafka-topics --create --topic demo.10 --bootstrap-server localhost:9092 --partitions 2```\\n\",1646686750.451479,1646816575.358089,U01DFQ82AK1\\n049df771-7747-4302-af23-558b6e0e1183,U01AXE0P5M3,,,\"u can check other scripts like\\n```root@broker:/usr/bin# ls | grep \"\"kafka\"\"\\nkafka-acls\\nkafka-broker-api-versions\\nkafka-configs\\nkafka-console-consumer\\nkafka-console-producer\\nkafka-consumer-groups\\nkafka-consumer-perf-test\\nkafka-delegation-tokens\\nkafka-delete-records\\nkafka-dump-log\\nkafka-leader-election\\nkafka-log-dirs\\nkafka-mirror-maker\\nkafka-preferred-replica-election\\nkafka-producer-perf-test\\nkafka-reassign-partitions\\nkafka-replica-status\\nkafka-replica-verification\\nkafka-run-class\\nkafka-server-start\\nkafka-server-stop\\nkafka-streams-application-reset\\nkafka-topics\\nkafka-verifiable-consumer\\nkafka-verifiable-producer```\",1646686750.451479,1646816609.301039,U01DFQ82AK1\\n44f4a9d8-d73e-4797-ae08-67506387e3bf,,12.0,,i have a challenge with my GCP . Can that affect my project in any way,1646817149.444839,1646817149.444839,U02AUCL9ZQF\\n68e7acfe-d126-47d2-a404-1d9bf17678d8,U02SMBGHBUN,,,It is an open ended solution. Feel free to implement a solution which gives you the most exposure to stream processing,1646807293.427729,1646819397.343039,U01DFQ82AK1\\n4ae34dae-de67-411f-af9f-7001a30a63ac,U02U5SW982W,,,I would suggest you to use virtual environments to have a clean start,1646792207.912569,1646819466.698369,U01DFQ82AK1\\nbb870d6e-1190-45d1-95ff-8ea189485654,U02AUCL9ZQF,,,What\\'s the challenge?,1646817149.444839,1646822213.501229,U01AXE0P5M3\\n8ef1fe00-e376-48d4-aa91-cf5ca89e33c7,U02R77FP943,,,I am on windows machine. I set \\'Full control\\' from folder Properties -&gt; Security -&gt; Permissions for Authenticated users,1646769777.750379,1646822363.639779,U02R77FP943\\nf9727534-e842-4873-bda3-00359018fcee,,13.0,,\"Good day everyone,\\n\\nI just started the course and I am trying to run the initial docker command to start up postgres but i am getting invalid reference format as the error. I have tried different things to no avail.\",1646822525.611199,1646822525.611199,U035VPYAEBY\\na0b440b0-0b64-4a33-86b8-0604006f6690,U02AUCL9ZQF,,,ive been trying to load my credit card but it does not go through. Ive tried getting help but all have failed. Without gcp i cannot setup terraform,1646817149.444839,1646823016.967859,U02AUCL9ZQF\\n97d13965-165d-49ef-bb9c-80138c0ff1ff,U02SMBGHBUN,,,any method. All the solutions are not covered in videos,1646802814.414629,1646823332.948749,U01DFQ82AK1\\n2544a2d8-913f-474a-b341-aba25d506b03,U02AUCL9ZQF,,,\"In principle you can do things locally, but it\\'ll look much better for your portfolio if you do it in the cloud. So maybe you can get a virtual card? Or use another cloud?\",1646817149.444839,1646823609.735429,U01AXE0P5M3\\nec0d8d4d-2af9-4944-bd9c-547f86beab75,U02AUCL9ZQF,,,Let me see if i can get a virtual card,1646817149.444839,1646823678.602869,U02AUCL9ZQF\\n414eb8cf-fb43-4664-b118-7b5fd4a0c70c,U02AUCL9ZQF,,,Any links that can help me get virtual cards ?\\',1646817149.444839,1646825171.741899,U02AUCL9ZQF\\naf58f0b4-cf84-4af9-9aef-8c8341bbf79e,,9.0,,\"Hi everyone, I just join the data engineering channel and start watching the Youtube contents.\\n(1) May I know where can I get instruction to install the Docker in Windows to follow the course properly?\\n(2) When we run  `docker built -t test:pandas` , is the \\'test\\' a filename and the \\'pandas\\' is the tag name? What is the purpose of using the semicolon \\':\\'?\",1646825218.588189,1646825218.588189,U02A3RTSE5D\\n8224345c-b9de-493e-b343-84f4981797b8,U02A3RTSE5D,,,Have you tried the docker documentation,1646825218.588189,1646825836.296389,U02AUCL9ZQF\\n03fff86a-7ca8-4928-9113-4c69d668a75a,U02A3RTSE5D,,,The docker documentaation has steps on how to install it on windows,1646825218.588189,1646825866.115929,U02AUCL9ZQF\\nc06bedff-e3d6-4517-b845-b0f0403ab92c,U02A3RTSE5D,,,\"Hi <@U02AUCL9ZQF>, do you mean I should install the Docker Desktop in my laptop as explained in this link,\\n<https://docs.docker.com/desktop/windows/>?\\n\\nCould you share some information for my second question in my first post, please?\",1646825218.588189,1646826169.240039,U02A3RTSE5D\\n0386a294-a0f9-4fad-a157-0eb95b29c742,U02AUCL9ZQF,,,\"It depends on the country where you live. I\\'d start with googling \"\"virtual bank card {country name}\"\"\",1646817149.444839,1646826475.521009,U01AXE0P5M3\\nef570c74-ae9a-44c2-884e-38c81d40d1ec,U02AUCL9ZQF,,,\"<@U01AXE0P5M3>, sorry to interrupt. Are we supposed to use the free-trial of the GCP to complete the project?\",1646817149.444839,1646826715.253569,U02A3RTSE5D\\n4e2cc6e8-2c31-4292-8107-176d40aa719e,U02AUCL9ZQF,,,\"You decide.  You don\\'t have to, it\\'s up to you what to use\",1646817149.444839,1646826734.068859,U01AXE0P5M3\\nf0b683fb-d39e-445c-a98c-08d81d822ed7,U02A3RTSE5D,,,\"<@U01AXE0P5M3>, could you share some information about my two questions? I do not think I can sleep if I cannot get answers today.\",1646825218.588189,1646826936.525059,U02A3RTSE5D\\n43f45ba8-7617-4977-8b9c-44e73e5f3244,U02A3RTSE5D,,,\"I don\\'t have instructions for windows. The link looks correct, it probably will work\",1646825218.588189,1646827288.349709,U01AXE0P5M3\\ne4e66123-e5df-45a2-bab1-f5b37beefce1,U02A3RTSE5D,,,Test is not the filename. This is the name of your docker image. The second part after the colon is a tag. Think of different versions of the same image,1646825218.588189,1646827354.668989,U01AXE0P5M3\\nb29e817d-9833-4335-80dd-c729f0f9e801,U035VPYAEBY,,,Have you checked the FAQ?,1646822525.611199,1646827386.969529,U01AXE0P5M3\\n49192ec8-5699-41d6-9422-5ea741f55c98,U02A3RTSE5D,,,\"Do you mean imagine it as ...\\nan image called \\'test\\' and its version called \\'pandas\\'?\\nWhere is the test\\' image file come from, Alexey?\\n\\nAs for the syntax, can we use dash (-) and not semicolon (:) by writing `docker build -t test-pandas`\\n?\",1646825218.588189,1646827765.503759,U02A3RTSE5D\\ne4b00756-eca9-4f86-86a0-0801786a3442,U02A3RTSE5D,,,In this case the image name will be test-pandas:latest,1646825218.588189,1646828268.414639,U01AXE0P5M3\\n7ed6afd4-fa1f-47d0-be5a-f96313b36be7,,4.0,,\"For the extra file from week 3 to directly put data into GCS without airflow, why does this iterate to 13 instead of 12 (12 months)? The files for month 13 have errors for me\\n```def web_to_gcs(year, service):\\n    for i in range(13):\\n        month = \\'0\\'+str(i+1)\\n        month = month[-2:]\\n        file_name = service + \\'_tripdata_\\' + year + \\'-\\' + month + \\'.csv\\'\\n        request_url = init_url + file_name\\n        r = requests.get(request_url)\\n        ... ```\",1646831287.988489,1646831287.988489,U02TVGE99QU\\n576480ba-e0b6-4688-bcb9-7e634993bb6c,,12.0,,\"Finally got kafka up and running, but when i try to run producer.py, it says it cannot find kafka. I have installed everything. I am on macOS. Do I need to fix path?\",1646836356.583069,1646836356.583069,U02UBQJBYHZ\\n8CA980F1-590B-41C9-AAA4-AEDA07723277,U02UBQJBYHZ,,,If you are running Kafka on docker compose there is no need ..are the producer application on the same folder ? Did u check localhost:9092 whether cluster is up and running ?,1646836356.583069,1646837041.700849,U02AGF1S0TY\\n713527ce-7b18-4090-8870-d4500ea7c258,U02UBQJBYHZ,,,Yes cluster is up and running.,1646836356.583069,1646837284.410229,U02UBQJBYHZ\\neaff51e7-dd55-47a3-a063-fac43e92fd69,U02UBQJBYHZ,,,\"The error is \"\"kafka\"\" not found in the line \"\"from kafka import KafkaProducer\"\"\",1646836356.583069,1646837365.441739,U02UBQJBYHZ\\n81c9ed9c-d530-4b3f-b235-75996c0d3bbb,U02UBQJBYHZ,,,`ModuleNotFoundError: No module named \\'kafka\\'`,1646836356.583069,1646837446.499459,U02UBQJBYHZ\\n4280520d-666f-434e-b4dc-1f733d80afee,U02UBQJBYHZ,,,Do I have to run from shell inside of docker?,1646836356.583069,1646837548.849679,U02UBQJBYHZ\\ne8bf93e6-8c28-4ea7-9f95-78314027b179,,,,Maybe I should start over - how do we set up kafka? Their web site is incomprehensible to me. I ran pip install confluent-kafka. Then I installed everything from requirements.txt. I can\\'t get it to run.,,1646837736.190219,U02UBQJBYHZ\\n950e1a0a-6705-42da-b7aa-1a0b9c657af2,,1.0,,\"I mean I can get it to run, the control center comes up on port 9092, but \"\"kafka\"\" is not recognized by zsh on macos.\",1646837802.797989,1646837802.797989,U02UBQJBYHZ\\nE4A9DBDE-BDDE-4995-A86D-7B22A50F1A9A,U02UBQJBYHZ,,,Did u install all the libraries in requirement.txt ,1646836356.583069,1646838867.777919,U01DFQ82AK1\\n97405657-2F71-4BE4-BD18-7B24C3BB5408,U02UBQJBYHZ,,,So it is not a docker issue? It is python library issue ,1646837802.797989,1646839040.736259,U01DFQ82AK1\\n649A5066-5685-446B-A5B6-BD7EF5486E94,U02UBQJBYHZ,,,\"Suggest following steps \\n\\nCd to directory where requirements.txt is located\\nActivate your virtual env \\nRun pip install -r requirements.txt in terminal \",1646836356.583069,1646840092.093909,U02AGF1S0TY\\n35afc9b5-6a58-428e-8f78-e97e4197d3e2,U02UBQJBYHZ,,,\"To create a virtual env and install packages (run only once)\\n\\n```python -m venv env\\nsource env/bin/activate\\npip install -r requirements.txt```\\nTo activate it (you\\'ll need to run it every time you need the virtual env):\\n\\n```source env/bin/activate```\\nTo deactivate it:\\n\\n```deactivate```\\nThis works on MacOS, Linux and Windows - but for Windows the path is slightly different (it\\'s `env/Scripts/activate`)\",1646836356.583069,1646841472.620539,U01AXE0P5M3\\nea6c6994-89e6-4cc2-b998-90defc211617,U02U5SW982W,,,\"maybe try doing it with virtual env?\\n\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1646841472620539?thread_ts=1646836356.583069&amp;cid=C01FABYF2RG>\",1646802335.624369,1646841630.163899,U01AXE0P5M3\\n1a8997b6-1702-4cbf-86df-38f0a9000c3a,U02AUCL9ZQF,,,ive tried my best but it cant work from my end. Is there anyone who can help me out,1646817149.444839,1646843594.454689,U02AUCL9ZQF\\na9dc32bd-0a77-477b-81de-73bfc11ed2f9,U02SMBGHBUN,,,<@U01DFQ82AK1> Thanks!,1646807293.427729,1646844778.493159,U02SMBGHBUN\\n6192db6e-7d25-405d-b952-feb446cac52d,U02SMBGHBUN,,,got it :thank_you:,1646802814.414629,1646844792.842869,U02SMBGHBUN\\nbe1df24c-ee9a-430b-b986-7c43591dc01c,,,,Hi everyone :slightly_smiling_face: I\\'m just starting the course.,,1646848118.202339,U035B3MB92M\\naed34231-49d9-4186-b031-bd8450e7ec6b,,3.0,,\"So the last activity on Faust was in October 2020 it seems :upside_down_face:\\n\\n<https://github.com/robinhood/faust/commits/master>\",1646850614.641829,1646850614.641829,U02TATJKLHG\\n25f5fb1a-ae0b-442b-84ca-06e3882d8851,U02U3E6HVNC,,,\"Oh no, I was hoping you could swoop in with the answer :laughing: Yes, this does seem to be a difficult question for those not experienced in Kafka!\",1646418691.258709,1646850726.581909,U02U3E6HVNC\\n9053DA8B-0C28-47EF-A7DC-686D22D31399,,2.0,,Please when is the deadline for the final project?? ,1646850921.471979,1646850921.471979,U02TBCXNZ60\\nE6D36B69-5C6A-452D-B734-7E599BCA97C2,U02TATJKLHG,,,I know. Unfortunately that is the best python lib I knew for Kafka. Best is to use the official jvm lib. ,1646850614.641829,1646853800.598499,U01DFQ82AK1\\nca71833a-92ed-44d7-a8cd-476a6fc262e5,U02TATJKLHG,,,Yeah no other options. So any particular reason why there are no good libraries despite Python being so widely used? Is there something that JVM offers that python can\\'t?,1646850614.641829,1646855268.121309,U02TATJKLHG\\n0a4f1918-b19e-4314-8d2c-42012e3e2c0c,U02T70K8T61,,,could be laptop issues . it went off since  that day.let me try again,1646673177.671119,1646858821.602419,U02T70K8T61\\n47f33155-3291-439c-b53e-676ef46aa74e,U035VPYAEBY,,,yes i have,1646822525.611199,1646859998.769429,U035VPYAEBY\\nbfd253d9-dc60-461f-a0cd-b5b86da362d3,U035VPYAEBY,,,can you show what you run and the error you get? no screenshots please,1646822525.611199,1646861383.246849,U01AXE0P5M3\\n8638283c-cd01-46ec-a658-7c45309604f1,U02U5SW982W,,,I had done that <@U01AXE0P5M3> - however this did not fix the problem. It was only happy when I installed an older version of kafka-python. I don\\'t know why robinhood is incompatible with the latest version of python-kafka? Thanks for trying to help though :),1646802335.624369,1646863699.109859,U02U5SW982W\\n5db1f8f1-2348-4b61-bd1f-78f5d5380b58,U02U5SW982W,,,\"Hi <@U01DFQ82AK1> I did do that - but the issue was with robinhood when I installed with pip it gives me an error:  `ERROR: robinhood-aiokafka 1.1.6 has requirement kafka-python&lt;1.5,&gt;=1.4.6, but you\\'ll have kafka-python 2.0.0 which is incompatible.` When I installed kafka-python 1.4.6 it was happy (at least for now)- but it seems weird that I cannot use the new version of kafka-python...\",1646792207.912569,1646863883.500129,U02U5SW982W\\n2b6d3e76-786c-4fe0-83d9-c7dc950bbf18,U02ULFC9N5P,,,\"March 27th is the tentative date, according to what Alexey said in the week 8 office hour video.\",1646775937.386769,1646865137.663869,U02U3E6HVNC\\n2df9f367-8569-4e17-a768-c4367557526f,U02TBCXNZ60,,,\"March 27th is the tentative date, according to what Alexey said in the week 8 office hour video.\",1646850921.471979,1646865159.772279,U02U3E6HVNC\\n3ea1ad17-8422-48a2-af1a-14391ec183be,,7.0,,\"I\\'m having some trouble understanding Kafka concepts, would love some recommendations of contents explaining more about it\",1646865736.378139,1646865736.378139,U02TC704A3F\\n564a71fb-94d1-4fb5-ab03-b1238adf7987,U02UBQJBYHZ,,,\"Let me know if this is correct, because now control-center is not coming up for me. 1. Restarted. 2. Open terminal window and cd to the directory with the week 6 files. 3. run the lines above to make the virtual env and install the requirements. 4. In the same window, run docker-compose up.\\nResult: all kinds of error messages from docker-compose. timeouts, telling me things are DEAD, and control-center doesn\\'t come up properly on localhost:9021. Should I do these steps in a different order?\",1646836356.583069,1646869247.163709,U02UBQJBYHZ\\nda3384ac-f6ec-4b68-b9a2-fb26c51cb84b,U02UBQJBYHZ,,,\"When i say restarted, I mean I restarted the computer.\",1646836356.583069,1646869263.583089,U02UBQJBYHZ\\n9af2dee6-9580-412e-bd7d-37d0e01ddaff,U02TC704A3F,,,<@U02TC704A3F> did you check this <http://www.gentlydownthe.stream/#/11>,1646865736.378139,1646878129.105749,U02T8HEJ1AS\\n3f5e2ee0-bf5c-42f0-81ba-3332850e75c8,U02TC704A3F,,,What is this?,1646865736.378139,1646878193.197099,U02TC704A3F\\na902893f-35f7-42be-ac0a-0e33a66b8d1f,U02TVGE99QU,,,range(13) is 0-13.,1646831287.988489,1646879913.572869,U033QG69HRQ\\nfb9aa9ff-6162-4467-82f4-8a9219faed7e,U02TVGE99QU,,,\"```&gt;&gt;&gt; list(range(13))\\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\\n\\n&gt;&gt;&gt; list(range(1, 13))\\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]```\",1646831287.988489,1646879928.787049,U033QG69HRQ\\n321c5647-048a-4c14-b741-4a20bd26ed2a,U02TC704A3F,,,It\\'s awesome <@U02TC704A3F> I\\'d highly recommend it - good recommendation <@U02T8HEJ1AS>,1646865736.378139,1646879959.653549,U02U5SW982W\\nc5a60bc1-c129-4208-86bd-eb78042c7ed8,U02TC704A3F,,,I like it because it is pretty cute but I think the metaphor works well too,1646865736.378139,1646880013.597199,U02U5SW982W\\nb4a5f48f-7013-4d7a-a338-89d42d18c19a,U02AUCL9ZQF,,,Not sure if anyone else can. It depends on the country you live in; you need to have a VISA/Mastercard enabled card.,1646817149.444839,1646889081.370119,U02HB9KTERJ\\n37cf1f5c-13c5-4fb0-8068-987b5208d609,U035VPYAEBY,,,so basically i am running the first docker command to start the postgres container.,1646822525.611199,1646895040.011009,U035VPYAEBY\\n89615be9-0d6f-4c65-95db-1a03847ebbd8,U035VPYAEBY,,,i am running the command straight from the github page,1646822525.611199,1646895053.517959,U035VPYAEBY\\n40b91557-cb76-4f9d-9072-cd3e210a695e,U035VPYAEBY,,,yet it says invalid reference,1646822525.611199,1646895059.307879,U035VPYAEBY\\n597221ed-42cd-480b-be16-342404d6a939,U035VPYAEBY,,,i have tried to debug by removing the volume tag thinking the issue is coming from there,1646822525.611199,1646895080.638509,U035VPYAEBY\\n6c43c467-40bc-45e8-8996-18da3ce0f1fd,U035VPYAEBY,,,it still persists,1646822525.611199,1646895085.496369,U035VPYAEBY\\nbf4c1236-565b-41a6-bd79-4da9f8e92f8d,U035VPYAEBY,,,Which OS you\\'re on?,1646822525.611199,1646895283.606969,U01AXE0P5M3\\n468cfb3f-b1ab-405d-9ea5-722e3264a51c,U02TVGE99QU,,,\"Since there\\'s +1, it indeed tries to process month 13. Can you please create a PR with a fix?\",1646831287.988489,1646895689.409129,U01AXE0P5M3\\na02caca5-b0d0-417e-bf83-808013e96194,U035VPYAEBY,,,MAC os,1646822525.611199,1646896074.635779,U035VPYAEBY\\nee2cff51-660f-4d33-b1ce-a2af325a8c5d,U035VPYAEBY,,,\"i sorted that out. It had to do with me using string quotes for POSTGRES_USER = \"\"root\"\". should be without string quotes\",1646822525.611199,1646896108.542129,U035VPYAEBY\\n61a88a0a-a6b5-42dc-bbfb-2aa2377646ba,U035VPYAEBY,,,the next issue is the -v is refusing to mount saying the folder is not shown to docker. added the folder to the file sharing tab as recommended on docker desktop but it hasnt worked yet. I am still figuring it out,1646822525.611199,1646896150.438449,U035VPYAEBY\\nbe15a642-d156-4946-8015-a5f46f3bd3fb,U035VPYAEBY,,,sorted. followed the FAQ and created a local volume and it worked,1646822525.611199,1646896539.439679,U035VPYAEBY\\n6d44e50c-c3e8-4803-a4e1-57a15e634f4b,U02SEH4PPQB,,,what do you mean? aside of the project for week 4?,1645061263.390639,1645087178.087489,U01B6TH1LRL\\n89d3eff3-df3f-4440-90fa-fcc6c662ea3c,U02T65GT78W,,,,1644957970.026469,1645087380.336179,U01B6TH1LRL\\n2a5fa8f0-41bf-456d-b9ba-0548a7c442b1,,4.0,,\"hello all,\\nHow exactly am I supposed to use the docker-setup file provided for week_4 as there is little reference to it on the vdeos\\nI am working on a VM.\\nPlease clarify.\",1645088736.844399,1645088736.844399,U02V90BSU1Y\\nbd0b8d71-1284-4def-b8a8-3be0273bb147,U02V90BSU1Y,,,\"It\\'s only another option, you can use dbt cloud as shown in the videos, install dbt locally or use a docker image\",1645088736.844399,1645088795.267559,U01B6TH1LRL\\nd7bcaaf1-adf2-49d3-9c3d-edc80fd2ceaf,U02V90BSU1Y,,,\"will I have all the required dependencies if I just install dbt locally on a VM using the below command?\\n\\npip install \\\\\\n  dbt-core \\\\\\n  dbt-postgres \\\\\\n  dbt-redshift \\\\\\n  dbt-snowflake \\\\\\n  dbt-bigquery\\n\\nNote: The below code in docker-compose file references the service_account credentials, how do I achieve same with local installation?\\n\\n- ~/.google/credentials/google_credentials.json:/.google/credentials/google_credentials.json\",1645088736.844399,1645089146.121089,U02V90BSU1Y\\ne0e52b7a-8fa7-49ec-8f3a-01f79217344b,U02V90BSU1Y,,,\"You can install locally all dependencies, but there\\'s no need to. Install only dbt-bigquery adapter. Links to the official instructions on how to proceed with a local installation are in the readme\",1645088736.844399,1645089477.558149,U01B6TH1LRL\\ndc5c4872-9ca6-4d79-baed-162007980d8c,U02SEH4PPQB,,,\"Hi <@U01B6TH1LRL>,  I meant to ask if the modelling we have done in week 4 represents a star/snowflake schema or it is a different procedure?\",1645061263.390639,1645091648.474309,U02SEH4PPQB\\n9e67f3e0-add5-405d-94fd-82c052fe7951,U02UZ493J56,,,\"<@U01B6TH1LRL> Creating tables from external ones solved my problem of \"\"globbing\"\", however it caused another problem. The count number for the fact_trips was way too low compared to your results in the video. So I added the \"\"viewer\"\" role to the service account, and used the external tables directly. It changed the number to 61,604,279. This is still lower than your number. Why do you think this is happening? I am using the same dbt files as yours. I am not sure whether BigQuery making changes in the tables while creating them, like removing duplicates etc. Any help would be appreciated.\",1644861631.604849,1645091812.935159,U02UVKAAN2H\\ncadf9397-daed-4a0b-96aa-3cc028a6e1be,U02UJGGM7K6,,,\"This is the current W4Q3 description  - so you do not need to bother yourself with filtering the dataset (of course if your `fhv_trips` fact table consists of 2019 data only).\\n\\nBTW: If you read this and have different counts for W4 Q3/Q4/Q5 then make sure your models are correct - I\\'ve been there, too but finally located the issue and got correct results.\",1644759646.413779,1645092979.620669,U02UKBMGJCR\\n1f5e4996-8b97-43d1-a2f9-fe5182c134c6,U02S9JS3D2R,,,\"Same here.\\n`green_external` - 7,778,101\\n`yellow_external` - 109,047,518\\n`fact_trips` - 61,602,987\\n\\n`stg_*` distribution 10.2% GREEN / 89.8% YELLOW\\n\\nDoes numbers and GBs processed in your DBT log look the same?\",1645039735.829189,1645094966.845119,U02UKBMGJCR\\n32f3ff40-5a94-4503-8219-cfc8566f1725,U02S9JS3D2R,,,<@U02T9550LTU> Your number is very close to the one in the videos. How did you create your green_tripdata and yellow_tripdata files in BQ? I am not sure whether BQ makes some changes during table creation that I don\\'t realize.,1645039735.829189,1645095087.195289,U02UVKAAN2H\\n,USLACKBOT,2.0,tombstone,This message was deleted.,1645100199.551459,1645100199.551459,USLACKBOT\\n11e63f36-48d7-47c3-912a-fc38db359cf5,U026040637Z,,,\"<@U02TC704A3F>: I tried doing that but it gave me a different error `Dataset project:production not found`. I undid that, and instead tried <@U02ULMHKBQT>’s suggestion and that worked. Thanks to the both of you!\",1645063164.448789,1645104944.029739,U026040637Z\\n2d46f5ef-2557-47f9-9d86-fb83581d512c,U02TNEJLC84,,,\"<@U02TC704A3F> hello! I have that 404 problem with US as default server. I guess my server is europe-west or europe-west6, but setting it so doesn\\'t help. What is the convention for setting a location, or is there a page that explains this issue?\",1644973280.026019,1645105174.247079,U02UECC4H6U\\nd53f5aea-52e8-46f3-a620-df7307530b4f,,5.0,,I seems to be having permission issues for my data_ingestion_gcs_dag. Can anyone kindly help or point me to the solution?,1645106096.548269,1645106096.548269,U02SZARNXUG\\n7602279c-4165-477d-ab6f-0221caa48708,U02TNEJLC84,,,\"Oh well, I read through the threads above and rerun with `europe-west6` as a server location, and it worked. Its either dbt cloud didn\\'t update configs or I messed something up, but it\\'s resolved now\",1644973280.026019,1645106692.746109,U02UECC4H6U\\n7de58f8c-4fc2-4088-89bb-2c8b8005a488,U02SZARNXUG,,,Can you be a bit more specific? What \\'permission\\'?,1645106096.548269,1645108103.659279,U02HB9KTERJ\\na26fdd6c-ae75-400b-b603-5a164e74a9f1,USLACKBOT,,,*ml-zoomcamp*? Either you are in the wrong slack channel or wrong repository...,1645100199.551459,1645108173.949969,U02HB9KTERJ\\n8c3b0689-1b40-4f65-a2bc-36cf1dfedf19,U02SZARNXUG,,,,1645106096.548269,1645110238.955669,U02SZARNXUG\\n7fe6babe-506f-42ad-a3d8-1533571d5cb2,U02SZARNXUG,,,<@U02HB9KTERJ> seems the issue is the specified service account. Looks strange. That\\'s not my service account I think,1645106096.548269,1645110282.715789,U02SZARNXUG\\n6118d1c1-c22a-4014-a527-1fb9e278ea83,U02SZARNXUG,,,\"I\\'m using a VM and checking IAM there service account in the error message isn\\'t my VM\\'s service account, although the service account in the error message is listed as one of my service accounts\",1645106096.548269,1645110560.192479,U02SZARNXUG\\n5bae218a-c419-49b6-85b8-83e9e39a6c14,USLACKBOT,,,oh.. sorry,1645100199.551459,1645112662.388869,U02DFL7BT5W\\na6298ea4-ea96-4c1a-ac90-fc8cfe9d938d,U02SZARNXUG,,,Resolved! It\\'s the service account. I authenticated with a different key,1645106096.548269,1645114348.477269,U02SZARNXUG\\n924a768f-3086-4dee-abc7-dc283719c8fb,U02S9JS3D2R,,,\"61,635,405 for me, still doesn\\'t match any of the HW choices nor videos, as well as Vic\\'s count is not in the answers :thinking_face:\",1645039735.829189,1645116564.843949,U02UECC4H6U\\n64324b62-2ca1-4431-b30a-c77652798fac,U02U3E6HVNC,,,\"I have a silly question related, if I write a query for the number of row in bigquery I get to a number very close to what is available (with a few row difference but I assume it is related to a similar issue from the other week).\\nHowever if I run the production query I made, I am missing like 5m rows and I don\\'t quite see where the issue is happening though it must be obvious.\\nThe fact_fhv_trips is just like for fact_trips except that we are only using fhv data but otherwise it should be almost an identical query right?\",1645062231.005189,1645120154.938079,U02TC8X43BN\\n918c1789-e273-4aa2-be60-635c8636f24a,U02U3E6HVNC,,,\"Maybe it is related to the row_number() over, I am not too sure if I understand fully its purpose. Is it a way to make sure there are no duplicates in the data?\",1645062231.005189,1645120498.675729,U02TC8X43BN\\n81978708-35ee-4628-b775-27e968a8ab68,U02S9JS3D2R,,,<@U02T9550LTU> Thanks for the reply. I used csv files from the bucket which I filled  using terraform. I will try with your way now.,1645039735.829189,1645123155.374099,U02UVKAAN2H\\ne507fc01-3bcf-4731-a0dc-f4020d3c6b84,U02U3E6HVNC,,,\"I assume the production dataset has fewer rows because of removed duplicates, yes. I\\'m curious what other people used as their primary key, because that could have a big effect on that.\\nAnd yeah none of my counts match exactly to the homework answers :sweat_smile: I can\\'t decide if I want to put more time in this, or just put in my best guesses...\",1645062231.005189,1645123861.624969,U02U3E6HVNC\\nbb391edc-3d07-43b5-a260-bed8c6eb97ad,U02U3E6HVNC,,,Ok after I removed the restriction on row number I got an answer that matches one of the options.,1645062231.005189,1645124270.182499,U02TC8X43BN\\n56bdb4eb-1b6c-4584-8076-5279a5bca62f,U02U3E6HVNC,,,Hmm... Do you mind sharing what fields you had referenced in that restriction?,1645062231.005189,1645124834.807019,U02U3E6HVNC\\n6b9c0cc0-4daa-4d76-bb34-5d246dd37169,,2.0,,\"It seems I messed up with my service accounts. Trying to move data from my GCS bucket to the Data Warehouse in BigQuery. And I\\'m getting this error:\\n\\n`\\n```<mailto:dtc-de-user@healthy-fuze-339218.iam.gserviceaccount.com|dtc-de-user@healthy-fuze-339218.iam.gserviceaccount.com> does not have storage.objects.list access to the Google Cloud Storage bucket.```\\nWhat permissions do I need to add to my service account? I checked and can\\'t find specifically the Google Cloud Storage bucket permissions.\",1645126441.317339,1645126441.317339,U02RSAE2M4P\\nfb08354c-a2c8-4a3d-83a6-ee7ca8bebfa4,U02U3E6HVNC,,,\"Well I was just copy pasting the code of victoria where she do a:\\nwhere rn=1\\nand I removed it and I find an answer that matches one of her possibilities.\\nbut I am not sure it is the right answer, it kind of made sense to remove them in the first place, however without that restriction i gained 5m rows (unless I screwed something else)\",1645062231.005189,1645129647.730579,U02TC8X43BN\\n7348fd7d-188c-4e13-8a2f-a0fa324b550f,,1.0,,Its dbt cloud down?,1645131513.585449,1645131513.585449,U02T0CYNNP2\\nd1e5c364-3b7f-4132-bc13-5cecb03a2c70,U02T0CYNNP2,,,<https://status.getdbt.com>,1645131513.585449,1645133243.469869,U02UKBMGJCR\\n27be9470-6e43-4b3a-87e5-0bbe26a4d3b0,U02TJ69RKT5,,,\"I tried to reconfigure all, and write again the models. But I don\\'t know why I obtain always the same error.\\nWhen I run the queries in BigQuery all goes good. :smiling_face_with_tear:\",1644968876.461069,1645138115.803799,U02TJ69RKT5\\n5cc31337-305a-4fcc-9d49-8e0fb114a900,,10.0,,\"Does anyone got this error, while running the `fact_trips.sql` model.\\n```Permission denied while globbing file pattern.```\",1645138166.157909,1645138166.157909,U0297ANJF6F\\n382251df-5f97-4cee-a961-dc414db30d9f,U0297ANJF6F,,,You must Add the Role Viewer to the account service,1645138166.157909,1645139276.647289,U02DBNR22GN\\n0c1eb73f-595c-4b20-a4b3-eed8fae3c690,U0297ANJF6F,,,\"I think this error is misleading. I\\'m getting the same.\\n```Database Error in model fact_trips (models/core/fact_trips.sql)\\n  Access Denied: BigQuery BigQuery: Permission denied while globbing file pattern.\\n  compiled SQL at target/run/ny_taxi_rides/models/core/fact_trips.sql```\\nSo okay, I know I granted the permissions correctly. I\\'ll just run it in BigQuery directly. Then I get.\\n```Cannot process data across locations: us-east1,US```\\n\",1645138166.157909,1645142662.942159,U02TNEJLC84\\n86ccf998-047c-4883-ae00-d493d4fdc777,U02RSAE2M4P,,,Cloud Storage object or Cloud Storage Admin for full access,1645126441.317339,1645143144.249089,U02TA7FL78A\\na3f7e996-2b1e-406d-b4fb-85380359921e,U02S9JS3D2R,,,\"Okay I rebuilded some stuff and now my numbers are:\\n• fact_trips for 2019 and 2020 pickup dates - 61567786;\\n• Proportion - yellow-&gt; 56100630, green -&gt; 6304783;\\n• count of records in stg_fhv_tripdata - 42032656;\\n• Rows in fhv_tripdata_2019 - 22,667,698.\\nFILE TYPE - Parquet, all of them uploaded to Cloud Storage using Airflow DAGs.\\n\\nExternal tables created in Big Query.\\n\\nIn DBT query, \\'where rn = 1\\' was disabled for FHV data.\\n\\nThat\\'s it.\\n\\nDon\\'t know much what to do next. Maybe rebuild using CSV files?\",1645039735.829189,1645143548.109469,U02TC704A3F\\nf8e8a490-e5a7-4211-bf68-d2fd8651e668,U0297ANJF6F,,,\"Have to put this down for the night. What I\\'ve come up with is that my external tables are using Parquet files that are stored in a different location. I tried to give my service account admin rights on the bucket, but still no dice. I tried to make \"\"materialized\"\" tables from the external tables, but Big Query didn\\'t like that. My next course of action is to create a second bucket, set it\\'s location properly and copy over the files from the original bucket. I have the second bucket created, but I am struggling to copy the files. If you know of an easy way to do this please share. Otherwise I\\'m just going to update my airflow script tomorrow and rerun. Good luck and good night friend.\",1645138166.157909,1645145630.422529,U02TNEJLC84\\n5acda9e0-900e-4a7f-9c7e-e487a57e14a8,,4.0,,\"``` CREATE TABLE has columns with duplicate name locationid at [8:6]```\\nHW 4 Q4 I feel like this should be an easy fix... but can\\'t figure it out rn. I was trying to follow how we created the fact_trips using join but i\\'m getting this error... my sql statment is attached in the thread, any help would be appreciated\",1645148944.544799,1645148944.544799,U02T9550LTU\\n9814898d-d191-4d41-bc39-54da9fc9df93,U02T9550LTU,,,,1645148944.544799,1645148975.172479,U02T9550LTU\\nf1cb9349-fc19-4387-8f1e-5407407a2a37,U02T9550LTU,,,\"Works in bigquery... but logical error where there\\'s two location id\\'s which i circled... pretty sure its an issue with my SQL, anyone have anything to point me in the right direction?\",1645148944.544799,1645149068.416369,U02T9550LTU\\n84845c18-fddf-4679-8c90-356399259e62,,1.0,,\"Week 4 - DE Zoomcamp 4.3.1 - Build the First dbt models - compilation error. Hi All, I know this was asked previously by <@U02TNEJLC84> but I\\'m not sure if there was a resolution to this at all. I have the exact same Compliation Error in my model. I believe I have followed the steps exactly as outlined by Victoria in her video and made all of these steps explicit in my <https://learningdataengineering540969211.wordpress.com/2022/02/18/week-4-de-zoomcamp-4-3-1-build-the-first-dbt-models-part-1/|blog> for the course for this week. However, it still is complaining with pretty much exactly the same message as what <@U02TNEJLC84> reported. Any ideas on where I might go to resolve this issue? I thought all of my naming was fine but obviously it cannot find it...\",1645156164.293259,1645156164.293259,U02U5SW982W\\n7a015719-0bb1-44f6-ac72-10c965bf0b96,U02U5SW982W,,,Rookie error guys. I had copy pasted and didn\\'t realise that I hadn\\'t changed the \\'models\\' to \\'sources\\' in the .yml file :woman-facepalming:,1645156164.293259,1645156542.627009,U02U5SW982W\\n440540ea-4158-4133-99c4-244fbbf7256f,,,,\"Week 4 - Build the first dbt model - location error. Hi All, I know that <@U02ULMHKBQT> raised this a few days ago but I\\'m playing catch-up - as per usual. I\\'ve faced the same problem. I\\'m using dbt Cloud so no access to profile.yml. I originally got <@U02ULMHKBQT>\\'s location error about not being in US. I then went in to my settings and changed it to be where my GCP datasets are \\'australia-southeast1\\' and now it\\'s complaining that it can\\'t find it in this location either!. I\\'ve <https://learningdataengineering540969211.wordpress.com/2022/02/18/week-4-de-zoomcamp-4-3-1-build-the-first-dbt-models-part-1/|blogged> all my steps - I may have gone astray somewhere but I\\'m at a loss. Any ideas?\",,1645163008.873689,U02U5SW982W\\nEAA4A7D9-5A1A-44AC-82B4-539367A2D585,U0297ANJF6F,,,\"I\\'m having the same issue. If it was due to permissions on bq why did green_stg and yellow_stg model ran successfully? I am calling it a night, will update you guys if I get a chance to work on it tomorrow. Have a good one!!\",1645138166.157909,1645163392.838909,U02UM74ESE5\\nf26142c1-fb7d-430d-b4e2-d970fa12a582,U02S9JS3D2R,,,61604282,1645039735.829189,1645166315.001469,U02T0CYNNP2\\nbfca466f-2de3-4602-be5b-276f50711884,U02TJ69RKT5,,,What model exactly are you running? Did you get to find the compiled code under the target folder?,1644968876.461069,1645168378.241859,U01B6TH1LRL\\n15e3f057-e841-4f1b-8b77-f4725abad9a6,,4.0,,\"morning :slightly_smiling_face: in `Question 1: What is the count of records in the model fact_trips after running all models with the test run variable disabled and filtering for 2019 and 2020 data only (pickup datetime)`  I get a result which is close to three of the options, but non exactly the same. I followed the video of Victoria carefully and checked everything again, the only problem might be again due to the format of the data coming from GCS (in my case I have yellow as parquet and green as csv), otherwise I can\\'t explain why. How do we deal with that if it happens?\",1645171121.759679,1645171121.759679,U02UA0EEHA8\\n3f6060c7-d7da-4159-af10-610c8f9d5a85,U0343G3CH32,,,thanks Alexey :slightly_smiling_face:,1645774906.471079,1645785072.372979,U0343G3CH32\\n2b37600a-010e-4ab5-bec5-3f09a77c1714,U02U5SW982W,,,Are you guys ok? I hope the damage was minimal.,1645760064.750309,1645785181.368229,U02BVP1QTQF\\n42e34342-6641-482b-9b87-8ec1b1d3157a,U02T941CTFY,,,I followed Alexey\\'s instructions closely but I couldn\\'t get it to work by nature,1645512707.679389,1645791007.666479,U02T941CTFY\\nb18f785c-6c13-472c-ad51-bb37494a2d4e,,1.0,,\"I\\'ve been maintaining a git repo with all the Spark related notes, questions and guidelines for over a couple of years. If you want to learn, refresh or find resources to any Spark topics, you can look at this guide. Also, you can share some feedback if you think there is room for improvement :smile:\\n\\nThought I\\'d share this since we are well into week 5 and some of the folks might want to deep dive a little.\\n\\n<https://github.com/ankurchavda/SparkLearning>\",1645791753.206819,1645791753.206819,U02TATJKLHG\\n235db770-2460-431c-bbb9-94fbadbbec4f,,14.0,,\"(SOLVED - answer in the threat - and added to the FAQ)\\nHello,\\nHope everyone is doing well.\\nI have installed Java, Hadoop, Spark and Pyspark on my windows machine and I can run pySpark on my command line and anaconda prompt. But then I can\\'t run the jupyter notebook script to test it. It gives a java error (error in the thread)\\n\\nDid anyone who installed pyspark in windows have the same problem?\",1645800493.618169,1645800493.618169,U02R09ZR6FQ\\n63b8e456-2abe-4b5d-81c8-11d68ec9bdae,U02R09ZR6FQ,,,\"The script:\\n```import pyspark\\nfrom pyspark.sql import SparkSession\\n\\nspark = SparkSession.builder \\\\\\n    .master(\"\"local[*]\"\") \\\\\\n    .appName(\\'test\\') \\\\\\n    .getOrCreate()\\n\\ndf = spark.read \\\\\\n    .option(\"\"header\"\", \"\"true\"\") \\\\\\n    .csv(\\'taxi+_zone_lookup.csv\\')\\n\\ndf.show()```\",1645800493.618169,1645800522.678249,U02R09ZR6FQ\\nd6bfb294-0aac-4156-82c9-7ce92a79efac,U02R09ZR6FQ,,,\"The output:\\n```RuntimeError                              Traceback (most recent call last)\\nInput In [2], in &lt;module&gt;\\n      1 import pyspark\\n      2 from pyspark.sql import SparkSession\\n----&gt; 4 spark = SparkSession.builder \\\\\\n      5     .master(\"\"local[*]\"\") \\\\\\n      6     .appName(\\'test\\') \\\\\\n      7     .getOrCreate()\\n      9 df = spark.read \\\\\\n     10     .option(\"\"header\"\", \"\"true\"\") \\\\\\n     11     .csv(\\'taxi+_zone_lookup.csv\\')\\n     13 df.show()\\n\\nFile c:\\\\users\\\\gustavo\\\\appdata\\\\local\\\\programs\\\\python\\\\python39\\\\lib\\\\site-packages\\\\pyspark\\\\sql\\\\session.py:228, in SparkSession.Builder.getOrCreate(self)\\n    226         sparkConf.set(key, value)\\n    227     # This SparkContext may be an existing one.\\n--&gt; 228     sc = SparkContext.getOrCreate(sparkConf)\\n    229 # Do not update `SparkConf` for existing `SparkContext`, as it\\'s shared\\n    230 # by all sessions.\\n    231 session = SparkSession(sc)\\n\\nFile c:\\\\users\\\\gustavo\\\\appdata\\\\local\\\\programs\\\\python\\\\python39\\\\lib\\\\site-packages\\\\pyspark\\\\context.py:392, in SparkContext.getOrCreate(cls, conf)\\n    390 with SparkContext._lock:\\n    391     if SparkContext._active_spark_context is None:\\n--&gt; 392         SparkContext(conf=conf or SparkConf())\\n    393     return SparkContext._active_spark_context\\n\\nFile c:\\\\users\\\\gustavo\\\\appdata\\\\local\\\\programs\\\\python\\\\python39\\\\lib\\\\site-packages\\\\pyspark\\\\context.py:144, in SparkContext.__init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\\n    139 if gateway is not None and gateway.gateway_parameters.auth_token is None:\\n    140     raise ValueError(\\n    141         \"\"You are trying to pass an insecure Py4j gateway to Spark. This\"\"\\n    142         \"\" is not allowed as it is a security risk.\"\")\\n--&gt; 144 SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\\n    145 try:\\n    146     self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\\n    147                   conf, jsc, profiler_cls)\\n\\nFile c:\\\\users\\\\gustavo\\\\appdata\\\\local\\\\programs\\\\python\\\\python39\\\\lib\\\\site-packages\\\\pyspark\\\\context.py:339, in SparkContext._ensure_initialized(cls, instance, gateway, conf)\\n    337 with SparkContext._lock:\\n    338     if not SparkContext._gateway:\\n--&gt; 339         SparkContext._gateway = gateway or launch_gateway(conf)\\n    340         SparkContext._jvm = SparkContext._gateway.jvm\\n    342     if instance:\\n\\nFile c:\\\\users\\\\gustavo\\\\appdata\\\\local\\\\programs\\\\python\\\\python39\\\\lib\\\\site-packages\\\\pyspark\\\\java_gateway.py:108, in launch_gateway(conf, popen_kwargs)\\n    105     time.sleep(0.1)\\n    107 if not os.path.isfile(conn_info_file):\\n--&gt; 108     raise RuntimeError(\"\"Java gateway process exited before sending its port number\"\")\\n    110 with open(conn_info_file, \"\"rb\"\") as info:\\n    111     gateway_port = read_int(info)\\n\\nRuntimeError: Java gateway process exited before sending its port number```\",1645800493.618169,1645800549.023939,U02R09ZR6FQ\\n00232098-eed0-41f3-b4c1-42d7088177ac,U02R09ZR6FQ,,,\"I wasn\\'t able to define the variables through  `export` commands on bash (it ran but did nothing) so I define that manually.\\nThat being said, I didn\\'t define the PYTHONPATH since the 2nd line overwrites the first one:\\n```export PYTHONPATH=\"\"${SPARK_HOME}/python/:$PYTHONPATH\"\"\\nexport PYTHONPATH=\"\"${SPARK_HOME}/python/lib/py4j-0.10.9-src.zip:$PYTHONPATH\"\"```\",1645800493.618169,1645800795.828229,U02R09ZR6FQ\\n952a7e65-a18a-4105-a3f9-4fd152478191,U02R09ZR6FQ,,,I have searched the error but haven\\'t found a working solution for my case,1645800493.618169,1645800964.563019,U02R09ZR6FQ\\na6ebed71-0f1e-4dee-a24b-c1e5a33cb0d9,U02R09ZR6FQ,,,\"When you run\\n```pyspark.__file__```\\nin your notebook what is the output?\",1645800493.618169,1645801663.620589,U02TNEJLC84\\n0169cb4e-f7d8-4942-9ee4-6f83938c3f21,U02R09ZR6FQ,,,\"I have just solved it by adding\\n```import findspark\\nfindspark.init()```\\nto the start of the script!\",1645800493.618169,1645801749.767439,U02R09ZR6FQ\\n33ee7a24-6ca5-4f96-9880-a55ed672dd2a,U02R09ZR6FQ,,,\"And <@U02TNEJLC84> the output is:\\n```\\'c:\\\\\\\\users\\\\\\\\gustavo\\\\\\\\appdata\\\\\\\\local\\\\\\\\programs\\\\\\\\python\\\\\\\\python39\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\pyspark\\\\\\\\__init__.py\\'```\\nIs it correct?\",1645800493.618169,1645802288.880719,U02R09ZR6FQ\\n3413fd94-9b4a-4658-ab0e-fe05232a3e04,U02R09ZR6FQ,,,On one of the attempts I installed pyspark using pip,1645800493.618169,1645802365.810229,U02R09ZR6FQ\\nde7a113c-8209-4e72-aad9-062897b89cbc,U02R09ZR6FQ,,,\"Hey Gustavo. This might be long. I\\'ve installed spark for other courses so didn\\'t install at first and everything worked fine. I saw HW Q1 was to get the version of Spark so I went ahead and reinstalled following the instructions. Then I got a similar message, not the same one, but similar. I went through my bashrc file and saw that I had spark installed in /opt and in my home directory. I had a bunch of other exports from previous projects so I went through them commenting them out and trying to rerun my not book again. Through a combination of that and deleting pyspark in my site-packages I was able to get it to run normally again.\\nSo I think your issue is similar. Basically between spark, java and pyspark the components are quite lining up or where they should be according to the configuration. I plan to dig into my issue a bit more if I have time just because I like to get better understanding of stuff like this.\\nOut of curiosity, did you download the Oracle version of Java and set JAVA_HOME?\",1645800493.618169,1645802928.119349,U02TNEJLC84\\nfb3e4817-31d8-45e4-91f3-409ec61efbff,U02R09ZR6FQ,,,\"Thank you for the input Michael. Yes, I did download the Oracle version of Java and (manually) set JAVA_HOME\",1645800493.618169,1645803113.321369,U02R09ZR6FQ\\nfd382c16-430a-4cca-89cb-78c44fbc33ae,U02R09ZR6FQ,,,\"Well, so I might not be using the correct pyspark installation (from what I understand) -  I\\'ll have to check it too\",1645800493.618169,1645803224.883249,U02R09ZR6FQ\\n,,2.0,,\"Hello again,\\n(It might be a simple question but...)\\nDoes anyone know how to manually add these PYTHONPATHS in the windows\\' environmental variables? I don\\'t understand what is the `$PYTHONPATH` at the end of the export path\",1645803682.736359,1645803682.736359,U02R09ZR6FQ\\n46a3d41a-3335-4817-9460-72b6d50f49a8,U02R09ZR6FQ,,,\"Off topic and not sure what spec your PC is, but years ago I downloaded VMWare Player to spin up VMs. I believe Windows has an evaluation iso too. Just in case you ever get curious about stuff like this like I do you can spin up a VM try different things and see what works. If it all blows up, it\\'s cool cause it\\'s just a VM. Delete it and spin up a new one and try something else.\",1645800493.618169,1645803987.268339,U02TNEJLC84\\nd6848923-0cd0-48b9-986c-5b53f9e1d340,U02R09ZR6FQ,,,\"Thank you for the suggestion, with more time I might give it a try\",1645800493.618169,1645804484.843129,U02R09ZR6FQ\\nF5867615-F0D3-4A14-8BF4-182813B0866D,U02R09ZR6FQ,,,Do you need hadoop for the lesson?,1645800493.618169,1645805875.790659,U02U6DR551B\\n89818210-83F3-4883-903C-855394A45BE1,,1.0,,Do you need Hadoop for week 5?,1645806242.319589,1645806242.319589,U02U6DR551B\\nf5f71f84-4457-4688-a403-846155a09002,,13.0,,Any particular reason why Alexey is not using the `inferSchema` option in the spark dataframe reader to read csv files with suggested schema rather than pandas?,1645807526.556439,1645807526.556439,U02TATJKLHG\\na0a9e9ed-efef-427e-bbc7-a273a06609bc,U02TATJKLHG,,,Does it work for you? For me it inferred everything as string,1645807526.556439,1645807689.409559,U01AXE0P5M3\\nfa31917a-55bc-474d-bf18-a99772554517,U02TATJKLHG,,,\"Oh okay. I tried this with the high volume data and got this after inferring the schema.\\n\\n```root\\n |-- hvfhs_license_num: string (nullable = true)\\n |-- dispatching_base_num: string (nullable = true)\\n |-- pickup_datetime: string (nullable = true)\\n |-- dropoff_datetime: string (nullable = true)\\n |-- PULocationID: integer (nullable = true)\\n |-- DOLocationID: integer (nullable = true)\\n |-- SR_Flag: integer (nullable = true)```\",1645807526.556439,1645807908.594589,U02TATJKLHG\\n0be17b78-5943-49e1-a945-5e4fdbcfb25b,U02TATJKLHG,,,\"I also tried for green data and got this -\\n\\n```root\\n |-- VendorID: integer (nullable = true)\\n |-- lpep_pickup_datetime: string (nullable = true)\\n |-- lpep_dropoff_datetime: string (nullable = true)\\n |-- store_and_fwd_flag: string (nullable = true)\\n |-- RatecodeID: integer (nullable = true)\\n |-- PULocationID: integer (nullable = true)\\n |-- DOLocationID: integer (nullable = true)\\n |-- passenger_count: integer (nullable = true)\\n |-- trip_distance: double (nullable = true)\\n |-- fare_amount: double (nullable = true)\\n |-- extra: double (nullable = true)\\n |-- mta_tax: double (nullable = true)\\n |-- tip_amount: double (nullable = true)\\n |-- tolls_amount: double (nullable = true)\\n |-- ehail_fee: string (nullable = true)\\n |-- improvement_surcharge: double (nullable = true)\\n |-- total_amount: double (nullable = true)\\n |-- payment_type: integer (nullable = true)\\n |-- trip_type: integer (nullable = true)\\n |-- congestion_surcharge: double (nullable = true)```\",1645807526.556439,1645811288.314639,U02TATJKLHG\\nf69c0528-d020-44f7-982c-a75d19190224,U02TATJKLHG,,,Looks nice. Maybe it\\'s worth adding a note about it. Can you please create a PR?,1645807526.556439,1645811499.920099,U01AXE0P5M3\\n96327760-2715-47f2-9e86-6436b8c6ae84,U02TATJKLHG,,,Sure I can :smile:. Where do you want me to add the note?,1645807526.556439,1645811563.303489,U02TATJKLHG\\nc70288f8-759b-4e24-afbf-bc060ee70bba,U02TATJKLHG,,,In the README file near the video I suppose,1645807526.556439,1645811668.059209,U01AXE0P5M3\\n4929425d-7523-4c51-8c4d-bc5deae3803d,U02TATJKLHG,,,Sure,1645807526.556439,1645811729.529499,U02TATJKLHG\\nc4a9bef1-d368-4ab8-b4d7-6a3f3a99ee11,U02TATJKLHG,,,\"Raised a PR\\n\\n<https://github.com/DataTalksClub/data-engineering-zoomcamp/pull/100>\",1645807526.556439,1645812940.900749,U02TATJKLHG\\n4a06fd59-8ca5-4928-95fb-df0eb9f067ea,U02TATJKLHG,,,\"Can you add something similar to the FAQ? Like, \"\"Do I really need to use the inferschema option in Spark?\"\"\",1645807526.556439,1645815364.284959,U02CD7E30T0\\n84473349-3147-4a09-b105-5f9693680078,,2.0,,Hi. I am new to the course and I just started watching the videos and following the examples from week 1. I want to download the data but it seems the link is missing from the Github repo. Who can help with the data?,1645816959.816319,1645816959.816319,U034VEFGMSM\\neec73291-12b3-48f7-800e-50dab0308b76,U02TATJKLHG,,,This is awesome!! thank you!!,1645791753.206819,1645817103.294149,U02T9550LTU\\n8cbe66ce-949d-4070-a5ac-bd1378c13d7c,,2.0,,Shall we use Google Colab for week 5. I am having hard time on downloading the datasets in local setup?,1645818891.617119,1645818891.617119,U02S9JS3D2R\\ne83eecf7-184d-4a2f-b4b7-65fafd453220,U034VEFGMSM,,,<https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup/2_docker_sql#ny-trips-dataset|https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup/2_docker_sql#ny-trips-dataset>,1645816959.816319,1645819273.653239,U01AXE0P5M3\\n289df20e-bf99-4ef7-980f-b4929152bcad,U02S9JS3D2R,,,\"Does it have pyspark? If yes, it should be fine\",1645818891.617119,1645819304.504589,U01AXE0P5M3\\n2e2e7040-0c13-4ed4-b675-decc2fa14fec,U02S9JS3D2R,,,\"Yes, Installed Pyspark and following up with the videos. It\\'s really quick and downloaded the datasets in less than a minute.\",1645818891.617119,1645819642.615119,U02S9JS3D2R\\n660f29d4-d9f6-49c9-a5ae-149c3efa6bbc,U034VEFGMSM,,,Thanks,1645816959.816319,1645821252.028449,U034VEFGMSM\\n6a714f06-1613-4d0e-afbd-f17eafe68b01,U02U6DR551B,,,On windows you need some hadoop\\'s binaries. On Linux you don\\'t need it,1645806242.319589,1645823765.284049,U01AXE0P5M3\\nd9e35f31-f366-4f49-b208-9b3ff7ff974d,U02R09ZR6FQ,,,\"you sort of prepend it to the beginning of your PYTHONPATH\\n\\nlet\\'s say your PYTHONPATH is empty\\n\\nafter executing the first command it becomes\\n\\n```${SPARK_HOME}/python/:```\\nafter executing the second, it becomes\\n\\n```${SPARK_HOME}/python/lib/py4j-0.10.9-src.zip:${SPARK_HOME}/python/:```\\nso for you you\\'d need to create a PYTHONPATH variable with these paths and separate them by `;` - I think this is the separator used in windows\",1645803682.736359,1645823845.877549,U01AXE0P5M3\\n6f6d3e1b-09c2-43c5-ab12-503b6194144d,U02QL1EG0LV,,,<@U02QL1EG0LV> could you please help me to solve this issue?,1643223995.397300,1645824713.598349,U02CK7EJCKW\\n46180a1f-c6cf-4c94-8192-65cceba676c9,U02U5SW982W,,,<https://forms.gle/dBkVK9yT8cSMDwuw7>,1645645044.333119,1645825571.059889,U01AXE0P5M3\\n94509c82-044a-443c-a445-42a4579cb08d,U02U5SW982W,,,like that?,1645645044.333119,1645825572.555489,U01AXE0P5M3\\n23aa2f7b-beec-4923-95f9-96a08eddc4cd,U02U5SW982W,,,thanks for the idea!,1645645044.333119,1645825578.158319,U01AXE0P5M3\\n34068bf4-1fc6-46fe-8adf-e78f3595183c,,1.0,,\"Is anyone else having trouble with question 4 on the week 5 homework? It should be a fairly straightforward query, but none of the answers are even in my top ten trips.\",1645828558.420299,1645828558.420299,U02U3E6HVNC\\n5d21857a-dfb3-492d-a8f5-37ddd35740e3,,,,\"A question not totally related with the course but with this week. If Spark is Scala native but if Spark can work with Python and others languages... how important is for a Data Engineer to know Scala?\\nNote: In my opinion Spark is THE most important tool for Data Engineering.\",,1645829248.151279,U02CD7E30T0\\n52b527ed-8d39-4541-b1c9-ca1f53bff2d9,U02U5SW982W,,,We\\'re ok <@U02BVP1QTQF> but we\\'ve got more rain today. Basically flooding all across South East Queensland Australia. I\\'ve honestly never seen anything like it - so much water. As long as we are all sensible and don\\'t drive through flooded waterways :face_with_rolling_eyes: we should be fine. At least it puts my programming issues into perspective. Although it \\'seems\\' to be going much better this week. How about you?,1645760064.750309,1645831851.764459,U02U5SW982W\\n65550add-26c3-4a7e-b9a1-690d115e61cd,U02U5SW982W,,,\"That looks great <@U01AXE0P5M3> - a common hack for educators. Why do the work when you can get the students to do it for you? :wink: Also, though I think it is a pretty standard way of trying to get more community engagement going with your courses.\",1645645044.333119,1645835937.733639,U02U5SW982W\\nd7f25dd3-6275-45e4-96bf-3066c185b084,U02U34YJ8C8,,,\"God yes <@U02CGKRHC9E> you are so right. Very time consuming but I\\'m hoping it helps reinforce what I\\'m doing. I\\'m also \\'that\\' person they write the \\'dummies\\' series of books for. I have to really spell it out for myself, and document it in excruciating detail for my future self. I feel like I\\'m Merlin - forgetting more and more as I age. I have to write it down somewhere - like it is a data bank - so I can go back to it.\",1645587287.934269,1645836360.185159,U02U5SW982W\\nc7ec7c3b-8954-4349-90ac-9f3086139528,U02U34YJ8C8,,,<@U02CGKRHC9E> being based in Berlin is much better I imagine than being based in Australia for Slack activity... oh well we do what we can here Down Under I guess... :wink:,1645587287.934269,1645836416.291519,U02U5SW982W\\n4bfc5169-399a-488c-9aad-3eb2bb57d89d,,10.0,,Homework Week 5 Question 3 clarification,1645836479.191229,1645836479.191229,U02U5SW982W\\n7bfb1392-96b7-4cbd-a1fb-91c3727fe3c2,U02U5SW982W,,,Hi All just wanting to clarify the \\'How many trips on February 15\\' I am assuming that we count all trips that both started AND finished on February 15?,1645836479.191229,1645836573.303569,U02U5SW982W\\n3e64c01b-66df-44fb-a8f1-cfc17d4decf3,U02U5SW982W,,,And we are using the fhvhv - the \\'for hire vehicles\\' data that we downloaded in Question 2? Which I don\\'t think are really taxis?,1645836479.191229,1645836739.250249,U02U5SW982W\\nf05c0801-b8f1-4c60-ab16-bf4ae3e5082e,U02U5SW982W,,,Question says to consider only starting in Feb 15 and it is what I did.,1645836479.191229,1645836909.075939,U02TC704A3F\\n626f7408-143d-404e-bd61-8551c10ceb74,U02U5SW982W,,,I will try out both started and finished in feb 15 and see what happens,1645836479.191229,1645836952.275459,U02TC704A3F\\neb050171-cb73-4d5c-8129-595c1d24788d,U02U5SW982W,,,Awesome <@U02TC704A3F> - I could just game it until I get one of the answers but I\\'d like to understand what it\\'s really asking...,1645836479.191229,1645841897.541919,U02U5SW982W\\n44bda971-bf6d-4644-b954-e19ab5cb7171,U02U5SW982W,,,But yes <@U02TC704A3F> it does say \\'start on\\'. Which would typically imply that it starts on that date :face_with_rolling_eyes:  . For some reason I thought they were separate questions... oh dear,1645836479.191229,1645842183.034299,U02U5SW982W\\n167c6a83-2c76-481c-a70c-384781abdab9,U02U5SW982W,,,<@U02U5SW982W> stay safe,1645760064.750309,1645847250.319059,U02T8HEJ1AS\\nfa4bda65-5568-415b-b3b7-b0b3a59a362d,U02TATJKLHG,,,\"Hi Luis,\\n\\nThe PR got merged but I am happy to add to the FAQ doc. `inferSchema` is just a faster way to do this. I am not sure of any inherent advantages that it provides apart from speed and ability to read larger data.\",1645807526.556439,1645849703.279839,U02TATJKLHG\\n7da96dde-570f-4da5-be94-f7d1395b60cc,U02U5SW982W,,,Starting on Feb 15 but not necessarily finishing on Feb 15. Just like we did for homework 1,1645836479.191229,1645854079.406309,U01AXE0P5M3\\nf83d03c2-38a0-4a0d-9a9b-1bf6fba4f5d0,U02U5SW982W,,,Okay - all good <@U01AXE0P5M3>. And is it just for fhvhv data - like a follow-on from Question 2?,1645836479.191229,1645854988.706149,U02U5SW982W\\n8a4f981e-b5cd-4371-bbf7-5d90638a9ebf,U02U5SW982W,,,Yep,1645836479.191229,1645855336.342459,U01AXE0P5M3\\n59ea780a-985f-46c7-a00f-cdd111fb55c9,U02U5SW982W,,,Excellent - thank you!,1645836479.191229,1645858107.383069,U02U5SW982W\\n2cefc2af-97ca-412b-8efe-7525c8530f4b,U0297ANJF6F,,,\"it looks okay, but I think you miss a few more steps. I\\'d also use \"\"show\"\" instead of \"\"write\"\" here to just see the result\",1646227839.083989,1646298826.408779,U01AXE0P5M3\\n32d74b4d-20f7-483d-b2ad-77817ddb5da0,U0297ANJF6F,,,and to be honest I\\'d just go with SQL here,1646227839.083989,1646298851.215179,U01AXE0P5M3\\n20061a3b-51e7-4800-b597-a869abc98ae2,,6.0,,\"Hi, in the video I saw Alexey opened a tab that show currently active ports in Visual Code. How do I do that?\",1646305478.371349,1646305478.371349,U03133B2HK8\\n7138eddc-525d-4d64-804a-4e658891bc13,U03133B2HK8,,,what do you mean by active points? can you show a screenshot?,1646305478.371349,1646306108.538159,U01AXE0P5M3\\n396d7331-0119-4e01-9e9c-dc325187e2cc,,,,\"Hi, I\\'ve been unable to set up my GCP for 2 weeks now. I can\\'t set up the billing account. I have contacted Google support but they only asked me to keep trying. \\nIs there anything else I can do ?\",,1646309340.794329,U02UC2BH8BW\\nbb4e651a-6479-4e7d-98fc-bf1793ee657e,U033VBQCGQK,,,\"Expanding on what Alexey said: Once you go through video 1.4.1, you will be an expert ( :stuck_out_tongue:  ) on GCP and you can run docker there. Use port-forwarding to access that on your local machine.\",1646235795.512249,1646310501.760009,U02HB9KTERJ\\n23cbd16e-a469-498c-ba56-16b9795a4fee,,,,\"\"\"fal allows you to run python scripts directly from your <https://www.getdbt.com/|dbt> project.\"\"\\n<https://blog.fal.ai/introducing-fal-dbt/>\",,1646313436.727349,U030FNZC26L\\nd67051cd-b053-4129-af7b-60bf7d9b142c,,4.0,,How do I connect data ingestion script to docker compose build in order to ingest data in there?,1646313460.592209,1646313460.592209,U034Q6ZKR9C\\ne775215d-db74-45b6-8f97-91d16f09ca54,U034Q6ZKR9C,,,\"Followed the steps in first week, everything works and compiled correctly, except that I do not have table with data\",1646313460.592209,1646313508.865389,U034Q6ZKR9C\\nb8ed4333-e121-4908-8f88-0c23962641bf,U034Q6ZKR9C,,,\"<https://youtu.be/tOr4hTsHOzU|https://youtu.be/tOr4hTsHOzU>\\n\\nMaybe this will help?\",1646313460.592209,1646313697.270519,U01AXE0P5M3\\n188eb9a4-cabd-4328-b3e2-0c5799aebc3d,U034Q6ZKR9C,,,Thanks! I\\'ll have a look,1646313460.592209,1646313814.764279,U034Q6ZKR9C\\n71970dbc-ab0d-4003-b93b-4b1dcfdb2b75,U0297ANJF6F,,,\"<@U01AXE0P5M3> Yeah , that right. I was thinking of doing it hard way.\\nSQL will be clear code and understandable.\\nThanks\",1646227839.083989,1646313960.790549,U0297ANJF6F\\n53f8dca7-1bd1-4ba7-b354-6d7ebbf3b0d7,,1.0,,\"Hi, when we have to start the final project?\\n\\nAny ideas about the large datasets related to NLP or Text Mining? Where data is categorized according to months, years.\",1646314108.934809,1646314108.934809,U0297ANJF6F\\n7e785f37-3501-42be-af65-921edaab9d98,U034Q6ZKR9C,,,\"<@U01AXE0P5M3> That was easy, thanks again)\",1646313460.592209,1646314417.551189,U034Q6ZKR9C\\n986161fd-1be5-44cb-89f2-59fdd3f978af,U03133B2HK8,,,\"Here, in the First Look at Spark video. I\\'m just curious how you have that `PORTS` tab :slightly_smiling_face:\",1646305478.371349,1646315318.596879,U03133B2HK8\\nd903dfd0-7716-4970-954e-b58774494b76,,2.0,,\"Though I am understanding all the lectures and practicing, I am completely blind to the project side, as I am from mechanical background. Any presentation on typical project workflows, with examples can give clarity and can encourage people like me to do new projects. Any links to resources for doing projects helps too.. thank you\",1646320735.860349,1646320735.860349,U0300EGP2EL\\n833a735c-42a4-4346-a1d4-954448d7faa6,U03133B2HK8,,,It\\'s basically `ctrl+``,1646305478.371349,1646323423.677859,U02TATJKLHG\\n19672f9a-3abc-483b-8bdc-d68220ba31f7,U0300EGP2EL,,,\"I am not sure how much this will help you, but a project could be anything that grabs your interest. You don\\'t have to think in terms of how the project flow is going to look like or how you are going to present it. You can just pick a simple use case and build your solution around it.\\nLet\\'s say cricket interests you, so you just find a cricket dataset and use it to derive answer to simple questions like who is the highest scoring batsman, who took the most wickets, which stadium is favourable to the batsmen, which over in a 50 over match is the most likely to be a costly one? You can extend this idea to any of the topic that interests you, I am sure you\\'ll find some form of dataset.\\nNow, how you want to do it, is another aspect. Maybe you want to do it using spark, or dbt, or both, or bigquery, or spark jobs triggered from airflow etc.\\nOnce you have these two things figured out, it\\'d be just about incrementally building your \"\"project\"\".\",1646320735.860349,1646324002.046049,U02TATJKLHG\\nb315ae2d-ef83-4a8b-9bff-c23a0d5a0509,U0297ANJF6F,,,\"Maybe this should help-\\n\\n<https://datatalks-club.slack.com/archives/C02V1Q9CL8K/p1643721123738689>\",1646314108.934809,1646324181.768459,U02TATJKLHG\\nd0d33966-13a0-40a8-b294-64a1b2a5800b,U03133B2HK8,,,\"Yes, thanks\",1646305478.371349,1646328429.477799,U01AXE0P5M3\\n5ae4209d-45b3-4ac9-b688-710669c66499,,8.0,,\"I am on the video 1.3.2: creating GCP infrastructure with Terraform.\\n\\nI am getting the error below after hitting  terraform apply command.\\n\\nError: googleapi: Error 403: The billing account for the owning project is disabled in state absent, accountDisabled\",1646329299.706539,1646329299.706539,U0330DCCDN1\\nbab555eb-af06-4f10-820b-ccfa4505dc78,U0330DCCDN1,,,\"Check the Project Name <https://datatalks-club.slack.com/archives/C01FABYF2RG/p1642690115362900> Think a lot of us ran into this at the beginning. :slightly_smiling_face: Click on the \"\"view message\"\" at the bottom.\",1646329299.706539,1646330090.062289,U02TNEJLC84\\n05cdabec-b60d-4fc8-af2d-0235617bbafc,U0330DCCDN1,,,Can you please put it to FAQ if it\\'s not there already?,1646329299.706539,1646330444.263779,U01AXE0P5M3\\nf9482fd8-9ad7-4423-a937-7909a09be420,U0330DCCDN1,,,\"Hey, is it mandatory to give your credit card details to Gcp in order to run this ??\",1646329299.706539,1646330445.284999,U0330DCCDN1\\na6545705-afc6-453b-81eb-be09aae6cd4d,U0330DCCDN1,,,\"Yes, but you\\'ll get $300 in free credit.\",1646329299.706539,1646330527.545649,U02TNEJLC84\\n34aa6af1-981c-4022-8910-23980c086db4,U0330DCCDN1,,,<@U01AXE0P5M3> Adding it to the FAQ now.,1646329299.706539,1646330542.946899,U02TNEJLC84\\n298132a9-f1ce-45b9-bcdc-4138d1fc7d8c,U0330DCCDN1,,,\"<@U02TNEJLC84> bummer, my card is not getting accepted.\",1646329299.706539,1646330666.711399,U0330DCCDN1\\neb5688e0-f352-4e9d-945a-1abdb360d7e8,U0330DCCDN1,,,Maybe helpful?? <https://www.quora.com/Google-Cloud-Platform-is-not-accepting-my-debit-card-Visa-How-can-I-start-my-free-trial>,1646329299.706539,1646331184.850749,U02TNEJLC84\\n326f1a35-55db-46c4-a986-97334013850b,,6.0,,I see that Google has a cancer image database. Is there any chance that this would make a good project? This course has not been a walk in the park for me so I anticipate I would be stuck in places. Also we have not done image processing.,1646336581.292489,1646336581.292489,U02UBQJBYHZ\\nbb50d0ec-3a69-4bb4-b468-edbde6a11127,U02UBQJBYHZ,,,\"Do you have some ideas what you\\'d like to do with this data? Since it\\'s not structured (images), it\\'s not really easy to use SQL, Spark and other things we cover here\",1646336581.292489,1646336693.761749,U01AXE0P5M3\\n49046ff9-ade8-4d0d-b19e-422db5d6958d,U02UBQJBYHZ,,,\"My husband is an oncologist and former software engineer, so I\\'m interested in the subject matter and I would have someone to talk to about it.\",1646336581.292489,1646336871.422879,U02UBQJBYHZ\\n1c0b68f3-739f-4dea-b049-70311b5a781c,U02UBQJBYHZ,,,But my other idea is to get a CD from the Massachusetts Secretary of State that has all the registered voters in it and all the elections they\\'ve voted in probably since 1970. About 5 million.,1646336581.292489,1646336968.434889,U02UBQJBYHZ\\n9eb52e2a-e9a9-48f6-8167-05b13172b16f,U02UBQJBYHZ,,,Maybe not a CD. I know it exists and I\\'ve worked with part of it.,1646336581.292489,1646337052.790929,U02UBQJBYHZ\\n2bab351d-1365-402f-bc96-15ae0761748f,U02UBQJBYHZ,,,\"As long as you manage to find a CD drive, should be fine :smile:\",1646336581.292489,1646337516.154409,U01AXE0P5M3\\ncd5a01d0-63a4-4db0-8428-79e46afbd9a6,U02UBQJBYHZ,,,That\\'s more structured for the project than pictures. You\\'d need to do a lot of extra stuff (not covered in the course) do use images. That will take a lot more time than just two weeks I think,1646336581.292489,1646337648.538659,U01AXE0P5M3\\nfad0a1e7-6904-4e77-b7b7-5a7bce592e4a,,2.0,,\"Hello everyone, any ideas about articles, blogs, videos about examples of ETL/ELT process from a architecture perspective? I\\'m a bit confused with all the tools we learned and the different libraries that are available out there (Prefect, Airflow, Spark, Kafka, DBT, Meltano, AirByte, etc). Sometimes I don\\'t know if one of these can be combined with other one or replace it, so I think perhaps read some real examples could help. Thanks :slightly_smiling_face:\",1646341588.083949,1646341588.083949,U032Q68SGHE\\n274ad397-949e-4fc0-bcdf-13ed8d4e181d,,,,For those still on Spark week (like me) this might be useful. It’s basically selected chapters from “Spark: The Definitive Guide” aimed at Data Engineers,,1646343200.957999,U02U34YJ8C8\\n4caa4670-c2a2-46a0-bb08-95136cef9e6a,U02U3E6HVNC,,,\"I know I\\'m late here, but I\\'d like to put in my 2 cents.  I logged on to the cloud dbt site and went to:  Account Settings -&gt; Selected my ny taxi project -&gt; clicked on the link associated with \"\"connection\"\" -&gt; clicked on the \"\"Edit\"\" button, edited the Location (Optional) field to \"\"us-west1\"\" and re-uploaded my JSON key file.  That seemed to do the trick for me.\",1644432387.132949,1646345120.076699,U02TP0ZT4TW\\n7254E1B8-2353-4267-8B85-090C83A08313,U02U34YJ8C8,,,<@U02U5SW982W> Sorry missed this. Behind on everything right now! Hoping to finish week 5 stuff tomorrow. Then maybe onto the Kafka stuff over the weekend. How you getting on?,1645213874.310119,1646347304.075849,U02U34YJ8C8\\n4be206ee-e731-4977-8f1d-33d414e94b35,U02U34YJ8C8,,,\"No worries <@U02U34YJ8C8> I\\'m doing ok but I pretty much had the day off on the course yesterday. So we both know what that means. Things will get away from me again. Plan was to try and re-blog my issue with dbt Cloud and location (which I got a response back that seems to work), write a reflection for week 5 and then get on with Kafka. Although I want to get all that done I\\'m probably being overly optimistic :confused: We will see how I go.. .\",1645213874.310119,1646349065.546379,U02U5SW982W\\n611ab5db-fbd3-4fe7-8a68-cbdd953a799e,U03133B2HK8,,,\"Yeah I know it\\'s to open the terminal, but somehow I don\\'t have the Ports tab next to it\",1646305478.371349,1646357561.644059,U03133B2HK8\\n161648da-fe82-4543-8108-7e84f3656e24,,4.0,,\"I\\'m reviewing some of the terraform materials to prep for the final project. Can anyone explain what this means in the `<http://main.tf|main.tf>` ?\\n\\n```  lifecycle_rule {\\n    action {\\n      type = \"\"Delete\"\"\\n    }\\n    condition {\\n      age = 30  // days\\n    }\\n  }```\\nI\\'m having trouble understanding the documentation online. It seems like it might mean delete after the bucket is 30 days old, but mine is still going strong after ~35 days\",1646367680.437609,1646367680.437609,U02T9GHG20J\\n93e36992-9169-477e-a075-cec6c872d0ec,U02ULP8PA0Z,,,HI <@U02ULP8PA0Z> - thanks so much for sharing this with us. I just read it after I watched <@U01DFQ82AK1> intro video and it helped a lot in reinforcing the concepts - at  least in my head anyway... :),1646099641.598249,1646368171.071189,U02U5SW982W\\n8bfecc4b-4bc9-40ed-861e-c1a397655bcc,,3.0,,Is there any way to get GCP Free Trial without having US debit card? I\\'m told that there is regional restrictions.,1646369026.483969,1646369026.483969,U034Q6ZKR9C\\na406b909-1842-4934-bb2a-be5b4183e5f2,U02T9GHG20J,,,Maybe.... if there\\'s no activities in that bucket after 30 days,1646367680.437609,1646371444.969189,U02TA7FL78A\\n6eaff74d-ca5a-4467-b12d-41137456af82,U034Q6ZKR9C,,,Card with Visa logo works for me in south east Asia,1646369026.483969,1646371514.242799,U02TA7FL78A\\nc8e46120-fd5d-42f9-ba3f-9fd48de6903a,U02T9GHG20J,,,I believe files will be deleted 30 days after they are uploaded/updated,1646367680.437609,1646378058.174729,U01AXE0P5M3\\nb74e3f79-e4f2-48f4-95c4-c9e2f719a229,U02T9GHG20J,,,Oh could it be 30 days after last access? Or maybe it\\'s just not working :sweat_smile:,1646367680.437609,1646378230.486639,U01AXE0P5M3\\nea61aaea-beda-42b9-ae5b-e813862c49c4,U03133B2HK8,,,\"I think someone had this problem. Try checking this channel. And if you find the solution, please put it to FAQ\",1646305478.371349,1646378345.478329,U01AXE0P5M3\\nb42df2dd-131f-4181-9053-09079e70253e,U034Q6ZKR9C,,,You need a visa/mastercard associated card to go through. but the course does have option to go without gcp - but that path is less interesting i guess...,1646369026.483969,1646378755.476119,U02HB9KTERJ\\n82aafcc2-2d38-4ee4-9c84-43800f4f05ba,U02TATJKLHG,,,I believe confluent focuses more on JVM stack rather than python. All the other libraries are initiatives from different companies which can be left feature incomplete,1646850614.641829,1646902597.610619,U01DFQ82AK1\\na9fa7647-1d2a-4bad-9c75-1783aeeca4ed,U02UBQJBYHZ,,,What is the error? Can you please paste logs,1646836356.583069,1646902641.824849,U01DFQ82AK1\\n9db72453-8800-4f73-ab9e-7d8914bc6c06,U02TC704A3F,,,Very cool :slightly_smiling_face:,1646865736.378139,1646904118.835899,U02CD7E30T0\\n1527a73d-a9a4-462d-a21a-a2c8301df116,,6.0,,\"If anyone struggling with creating virtual environment for week 6, please refer to this comment\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1646841472620539?thread_ts=1646836356.583069&amp;cid=C01FABYF2RG>\",1646904198.505189,1646904198.505189,U01DFQ82AK1\\nfbb84fa6-8f0f-4c27-af69-d83675026cfe,,2.0,,\"hi\\nfor week 7 if I use some library from github  for get data using API it\\'s fair ?? for example like\\n<https://github.com/plamere/spotipy>\",1646904808.053879,1646904808.053879,U02SQQYTR7U\\nc67a53f2-c606-40b0-b2c0-9c0ad4d4d516,U02SQQYTR7U,,,Looks good to me,1646904808.053879,1646904828.153389,U01AXE0P5M3\\n55d5c921-650c-45ee-af96-e5137ef74036,U02SQQYTR7U,,,ok thank you.:pray:,1646904808.053879,1646905086.544819,U02SQQYTR7U\\n698c8e35-278b-4f50-ad97-c0c45f1201ad,,3.0,,\"Hi, anyone know a python library to create tables in Big Query from Google Cloud Storage files? I\\'m searching something agnostic to the orchestration tool (Airflow, Prefect, etc)\",1646911606.789309,1646911606.789309,U032Q68SGHE\\n607d941e-bb7b-4f1a-b394-745aafad1265,U01DFQ82AK1,,,\"It didn\\'t work for me though, it actually made things worse.\",1646904198.505189,1646912842.409439,U02UBQJBYHZ\\n9287f3c9-5df0-4368-a4a6-52868d35c04f,U01DFQ82AK1,,,\"Before, I could get control center up on port 9021 and create a new topic. I just could not run the python file because \"\"kafka\"\" wasn\\'t recognized. Now I can\\'t get the control center to connect to anything.\",1646904198.505189,1646912964.495449,U02UBQJBYHZ\\n4ee969f2-bcf2-416f-a8ca-43e521b8af6f,U032Q68SGHE,,,BigQuery has python API.,1646911606.789309,1646913027.919559,U0290EYCA7Q\\nfaae6b12-9823-48b9-9cb9-47c5081ae47c,U032Q68SGHE,,,<https://cloud.google.com/bigquery/docs/schemas#python>,1646911606.789309,1646913043.809299,U0290EYCA7Q\\nd30377bf-3c4f-48fc-8378-ee4bae0a9134,U032Q68SGHE,,,Thanks <@U0290EYCA7Q>! I didn\\'t know about load_table_from_uri method,1646911606.789309,1646914598.044079,U032Q68SGHE\\na9cf7c05-a5c5-4546-bce9-eec003626194,,,,Hello guys i am having troubles with my docker.Restarting it has lead to this warning,,1646914784.065699,U02T70K8T61\\n87cdc6b0-8ba7-405d-af23-c1185621a011,,,,60% of the project is finding a suitable dataset that can go along with the project (e.g. one that can be ingested on a interval) :slightly_smiling_face: :stuck_out_tongue:,,1646917912.413969,U02HB9KTERJ\\n968f88b6-15ba-4aaa-812c-a81d1fb33259,U01DFQ82AK1,,,Starting docker images is different from setting up a virtual environment. So it is best not to confuse them together,1646904198.505189,1646918356.412449,U01DFQ82AK1\\n2ea0ff70-8252-4ae8-99e0-da06e8139e09,U01DFQ82AK1,,,\"• please check all docker images are running \\n```docker ps```\\n• Checks logs for any error\\n• Wait a few minutes to let all images get started (sometime it takes couple of minutes)\\nFor running code:\\n• Create virtual environment and source into it\\n• Install requirements\\n• run code\",1646904198.505189,1646918438.431459,U01DFQ82AK1\\nd9e89243-f09c-4bc9-8387-98f36625a748,,3.0,,\"I don\\'t understand the role of spark here. For instance, in Week 5, we downloaded csv, converted it to parquet, manipulated datatype. But all of this was done using airflow/dbt. I guess we can forgo it as far as the project is concerned, right? (and I don\\'t plan to touch Kafka in this life; maybe after rebirth)\",1646919057.693349,1646919057.693349,U02HB9KTERJ\\ne159cafc-b061-4fc5-a73a-67dc918cb388,U02HB9KTERJ,,,You can use dbt instead of spark indeed. They both play a somewhat similar role,1646919057.693349,1646919949.462639,U01AXE0P5M3\\na8a7db07-529a-4b07-93d5-9866b09c07b5,U02TBCXNZ60,,,what exactly is expected from our project,1646850921.471979,1646920195.889379,U02AUCL9ZQF\\nCABFA6DB-B9BB-4AA0-96F0-926B91401DBD,U02HB9KTERJ,,,\"I also think it\\'s unlikely I\\'ll touch Kafka in this life (no offense <@U01DFQ82AK1> :sweat_smile: )\\n\\nI like Vicki’s take on it. As with all technologies it\\'s as important to know when it\\'s needed as when it\\'s not. \\n\\n<https://vicki.substack.com/p/you-dont-need-kafka?s=r|https://vicki.substack.com/p/you-dont-need-kafka?s=r>\",1646919057.693349,1646925592.968299,U02T9GHG20J\\nB64F7BFF-6016-4E90-B482-DFF8510C9497,,2.0,,\"What is the best practice / recommendation for running dbt models on a schedule? Should that be done with airflow, or does dbt cloud have scheduling?\",1646925812.725779,1646925812.725779,U02T9GHG20J\\n48abf0fb-cf08-4423-a9cc-0df6ff537e75,U02HB9KTERJ,,,No offense taken. Use the best tech for your use case,1646919057.693349,1646925822.134689,U01DFQ82AK1\\n438b81cf-5c80-4da6-8514-5c4f92dc3b03,,3.0,,Hello guys please do you know any good python package to move files from one s3 bucket to another s3 bucket using airflow ?,1646931135.246599,1646931135.246599,U02U8P50W67\\na116c66f-2a6b-4f98-9726-f43603866bae,,,,\"404 POST <https://bigquery.googleapis.com/bigquery/v2/projects/bright-task-339417/jobs?prettyPrint=false>: Routing destination for europe-west2  is not found\\n\\nPlease helse has encounter this.\",,1646931407.640789,U02UKLHDWMQ\\nbe9f838e-ed0a-40f0-a89e-3ea40e099a22,,2.0,,Hi Week 2 : Is lighter version of Airflow(alexey  video) enough for  the course,1646931999.206079,1646931999.206079,U033WHDKP0T\\n10f9ead3-2bb1-4bef-94af-e4ea19419562,U033WHDKP0T,,,\"Yes, it is enough\",1646931999.206079,1646932971.934639,U02SUUT290F\\n57c53a98-9c77-4d4f-bcbd-f7ea597765be,U02ULFC9N5P,,,Thank you.,1646775937.386769,1646936037.163639,U02ULFC9N5P\\n3fed0c3e-b3c8-42ec-889d-573e10afdd15,U02U8P50W67,,,\"I don\\'t know with Airflow, but with pandas you can read the csv ( dataframe.read_csv(...parameters) ) and then upload it (<http://dataframe.to|dataframe.to>_csv(...parameters))\",1646931135.246599,1646939336.710189,U032Q68SGHE\\n3b0d5dd1-5d88-403f-a956-3626eda54c80,,,,\"Hi, for the last question in week 6, any material to guide us, because i dont understand the question\",,1646940579.432509,U02T0CYNNP2\\nbafaf864-5e1b-48eb-aa99-ee979c5adc51,U02U8P50W67,,,\"I don\\'t really have experience with it, but from what I understood, boto3 should be able to do a lot of aws stuff <https://pypi.org/project/boto3/|https://pypi.org/project/boto3/>\",1646931135.246599,1646948362.086579,U0312TK7WHK\\n3e0cd752-d178-4315-abfd-22bcfc178298,U02TC704A3F,,,Okay this is AMAZING,1646865736.378139,1646948933.747969,U02TC704A3F\\n3284e406-792f-4b73-b9d2-750ec6fce265,U02T9GHG20J,,,\"This is shown in the videos with dbt cloud but it could be done with any scheduler, even a simple Cron job. \\nFor airflow specifically, there\\'s a dbt package as well\",1646925812.725779,1646951873.881709,U01B6TH1LRL\\n51A758C1-200C-4C7E-BDD7-F2AC840A1808,U02T9GHG20J,,,\"Thanks Vic, I\\'ll review the videos. I\\'m thinking for the project it may make more sense to keep it all in airflow. We’ll see how it goes\",1646925812.725779,1646953617.944369,U02T9GHG20J\\n3e6aa6a9-eba1-443c-a216-4cd730b87000,,5.0,,\"Trying to run the stream example in week 6. I created the topic datatalkclub.yellow_taxi_ride.json in confluent, all the requirements from requirements.txt are installed and producer_tax_json.py is running. I\\'ve tried running the stream with\\n```python3 stream.py worker```\\nbut get the error\\n```Traceback (most recent call last):\\n  File \"\"stream.py\"\", line 6, in &lt;module&gt;\\n    topic = app.topic(\\'datatalkclub.yellow_taxi_ride.json\\', value_type=TaxiRide)\\n  File \"\"/home/gary/.local/lib/python3.8/site-packages/faust/app/base.py\"\", line 781, in topic\\n    return self.conf.Topic(\\n  File \"\"/home/gary/.local/lib/python3.8/site-packages/faust/app/base.py\"\", line 1796, in conf\\n    self._configure()\\n  File \"\"/home/gary/.local/lib/python3.8/site-packages/faust/app/base.py\"\", line 1740, in _configure\\n    conf = self._load_settings(silent=silent)\\n  File \"\"/home/gary/.local/lib/python3.8/site-packages/faust/app/base.py\"\", line 1752, in _load_settings\\n    return self.Settings(appid, **self._prepare_compat_settings(conf))\\n  File \"\"/home/gary/.local/lib/python3.8/site-packages/faust/types/settings.py\"\", line 768, in __init__\\n    self.Worker = cast(Type[_WorkerT], Worker or WORKER_TYPE)\\n  File \"\"/home/gary/.local/lib/python3.8/site-packages/faust/types/settings.py\"\", line 796, in __setattr__\\n    object.__setattr__(self, key, value)\\n  File \"\"/home/gary/.local/lib/python3.8/site-packages/faust/types/settings.py\"\", line 1204, in Worker\\n    self._Worker = symbol_by_name(Worker)\\n  File \"\"/home/gary/.local/lib/python3.8/site-packages/mode/utils/imports.py\"\", line 262, in symbol_by_name\\n    module = imp(  # type: ignore\\n  File \"\"/usr/lib/python3.8/importlib/__init__.py\"\", line 127, in import_module\\n    return _bootstrap._gcd_import(name[level:], package, level)\\n  File \"\"&lt;frozen importlib._bootstrap&gt;\"\", line 1014, in _gcd_import\\n  File \"\"&lt;frozen importlib._bootstrap&gt;\"\", line 991, in _find_and_load\\n  File \"\"&lt;frozen importlib._bootstrap&gt;\"\", line 975, in _find_and_load_unlocked\\n  File \"\"&lt;frozen importlib._bootstrap&gt;\"\", line 671, in _load_unlocked\\n  File \"\"&lt;frozen importlib._bootstrap_external&gt;\"\", line 848, in exec_module\\n  File \"\"&lt;frozen importlib._bootstrap&gt;\"\", line 219, in _call_with_frames_removed\\n  File \"\"/home/gary/.local/lib/python3.8/site-packages/faust/worker.py\"\", line 19, in &lt;module&gt;\\n    from aiokafka.structs import TopicPartition\\n  File \"\"/home/gary/.local/lib/python3.8/site-packages/aiokafka/__init__.py\"\", line 4, in &lt;module&gt;\\n    from .client import AIOKafkaClient\\n  File \"\"/home/gary/.local/lib/python3.8/site-packages/aiokafka/client.py\"\", line 10, in &lt;module&gt;\\n    import aiokafka.errors as Errors\\n  File \"\"/home/gary/.local/lib/python3.8/site-packages/aiokafka/errors.py\"\", line 4, in &lt;module&gt;\\n    from kafka.errors import (\\nImportError: cannot import name \\'ConnectionError\\' from \\'kafka.errors\\' (/home/gary/.local/lib/python3.8/site-packages/kafka/errors.py)```\\nSearching this channel I\\'ve seen a few questions related to it, but haven\\'t been able to locate the solution. Does anyone know how to resolve this?\",1646957620.422529,1646957620.422529,U02TNEJLC84\\n53a2918e-2bbb-4d1a-994c-c5ef12105459,U02TNEJLC84,,,Hi <@U02TNEJLC84> I got the EXACT same error and posted the solution I used in order to recover from it. I believe it has to do with robinhood not playing nicely with the latest version of kafka-python. I detail how to overcome this in my blog but essentially I uninstalled teh latest version of kafka-python and reinstalled the version that robinhood will work with. After this happy days :slightly_smiling_face: . See my blog <https://learningdataengineering540969211.wordpress.com/|post> if you want more detail but this was my solution. Let me know how you get on...,1646957620.422529,1646958357.651569,U02U5SW982W\\nf20161e5-bf8a-4141-8711-ed83594155e2,U033WHDKP0T,,,To save yourself a lot of pain and agony I would perhaps go on this path ... ;),1646931999.206079,1646958450.695479,U02U5SW982W\\n5b254059-38db-4673-9713-1615527a7d3f,U02TNEJLC84,,,Thank you <@U02U5SW982W> and nice blog! Specifying version 1.4.6 did the trick! I also submitted a pull request to update the requiremnts.txt file for this week so hopefully other students won\\'t be bumping into this going forward.,1646957620.422529,1646959267.590059,U02TNEJLC84\\n7b4f25fd-0300-4402-a245-06caf419bd44,,,,\"Hey everyone\\n\\nJust a heads up for Anaconda users like me: confluent-kafka and Conda-envs doesn\\'t go smooth.\\n\\nI added the problems I faced into the FAQ, but I will add here too.\\n\\n*ImportError: DLL load failed while importing cimpl*\\n\\nSOLUTION: $env:CONDA_DLL_SEARCH_MODIFICATION_ENABLE=1 in Powershell.\\nYou need to set this DLL manually in Conda Env for execution.\\n\\n*ModuleNotFoundError: No module named \\'avro\\'*\\n\\nSOLUTION: pip install confluent-kafka[avro].\\nFor some reason, Conda also doesn\\'t include this when installing confluent-kafka via pip.\\n\\nSOURCES:\\n<https://github.com/confluentinc/confluent-kafka-python/issues/590>\\n<https://githubhot.com/repo/confluentinc/confluent-kafka-python/issues/1186?page=2>\\n<https://github.com/confluentinc/confluent-kafka-python/issues/1221>\\n<https://stackoverflow.com/questions/69085157/cannot-import-producer-from-confluent-kafka>\",,1646960059.704439,U02TC704A3F\\n0a4ff8ce-be1d-46be-9c07-a90c7bb886fb,U02TNEJLC84,,,That\\'s awesome <@U02TNEJLC84> - love your work!,1646957620.422529,1646960531.442099,U02U5SW982W\\n99be614c-a436-4a6e-8eef-9e10ac24d5a9,,1.0,,\"Hi everyone, is week 5 correlated with week 6? I\\'m planning to skip week 5 and straight to week 6 because I didn\\'t follow the course in the last few weeks and I\\'m afraid I can\\'t follow the project if I do week 5\",1646964877.879839,1646964877.879839,U02T8ANTJGM\\ne30296d3-b44b-4d15-893e-853c2cb7ed26,U02T8ANTJGM,,,They are not correlated.,1646964877.879839,1646965746.271219,U02TC704A3F\\nb9250a65-c08a-46f7-b0af-c69c710f742b,U02U8P50W67,,,Thank you very much really appreciate. i will carefully work on the suggested links,1646931135.246599,1646973310.153799,U02U8P50W67\\nf10d7aa1-1b22-4e47-9d8e-391edcc66017,,1.0,,\"Good morning,\\n\\nI am having issues getting the postgres image to work. I have tried many things and even followed the FAQ but i keep getting password not authenticated for user \\'postgres\\'. i made the user postgres when i was running the container\",1646983548.339739,1646983548.339739,U035VPYAEBY\\n90cae4ef-c46d-40bb-b0fc-30c3ab4394bf,,,,I tried connecting via python and pgcli but i keep getting the same thing,,1646983572.790099,U035VPYAEBY\\n7da64eda-635d-4d52-9b57-924e55bc10b7,U035VPYAEBY,,,\"Sorted. I entered the database in the docker image with `docker exec -it &lt;hash&gt; psql -U &lt;user&gt; &lt;dbname&gt;` .\\n\\nthen ran the sql command `ALTER ROLE &lt;user&gt; WITH PASSWORD \\'&lt;password&gt;\\'` .\",1646983548.339739,1646984587.258129,U035VPYAEBY\\ndb7d7e97-961a-40f9-a4f0-2683468842c2,,,,i hope this is the right channel for that question,,1646727115.438469,U033QG69HRQ\\n2e180ba2-a6db-4b8b-ad6f-3addac635ca4,U033QG69HRQ,,,\"I think 10 hours is sufficient. Every tool in this course was new to me, and that was the amount of time I spent per week on average.\",1646723933.318809,1646727342.795719,U02T941CTFY\\ne0ec2ab7-b6d2-4141-9c60-693b972b6cbd,,13.0,,\"Hello everyone, I was wondering when is the deadline for homework 6? Also will there be a more detailed explanation of the coding part?\\n\\n\"\"(Attach code link) Practical: Create two streams with same key and provide a solution where you are joining the two keys\"\"\\n\\nIs there any template we should use? This task seems a bit vague to me.\",1646731420.426559,1646731420.426559,U01T2HV3HNJ\\n6f242e8c-5c39-4bbd-8aee-4868d6cd3f50,U01T2HV3HNJ,,,I\\'ve added a deadline to the github page. It\\'s 14 of March. Let\\'s wait for Ankush to answer the second question,1646731420.426559,1646732031.609039,U01AXE0P5M3\\n5214b918-ec72-4640-b70c-30e57644b067,U033QG69HRQ,,,\"I agree, and depending on your previous skills and experience, this may be less than 10 hours. Depending on the week, for me this has been between 5 and 10 hours. Of course, you can \"\"invest\"\" as much time as you wish reading extra documentation.\",1646723933.318809,1646732874.424409,U02LQMEAREX\\nafff7610-7d5c-463a-a5e2-05827dbb825f,U033QG69HRQ,,,\"hey, thanks for the replies!\",1646723933.318809,1646735944.167879,U033QG69HRQ\\n4e2698ef-c3c5-4c25-ac57-820b1626124d,U033QG69HRQ,,,\"yeah those \"\"extras\"\" are the one i\\'m most afraid of, as i have no background in computer science (although i self taught in python &amp; many other things), which means i have knowledge gaps &amp; experience told me i have tendencies to dig into the \"\"why\"\"s more often than not\",1646723933.318809,1646736175.438209,U033QG69HRQ\\n462628f5-7fa7-41fe-9967-57889eedef3f,U01T2HV3HNJ,,,\"As I imagine it, I will make a taxi data stream with rides and join it with stream or ktable of zones\",1646731420.426559,1646744938.397999,U02T65GT78W\\n4b118ea1-774f-417f-95c7-e56fac7ffad8,U01T2HV3HNJ,,,\"I thought of doing something like this but it is not exactly joining two streams if one table needs to be fully there, or is it ?\\n\\n\\nI have tried to figure out and understand the syntax of it in faust, I looked through all github for an example as well as the documentation but I have not found a single one.\\n\\nIt is possible to get both steams together doing:\\n```joined_topics=app.topic(\\'topic1\\',\\'topic2\\')\\n@app.agent(joined_topics) ```\\nbut after that I am lost.\\nI suppose we have to wait for keys to match to make the a joined something.\\n\\nIt is well explained using jvm but I am not familiar at all with it and I assume it should be possible in faust except no one has ever documented it fully.\",1646731420.426559,1646758805.912859,U02TC8X43BN\\naab99d15-c3b8-4c11-9056-4afcb7948159,U01T2HV3HNJ,,,\"<@U02TC8X43BN> Is this something that you are looking for? <https://faust.readthedocs.io/en/latest/reference/faust.joins.html>\\n\\nI\\'ve just started looking into the coding part myself. So will update if I find anything\",1646731420.426559,1646759891.397359,U02TATJKLHG\\nccf78a2c-f268-46c8-b623-e5fdba9c1efa,U01T2HV3HNJ,,,I know that link but to me it is not helpful at all. Maybe someone more experienced understand the logic of it.,1646731420.426559,1646760291.990289,U02TC8X43BN\\nb9fcb1f1-93c9-4663-a13d-bf394643f68e,U01T2HV3HNJ,,,I agree. Even I am unable to gather how to perform a join from this link.,1646731420.426559,1646760679.715979,U02TATJKLHG\\n1f1b70fa-c59a-43e5-998a-eecfe8adaeb2,U01T2HV3HNJ,,,\"I am not too sure but Faust joins source code doesn\\'t look complete to me.\\n\\n<https://github.com/robinhood/faust/blob/master/faust/joins.py>\",1646731420.426559,1646761458.719529,U02TATJKLHG\\n0448986d-83b3-45fe-b34b-5e62f7dcdeb7,,3.0,,\"If we need to add another program to our airflow worker do we put it in the Dockerfile? Been trying since last night. I changed\\n```RUN apt-get update -qq &amp;&amp; apt-get install vim -qqq ```\\nto\\n```RUN apt-get update -qq &amp;&amp; apt-get install vim -qqq &amp;&amp; apt-get install unzip```\\nRebuild the image and run airflow-init but get the error in my Dag log stating\\n```bash: unzip: command not found```\\nShould I be adding the apt-get install unzip somewhere  in the docker-compose file?\",1646763790.714439,1646763790.714439,U02TNEJLC84\\n98ac4ee0-1f57-4cfa-8ca9-3e85866124e2,,,,Hello all! I started the course around a week or so ago. I\\'ve really enjoyed it so far!,,1646763917.836659,U035695DTHQ\\ndf74afe6-70f5-41cb-b8ce-4f268fca5716,U02TNEJLC84,,,\"I\\'d install it this way:\\n\\n```RUN apt-get update -qq &amp;&amp; apt-get install -y vim unzip ```\\n(not sure what `-qqq` means, probably \"\"quiet\"\")\\n\\nand I\\'m also wondering why it needs vim... But this should work.\\n\\nAnother thing - with docker-compose it\\'s sometimes difficult to make sure it doesn\\'t use a cached version. Maybe try to delete all images first?\",1646763790.714439,1646766168.538489,U01AXE0P5M3\\nbeb40b78-c2c2-41ae-9eeb-534971db3065,,7.0,,\"Hello All,\\nFor week 6, I set up kafka using docker and kafka running properly. I abled to load the kafka control center in my browser. Next I tried to run pip install -r requirements.txt and getting an error\",1646769777.750379,1646769777.750379,U02R77FP943\\n1ca71e10-dfb1-4177-890c-3807a7b5192a,U02R77FP943,,,\"```Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\\nUsing legacy \\'setup.py install\\' for confluent-kafka, since package \\'wheel\\' is not installed.\\nUsing legacy \\'setup.py install\\' for opentracing, since package \\'wheel\\' is not installed.\\nInstalling collected packages: multidict, idna, frozenlist, yarl, six, colorama, charset-normalizer, attrs, async-timeout, aiosignal, python-dateutil, mypy-extensions, kafka-python, colorlog, aiohttp, venusian, urllib3, termin\\naltables, robinhood-aiokafka, opentracing, mode, croniter, colorclass, click, certifi, aiohttp-cors, requests, faust, fastavro, confluent-kafka, avro\\n    Running setup.py install for opentracing ... done\\n    Running setup.py install for confluent-kafka ... error\\n    ERROR: Command errored out with exit status 1:```\\n\",1646769777.750379,1646769858.154949,U02R77FP943\\n7e7c2437-fa4a-45a0-b558-701339d8a55f,U02R77FP943,,,\"Maybe try to update pip?\\n\\nPip install -U pip\",1646769777.750379,1646771246.867559,U01AXE0P5M3\\ne050b7fb-cb20-4883-92a9-59125ec7224a,,2.0,,\"Hi, what’s the deadline for the final project?\",1646775937.386769,1646775937.386769,U02ULFC9N5P\\nDE87AB60-FAB8-4799-B4D1-F0D56F3F1139,,,,\"Is the description for the project complete?\\nShould we start working on it?\",,1646778030.855329,U02U6DR551B\\nd156d966-36a4-4796-bdd6-9250edff1e0c,U033QG69HRQ,,,\"Yeah it really depends on how new you are to the tech, every tool was also new to me and it was about 10 hours per week for me as well , keep in mind it\\'s tough if you work full-time which i do\\n\\nbut the stack you learn here is so valuable\",1646723933.318809,1646782599.568699,U02T9550LTU\\n944fab41-b1f1-4c28-bb61-25982cd95ce3,U02TNEJLC84,,,\"Thanks for the reply <@U01AXE0P5M3> tried it 5 times now and it\\'s finally working. Feel like I did the same steps each time. Deleted all images, did a docker-compose build, docker-compose up airflow-init and then docker-compose up. Not sure what the difference was on the last run, but I\\'ll take it! Thanks as always.\",1646763790.714439,1646785691.830959,U02TNEJLC84\\n9c755005-6392-473b-b9b4-5334b5cf9bf5,U02UBQJBYHZ,,,\"I\\'m trying to follow the video for Week 6. Ankush starts with docker-compose up in terminal. I\\'m thinking there is something that I should know from previous weeks that I just didn\\'t retain. For example, is there something I need to build? Do I have to have something running in the background? I had no trouble with docker-compose when we were working with Airflow.\",1646703018.926709,1646788233.205309,U02UBQJBYHZ\\n8a04f308-6f5c-412f-aa4a-ac5355605b09,U02UBQJBYHZ,,,\"I tried to do a docker-compose build, but absolutely nothing happened. Do I need a Dockerfile that I write myself?\",1646703018.926709,1646788452.235729,U02UBQJBYHZ\\n45eaaa99-d44e-4fdd-9bfd-6a958f30a971,U02UBQJBYHZ,,,I opened Docker desktop. Somehow that started something and I am running docker-compose now.,1646703018.926709,1646791771.926059,U02UBQJBYHZ\\n7a24390b-d744-4552-9b37-e70f086cb0c9,U02U3E6HVNC,,,Sorry <@U02TC8X43BN> and <@U02U3E6HVNC> - I just realised I was a week behind - I thought you were asking about week 5 :woman-facepalming:,1646418691.258709,1646792098.168379,U02U5SW982W\\n16b61877-1d50-432e-9b8a-3063e94455c6,U02U3E6HVNC,,,I think I\\'m just about to feel your pain...,1646418691.258709,1646792107.999839,U02U5SW982W\\n3bc40ce4-a105-4eaf-b19e-ffd74a3d51d3,,7.0,,week 6 error running  $ python3 producer_tax_json.py,1646792207.912569,1646792207.912569,U02U5SW982W\\nfeac7e4f-75a0-4108-a6ad-a14accec283c,U02U5SW982W,,,\"Hi All, just wondering if anyone is getting a Syntax Error running producer_tax_json.py. I am running it unchanged from a cloned repo. Here is the output:\",1646792207.912569,1646792309.864719,U02U5SW982W\\n999a67a3-8d2d-4333-8c24-54fb49805bc7,U02U5SW982W,,,\"```Traceback (most recent call last):\\n  File \"\"producer_tax_json.py\"\", line 3, in &lt;module&gt;\\n    from kafka import KafkaProducer\\n  File \"\"/home/sandy/.local/lib/python3.8/site-packages/kafka/__init__.py\"\", line 23, in &lt;module&gt;\\n    from kafka.producer import KafkaProducer\\n  File \"\"/home/sandy/.local/lib/python3.8/site-packages/kafka/producer/__init__.py\"\", line 4, in &lt;module&gt;\\n    from .simple import SimpleProducer\\n  File \"\"/home/sandy/.local/lib/python3.8/site-packages/kafka/producer/simple.py\"\", line 54\\n    return \\'&lt;SimpleProducer batch=%s&gt;\\' % self.async\\n                                              ^\\nSyntaxError: invalid syntax```\",1646792207.912569,1646792315.498759,U02U5SW982W\\n3b319d59-1981-45af-9749-ba4d9aa1dc31,U02U5SW982W,,,I did find <@U02TNEJLC84> post about the same thing. I did uninstall kafka (for some reason it was there) and then reinstalled kafka-python (requirement already satisfied) but now I get the error following error:,1646792207.912569,1646794051.066139,U02U5SW982W\\nf1a2f0c4-2355-4e66-aad1-66b94152c04d,U02U5SW982W,,,\"```Traceback (most recent call last):\\n  File \"\"producer_tax_json.py\"\", line 3, in &lt;module&gt;\\n    from kafka import KafkaProducer\\nImportError: cannot import name \\'KafkaProducer\\' from \\'kafka\\' (unknown location)```\",1646792207.912569,1646794058.922579,U02U5SW982W\\nee4b2999-43ff-408a-9652-a5c5e2cb7e2a,U02U5SW982W,,,Hi All - I will now be that person that asks and answers their own questions because they are so weird that no-one else has the slightest idea what they are on about :wink: . For some reason I had to uninstall both kafka AND kafka-python and then reinstall kafka-python. It all then seemed to work. No idea why... :woman-shrugging:,1646792207.912569,1646795568.535079,U02U5SW982W\\n98baeab1-4e36-4cec-9161-59837e0a8257,,6.0,,week 6 ImportError problem when running $ python3 stream.py worker,1646802335.624369,1646802335.624369,U02U5SW982W\\nfb3e316f-d3e8-490c-a4bc-66bf3e9aa2ec,U02U5SW982W,,,I seem to be having a lot of these problems. I\\'ve even set myself up in a virtual environment to try to help myself out but even though I can run producer_tax_json.py I now cannot run stream.py. This is the exact error I get:,1646802335.624369,1646802449.066239,U02U5SW982W\\n84a4950d-7c2c-4cd8-a43c-29c924a73706,U02U5SW982W,,,\"```Traceback (most recent call last):\\n  File \"\"stream.py\"\", line 6, in &lt;module&gt;\\n    topic = app.topic(\\'datatalkclub.yellow_taxi_ride.json\\', value_type=TaxiRide)\\n  File \"\"/home/sandy/git_week_6/data-engineering-zoomcamp/week_6_stream_processing/kafka_venv/lib/python3.8/site-packages/faust/app/base.py\"\", line 781, in topic\\n    return self.conf.Topic(\\n  File \"\"/home/sandy/git_week_6/data-engineering-zoomcamp/week_6_stream_processing/kafka_venv/lib/python3.8/site-packages/faust/app/base.py\"\", line 1796, in conf\\n    self._configure()\\n  File \"\"/home/sandy/git_week_6/data-engineering-zoomcamp/week_6_stream_processing/kafka_venv/lib/python3.8/site-packages/faust/app/base.py\"\", line 1740, in _configure\\n    conf = self._load_settings(silent=silent)\\n  File \"\"/home/sandy/git_week_6/data-engineering-zoomcamp/week_6_stream_processing/kafka_venv/lib/python3.8/site-packages/faust/app/base.py\"\", line 1752, in _load_settings\\n    return self.Settings(appid, **self._prepare_compat_settings(conf))\\n  File \"\"/home/sandy/git_week_6/data-engineering-zoomcamp/week_6_stream_processing/kafka_venv/lib/python3.8/site-packages/faust/types/settings.py\"\", line 768, in __init__\\n    self.Worker = cast(Type[_WorkerT], Worker or WORKER_TYPE)\\n  File \"\"/home/sandy/git_week_6/data-engineering-zoomcamp/week_6_stream_processing/kafka_venv/lib/python3.8/site-packages/faust/types/settings.py\"\", line 796, in __setattr__\\n    object.__setattr__(self, key, value)\\n  File \"\"/home/sandy/git_week_6/data-engineering-zoomcamp/week_6_stream_processing/kafka_venv/lib/python3.8/site-packages/faust/types/settings.py\"\", line 1204, in Worker\\n    self._Worker = symbol_by_name(Worker)\\n  File \"\"/home/sandy/git_week_6/data-engineering-zoomcamp/week_6_stream_processing/kafka_venv/lib/python3.8/site-packages/mode/utils/imports.py\"\", line 262, in symbol_by_name\\n    module = imp(  # type: ignore\\n  File \"\"/usr/lib/python3.8/importlib/__init__.py\"\", line 127, in import_module\\n    return _bootstrap._gcd_import(name[level:], package, level)\\n  File \"\"&lt;frozen importlib._bootstrap&gt;\"\", line 1014, in _gcd_import\\n  File \"\"&lt;frozen importlib._bootstrap&gt;\"\", line 991, in _find_and_load\\n  File \"\"&lt;frozen importlib._bootstrap&gt;\"\", line 975, in _find_and_load_unlocked\\n  File \"\"&lt;frozen importlib._bootstrap&gt;\"\", line 671, in _load_unlocked\\n  File \"\"&lt;frozen importlib._bootstrap_external&gt;\"\", line 848, in exec_module\\n  File \"\"&lt;frozen importlib._bootstrap&gt;\"\", line 219, in _call_with_frames_removed\\n  File \"\"/home/sandy/git_week_6/data-engineering-zoomcamp/week_6_stream_processing/kafka_venv/lib/python3.8/site-packages/faust/worker.py\"\", line 19, in &lt;module&gt;\\n    from aiokafka.structs import TopicPartition\\n  File \"\"/home/sandy/git_week_6/data-engineering-zoomcamp/week_6_stream_processing/kafka_venv/lib/python3.8/site-packages/aiokafka/__init__.py\"\", line 4, in &lt;module&gt;\\n    from .client import AIOKafkaClient\\n  File \"\"/home/sandy/git_week_6/data-engineering-zoomcamp/week_6_stream_processing/kafka_venv/lib/python3.8/site-packages/aiokafka/client.py\"\", line 10, in &lt;module&gt;\\n    import aiokafka.errors as Errors\\n  File \"\"/home/sandy/git_week_6/data-engineering-zoomcamp/week_6_stream_processing/kafka_venv/lib/python3.8/site-packages/aiokafka/errors.py\"\", line 4, in &lt;module&gt;\\n    from kafka.errors import (\\nImportError: cannot import name \\'ConnectionError\\' from \\'kafka.errors\\' (/home/sandy/git_week_6/data-engineering-zoomcamp/week_6_stream_processing/kafka_venv/lib/python3.8/site-packages/kafka/errors.py)```\",1646802335.624369,1646802483.045509,U02U5SW982W\\n108d5543-8e6f-45fa-8d5e-ac60748e0a35,,2.0,,\"One of the quiz questions, seems to be a bit ambiguous - “From where all can a consumer start consuming messages from” - Is the answer based only on the video or any method if possible is acceptable. For example “<https://developer.confluent.io/tutorials/kafka-console-consumer-read-specific-offsets-partitions/confluent.html>”\\n\\nCan someone chime in please?\",1646802814.414629,1646802814.414629,U02SMBGHBUN\\nbe284a3c-d18b-4624-b79a-b543cafc10ed,U02U5SW982W,,,\"When I did the install with pip `$ pip3 install kafka-python` it said  `ERROR: robinhood-aiokafka 1.1.6 has requirement kafka-python&lt;1.5,&gt;=1.4.6, but you\\'ll have kafka-python 2.0.2 which is incompatible.` I wonder if this is the problem? Did anyone else get this error?\",1646802335.624369,1646803096.081679,U02U5SW982W\\n1bc15325-9f9c-41d4-a369-c04dfb0388e7,U02U5SW982W,,,So the solution to this seems to be a `pip install -Iv kafka-python==1.4.6`,1646802335.624369,1646803675.172299,U02U5SW982W\\ndba3f63c-0734-4dd1-bda3-71a47758af48,U02R77FP943,,,It worked. thanks,1646769777.750379,1646805801.665779,U02R77FP943\\n0e61a7f1-2185-41b4-a7a4-8db148dee13f,U02R77FP943,,,\"I also need to set full control for the folder to fix Access denied error while running the command\\n`pip install -U pip`\\n is it fine?\",1646769777.750379,1646806150.043449,U02R77FP943\\ndcbd23fe-3568-4a0b-93a0-9b6d05159536,,2.0,,\"For the Week 6 Question: Create two streams with same key and provide a solution where you are joining the two keys\\nthe joins are not implemented in Faust :slightly_smiling_face:\",1646807293.427729,1646807293.427729,U02SMBGHBUN\\n8e5ce217-87df-41d6-a7a9-c60ccb8c793b,,,,I am reading the question as instead: “Create two streams with same key and provide a solution where you are *merging the values* from the two keys”. .. Does this sound accurate?,,1646807336.421359,U02SMBGHBUN\\nfd948c0d-1eed-40e3-b446-7c2991c2a16f,U02R77FP943,,,I\\'m not sure what you mean,1646769777.750379,1646808320.298909,U01AXE0P5M3\\ndd599a1d-35ab-433f-8470-94afa305da74,U02R77FP943,,,Like running it with sudo? Probably not a good idea,1646769777.750379,1646808368.628769,U01AXE0P5M3\\n56343a90-25ae-4fb9-8137-c5402a4ac650,U02TNEJLC84,,,I remember having similar problems with docker compose and the solution was to build the images separately with just docker and then using tags in compose yaml,1646763790.714439,1646808474.453829,U01AXE0P5M3\\n105f7b4e-228b-46d9-8fd3-ef0da7cfc8e5,U033QG69HRQ,,,\"<@U02T9550LTU> yeah i only have 1d/7d (10hr) because of work, python, bash, docker, API, VS Code are not new to me but still\",1646723933.318809,1646811061.947399,U033QG69HRQ\\n7a3fd660-a8df-40d6-9b16-02282fcf0415,U033QG69HRQ,,,\"the toughest (hence time consuming) is always about understanding the underlying reasons, otherwise just using tech has never been that much of a problem to me\",1646723933.318809,1646811173.623319,U033QG69HRQ\\ne8b4db58-627d-49fb-a712-43bbfd718172,,,,\"Hello I\\'m Elizabeth from Abuja, Nigeria. Feels good to be here\",,1642062498.375900,U02TPSG86QL\\ndc04fb05-22fd-4744-af20-2ffba0359fe9,,,,\"I\\'m Dendi from Bandung, Indonesia. Let\\'s have some fun.\",,1642063403.376800,U02TV0FJ3UJ\\n15b363af-6f2f-46db-94a4-e9cd02c774f4,,7.0,,\"Google Cloud subscription: Check :slightly_smiling_face:\\nI already had the docker and terraform installed.\",1642064217.378200,1642064217.378200,U02CD7E30T0\\n38e92837-54b2-43ce-adde-e0ded0b5db04,,2.0,,\"Hi, I haven’t receive any mails after subscription on <https://github.com/DataTalksClub/data-engineering-zoomcamp>, where can I find the details of the training starting on 17th Jan?\",1642065559.379000,1642065559.379000,U02TG8SSMAS\\n8dd0f1c8-c071-4886-a91a-346370f0700b,,3.0,,\"Questions:\\n• It is a good ideia to fork this Git Hub project to our personal Git Hub?\\n• Should we build an empty Git Hub project just for this (in our personal Git Hub)? \",1642066774.381300,1642066774.381300,U02CD7E30T0\\n32c163a3-fbe5-4957-8dcf-9cb34a82332f,U02TG8SSMAS,,,The link contains all the details. You\\'ll get more information here in slack and on email on Sunday,1642065559.379000,1642067097.381500,U01AXE0P5M3\\n7cfd7a48-dadf-4441-90c8-11467c769694,U02CD7E30T0,,,I\\'m not sure how a fork would be helpful. But it\\'s a good idea to have your own repo - to follow us along and commit all the code,1642066774.381300,1642067145.381700,U01AXE0P5M3\\nfb98707a-997f-41f0-9630-9dc93e24c610,U02CD7E30T0,,,\"Python as well? \\n\\nI recommend anaconda\",1642064217.378200,1642067166.381900,U01AXE0P5M3\\nade90376-cac1-4d82-9469-6bf8b559e233,U02CD7E30T0,,,Thank you Alexey :wink:,1642066774.381300,1642067786.382400,U02CD7E30T0\\n0f4ade10-d2dd-4011-ae83-d822f211a243,U02CD7E30T0,,,\"Ah, Of Course :slightly_smiling_face: Python is my daily tool\",1642064217.378200,1642068087.383000,U02CD7E30T0\\nce05f0ee-f226-40af-baf2-d589c9fef6d4,U02EHEM7BP1,,,Azure doesn\\'t have big query but there is probably an alternative.  So you can try,1642046773.367700,1642068247.383200,U01AXE0P5M3\\nf8c39bdd-eb25-4016-a11b-6ead9147ac8e,U02JRJJHFL6,,,Both wsl2 and plain windows should work,1642038323.365400,1642068292.383400,U01AXE0P5M3\\nd34fb8bb-1dc4-44a4-8e51-fb62d299c149,U02EHEM7BP1,,,\"There is Azure Synapse Analytics providing many things of the box - Serverless SQL pools, MPP DB - Dedicated SQL Pool (formerly known as Azure DWH), Spark pools, ETL/ELT tool functionally identical to Azure Data Factory, seamless integration with Cosmos DB, Databricks and PowerBI, etc.\\n\\nI am not sure however if the things, which will be taught here with GCP, can be easily translated to Azure. Also, keep in mind, that they might have different billing schemas, and what is free in GCP, might cost a thing in Azure. For instance, the use of Dedicated pools is billed per time used and one can get a huge bill simply forgetting to shut them down when all activities are done.\",1642046773.367700,1642069527.383900,U02Q7JMT9P1\\nab150def-bdd7-4c0b-af87-1fa80a0c0d78,U02EHEM7BP1,,,\"Under the hood all clouds use or integrate the same tools, as the open source stack rules, but their philosophy isn\\'t certainly the same! Bottom line: you can do the same stuff, but differently, and at a distinct pricing!\",1642046773.367700,1642072801.384600,U02GVGA5F9Q\\nd6f96cc0-c9e7-4cc6-aedf-cb48c6813e68,U02CD7E30T0,,,Maybe a clone? :wink:,1642066774.381300,1642072920.384800,U02GVGA5F9Q\\nf1940a75-ca91-4b4b-8a35-b64393c135d4,U02CD7E30T0,,,Do we have to install Docker and  terraform locally on laptop?,1642064217.378200,1642074393.390800,U02TNA9H75E\\n7bf6c189-ede8-440f-b54e-65067546b9f1,U02CD7E30T0,,,Yes,1642064217.378200,1642076259.391000,U01AXE0P5M3\\n3a9a1f6e-30d7-4e53-8ad8-68667aaef19f,U02CD7E30T0,,,\"Also Python, don\\'t forget about it\",1642064217.378200,1642076273.391200,U01AXE0P5M3\\nD36459FD-8BC8-4A99-846D-E4E65A676ED5,,2.0,,\"Hey everyone! I\\'m Dan, a Canadian data analyst looking to expand my stack and make the transition into DE. Very excited \",1642076386.392500,1642076386.392500,U02TVGE99QU\\n19C5776E-0E88-4FA7-A932-78320974F13B,,1.0,,\"Not sure if the question got answered, but what’s the general opinion on OS/doing this in a VM or not? \",1642076452.393900,1642076452.393900,U02TVGE99QU\\nb031f5c9-3db8-4b8a-857e-3926801aa1b7,U02QR8AF4N6,,,me too :slightly_smiling_face:,1641902264.255900,1642076457.394000,U02S9ND0Q03\\n43bbd580-03d1-487f-82eb-5a4141240f06,U02TVGE99QU,,,<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1641913480265900?thread_ts=1641913480.265900&amp;cid=C01FABYF2RG|https://datatalks-club.slack.com/archives/C01FABYF2RG/p1641913480265900?thread_ts=1641913480.265900&amp;cid=C01FABYF2RG>,1642076386.392500,1642077179.394300,U02G4HSDY0G\\ndc46f7cd-49fb-45e9-99e8-47ab28c390eb,,2.0,,\"Is there anybody like me, who can\\'t do subscription to Google Cloud Platform with PayPal as payment method in Germany? Just asking, in case someone had the same problem and found a solution around.\",1642078611.396700,1642078611.396700,U02DTUQL1EV\\n2a7a5880-2ff2-423c-b468-0b87668d0edd,U02TG8SSMAS,,,Thanks <@U01AXE0P5M3>,1642065559.379000,1642079223.396900,U02TG8SSMAS\\nde118bcf-39ec-45f3-a3d9-96227c15984b,U02DTUQL1EV,,,PayPal didn\\'t work for me either - had to use a credit card (issued in Germany),1642078611.396700,1642081204.397600,U02Q7JMT9P1\\nf5d41c02-9d12-41f1-9230-f6fa4efde02d,,,,Hello all I\\'m Aya from Egypt I\\'m a software developer (ML focused for a couple of years). I\\'m here because I hate being clueless about DE :slightly_smiling_face:.looking forward to the zoomcamp,,1642082659.400000,U02TWFZURD1\\n61ab21f1-64b7-4e49-be84-45a4cc608d15,U02DTUQL1EV,,,Thanks. Good to know that it is not specific to me.,1642078611.396700,1642085016.403200,U02DTUQL1EV\\n997be158-07b6-4799-90b1-613695b634ab,,1.0,,Hi everyone! I\\'m Alex from Canada. Currently a data analyst but trying to move into the DE space. Big thank you for organizing this program :slightly_smiling_face:,1642085335.404300,1642085335.404300,U02TG61D2J3\\nff594d2d-f50e-4da8-914a-d397121563d2,,1.0,,:wave:,1642087734.405800,1642087734.405800,U02ULQFCXL0\\n086707AB-7A5C-4F15-9642-28382D954F7C,U02TVGE99QU,,,\"There is not os requirement, u are going to need python, docker and terraform installed for the course and spark I guess\\n\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1641924779284500?thread_ts=1641917776.277100&amp;cid=C01FABYF2RG|https://datatalks-club.slack.com/archives/C01FABYF2RG/p1641924779284500?thread_ts=1641917776.277100&amp;cid=C01FABYF2RG>\",1642076452.393900,1642091886.408700,U02TRHFHDH7\\nf3c96503-164b-45dd-aa44-9e3fdfb65db6,,,,HI everyone. Greetings from Colombia.,,1642092811.410000,U02TEKL21JQ\\n97ad9063-547b-4a3b-b594-528ada84a0f0,U02ULQFCXL0,,,Nice picture!,1642087734.405800,1642095152.410500,U01AXE0P5M3\\nbe94634e-9689-492e-8adb-fe1c48f38aac,U02JRJJHFL6,,,\"<@U02TRHFHDH7> Yes, I just wanted to know if that\\'s necessary. Thank you!\",1642038323.365400,1642100124.411800,U02JRJJHFL6\\n6357b555-ad6a-488f-81ca-56daf296d304,U02JRJJHFL6,,,Thank you very much Alexey! I\\'ll stick on to Windows then.,1642038323.365400,1642100173.412000,U02JRJJHFL6\\n44c26822-f29f-4fba-b8dc-39341514e376,,,,\"Hi, This is Ipek from Turkey\",,1642100460.412700,U02TYEX3D5H\\n8a2641f6-2b91-44da-9304-c82566ad0875,,13.0,,\"Hello <@U01AXE0P5M3> I downloaded terraform on my laptop, but I couldn\\'t Install it\",1642100796.413900,1642100796.413900,U02TNA9H75E\\nab3660a2-1c91-4706-8edc-6a048e471e34,U02TNA9H75E,,,I think you just need to unpack it and that\\'s it,1642100796.413900,1642101098.414000,U01AXE0P5M3\\n6090c980-d837-410a-9bc5-6f61ac74e730,U02TNA9H75E,,,How do i unpack it?,1642100796.413900,1642101510.414300,U02TNA9H75E\\n9b87c331-a6a4-4998-baeb-349d5e318a5c,U02TNA9H75E,,,What i did was to unzip it from the zipped folder,1642100796.413900,1642101553.414500,U02TNA9H75E\\n64b82e17-41d8-4719-9f98-3a8fa3fb9bf1,U02TNA9H75E,,,Then you need to put it to a folder that\\'s in your PATH variable. Which OS do you use?,1642100796.413900,1642103709.415000,U01AXE0P5M3\\nad8a976a-6a68-40fd-94a2-38bba2414cb3,U02TNA9H75E,,,Window 10,1642100796.413900,1642104245.415300,U02TNA9H75E\\n0c88008c-7ed1-4769-9412-10405b043002,U01AXE0P5M3,,,\"I\\'ll be using my day-to-day Ubuntu system, but will use Windows 11 with WSL2 too, to get acquainted with it. Something tells me it will be my next work\\'s architecture... I love Manjaro too, and it was my full stack web dev system of choice, but I\\'ve to many things in hands and Arch sometimes requires some attention :wink:.\",1641913480.265900,1642104998.415700,U02GVGA5F9Q\\n874c4d8e-8e8a-4eb8-af89-48598ed36c66,U02TNA9H75E,,,<@U02TNA9H75E> <https://m.youtube.com/watch?v=ljYzclmsvF4|https://m.youtube.com/watch?v=ljYzclmsvF4>,1642100796.413900,1642109039.417000,U02S848271C\\n85c62ade-fe2e-4b96-b7df-1cf7a41a7d3b,,,,\"Hi everyone, I\\'m David, a data analyst in Nigeria looking to transition to DE. Super excited!\",,1642110114.418900,U02UC164U2D\\nbec945c9-b497-465d-bd93-e90cd63ae84e,U02TNA9H75E,,,\"This also could be useful \\n<https://stackoverflow.com/questions/44272416/how-to-add-a-folder-to-path-environment-variable-in-windows-10-with-screensho|https://stackoverflow.com/questions/44272416/how-to-add-a-folder-to-path-environment-variable-in-windows-10-with-screensho>\",1642100796.413900,1642110837.419500,U01AXE0P5M3\\n2377FEBB-43C0-4924-813B-BCAAB3E8C757,,9.0,,\"Hi <@U01AXE0P5M3>, \\n\\nI believe I shouldn\\'t have much concern with using a Linux machine?\",1642114905.421300,1642114905.421300,U02SDHJMASG\\ncb8d91fe-21fc-461d-87fe-2252777eff47,U02SDHJMASG,,,Why should you? Linux is the natural environment for all this tools.,1642114905.421300,1642119540.423700,U02GVGA5F9Q\\ne183590e-608f-49d1-8ce5-c8ae4a430a2b,U02SDHJMASG,,,Great. It\\'s my second week ditching windows for Linux.  I now have all environment set up.,1642114905.421300,1642121824.424100,U02SDHJMASG\\n1ae24830-cd3d-44a7-baec-ea72319ac2e5,,,,\"Hello all, I am Eduardo and I am from Brazil. Glad to be here.\",,1642124987.424900,U02U072HLV8\\n8f098ca6-aea1-44e9-ab2b-08612f4e1934,U02TNA9H75E,,,Thank you,1642100796.413900,1642134785.425700,U02TNA9H75E\\nc29a59b8-06a7-4808-896e-01d6a4f79ad8,,,,\"Hello Everyone, Nakul from India!\\nLooking forward to a wonderful learning experience :hearts:\",,1642140367.427600,U02T49DENET\\nc51bf544-c7c5-4032-9c7f-7b795a86a07b,,,,\"Hi everyone, my name is Ariyo Kabir, I\\'m from Nigeria. Very happy to join the data engineering course team. Thanks to the organizers.\",,1641729372.027900,U02STT0TPGX\\ne4eff026-60aa-4cb3-a110-1318a20d22f8,U01UMAXUPSQ,,,\"Most of the things I need from the Linux world are available on Windows + WSL, so Windows should be fine\",1641638240.019000,1641730208.028100,U01AXE0P5M3\\nc89b9e8d-32f9-430e-ba72-5a0e4648457c,U01UMAXUPSQ,,,\"I\\'ll probably try to use both systems, as there are some minor quirks and issues, as Alvaro said before, that I\\'m interested to be acquainted with. I personally use Linux for everything, but many companies use Windows, so WSL2 is a strong contender!\",1641638240.019000,1641731262.028500,U02GVGA5F9Q\\na701840b-2e88-4fe7-8fb1-5ad4b0543018,U02RHT0M3M5,,,\"Oh so sorry guys!! I talked to Alexey and he told me all what we seek about will be in week 1 , so i dont think that workshop will make sense for us before the training itself!! \\n\\nI wish you all happy learning zoom cam! So sorry for late reply guys\",1641159804.189100,1641732163.031700,U02RHT0M3M5\\nefb7c8a4-d06a-4dab-95d7-1764bf7fcfd2,,,,\"Hello everyone, I\\'m Adanna. I just finished university and I\\'m looking forward to learning from this course and eventually building a career in Data Engineering.\",,1641732370.032300,U02SE3ZKQVA\\nebd7240a-6ae7-44b2-84cd-ab7de327bbf0,U02R65WFT16,,,\"Hey, the course will not cover relational db and NoSQL concepts.\\nAs part of the prerequisites, we expect the attendees to be already familiar with at least the basic concepts of writing SQL. And we will cover some of the necessary concepts around writing SQL and data modelling to level the knowledge and make sure everyone can understand the project we\\'re presenting.\\nYou can find more details of the specific topics we\\'ll cover in the <https://github.com/DataTalksClub/data-engineering-zoomcamp|course repo>. Let me know if you have any questions.\",1641433476.001000,1641732908.032500,U01B6TH1LRL\\n83cff8e6-e9b5-4c66-978c-07aa88d96f97,U01AXE0P5M3,,,Thanks <@U01AXE0P5M3>,1641566368.013200,1641735772.034200,U02T76JHS4S\\nedfbff94-11d4-4db1-b53b-b62dd6caa6fd,,,,\"Hello, everyone. I\\'m Olena from Ukraine. At the moment, I am working as a Full Stack Engineer. Glad to be here and work with data.\",,1641745139.035700,U02T23N4RCN\\n541f8cb4-57d8-45b9-b6ec-69031b84be9e,,,,\"Hello everyone, I\\'m Balogun Taiwo from Nigeria. Currently working as a Data analyst. Open to learning new stuffs related to data\",,1641761588.039500,U02TNA9H75E\\n69aa413a-192a-4925-85c3-34faea9f3071,,,,\"I finally sent the first confirmation email for the course - here it is\\n\\n<https://mailchi.mp/datatalks.club/dezoomcamp-1-0>\\n\\nIf you didn\\'t receive - don\\'t worry. Since you\\'re in Slack already, it okay if you don\\'t have it. But do let me know if you were expecting it but couldn\\'t find it\",,1641765666.041400,U01AXE0P5M3\\n9f546f4e-44c8-4781-aab1-99f315f5a06c,,,,\"Hi everyone, I\\'m Angela, a data analyst. 2022 goal is to become a data engineer.\",,1641766299.044300,U02TMP4GJEM\\n5653cd50-85a2-4f61-b113-f4fe6e7d6649,,,,\"Hello, I\\'m Jason.  I\\'ve done a lot of different things in my career, trying to transition now to a data focused role.  Just completed a development bootcamp that dipped a little in to devops &amp; data engineering - looking forward to going further with the latter\",,1641766441.046100,U02SUFM2GT1\\n87398590-17c3-4b40-bdbe-1d18d3bae9b2,,,,\"Hello all, I\\'m Angela from the US. I\\'m excited to take the data engineering course to help grow more as a data engineer, thank you for this course!\",,1641766770.048000,U02TBD1RG2Y\\nfd1b8ed6-9e0d-41eb-ad0a-a3de1a019750,,1.0,,\"Hello all ! I am Daniela , living in Canada. I  am a Data Scientist and am hoping to learn new skills  for a position as a Data Engineer.\",1641766861.048900,1641766861.048900,U02T2DX4LG6\\n1CAB04AD-948E-47A7-99F1-D723D784964E,,,,\"Hello! I’m living in Seattle, currently working as a data analyst. I’ve been the only guys that’s building out our data pipeline, which sparked my interest in data engineering. I’m hoping to move towards being a data engineer. Nice to meet you all! \",,1641767276.052200,U02T94VFDK4\\n704af461-a65c-441d-a157-3aab3799fe4d,,,,Hi Koffi from France...already a data engineer but want to level up on GCP,,1641767286.052500,U02S9RWSNKD\\n5BD52D39-6D0A-4CF8-9E16-BB1CB21AE349,,3.0,,\"Hey everybody,\\n\\nDoes anybody know what day, what time and what time zone the course will occur every week?\",1641767761.054300,1641767761.054300,U02T94TSC6N\\n0364c05a-67a5-4ea2-95c9-156c56afbba0,U02T94TSC6N,,,\"This is what the FAQ says in the GitHub readme:\\n\\n&gt; Q: At what time of the day will it happen?\\xa0A: Most likely on Mondays at 17:00 CET. But everything will be recorded, so you can watch it whenever it’s convenient for you\",1641767761.054300,1641768445.055600,U02SE2PSSTC\\n08f81bf1-0e9e-4fdd-9fad-8d66b74b18ad,,2.0,,\"Hey, Matt here. I\\'m a data scientist but looking to learn more about data engineering. Thanks for building this course <@U01AXE0P5M3> <@U01DHB2HS3X>. The github repo looks great!\",1641768630.056700,1641768630.056700,U02S51KRTNZ\\n4321237c-821a-4ba2-a21f-2fd15ea5fcd5,,2.0,,\"Very much looking forward to this course, and especially learning more of DBT and BigQuery.\\nAlso, great work with the Repo <@U01AXE0P5M3> <@U01DHB2HS3X>\",1641768851.058400,1641768851.058400,U02QNEVDSMA\\nbf4396fe-e6d1-4d3c-8885-222619f4a66b,,,,\"Hey, I am Sunil here. Very much looking forward to this course.\",,1641773386.063400,U02T991SQHH\\n2162D821-8BF7-479D-9511-E34711BA67BF,U02T94TSC6N,,,Thank you <@U02SE2PSSTC>. I saw it but was not sure if it is already final as it was referring to “most likely”.,1641767761.054300,1641774167.065200,U02T94TSC6N\\n5e5004fa-1b17-4364-9b55-7c3d1d6eb4bd,,,,\"Hello Everyone, I am Sumanth Shivashankar, currently working as a Data Analyst in USA. Excited to learn more on Data Engineering tools and concepts. Thank you for providing this opportunity. Looking forward to learning.\",,1641776113.068200,U02T9ACK727\\n2150f6bf-6687-4397-8517-81116d60999c,U02R65WFT16,,,<@U01B6TH1LRL> thanks a lot for the address!,1641433476.001000,1641779278.070700,U02R65WFT16\\nf4fc0b7b-b366-440a-a7ff-dbf03aae7d43,,,,\"Hi All, this Anjaiah and exited to begin this journey\",,1641780611.071800,U02QPGN7J3Z\\n3C0BAD8E-8A2A-4D85-A717-C970B64E7AA4,,,,Hello! Femi Fajolu here. Currently a Data Analyst and looking to add data engineering skills to my Arsenal this year. I appreciate this opportunity. Thank you very much. ,,1641782248.074600,U02SUR1D4F9\\n033c103e-6992-4805-8cee-c4ef7ec7512d,,,,I am Shijin from India. Looking forward to learn and grow professionally with this course. Thanks for this amazing opportunity,,1641782675.076200,U02R8GHJE3W\\n9fc0db56-9a43-4399-9a1d-2ca26ada3b53,,,,\"Hi guys, I am Shourya, a Data Engineer, looking forward to this course and excited to see how it pans out. Let\\'s make something great :tada:.\",,1641785578.079000,U02T2RFCJ3Y\\n95ab7f8e-c8aa-479b-ab7a-777dd58fa05f,,,,\"Hello, all. Will from the US, checking in.\",,1641786274.080100,U02T99T90Q2\\n6dc53953-658d-49c3-a380-549ae97a49c3,,,,\"Hello everyone, I am Gopakumar Nair working as a Technical Manager looking forward to learn from this course.\",,1641788163.081700,U02TBQ9J3HS\\n4fecb9cc-19bc-423a-ab47-2b10e4d67b1d,,,,\"Hello everyone, I am looking forward to network and learn data engineering. I am currently a fresher.\",,1641788995.084500,U02T6M636ER\\nd1e8d5c4-5ae5-47e6-a8be-1ba7d6dceb85,U01UMAXUPSQ,,,\"<@U01AXE0P5M3> a quick question not directly related though. If I\\'m using WSL on Windows, do I also have access to USB hardware?\",1641638240.019000,1641789638.084600,U01SPC4FKLK\\n3314345e-707f-4ef1-956f-18c6347ec12d,,,,\"Hello Everyone, looking forward to join data engineering zoomcamp\",,1641791077.086700,U02J4323PHS\\nf0872faf-8f4d-450c-b36e-9c510f54e888,,,,\"hi guys, i\\'m arbi excited to learn new things here.\",,1641794241.090000,U02TC0Q1YJ0\\n80a8e4ca-3fdd-4128-958f-00268f6abbae,U02T2DX4LG6,,,\"You got this.  My personal opinion, knowing data science first makes for an easier jump to data engineering\",1641766861.048900,1641795134.090600,U02RYKKPT5W\\nfe7c11ed-4613-44db-81e7-ec232517c07c,U01UMAXUPSQ,,,\"I\\'ve never tried, but I think so\",1641638240.019000,1641795519.090900,U01AXE0P5M3\\n37086926-1726-41d8-bccb-1f4bcc98e7c8,U02T94TSC6N,,,\"I\\'ll update it soon and remove \"\"most likely\"\" =)\",1641767761.054300,1641795743.092200,U01AXE0P5M3\\nc42315ba-bd95-4a50-adc5-049596b9fbd2,,,,\"Hi All, Nazri from Kuala Lumpur. Thanks to organizer. Looking forward to this course\",,1641795764.092800,U02QNSQ40HJ\\n0f099da6-7269-4043-947d-60687f30cb71,U02S51KRTNZ,,,You\\'re welcome! And don\\'t forget about Ankush and Victoria as well =),1641768630.056700,1641795766.092900,U01AXE0P5M3\\n30f2bad3-8f69-49a0-996a-179a2398312a,,,,\"Hi everyone, am super excited to learn and grow professionally with this course.\",,1641797150.096100,U02T9TPGBBM\\n88417da7-dbbd-4601-9021-466ad942c27f,,,,\"Hi All, Looking forward to learn a lot from the zoom camp\",,1641797459.096700,U02T378EZL6\\ne9bc10c6-0d2e-4edb-b2b4-7f626c7d8901,,,,lets go!!!,,1641798082.097400,U02RTJPV6TZ\\nfcd9fa71-e28e-4aec-8233-80b16f114e4f,,8.0,,\"• newbie here, how do i setup the following as requested: Docker\\xa0\\n• Terraform\\n• Python 3\\n\",1641798212.098100,1641798212.098100,U02RTJPV6TZ\\n2b312443-891d-47ef-a8e2-a7719f848626,,,,any guides,,1641798244.098300,U02RTJPV6TZ\\nE8AB4214-129F-4EFE-B7D4-BD3C38198B66,,,,\"Hello everyone, this is Pelin from Turkey. Excited to learn new things..\",,1641798763.100600,U02RB70NHPS\\ne3f67bdc-7829-40cc-ba0d-7bac41b08e6a,,,,\"Heya, Grégoire here, looking forward polishing some engineering concepts and becoming a master kuberneters. I am also very curious to check the flow of Alexeys\\'s course as the talks he organizes are always very interesting and the one one machine learning was also very enlightening. I am based in Berlin.\",,1641799794.103300,U02TC8X43BN\\nd6dfd991-0016-4d42-b632-bb9fe940f451,,,,Hello everyone from Madrid (Spain)!!! Happy to be here and looking foward to improve our data-engineering skills with this course,,1641800327.105500,U02TCA6QKEY\\n2264907b-a26c-4c8e-8b02-8166fecc42c4,,,,\"Hey everyone, I am Mannan Bansal from India currently a 4th year student. I am looking forward to get my hands dirty in data science field and awesome learning experience from this course.\",,1641800577.107900,U02QYKBJWN6\\nB177789A-1FB4-46EC-9696-50925C84CFCB,U02RTJPV6TZ,,,\"You can download them online. Just google. For instance Python download, Docker download.\\n\\nI haven\\'t downloaded Terraform before but I believe it should be as easy as others \",1641798212.098100,1641800653.109700,U02TBCXNZ60\\n53235513-6A9D-45C3-B6E3-7E0600B19D84,,,,Hey everyone. I am Ahmed from India. Currently looking for a data analyst job. I am looking forward to learn more about data. ,,1641801259.111800,U02QHQU3LF7\\nd8caa7b8-6186-4a7d-9dc6-da0068ebd7ef,,5.0,,\"week 4\\nplease did anyone encounter tis error when trying to run their model?\",1646043212.372849,1646043212.372849,U02TA6MJZHR\\n1400cfea-e53e-4f93-a724-93a9839124b8,U02TA6MJZHR,,,Have you checked in the channel or the FAQ? I remember someone posted this before,1646043212.372849,1646044900.544849,U01B6TH1LRL\\n449d48d1-3e65-46fd-8e05-491dbf203868,U02TA6MJZHR,,,alright will do..thank you,1646043212.372849,1646049561.943359,U02TA6MJZHR\\na18b3c1c-b147-47d3-9b9c-b9d75f061d51,U02TA6MJZHR,,,didn\\'t find anything for this,1646043212.372849,1646049652.753199,U02TA6MJZHR\\ne4cd0da7-e149-46ed-a58f-76d9ceeaf32a,U02TA6MJZHR,,,<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1645634338661579>,1646043212.372849,1646058520.510839,U01B6TH1LRL\\n807d10c4-f2bf-4a45-8ed6-d4592211b972,U02UBQJBYHZ,,,\"Yes Mac OS. I\\'ll have another go at it today, try wget, see what happens and report back. I\\'ve also been using \"\"du\"\" on terminal to get the folder size, maybe it gives a different result.\",1646014250.535359,1646068862.679959,U02UBQJBYHZ\\n237ce831-89b6-4f0a-897b-14a7b9bf48f0,U02QL1EG0LV,,,<@U01AXE0P5M3> it seems to vary on Mac OS as well. I marked the closest one.. in MacOS it was 24 files of 8.4MB each,1645954905.103999,1646070780.332539,U02SMBGHBUN\\n9edeb02f-c6e9-49b0-bd80-63ffef50817c,U02QL1EG0LV,,,I got 202Mb. Ubuntu 20.04,1645954905.103999,1646070986.662709,U02U3E6HVNC\\n96ddf80c-6527-4184-abb9-104416de12a1,,6.0,,I\\'m really at a loss for question 4. Is there something obviously wrong in my SQL code?,1646071455.536359,1646071455.536359,U02U3E6HVNC\\ncc34502f-bcdb-4a85-af4c-37cff5ef2dc5,U02U3E6HVNC,,,\"Seems good. Just one step is missing. Check week 1, there was something similar\",1646071455.536359,1646071525.871089,U01AXE0P5M3\\n00f2572a-4f2d-410d-9f87-54dc9a101aec,U02U3E6HVNC,,,\"Oh gosh I had \"\"sort by\"\" instead of \"\"order by.\"\" No idea what \"\"sort\"\" even does\",1646071455.536359,1646072039.964649,U02U3E6HVNC\\n112391ba-1e1d-487f-9367-82194f119ff7,U02U3E6HVNC,,,But why it works? :thinking_face:,1646071455.536359,1646072470.594179,U01AXE0P5M3\\n8588181c-ccc5-4b1c-b7f4-c0cb0b1d1969,U02U3E6HVNC,,,Looks like it\\'s doing the same thing as order by,1646071455.536359,1646072499.863329,U01AXE0P5M3\\n112a8eb9-1603-49a6-97db-f0f0b33b4468,,7.0,,\"Hi, is there a thread on datasets that could be useful for the project? I think it was mentioned on today\\'s office hours.\",1646074398.401919,1646074398.401919,U02CKHHRY6Q\\n789494f1-85bb-41f7-a756-39b8010d4d98,U02CKHHRY6Q,,,\"Hey <@U02CKHHRY6Q> the only ones I remember are:\\n<https://www.kaggle.com/c/avazu-ctr-prediction>\\nand\\n<https://commoncrawl.org/>\",1646074398.401919,1646074861.792809,U02TNEJLC84\\n0e779f93-e336-4fdf-941d-899b553511f1,U02U3E6HVNC,,,\"According to the <https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-sortby.html|docs>, the difference is that `SORT BY` doesn\\'t guarantee a total order in the output, as `ORDER BY` does.\",1646071455.536359,1646078341.362279,U02DNSC6Z52\\naa24369f-b7cb-48cd-a3cb-4f525cac85b1,,1.0,,Can a udf be used inside a spark sql query or not?,1646079659.023879,1646079659.023879,U02T9JQAX9N\\n3ca9d492-3405-4f53-bf40-8dbcda05098d,U02U3E6HVNC,,,\"Ah that makes sense, thanks for the clarification\",1646071455.536359,1646079766.842069,U02U3E6HVNC\\nb3a70ab5-a71a-45b5-a275-84018648c544,U02UBQJBYHZ,,,Give it a go <@U02UBQJBYHZ> and let us know how you get on.,1646014250.535359,1646079822.583329,U02U5SW982W\\n6e684ea6-c177-4b19-9b17-a54f43b5c6fa,U02CKHHRY6Q,,,<https://datatalks-club.slack.com/archives/C02V1Q9CL8K/p1643721123738689?thread_ts=1643721123.738689&amp;cid=C02V1Q9CL8K|https://datatalks-club.slack.com/archives/C02V1Q9CL8K/p1643721123738689?thread_ts=1643721123.738689&amp;cid=C02V1Q9CL8K>,1646074398.401919,1646081249.539919,U01AXE0P5M3\\nd2aa1ccf-64d8-43b4-affb-55178e31afc3,U02CKHHRY6Q,,,\"These messages will soon disappear, we should copy them\",1646074398.401919,1646081275.410919,U01AXE0P5M3\\n3f502998-d8a7-4b70-b8a3-6fb83e64a64c,U02CKHHRY6Q,,,\"Great, thanks!\",1646074398.401919,1646081471.157749,U02CKHHRY6Q\\n798064ec-aa83-439b-a60f-d36a6f80a22a,U02T9JQAX9N,,,\"Interesting. I haven\\'t tried it, but might test it tonight. <https://sparkbyexamples.com/pyspark/pyspark-udf-user-defined-function/>\",1646079659.023879,1646082413.690469,U02TNEJLC84\\nec29dab7-2c2b-45d2-bee9-2529abb87c74,,3.0,,\"Hello everyone.\\nIn Question 3 we should consider only day 15/Feb or all Feb?\\nI am asking this because we have this on the homework.md\\nBut then on the google form it says only \"\"Question 3: Records on Feb 2021\\xa0*\"\"\\nThank you advance. :)\",1646090410.914779,1646090410.914779,U02CD7E30T0\\ne29d21ff-b3c7-483a-8a39-5b47335870bf,,,,Just Feb 15 <@U02CD7E30T0> I believe,,1646093213.642679,U02U5SW982W\\n603f2f63-fdbb-437c-8b26-de911bf703ba,,2.0,,Homework week 5 - Q5 dispatching_base_num,1646096161.453369,1646096161.453369,U02U5SW982W\\n74da2e48-e6e2-45e6-8249-79f97a6f2c33,U02U5SW982W,,,\"Hi All,\",1646096161.453369,1646096178.420709,U02U5SW982W\\n17de9c7c-5877-4b68-aa9a-ced6895cf9c5,U02U5SW982W,,,(me and my auto &lt;Enter&gt; key :face_with_rolling_eyes:) - I will just answer my own question. I thought it was asking for the actual dispatching_base_num - which it isn\\'t. It\\'s asking for stages...,1646096161.453369,1646096291.561639,U02U5SW982W\\n9e2c594e-7807-46dc-aa88-417772d3a8bf,U02BRPZKV6J,,,from the docs i don\\'t think you can change locations after you have created a project..i had the same error and i had to create a new project to change my location,1645634338.661579,1646099471.483979,U02TA6MJZHR\\n7adf96af-2b63-4b0c-b790-196db560cb58,U02TA6MJZHR,,,thanks...we\\'re not supposed to change locations after creating a project that\\'s why,1646043212.372849,1646099517.272199,U02TA6MJZHR\\n741d852d-5c6b-48d1-a8bd-1dd884bd1c59,,3.0,,\"While I was going through the Kafka videos, I remembered seeing a children\\'s book on Kafka with Otters. It\\'s actually a fun read!\\n\\nHere\\'s a link for anybody interested: <https://www.gentlydownthe.stream/>\",1646099641.598249,1646099641.598249,U02ULP8PA0Z\\ndb708a0b-a3ac-4617-81bb-9a0d8fe10efd,,2.0,,\"Hello, I cant access the control center, any help\",1646118173.058739,1646118173.058739,U02U5G0EKEH\\n983b11ca-25cd-49e2-8b32-f2c26ee38373,U02CD7E30T0,,,It should be only 15 Feb..,1646090410.914779,1646120041.181969,U02T9JQAX9N\\nba0422a6-8dfa-4593-a8ac-cd4bf516388c,U02CD7E30T0,,,\"If its all of Feb, that would imply the whole dataset\",1646090410.914779,1646120064.894909,U02T9JQAX9N\\n9628b866-d8cb-4eba-9f51-604c9b896b48,,2.0,,\"Hi! Do I have to still sign the form on airtable to get the zoom link, or is this enough? I am already a member of the slack workspace/this channel\",1641894991.251000,1641894991.251000,U02CCSM8WSY\\n978f3213-c624-4cc8-803f-106171bd21dc,U02CCSM8WSY,,,Leaving your email and/or registering in slack is enough. I\\'ll send you all the links soon - and post them in the channel as well,1641894991.251000,1641895256.251300,U01AXE0P5M3\\nad2d29a6-10f7-4934-acaa-f7b4495955d0,U02CCSM8WSY,,,\"<@U01AXE0P5M3>, Grat work\",1641894991.251000,1641898631.252400,U02U6MTLE8G\\n4d822001-ea00-4139-aad7-e33781b6262c,,,,\"Hey there, I\\'m Siva from India, really excited to learn with you all, A special thenks to <@U01AXE0P5M3> and team, for this great initiative. :+1:\",,1641898854.254000,U02U6MTLE8G\\n03793df4-4a73-4c8c-9fb4-4ed570b1a19d,,2.0,,Hi evryone! I\\'m Timur from Saint Petersburg:wave:,1641902264.255900,1641902264.255900,U02QR8AF4N6\\nc2d5922d-73ba-4c55-8f9e-a995016feb70,,,,Hello everyone!! I am Pallavi from India,,1641903949.257400,U02T95AUF35\\n4641be46-5a02-4298-af0e-88a2eeebe34e,U02SW4N130F,,,welcome friend :flag-ar:,1641810834.142400,1641905200.259700,U02T2JGQ8UE\\n68ac5a71-7afc-46c7-960f-35b13a40bef2,U02QR8AF4N6,,,I\\'m also from Saint-Petersburg :slightly_smiling_face:,1641902264.255900,1641906457.260100,U02QL1EG0LV\\n423274ed-e6a8-404f-a2aa-acb37cdecb25,,,,Hello everyone,,1641907346.261300,U02QR0DLMEY\\n8fb52ae2-87a7-4aeb-8de7-a419e27515b7,,,,I am srikrishna from India,,1641907357.261700,U02QR0DLMEY\\n245348e7-eb6d-41ad-9dbb-820351256618,,,,\"Hello from Dhaka, Bangladesh\",,1641908947.262700,U02CK7EJCKW\\n910e6985-e214-4ee4-a37d-e6e5f4af9095,,,,\"Hi everyone, I\\'m Maulana from Indonesia\\nCan\\'t wait for the Zoomcamp!\",,1641909622.263600,U02TES0M8LV\\nb11b3c3d-7f5e-4393-a165-aad6b3bfc901,,12.0,,\"Which OS are you going to use for the course?\\n\\n:one:  Windows\\n:two:  MacOS\\n:three:  Ubuntu or other debian-based Linux\\n:four:  A different linux distro\\n:five:  Something else (who knows?)\",1641913480.265900,1641913480.265900,U01AXE0P5M3\\n6846435b-fd6d-443e-9e17-643f44220dcd,U01AXE0P5M3,,,I was thinking on using my laptop - Windows. But perhaps it makes more sense to me to use my VM using Ubuntu.,1641913480.265900,1641914226.270700,U02CD7E30T0\\n27FD16BF-7827-48C3-87AC-0BD892F87C15,U01AXE0P5M3,,,2,1641913480.265900,1641914273.271200,U01QPFCUXFZ\\nba66959c-8be0-4b78-a376-7a23c49f3f41,U01AXE0P5M3,,,What is the recommended option?,1641913480.265900,1641915489.272900,U02R4F43B0C\\n714318f7-e7f4-4280-beca-96956df5985c,U01AXE0P5M3,,,we expect that all (except 5) should work,1641913480.265900,1641915657.273300,U01AXE0P5M3\\n81dc36e9-b44d-48ea-b501-b6fce7dc5bbb,U01AXE0P5M3,,,I work with windows 11,1641913480.265900,1641915799.273500,U02T213S7RC\\n0c488af7-fc7c-461d-96b9-48065bcb8ea4,U01AXE0P5M3,,,<@U02QUBX2XHD> which distro do you use?,1641913480.265900,1641916247.273900,U01AXE0P5M3\\n6ae56003-5b4b-47a7-ba09-0e81c5bb0aa2,U01AXE0P5M3,,,Arch Linux,1641913480.265900,1641916266.274200,U02QUBX2XHD\\n93f8fedc-1f15-4e1f-b2e7-ff8165fc9246,,1.0,,\"How do we get a free GCP account, docker and terraform?\",1641917776.277100,1641917776.277100,U02TLSZJ3NY\\n8b57cfcf-ea14-4d3c-b7a6-93789d53da6c,,,,hello everyone ! im hira from indonesia,,1641918531.278700,U02R7MH54PJ\\nbaa4fa06-0a1a-4edf-a870-b4cf9dcd84f0,,,,Hi everyone I\\'m Napat from Thailand.:wave:,,1641924129.283900,U02TMLK8T5J\\nd6530ef8-938a-4d55-ab91-a43d8c4baa80,U02TLSZJ3NY,,,\"• <https://cloud.google.com/free|https://cloud.google.com/free>\\n• <https://docs.docker.com/get-docker/|https://docs.docker.com/get-docker/>\\n• <https://learn.hashicorp.com/tutorials/terraform/install-cli|https://learn.hashicorp.com/tutorials/terraform/install-cli>\",1641917776.277100,1641924779.284500,U01AXE0P5M3\\n8150B2D8-D3D6-4AB8-8AC9-EE0C87B1AC20,,1.0,,Is there any date when the zoom camp is going to start?,1641925294.285900,1641925294.285900,U02TRHFHDH7\\nf845a4a2-eaab-4e64-84cb-83f56389f696,U01AXE0P5M3,,,Windows with WSL (ubuntu),1641913480.265900,1641925336.286100,U02S9RWSNKD\\n2fc16247-ecf6-4486-8c2f-c28db06259b7,U02TRHFHDH7,,,\"The course starts on January, 17.\",1641925294.285900,1641925604.286500,U02RUGU2K5W\\nfab6a41b-1be0-4c3a-9f0e-57497bbee549,,,,\"Hi everyone! I am Emmanuel from DRC :flag-cd:\\nNice to be here!\",,1641925707.287100,U02TCT40ZNJ\\nc1e11779-eb76-4fab-b117-6fa9115652ca,,,,\"Hi everyone! Glad to be here, and thank you everyone whos making this course possible!\",,1641928682.289500,U02S8PRSDPE\\n0af5af09-03fe-4449-a970-ca6c1abdb3bc,,,,\"Hi everyone I’m Kunle in the Baltimore, USA. Transitioning to data engineering from software development. Excited for this course\",,1641930955.291400,U02TYL6DTED\\nffeac225-6218-43b2-a8d0-c3837b5dcfc6,,,,hello everyone! am Salman from Kuwait. am glad to be here ...excited for the course,,1641931155.292300,U02SX7G2WAK\\nb267f6dd-5960-44a6-bfc9-2819e63b6104,U01AXE0P5M3,,,I\\'ll be using windows with the Ubuntu Linux implementation.,1641913480.265900,1641936463.293300,U02TD4Q3CPM\\n6c1f2a9f-1920-462b-82ab-ac1aaebdcf58,,,,Hi everyone! I\\'m Arben from North Macedonia:flag-mk:,,1641940062.295300,U02R02KLCRJ\\n3F50A27A-39C0-4DA1-BDD7-B65A79EC0CFE,,,,Hi guys! Luis from Guatemala here. ,,1641941749.296100,U02TLV7685Q\\n5c275b74-6abf-4d6c-abc5-625d4923df5f,,,,\"Hello! I am Nurdaulet from Kazakhstan. 2nd year student at Suleyman Demirel University, Computer Science &amp; Junior Big Data Engineer at Beeline Kazakhstan (VEON)\",,1641942059.297700,U02RBKL48KE\\n8e6d1ba9-9e94-42b3-8852-258471c1f9f0,,,,Hi everyone. Carlos from Portugal - currently a data scientist and I want to learn more about moving pipelines into production,,1641943160.299800,U02TPAW5ESG\\n6dc6aae0-0adc-4f80-b4fd-2cfa23ee7dc3,U01AXE0P5M3,,,An inheritance of Arch: Manjaro,1641913480.265900,1641947660.300700,U02TES0M8LV\\naf84bfd1-cea2-4883-9131-a6904a414ba6,,,,Hi!. I am Tegar from Indonesia,,1641955245.303800,U02TKFAL22G\\n6180dc9b-8d09-4375-9d7e-86121ab29d85,,,,\"Hi everyone! Anshul from Sydney, AU\",,1641957632.304500,U02T73TJH4P\\nbfa363dc-0175-448a-b757-da1c2cdc176a,,,,\"Hi this is Ray from Jiutepec Morelos ,Mexico\",,1641960372.306000,U0297MFTTM1\\n9d32f198-00ce-47da-8851-697385719b1d,,,,Hi everyone!I am Aizad from Malaysia,,1641961724.307300,U02T77T3FTR\\ne3b1fbf5-4d3b-4d41-b381-665dee60363e,U02SC5JGJKW,,,Hey Good Idea!,1640859253.140000,1641967658.309600,U02SC5JGJKW\\n52d0c145-08cd-4729-be48-d7271267309d,U02QY444V8E,,,Hey Guys I\\'m Also from SA i have connected with <@U02QVFVHD3N>  We should keep in contact to help each other,1641053970.182800,1641967922.310900,U02SC5JGJKW\\n26ad5319-9049-46c2-8944-513b330191e2,U02QQCG250X,,,Hi Where are you from?,1641120601.186900,1641968005.311600,U02SC5JGJKW\\ne2c8a601-ad37-4ccb-b5b6-e85f4966d508,,,,\"Hi, I am Abed from Indonesia\",,1641969462.313700,U02TN6PQVU2\\nbdd34ee3-55df-4a48-9485-29a47be1fc07,,,,\"Hello, I am Alfaxad from Tanzania\",,1641972343.315000,U02UC32UBDE\\nd89dc718-76dc-4371-bda0-b59e12641ce6,,,,\"Hi, I am David from Russia\",,1641973340.315500,U02T7PWSGS3\\n8f39f1db-b28b-40dd-a1aa-90aa23873afe,U02U5SW982W,,,\"The company I work for uses Donedo. As I don\\'t work directly with this or the team, I don\\'t the know the use cases. However, looks like this has more capabilities than just transformation.\\n\\n<https://www.denodo.com/en/data-virtualization/how-it-works>\\n\\n(my head is spinning with various products, concepts and overlapping capabilities - Data Lake, Warehouse, Lakehouse, DataMesh, Virtualization)\",1645560296.440029,1645702126.263729,U02S83KSX3L\\ndca07ab8-3a2c-423e-9dd3-b47abe3e2c87,U02U5SW982W,,,\"Note that holistics is a bi tool, not transformation, so would be a replacement for looker, metabase or data studio. You could skip the transformation layer but as your data increases in both size and complexity a transformation layer becomes a bigger need.\\nI don\\'t know denodo, but also not an alternative to dbt as this is for data virtualization (tackles the use case of multiregion availability and consumption rather than transformation layer).\\nafaik, dbt is the only one open source at this moment and the most database agnostic (others are more vendor specific). But I\\'m very much looking forward to see more coming out soon.  You can always not use a tool and have your transformation layer as scripts with a set of stored procedures, airflow or cron jobs to schedule and mantain fresh data.\\nA good resource I found recently for finding alternatives for data stack is <https://www.moderndatastack.xyz/category/Data-Modelling-and-Transformation|this website>, check for data modelling and trasformation\",1645560296.440029,1645704390.187539,U01B6TH1LRL\\n19311804-0c1d-4bd1-a32a-dc2735e0a31a,U02U5SW982W,,,\"btw, holistics has some very good blogs and ebooks, one of the slides I made for the course has one image from their blog :wink:\",1645560296.440029,1645704434.128129,U01B6TH1LRL\\n4f65b9f6-6428-448d-8d77-b8632de415ad,U02U34YJ8C8,,,\"Sorry, <@U02U5SW982W>. You are still referring to Q1 of HW4? But that\\'s not about the FHV data!? There are some attempts to reproduce this variation in the channel, but I didn\\'t have the time to dig into it.\",1645587287.934269,1645705286.532339,U02CGKRHC9E\\na7928a59-6e61-4343-be8d-b36f7d355959,,2.0,,\"Hi all, I\\'m back working on this course and trying to get my airflow from week 2 &amp; 3 working again. when I run `docker-compose up airflow-init` I get quite a few rounds of this message for google stuff, not just the google-api-core. It\\'s going on half an hour of working now\\n```INFO: pip is looking at multiple versions of google-api-core to determine which version is compatible with other requirements. This could take a while.```\",1645708054.212869,1645708054.212869,U02TVGE99QU\\n9042b067-6c50-4041-9fa9-d65ad87976f2,U02TVGE99QU,,,\"Hi, I am in the same boat and just started week 2 after a long time. I faced the same error too. This seems like a pip issue, from StackOverflow searches. Just use this workaround(which worked for me). Replace the pip command in the docker file with the below one:\\nRUN pip install --no-cache-dir --use-deprecated=legacy-resolver -r requirements.txt\",1645708054.212869,1645708346.656069,U02UB8XDCHJ\\n8751654c-d4ed-45c7-b0f3-c94317bcd5ce,U02TVGE99QU,,,\"I followed your suggestion and updated the dockerfile but now `docker-compose build` is having the same issue EDIT: and then I got this:\\n```INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here:```\\nas far as I understand, these are the packages/dependencies that  are included with apache-airflow-providers-google, right?\",1645708054.212869,1645709127.469559,U02TVGE99QU\\n749994dd-90af-44b9-bc0f-001da8cbd035,U02TC704A3F,,,\"Hi <@U02SUUT290F> , <@U02TC704A3F>, is there any way to download the FHV and convert to parquet using this method as well?\",1644536483.340339,1645722290.406329,U02BRPZKV6J\\nb1734bd4-a4a1-49b2-b1f0-0d4dd09b4158,U02TC704A3F,,,\"Hey <@U02BRPZKV6J>, yes, you just have to 1) change `file_name` variable inside `web_to_gcs` so that it matches FHV complete URL  2) For the schema, I would probably load one of FHV csv files using pandas and check their data types using `dtypes` method (As Alexey did in a couple of videos) and based on that create the `table_schema`  variable that you\\'ll use in `format_to_parquet`  function.  I think that should do it. Let me know if that works.\",1644536483.340339,1645723390.988339,U02SUUT290F\\nb379a10d-872b-4aa7-8f07-6fd9beccb815,U02U34YJ8C8,,,HI <@U02CGKRHC9E> yes that\\'s the case. And no me neither. Too much else to catch up on I\\'m afraid - week 5 is going fast!,1645587287.934269,1645730853.783749,U02U5SW982W\\n7a564363-d369-4b8e-a9cb-797de4c88aeb,U02U34YJ8C8,,,\"Yep, I haven\\'t even started with week 5 videos yet. :grimacing:\",1645587287.934269,1645731204.983639,U02CGKRHC9E\\n6a51ba48-b98f-4c79-abdb-88dbdfa237fb,U02U5SW982W,,,Thanks for sending this along <@U01B6TH1LRL> and <@U02S83KSX3L>. That\\'s a lot more to look into :slightly_smiling_face: - definitely food for thought.,1645560296.440029,1645731375.071909,U02U5SW982W\\n360302e0-7955-4a19-8115-a9a65b4c1ef9,U02U34YJ8C8,,,\"Gosh you are up early too <@U02CGKRHC9E> - or not I guess - depending on where you are in the world. It\\'s 5:30 am here. I\\'m busy trying to watch the video\\'s, get everything working and <https://learningdataengineering540969211.wordpress.com/2022/02/24/week-5-de-zoomcamp-5-3-1-first-look-at-spark-pyspark/|blog> about it. I\\'m only up to the first video where we do anything :disappointed:\",1645587287.934269,1645731497.816639,U02U5SW982W\\n75db15c7-de78-4429-bac8-0f04282bb20c,U02SFFC9FPC,,,Yes <@U02SFFC9FPC> I put it down to the magical fairies in the internet - or some such thing :wink:,1643237452.453700,1645731571.607669,U02U5SW982W\\nd1be8c83-9551-4f50-ad68-eba8d2f18c9e,U02U34YJ8C8,,,\":smile: I am based in Berlin (UTC+1), so it\\'s a good time for slack activity. Great idea to document your learning experience, already for yourself. I often end up to search for commands and procedures I run several times before, but not often enough to remember them. But I can imagine writing a blog on top of parsing the videos and finishing the homework takes half of your week.\",1645587287.934269,1645731948.920539,U02CGKRHC9E\\n6b6b25c8-1046-43c6-a7a0-b4cef8de3d5f,,4.0,,brew install scala@2.11 did not work on my MacBook pro. There were errors. I had to run brew install scala. Then when I run spark-shell I get a ton of warnings. But it works. Just saying.,1645737059.827449,1645737059.827449,U02UBQJBYHZ\\n9be8fcd4-aec1-4b76-80ff-4e1cdf5008dc,U02UBQJBYHZ,,,Also there is no directory /usr/local/Cellar/apache-spark. My directions say MacOS but is this right?,1645737059.827449,1645737751.035329,U02UBQJBYHZ\\n044b840a-8117-4ad7-9e29-315608b8d6ba,U02UBQJBYHZ,,,OK reinstalled and found it in a different place. /opt/homebrew/Cellar/apache-spark/3.2.1,1645737059.827449,1645738212.078429,U02UBQJBYHZ\\n6973936b-1d80-4c5e-978e-cdcdf6579fbb,U02UBQJBYHZ,,,\"Hi <@U02UBQJBYHZ>, try using the below\\n\\n`export SPARK_HOME=/opt/homebrew/Cellar/apache-spark/3.2.1/libexec`\\n`export PATH=\"\"$PATH_HOME/bin/:$PATH\"\"`\",1645737059.827449,1645738624.581279,U02SEH4PPQB\\nc7dd86c2-4eb1-45d5-9306-8e2d4bf4e168,U02UBQJBYHZ,,,Thanks! I wonder why it worked anyway. A mystery. I have it in my .zshrc now.,1645737059.827449,1645738783.742609,U02UBQJBYHZ\\n04fc6822-28d9-42c9-9fb2-cdc7e0a65490,U02TC704A3F,,,\"Awesome, thanks <@U02TC704A3F>! I\\'m planning to deploy my project in GCP entirely and was going to start looking at Dataproc in the next week or so- definitely will start with your guide :slightly_smiling_face:\",1645659834.521759,1645743132.524739,U02TXLWFG86\\n2ecea0eb-d82d-4c5e-9bfc-2e0ff71fbddd,U02TC704A3F,,,\"You can find more resources in GoogleCloudDataproc github too\\n\\n<https://github.com/GoogleCloudDataproc/cloud-dataproc/tree/master/notebooks/python>\",1645659834.521759,1645747717.619779,U02TC704A3F\\ne01a9c4b-7042-4813-b367-a6348b789f78,U02TC704A3F,,,\"<@U02SUUT290F> Thanks a lot! Yes, it works well! :grinning:\",1644536483.340339,1645753012.104619,U02BRPZKV6J\\na020bc46-f8f8-4a53-92d0-e3d7664c2b70,,4.0,,A small diversion? Or perhaps a rather large one? Thought your programming was giving you a hard time? Well this has been my day ... flooding rains...,1645760064.750309,1645760064.750309,U02U5SW982W\\n737dab4d-d403-476b-8564-55a5a7517c9b,U02U5SW982W,,,\"So, while I\\'ve been beavering away inside on my blog for the course it appears we\\'ve had a flood outside - that I was very much unaware of.  Unedited video is in the <https://learningdataengineering540969211.wordpress.com/2022/02/25/week-5-de-zoomcamp-5-3-1-first-look-at-spark-pyspark-part-2/|blog>...\",1645760064.750309,1645760205.505449,U02U5SW982W\\n9c3a6364-9d76-4b00-850e-20549ceac720,U02BRPZKV6J,,,\"Hi <@U01B6TH1LRL>, thanks a lot for the prompt reply. It\\'s a very weird error. When I tried to change the region setting, I was prompted to enter my private key.\\n\\nWhen I copy-pasted my private key into the configuration setting, there seems to be something wrong during the process. It keeps showing this error.\\n\\nWhen I tried using the \"\"upload private key JSON\"\" function in the setting page, it seems to have resolved this issue.\",1645634338.661579,1645769611.184209,U02BRPZKV6J\\n2b8821f3-d57d-424f-a08d-8346f6f1756f,,,,\"Hi!\\nCan I know what is the estimated # of hours needed for each week to complete, please?\",,1645774406.609029,U033AKBJLBE\\n20fbb05c-de71-4177-b0fc-b8f4aa7d3c8f,,2.0,,\"Hello everyone, I have python 3.9.7 version and gcloud sdk is recommending 3.7. should i go ahead and install it or will i face any issues going forward?\",1645774906.471079,1645774906.471079,U0343G3CH32\\n4a9399a9-6ef7-439a-88c4-9ec16ccb693f,U0343G3CH32,,,I checked it with 3.9 and it seems to work,1645774906.471079,1645775144.834939,U01AXE0P5M3\\n741f9bce-d8d4-4de2-b2a0-3de8771c5c17,,,,\"Hello everyone, Afonso from Portugal :slightly_smiling_face:\",,1641981170.318400,U01EMDX53QU\\n959acc68-3e56-47bc-bc48-699ca15248eb,,,,Is there a Google Calendar integration link available for this course yet?,,1641983798.321100,U02SXPX4EBT\\ne3b834f2-03c9-42fe-a692-06c53322c2b7,U01AXE0P5M3,,,\"<@U01AXE0P5M3> Will we be covering *Kaggle*, *DockerFile*, and any specific *Knowledge Discovery Tools*?  Also, if possible, it would be nice to dive into the Stream Analytics field and go over some tools e. g. Michelangelo, Contentsquare, and talk about how they operate.\",1641566368.013200,1641984141.321300,U02S848271C\\n1f6d4e08-9cea-4ad2-a85c-90903ad3ca48,U01AXE0P5M3,,,Hi! There\\'s a link in the channel with the syllabus. We\\'ll be covering things from there and won\\'t be covering things that aren\\'t there,1641566368.013200,1641985114.322300,U01AXE0P5M3\\n31bbe78d-8da6-4880-994d-48a2cea198c7,,6.0,,I just created it - <https://calendar.google.com/calendar/?cid=ZXIxcjA1M3ZlYjJpcXU0dTFmaG02MzVxMG9AZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ>,1641985402.322900,1641985402.322900,U01AXE0P5M3\\n45e4dd65-dcee-454d-85f2-b9d666f22175,,8.0,,\"Out of curiosity I\\'d like to see where people are from in this course. As such, could you please put your country\\'s flag as a reaction here :thumbsup:\",1641985498.324400,1641985498.324400,U01SXD1FU9W\\n4bf0b5be-9d8f-4689-8ef1-16d22585f019,U01AXE0P5M3,,,Are all the sessions just 1h long? I noticed that on the Github repo some sessions were more than 1h.,1641985402.322900,1641985628.325000,U02QPBZ3P8D\\n74903a06-1303-4d6e-a79b-ff412e6d3d1a,U01AXE0P5M3,,,\"Also, seems like in the calendar the recurrence has no end and just goes on after the 6 weeks (I\\'m not sure if that\\'s the intention or not)\",1641985402.322900,1641985752.325400,U02QPBZ3P8D\\n9397521d-9f28-4c46-a125-680f1830a8c0,U01AXE0P5M3,,,\"Currently it\\'s recurring, it\\'s just simpler this way. We\\'re still figuring out the exact format, but most likely we\\'ll use this slot on Mondays as office hours where we\\'ll answer questions and the videos will be pre recorded\",1641985402.322900,1641986453.326200,U01AXE0P5M3\\n2545c4a4-17d2-437d-a9ab-696a6d0ff034,U01AXE0P5M3,,,\"Hi <@U01AXE0P5M3>  do we have to download Docker, terraform on our local device and can we use anaconda to replace python 3?\",1641566368.013200,1641987328.327700,U02TNA9H75E\\nabbadf9e-3935-4aec-9c89-c5c56e931cea,U01SXD1FU9W,,,:flag-ng:,1641985498.324400,1641987929.328600,U02TNA9H75E\\nf14cab7c-a75a-47c5-bb50-8e6f57ce1487,U01AXE0P5M3,,,Yes anaconda is fine and I\\'d actually suggest using it,1641566368.013200,1641988361.329600,U01AXE0P5M3\\n1e0c4e66-7cdf-47d4-9ef4-1ad843e36daa,U01AXE0P5M3,,,Yes you\\'ll been to download docker and terraform,1641566368.013200,1641988383.329800,U01AXE0P5M3\\nB9357B19-B84B-4889-BD58-F33AAF873C7B,,3.0,,\"What time do the course start in 17th, and will the link be posted or the link is already available \",1641992415.336300,1641992415.336300,U02QKMCV39R\\n96cd2d42-ed1c-44e5-b965-6a38b211de37,U01SXD1FU9W,,,:flag-ar:,1641985498.324400,1641992551.336400,U02TCC380GJ\\nbd9d8e01-44ea-4313-8582-01e4df5bba8a,U01SXD1FU9W,,,Wow this is amazing. Such diversity :muscle:,1641985498.324400,1641995020.337900,U01SXD1FU9W\\n710bf900-bed4-421d-a06b-ede7675cf868,U01SXD1FU9W,,,:flag-ng:,1641985498.324400,1641996216.338800,U01MFQW46BE\\ne555fbed-ceec-42b9-9835-59bc241860a8,U02QKMCV39R,,,I had sent an email to Alexey about this and he wrote:,1641992415.336300,1641996623.339700,U02TFN2LWAF\\nae57dd8b-d31f-40b0-a91d-df3222556fd1,U02QKMCV39R,,,\"It\\'s 17:00 CET on Mondays, but everything will be recorded\",1641992415.336300,1641996625.339900,U02TFN2LWAF\\n2934a62f-114e-4e1c-9e7e-09b56b567c02,U02QKMCV39R,,,Thanks for the info,1641992415.336300,1641997114.340600,U02TES0M8LV\\n2555b066-9987-4863-94d8-2a7ca1696ff2,U01SXD1FU9W,,,<@U02BVP1QTQF> which country is it? :slightly_smiling_face:,1641985498.324400,1641997777.341500,U01AXE0P5M3\\n1E6332AC-AD4B-4A3B-BB04-FA86C2FAA8D5,U01SXD1FU9W,,,\"Oops, got caught :upside_down_face:\",1641985498.324400,1641997834.342100,U02BVP1QTQF\\na895d6f5-6383-4011-98ae-356c670eaf4f,U01SXD1FU9W,,,<@U021G83J3BQ> Welcome :spock-hand: :alien: :sweat_smile:,1641985498.324400,1641999243.343700,U01SXD1FU9W\\n632a1f35-a9d8-4cf9-9fb3-51b0ca53522e,U01SXD1FU9W,,,\"<@U01SXD1FU9W>  :joy: :joy:  my intention was to show, that although we are very diverse, we all come from the same galaxy. And there might be others out there as well! Good idea to ask this question by the way.\",1641985498.324400,1641999955.344300,U021G83J3BQ\\ne0432218-c3b0-4525-a19d-8e5b046329c8,,1.0,,\"What are pre-requisite to keep ready before starting.\\nHope i didnt missed anything ?\",1642003648.348200,1642003648.348200,U02QRM8V5LL\\n3180d8a8-f8b9-437b-b52a-323c406f039b,,,,\"Whats the best way for professional DE\\'s to assist with the course (i.e. answer questions, help out beginners, etc.)\",,1642031804.362900,U02S8K9JBD0\\n5f68945f-2648-452b-9973-c84c340145c8,,,,How many people are in this course?,,1642037549.364300,U02S14MHV3N\\n5c5ec7ec-385f-45d0-8966-9996de6c321b,,4.0,,Is Windows Subsystem for Linux enough to take part in this course? Or is there a preference for any operating system?,1642038323.365400,1642038323.365400,U02JRJJHFL6\\n60739d1a-08d3-4f47-9471-185eae6d9ccb,,3.0,,Is it possible to do the course on Azure cloud ?,1642046773.367700,1642046773.367700,U02EHEM7BP1\\n9551c70a-b8ae-4e4b-ab99-d026a427594c,,,,\"One request, when we cover topics like Data Lake, can we also try to cover the architecture patterns(such as lake house, data landing zones, mesh) for a production deployment including how the Data lake should be structured for prod/non-prod data and CI-CD use cases ?\",,1642047190.370100,U02EHEM7BP1\\n24df7c6a-f747-4099-a588-ba705d6ce3c7,U02JRJJHFL6,,,what about if u give a try to install linux at your machine? u can make a dual boot pretty easy. Also windows should be fine if u can run docker and GCP cli.,1642038323.365400,1642055874.371900,U02TRHFHDH7\\n8c5f5c5c-736b-47bd-a18f-0d38ac97433f,U02QRM8V5LL,,,\"`GITHUB REPO` , I remember that I have seen some others, I will try to find again in the chat :+1: UPDATE =&gt; <https://datatalks-club.slack.com/archives/C01FABYF2RG/p1641924779284500?thread_ts=1641917776.277100&amp;cid=C01FABYF2RG>\",1642003648.348200,1642056103.372100,U02TRHFHDH7\\n9c19c19b-8217-4849-b81c-417b1cfabb4f,U02U5SW982W,,,:slightly_smiling_face:Thanks <@U02TATJKLHG>,1646538059.326969,1646554552.278609,U02U5SW982W\\n1fe947b6-3a3d-4b23-a1a2-69c569154c63,U0233FB1SB1,,,\"Hey Binal\\nNo not yet, still troubleshooting..\\nI will keep you posted\",1646486970.082019,1646565561.091209,U0233FB1SB1\\naabfaeb5-f943-4ae5-8bbf-f83e2986ba30,,1.0,,anyone when last submit for week 6 homework ?,1646574934.377499,1646574934.377499,U02SQQYTR7U\\nfe30b365-aada-452a-bc22-c5b1578f885b,,3.0,,\"Hi everyone,\\nI am a little late to party. I will be starting the course now:sweat_smile::sweat_smile:\",1646575694.308859,1646575694.308859,U02R8V9MF6E\\nd6e074dd-6ea7-46f1-8897-25f9fc610e8c,U02SQQYTR7U,,,14 March most likely,1646574934.377499,1646576289.543219,U01AXE0P5M3\\n222205cc-384d-47ab-9470-1f844e2ff364,U02R8V9MF6E,,,Have fun!,1646575694.308859,1646576301.246869,U01AXE0P5M3\\n24fbb44e-2b98-412d-b6be-ef4aec94e22c,U02R8V9MF6E,,,Any tips on how to cover up in the  best way?,1646575694.308859,1646576858.298859,U02R8V9MF6E\\n13b3f7e5-0855-49ea-ad61-497b7c70d206,,1.0,,\"Where should I get dags that shown in 2.3.2 video, just download them from github?\",1646577423.888929,1646577423.888929,U034Q6ZKR9C\\n32395aa7-3aa4-4a06-8175-4a5bb309d965,U02URV3EPA7,,,\"No problem with the typo!\\nMaybe because english it\\'s not my native language: I don\\'t understand the phrase:  \"\"Do not treat `WITH` clauses as prepared statements\"\".\\nAny example of that? What is the meaning of \"\"prepared statements\"\"?\",1646414115.669799,1646578757.623859,U02URV3EPA7\\n2ddaabba-c7f7-4b39-8af9-f1d8a79287b8,U034Q6ZKR9C,,,<@U034Q6ZKR9C> it should be here <https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_2_data_ingestion/airflow>,1646577423.888929,1646579148.754309,U02T8HEJ1AS\\nF4DB042A-7BEF-47E6-B3A4-DBE658047FD3,U0233FB1SB1,,,\"I am getting this error:\\ndocker: Error response from daemon: create ny_taxi_postgres_data\",1646479522.551359,1646579239.673559,U030RUDPYJJ\\nf0daa5a4-b904-4e3b-b438-4103cb55e41d,U02R8V9MF6E,,,\"<@U02R8V9MF6E> read the notes from students, FAQ and take your time <https://github.com/DataTalksClub/data-engineering-zoomcamp/commits?author=ziritrion>\",1646575694.308859,1646579253.622859,U02T8HEJ1AS\\nd845c6d7-602b-47ab-825b-b9e30a12181d,U0233FB1SB1,,,Hey <@U032TP2AS2Z>,1646486970.082019,1646583765.912719,U0233FB1SB1\\n8f701f46-0a6a-4de6-9907-9afd56f46cce,U0233FB1SB1,,,I was finally able to resolve it,1646486970.082019,1646583775.422869,U0233FB1SB1\\nb00e0a87-7fac-43a5-b811-959e381eae11,U0233FB1SB1,,,If you are still stuck,1646486970.082019,1646583789.915579,U0233FB1SB1\\nb5a2e5f8-8645-4c5c-a673-bf11d41ee153,U0233FB1SB1,,,\"Try changing the port on the host to 5431\\n```docker run -it -e POSTGRES_USER=\"\"root\"\" -e POSTGRES_PASSWORD=\"\"root\"\" -e POSTGRES_DB=\"\"ny_taxi\"\" -v d:/Github/Data-Engineering-Zoomcamp/01_basics_n_setup/2_docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data -p 5431:5432 postgres:13```\",1646486970.082019,1646583865.839819,U0233FB1SB1\\n72097ebe-7c9c-4b1f-9c3c-1a3b92a5b736,U0233FB1SB1,,,\"References\\n\\n<https://github.com/sameersbn/docker-postgresql/issues/112>\\n\\nthe 4th comment from eddex\",1646486970.082019,1646583915.535199,U0233FB1SB1\\nED9DCA22-D1C2-45C7-9343-B8F5117D5AF2,U0233FB1SB1,,,Thank you!! <@U0233FB1SB1> I will definitely try that. ,1646486970.082019,1646584047.519279,U032TP2AS2Z\\n06123c52-1b57-4926-a85f-cacd78cfec22,U0233FB1SB1,,,\"There were others too but this is the main one\\n\\nAlso <https://forums.docker.com/t/cant-connect-to-postgres-through-docker/120051>\\n\\nThis example pushed me to try it\",1646486970.082019,1646584062.461249,U0233FB1SB1\\n177c9ac2-00ce-4364-b98f-b1c0205b7ea2,U0233FB1SB1,,,It was exhausting to figure this out :stuck_out_tongue:,1646486970.082019,1646584093.547239,U0233FB1SB1\\n76c31e9f-7460-4a89-9ad9-939a087e57dd,U0233FB1SB1,,,Good luck,1646486970.082019,1646584096.813169,U0233FB1SB1\\n70d991a2-0dcf-47fa-b37c-365e85837ca2,U0233FB1SB1,,,and let me know if it works,1646486970.082019,1646584112.694619,U0233FB1SB1\\n589685b9-f2d6-417e-aba0-68e6049af361,U0233FB1SB1,,,It worked!! I\\'m connected to postgres! Thank you :smiley:,1646486970.082019,1646584489.729529,U032TP2AS2Z\\nb7ca2ce8-8a82-4320-8811-77660f074507,U0233FB1SB1,,,Great! Please add this in FAQ,1646486970.082019,1646584786.556419,U01AXE0P5M3\\n19aed499-28eb-4b28-8580-94f8e06ada54,U032Q68SGHE,,,Thank you <@U02LQMEAREX> !!,1644881953.727679,1646597150.329569,U02URV3EPA7\\nd430bfa0-9890-41f6-989b-b6082616c748,U035VQMUY9E,,,<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1646329299706539?thread_ts=1646329299.706539&amp;cid=C01FABYF2RG|https://datatalks-club.slack.com/archives/C01FABYF2RG/p1646329299706539?thread_ts=1646329299.706539&amp;cid=C01FABYF2RG>,1646404258.477979,1646598075.574469,U01AXE0P5M3\\n9935425b-672c-4fc8-9a23-886324863603,U0233FB1SB1,,,\"Tried all the pointers listed here, but its not working, sat on this for about like 2-3 hours max reading articles and stackoverflow answers. Think I will take some rest and continue my fight with this error tomorrow:sweat_smile:\",1646486970.082019,1646606724.351789,U033PKGGFS7\\n902c9386-fd17-4170-b123-4b8bc30f3383,,12.0,,\"Hi everyone,\\n\\nPlease trying to open up the database with pgcli:\\n`pgcli -h localhost -p 5432 -u root -d ny_taxi`\\n\\n\\nIt prompts for the password\\n\\n`Password for root: root`\\n\\nBut nothing else happens, I\\'ve uninstalled and installed pgcli.. please help.\\n\\nThanks\",1646637342.400109,1646637342.400109,U033VBQCGQK\\nec7c8996-39de-4d0b-b0c3-58db3deda3df,U02U3E6HVNC,,,\"Shining in late. Well it sounds as if Sandy sees it pretty clearly but I am also at a loss of where to go exactly.\\nI browsed through the video multiple times and even though there is some schematics to talk about joins, the fact there is no code to support it makes it really hard for me to visualize how it is implemented.\\nI tried to google it this weekend but strangely I cannot find any example. There is a lot of documentation and tutorial for jvm but nothing comes up for Faust besides the references Shura mentioned.\\n\\nOn top of that, Faust is actually replaced by its fork these days. <https://github.com/faust-streaming/faust>\\n\\n\\nThis week was really strange to me, lot of concepts and lengthy videos but very floaty illustration of where and why it matters.\\nProbably I am not getting it, like how does Avro ties with Faust and Kstreams. It seems it is very hard to use both Avro and Faust so when is one used over the other and is Avro dispensable if we use Faust.\\n\\nOr how is a ktable implemented exactly and to which library does it belong. There is nothing related to Ktable in Faust but there is with jvm. Is a table in faust a ktable? And what about global Ktable?\\n\\nThese principles I really don\\'t get, maybe it is just explained clearly but I feel really lost this week.\",1646418691.258709,1646637585.859209,U02TC8X43BN\\n982963fb-0971-47b7-8e9a-645f0761b79b,U033VBQCGQK,,,did you have a chance to check this channel if somebody had this issue already?,1646637342.400109,1646637681.553229,U01AXE0P5M3\\n0ee59aa5-c23b-4e39-b539-d787c5d0573d,U033VBQCGQK,,,\"Tried running this on both windows and vs code terminal:\\n\\n`pgcli -h localhost -p 5432 -u root -d ny_taxi`\\n\\n-&gt; connection to server at \"\"localhost\"\" (::1), port 5432 failed: FATAL: password authentication failed for user \"\"root\"\"\\n\\nWith: \\n`pgcli -h localhost -p 5431 -u root -d ny_taxi`\\n\\n-&gt; connection to server at \"\"localhost\"\" (::1), port 5431 failed: Connection refused (0x0000274D/10061)\\n        Is the server running on that host and accepting TCP/IP connections?\\nconnection to server at \"\"localhost\"\" (127.0.0.1), port 5431 failed: Connection refused (0x0000274D/10061)\\n        Is the server running on that host and accepting TCP/IP connections?\",1646637342.400109,1646639486.550889,U033VBQCGQK\\n636fad90-3d28-474a-8c00-73825854069e,U033VBQCGQK,,,\"docker container ls\\n\\nCONTAINER ID   IMAGE         COMMAND                  CREATED          STATUS          PORTS                              NAMES\\na6f61db0b76b   postgres:14   \"\"docker-entrypoint.s…\"\"   12 minutes ago   Up 12 minutes   5432/tcp, 0.0.0.0:5432-&gt;5433/tcp   fervent_dubinsky\",1646637342.400109,1646639998.904059,U033VBQCGQK\\n5a566f90-6cc7-4a3c-8a1f-65b65d54decb,,,,\"Hi everyone, I\\'m Martin from Nairobi, Kenya. Looking forward to learn best practices in the DE space.\",,1642148683.430400,U02TNF9M8RX\\n8d7e6596-1a59-4937-aedf-f1a2dbc7e1b0,U02SDHJMASG,,,\"I ditched windows for linux 8 years ago. Never looked back! At first I kept a dual boot, but never chose to boot windows. When I had to go there, say every 2 years, it needed to do tons of updates, so I really shut it down for good and live on Ubuntu permanently. For the very specific thing I had to use from windows world, I mounted a Virtualbox VM. Using it in seamless mode is the best of both worlds!\",1642114905.421300,1642158818.431900,U02GVGA5F9Q\\n5e63328b-40ea-459a-95fb-c909b6df0b2f,U02SDHJMASG,,,For this zoomcamp I\\'m preparing a windows 11 notebook with WSL2 too. It\\'s a promising solution. I ought to give it a try :grin:,1642114905.421300,1642158903.432100,U02GVGA5F9Q\\ne52e770d-f018-4ee0-9b2b-e9ba66210766,U02TNA9H75E,,,<@U01AXE0P5M3> i keep having this issue with docker,1642100796.413900,1642160189.432500,U02TNA9H75E\\ne00fc361-b485-42ea-bdf4-ab955b23e54c,U02TNA9H75E,,,,1642100796.413900,1642160229.432700,U02TNA9H75E\\n9b54b62f-7545-4cf3-ade0-0c59c26b2481,U02TNA9H75E,,,Did you try checking the link?,1642100796.413900,1642162282.433200,U01AXE0P5M3\\nc9a0225e-88de-4434-ad35-25116f873d5c,U02TNA9H75E,,,yes... till having bugs,1642100796.413900,1642164631.433800,U02TNA9H75E\\n56b23a37-4ab5-4b72-a246-4c2c8d8a67da,U01AXE0P5M3,,,Thanks <@U01AXE0P5M3>! Why doesn\\'t it stop recursion at the 6th week? Is it on purpose?,1641985402.322900,1642168156.434200,U02GVGA5F9Q\\n783566b5-21d7-43fb-8292-43af68537374,U02SDHJMASG,,,<@U02GVGA5F9Q> I find the WSL system a bit annoying personally I much prefer just natively using the terminal in Mac/Linux. Hope it works out better for you !,1642114905.421300,1642169424.434500,U02QK0E49L2\\nb802630c-c388-44be-a338-e735717dc0c3,U01AXE0P5M3,,,Mostly because of laziness but also it\\'s not yet 100% clear when exactly we should stop it,1641985402.322900,1642169754.435000,U01AXE0P5M3\\nf6794737-4096-4aeb-bc8e-048ed30de10d,,,,\"Hi guys, looking forward to this course. Thank you to sohaib for inviting me :dancer:\",,1642170906.436600,U02U2Q5P61Z\\n06651d92-731b-4cae-9492-940f25975a59,,6.0,,\"Does anyone know the exact time at which this is going to happen\\nso I can setup an alert on my alexa device.\",1642171213.437300,1642171213.437300,U02U2Q5P61Z\\n055b0433-b00b-400a-96d1-42361eeb6377,U02U2Q5P61Z,,,Hi <@U02U2Q5P61Z> I found this in Github repo:,1642171213.437300,1642171699.437500,U02UFAVT4MP\\n9d8c3809-ae7d-424e-af9c-bbb88fc7e19d,U02U2Q5P61Z,,,\"thank you franklin, so we can\\'t attend in person?\",1642171213.437300,1642171759.437900,U02U2Q5P61Z\\n796e97d0-7385-4169-b09d-fabcf65c18f7,U02U2Q5P61Z,,,\"In person? No, it\\'ll be online =)\",1642171213.437300,1642172249.438400,U01AXE0P5M3\\nf38c8c55-1a7a-4a8a-bd79-9dc2d0a1ecb4,U02U2Q5P61Z,,,\"sorry, I meant being there live instead of watching the recording.\",1642171213.437300,1642172361.438600,U02U2Q5P61Z\\n9bf5f43c-46bd-40bb-ab5c-1a6e2d7863be,U02SDHJMASG,,,\"Thanks <@U02QK0E49L2> for the tip. Yes, I\\'m expecting some annoyances and, honestly, I\\'m not betting a lot on it. My Ubuntu is ready to roll and I\\'ll use it as my main machine. I\\'m just curious on WSL2.\",1642114905.421300,1642172749.438800,U02GVGA5F9Q\\n828e2b49-6a68-4ab5-9037-9610a2d9dd36,U02U2Q5P61Z,,,<@U01AXE0P5M3> Are there going to be live Zoom classes or is it weekly recordings?,1642171213.437300,1642172924.439000,U02QK0E49L2\\ndf5733a1-cd1e-40c9-a2e4-bfc60351a6fa,U02U2Q5P61Z,,,The course will be recorded and on Mondays we\\'ll have office hours,1642171213.437300,1642172962.439300,U01AXE0P5M3\\n9d4d8e2f-f899-4718-87fc-7de308a8ac1c,U02SDHJMASG,,,\"I\\'m not a Mac guy, but I like its FreeBSD \"\"alike\"\" terminal :zany_face:. It\\'s powerful!\",1642114905.421300,1642172999.439500,U02GVGA5F9Q\\n48b772d9-231d-448f-9e16-4f138522f057,U01AXE0P5M3,,,\"Well, you can make it last many months very easily! :joy:\",1641985402.322900,1642173367.439800,U02GVGA5F9Q\\nde33d58d-d86f-458f-9cec-53f1705612d6,U02TNA9H75E,,,<@U02TNA9H75E> <https://stackoverflow.com/questions/39684974/docker-for-windows-error-hardware-assisted-virtualization-and-data-execution-p|https://stackoverflow.com/questions/39684974/docker-for-windows-error-hardware-assisted-virtualization-and-data-execution-p>,1642100796.413900,1642174047.440000,U02S848271C\\nfc7e152c-b83b-43c7-bdd6-6305adc540e3,,,,\"Hi everyone, I\\'m Tong from Thailand\",,1642177346.441900,U02U3F64GCT\\nD75F36AE-9C0D-4106-B6F5-D5F1D74C98B7,U02SDHJMASG,,,\"It was pretty easy for me to set up Linux for the course. As a first-timer, every of my issue has always been resolved with 2 to 3 clicks on a google search. \",1642114905.421300,1642178954.444700,U02SDHJMASG\\nB1DFDDF4-CD88-42E0-B723-5E50A04CE996,U02SDHJMASG,,,\"Unfortunately, my Mac is only for work. Can\\'t use it for personal stuff. My choice for Linux is to have experience close to what I have on my Mac.\",1642114905.421300,1642179101.446900,U02SDHJMASG\\n22e19a54-7073-4a26-84e8-23329b20256a,,,,\"Hello everyone, Majed here from Montreal, Canada\",,1642180027.447700,U02U5G6MXJ8\\n84296dce-e7aa-4324-9dcb-d4151000afcc,U02TVGE99QU,,,\"Hey Dan, same here :flag-ca: - trying to make DA to DE transition. Inbox me your linkedin, would love to connect :smile:\",1642076386.392500,1642185535.448700,U01HNUYV81L\\nbc44cdee-43c4-4718-b0a5-abe121d01600,U02TG61D2J3,,,\"Hey Alex, same here\\xa0:flag-ca:\\xa0- trying to make DA to DE transition. Inbox me your linkedin, would love to connect\\xa0:smile:\",1642085335.404300,1642185579.448900,U01HNUYV81L\\n77f0d362-b7d9-4cea-a1e0-2f3ae6b2964c,U02CD7E30T0,,,\"poetry isnt bad as well, I wonder how prevalent it is by data engineers.\",1642064217.378200,1642185904.449600,U02S6KXPH8W\\nf769d8d9-2a2b-46b6-a001-5fc6c56c726b,U02CD7E30T0,,,\"as a student i constantly shift between the two and it frustrates me , idk why :grin:\",1642064217.378200,1642185941.449800,U02S6KXPH8W\\nc7ae7bdb-1c27-4364-a886-61cf30e7cb93,,18.0,,\"If you have windows 10 home and have problems installing docker, this is what helped me - <https://docs.microsoft.com/en-us/answers/questions/29175/installation-of-hyper-v-on-windows-10-home.html>\\n\\nI have windows 10 home and have docker &amp; wsl2 configured\",1642186903.450800,1642186903.450800,U01AXE0P5M3\\n20f3dbed-e67e-4fb9-a5c4-135a39cae2b4,U01AXE0P5M3,,,Another option could also be renting an instance with Linux on GCP and just doing everything from there - the 300 EUR credit should be more than enough for that and everything else we\\'ll need for the course,1642186903.450800,1642187160.451100,U01AXE0P5M3\\naa5efdd5-b4d1-4b5a-b59f-dd5970c657da,,,,Hi everyone. I\\'m Gustavo from Portugal,,1642190997.452800,U02R09ZR6FQ\\n1a1e394d-c330-4f30-81de-2638a6d0dd19,U01AXE0P5M3,,,I\\'m using a windows 11 home on my second machine and the trick was to install docker in WSL2\\'s Ubuntu.,1642186903.450800,1642193707.453400,U02GVGA5F9Q\\n8075cd9b-2fa5-4bb4-8c91-abaa5f898c34,U01AXE0P5M3,,,Docker instructions: <https://docs.docker.com/desktop/windows/wsl/>,1642186903.450800,1642193721.453600,U02GVGA5F9Q\\n8a17c07e-601b-4940-9b70-8db2eaa4d305,U01AXE0P5M3,,,And Microsoft instructions: <https://docs.microsoft.com/en-us/windows/wsl/tutorials/wsl-containers>,1642186903.450800,1642193759.453900,U02GVGA5F9Q\\n063f75bf-aa67-47cf-ad96-b79903706873,,,,Hi,,1642201444.455600,U02T4CAPP4M\\n74e3d9d2-1bf9-43b4-bbc2-bd4cc62269e8,U01AXE0P5M3,,,<@U01AXE0P5M3> Is it possible to cover the installation of docker and terraform on gcp in the course?,1642186903.450800,1642230918.456800,U02QGA57GRY\\n9facc41d-5c6b-4a33-9b36-44a2b0789cd0,U01AXE0P5M3,,,\"Sunil, do you have problems installing docker? What doesn\\'t work for you?\",1642186903.450800,1642231523.457000,U01AXE0P5M3\\n4e0cd44b-0cb0-4c46-94ee-4a0fbe31bac1,U02U6DR551B,,,\"And where should we send it, please?\",1647123564.636869,1647159256.872189,U02R2PU9NLD\\nc0e430bd-cbda-4527-91e0-de044c849a95,U02TNEJLC84,,,Thank you <@U02U5SW982W> and <@U02TNEJLC84> for the fix :blush:,1646957620.422529,1647159627.223849,U031T71PEL8\\n62d3d8d4-89fa-4492-b0ca-9c23b4754528,U02TNEJLC84,,,You are very welcome <@U031T71PEL8>!,1646957620.422529,1647159680.780309,U02U5SW982W\\na04ecb84-6ed4-4f92-8d2f-101e6f9bed93,U02QL1EG0LV,,,\"I found. When you create table you can point columns wich you want\\nCREATE OR REPLACE EXTERNAL TABLE `****`\\n(\\n  column1 TYPE,\\n  column2 TYPE\\n)\\nOPTIONS(...............)\",1647099681.034019,1647163653.014479,U02QL1EG0LV\\n16ed7fca-f371-4bc4-9075-446d7c86278a,,3.0,,\"Week 6 Question\\n&gt; (Attach code link) Practical: Create two streams with same key and provide a solution where you are joining the two keys\\n&gt; Your answer\\nFor this question\\nthat means create some like on the video tutorial <@U01DFQ82AK1> create some delta time ?\",1647165895.987499,1647165895.987499,U02SQQYTR7U\\n80c6f2e7-e0de-48d2-8e9f-d2da6ed6592e,,2.0,,\"I was looking around for the project already, I have a `terraform` question: if I want to create already two schemas into Big Query, one for raw data and one for processed data how should I implement this in the terraform variables and main? I tried this:\\n```variable \"\"BQ_DATASET_RAW\"\" {\\n  description = \"\"BigQuery Dataset hsoting raw data\"\"\\n  type = string\\n  default = \"\"dtc_de_project_raw\"\"\\n}\\n\\nvariable \"\"BQ_DATASET_PROD\"\" {\\n  description = \"\"BigQuery Dataset hosting production data\"\"\\n  type = string\\n  default = \"\"dtc_de_project_prod\"\"\\n}```\\nreferencing two variables, and then creating two resources in the main:\\n```resource \"\"google_bigquery_dataset\"\" \"\"dataset\"\" {\\n  dataset_id = var.BQ_DATASET_RAW\\n  project    = var.project\\n  location   = var.region\\n}\\n\\n# prod\\nresource \"\"google_bigquery_dataset\"\" \"\"dataset\"\" {\\n  dataset_id = var.BQ_DATASET_PROD\\n  project    = var.project\\n  location   = var.region\\n}```\\nbut I am thrown an error as I am trying to create the same resource type twice. Do I need to create lists of variables and lists of resources in this case?\",1647183620.275039,1647183620.275039,U02UA0EEHA8\\nf6e0faf3-2d34-462f-9372-41e38b14977f,,3.0,,Anyone received a `Connection to node -1 (broker/172.18.0.3:29092) could not be established. Broker may not be available.`  &amp; `Expected 1 brokers but found only 0. Brokers found []` errors when running `docker-compose up`? I’m on a Mac M1. Tried clearing out all my docker images/containers and updating Docker Desktop but still the same issue.,1647187709.348309,1647187709.348309,U02U34YJ8C8\\n4b99f7bb-8c8b-4813-bba8-43447e430387,,3.0,,\"Hi everyone, I have some questions concerning kafka. Would be great if someone could help me.\\n• How would I run kafka in the cloud, for example on GCP?\\n• If connectors are not really used for production settings (this was mentioned in the video lecture), how would I store my kafka data in a DB, big query or in GCS? What is the best practice?\\n• I would like to use a connector to GCS in the docker compose setup we used for week 6. How do I need to adjust the setup to make this work? \",1647189014.334189,1647189014.334189,U01T2HV3HNJ\\nd44b5ad3-d093-4acc-98ed-83c6390fbef1,U02U34YJ8C8,,,\"That\\'s the error I was getting. I had docker images showing up in docker desktop that did not appear when I typed \"\"docker ps\"\" in terminal. I removed them, and things started working. I\\'m not sure it wasn\\'t something else, because I could get things to work before I did this and have had things not work since I\\'ve done that. Today things are working.\",1647187709.348309,1647190889.660169,U02UBQJBYHZ\\neb503ae5-7c15-4d6d-8c0a-6c06b9e8af12,U02U34YJ8C8,,,I have Mac M1.,1647187709.348309,1647190910.163779,U02UBQJBYHZ\\nff0eac19-8db6-4166-8ee2-3f8187373e72,U02SQQYTR7U,,,You can create anything two streams you feel like. Take this question as a revision of Kafka streams and producer,1647165895.987499,1647193420.020959,U01DFQ82AK1\\ne7ca3856-29e8-479f-8f90-7e7d0269148b,U01T2HV3HNJ,,,\"&gt; • How would I run kafka in the cloud, for example on GCP?\\nYou can start your own kafka cluster. Check this <https://medium.com/memsource-engineering/deploying-your-own-kafka-cluster-in-aws-via-terraform-and-ansible-e753f59fab97|post> an example\\n&gt; • If connectors are not really used for production settings (this was mentioned in the video lecture), how would I store my kafka data in a DB, big query or in GCS? What is the best practice?\\nDid i mention that? I think i mention that for KSQL, but connectors can and should be used in production setting.\\nThe best and easiest way in Connector. Check different connectors available in <https://www.confluent.io/hub/|confluent hub>\\n&gt; • I would like to use a connector to GCS in the docker compose setup we used for week 6. How do I need to adjust the setup to make this work?\\n• Deploy a kafka connect node (similar to confluent server)\\n• Check if the connector is available by default (if not add the jars to relevant folder)\\n• Make a curl request to start the connector\\n• Use the <https://docs.confluent.io/platform/current/connect/references/restapi.html|REST> endpoints to check the status\",1647189014.334189,1647193759.022219,U01DFQ82AK1\\nd5d89fc4-8b4d-4444-8f34-e1701ebbed35,U02UA0EEHA8,,,\"You are using \"\"dataset\"\" twice.\\nChange to \"\"dataset_dev\"\" and \"\"dataset_prod\"\"\",1647183620.275039,1647193832.242139,U01DFQ82AK1\\n40517673-ae68-4ddf-b030-da6bb2a81ba6,U02ULMHKBQT,,,\"You can read it like:\\nWhen joining KTable with KTable partitions should be DIFFERENT\\nWhen joining KTable with KTable partitions should be SAME\\nWhen joining KTable with KTable partitions should be DOES NOT MATTER\",1646996078.393879,1647194090.565629,U01DFQ82AK1\\n88364eb6-d6c8-42e4-8ea3-b44d6b28208e,U01T2HV3HNJ,,,\"thank you, concerning the project, what would be your suggestion if I want to use kafka? starting an own kafka cluster or use the local docker setup? Also, I guess the project would look like this: querrying an API and using kafka with a connector to save the data somehow on the GCP?\",1647189014.334189,1647194993.337749,U01T2HV3HNJ\\n4dcaae6e-e66e-446c-a43b-7a5e5becf68c,U02UA0EEHA8,,,Thanks a lot Ankush :blush:,1647183620.275039,1647195179.085979,U02UA0EEHA8\\n27a58efd-6a16-4c4d-a282-92e086bc6066,,1.0,,Week 6 homework - is it okay to use Confluent Cloud for this question?,1647205323.052199,1647205323.052199,U02U5SW982W\\n367e0bbf-c7e6-40f9-ba84-fc66cf977fdd,U02U34YJ8C8,,,\"Thanks <@U02UBQJBYHZ>. I’ve tried that but still not having any luck. Might just leave it for this week as I doubt I’ll get any of it done before the home work deadline, and I won’t be using Kafka for my project. Maybe I’ll try again tomorrow.\",1647187709.348309,1647210125.188739,U02U34YJ8C8\\n549297fd-63c3-4c9e-b378-4a53f434c608,,30.0,,I cannot find any documentation that makes sense on how to use faust join function. Help please.,1647224360.241129,1647224360.241129,U02UBQJBYHZ\\na86f75d5-5989-41b8-a39c-9756f51860a6,U02UBQJBYHZ,,,Hi Cris - you cannot do a join in faust,1647224360.241129,1647225537.136449,U02U5SW982W\\neccb636a-22b3-4878-8bcc-e3e1acac41a6,U02UBQJBYHZ,,,It\\'s been discussed here a bit,1647224360.241129,1647225544.093309,U02U5SW982W\\n6c6f2e48-2670-42f4-a1fe-7cea85011223,U02UBQJBYHZ,,,A bit of a giant pain but ...,1647224360.241129,1647225567.013329,U02U5SW982W\\ne6b8efb6-c491-4ae7-87db-9cc29f2d7e38,U02UBQJBYHZ,,,Joins have not been implemented in Faust so you need to use something else,1647224360.241129,1647225629.327209,U02U5SW982W\\n5dd1f4ee-ebef-4316-bdd5-eef9b6043bfb,U02UBQJBYHZ,,,@<https://app.slack.com/team/U02SMBGHBUN|Senthilkumar Gopal> provided one way of doing it and I can\\'t find it now but someone else used Spark. I\\'m trying out Confluent Cloud (god knows how long that will take me :().,1647224360.241129,1647225801.742819,U02U5SW982W\\n6769369e-3370-4280-9ff6-3083c40908cb,U02SQQYTR7U,,,\"thanks <@U01DFQ82AK1> for the explain\\nBut I still confused about relation Kstream or ktable heheh :joy:\",1647165895.987499,1647231596.482589,U02SQQYTR7U\\n9d87b207-a65b-4c56-8f35-53fc387c100a,U02UBQJBYHZ,,,hi <@U02UBQJBYHZ> you still stuck for practical question week 6 ??,1647224360.241129,1647231656.555469,U02SQQYTR7U\\nd1cde82a-a1f3-4686-8f4a-fe4de2945cf1,U02UBQJBYHZ,,,If yes me too hehe,1647224360.241129,1647231666.250899,U02SQQYTR7U\\n90d0d97c-5198-40ca-89cb-1eaed25fa71c,U02UBQJBYHZ,,,Hi <@U02SQQYTR7U> and <@U02UBQJBYHZ> - still taking too long. I have tried using confluent cloud but no success. I think I\\'m going to have a look at <@U02SMBGHBUN>\\'s way of doing it - I\\'d like to do it in confluent cloud but am stuck on an error ...,1647224360.241129,1647233929.685409,U02U5SW982W\\n363e5438-df75-4456-b7ac-19346144ff99,U02UBQJBYHZ,,,I\\'m wondering if there is any chance of an extension on the homework - even if for a day. My time has completely blown out...,1647224360.241129,1647233979.077399,U02U5SW982W\\n53b14900-3e34-4461-928d-e05c2403b16e,U02U5SW982W,,,Never mind I don\\'t think I\\'m going to end up using this - I can\\'t get past an error on this. Anyone else using this by any chance I wonder?,1647205323.052199,1647234029.051389,U02U5SW982W\\n68431aa5-5784-4ebe-b65c-a556961cb486,U02UBQJBYHZ,,,I hope data talks can be provide for solution for week 6,1647224360.241129,1647234599.987219,U02SQQYTR7U\\n6ba34107-7f52-450c-a7f4-e604b5d5f4d5,U02ULMHKBQT,,,thanks a lot Ankush!,1646996078.393879,1647237622.135989,U02ULMHKBQT\\nb9673e0d-3d9c-47f4-b504-84a077cefbb7,U02UBQJBYHZ,,,Hi <@U02SQQYTR7U> not sure there is one solution - many solutions I imagine. Have you tried <@U02SMBGHBUN> solution yet?,1647224360.241129,1647237686.268979,U02U5SW982W\\nA7132EA5-D297-4C50-BC9E-B79E63A1491A,U02SQQYTR7U,,,Can you please elaborate what are you confused about? ,1647165895.987499,1647238926.315899,U01DFQ82AK1\\na0f6aff7-c5b0-47f3-b69c-8bca3a4e6794,U01T2HV3HNJ,,,\"Hi, so regarding kafka deployment I would suggest to focus on what do you want to learn and focus your career towards, if devops/dataops deploying kafka in GCP or AWS would be a great experience. If focused more on data engineering/software engineering deploying Kafka on docker locally is fine (maybe try with multiple nodes to have a better experience).\\n&gt; querrying an API and using kafka with a connector to save the data somehow on the GCP?\\nu can check twitter API, as it has quite good data coming in realtime, u can also check kafka connect GCP sink connector\",1647189014.334189,1647246829.480949,U01DFQ82AK1\\n1c47446a-7f5b-42fe-bfd3-2d512225abd1,U02UBQJBYHZ,,,\"which one\\n?? <@U02U5SW982W>\",1647224360.241129,1647249303.813629,U02SQQYTR7U\\nd1e23643-6746-4643-8650-bbf3e9087af6,U02UBQJBYHZ,,,Hi <@U02SQQYTR7U> I think <@U02TATJKLHG> provides a little more structure for this. Sorry I don\\'t know how to put his thread here but here\\'s a repeat:,1647224360.241129,1647250473.560649,U02U5SW982W\\n943ce35e-c636-4483-b0ab-a15254ad21fa,U02UBQJBYHZ,,,\"Learn Spark Streaming - <https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html>\\nKafka + Spark Integration - <https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html> (edited)\",1647224360.241129,1647250478.091339,U02U5SW982W\\n5f28a639-4872-405d-bf9a-17f6b0a8dd32,U02UBQJBYHZ,,,I think I should have done this in the beginning rather than waste a whole day on what I was doing. But that\\'s learning I guess :woman-shrugging:,1647224360.241129,1647250630.678899,U02U5SW982W\\n39177584-f68c-4454-bbb1-f46661031243,U02UBQJBYHZ,,,\"Hi All,\\n\\nThe above two links should help you get started with Spark Streaming + Kafka. It\\'s relatively easy to setup and understand if you know Spark. It\\'ll take some reading but it\\'s not hard to do :smile:. I joined the rides stream with the zones stream just to get an idea of how things work. You can otherwise join the rides stream with a *static* zones file. That should work as well.\",1647224360.241129,1647250791.894289,U02TATJKLHG\\nf33f51e8-7f25-4b62-acb0-0c42f5f72ffc,U02UBQJBYHZ,,,so basically join is happen on spark ?,1647224360.241129,1647250958.248479,U02SQQYTR7U\\nf6e3ae28-71ea-407f-bd0f-7b383a15396a,U02UBQJBYHZ,,,correct <@U02TATJKLHG>,1647224360.241129,1647250967.804509,U02SQQYTR7U\\n6d4edf79-695a-4ca8-8434-91387a59865e,U02UBQJBYHZ,,,\"Yes, on Spark Streaming. The syntax would be very similar to how you\\'d join two dataframes in Spark. The intricacies would be handled by spark internally so you just worry about joining on the right columns. You\\'ll also have to consider watermark + event-time constraints while joining the two streams. You can read about that in the above link\",1647224360.241129,1647251078.473589,U02TATJKLHG\\nb9797d3b-a294-41cf-aff0-b44eb5b74e6f,U02UBQJBYHZ,,,\"Hi, I am using kafka-python and implementing the join by myself as explained in the video, just as an exercise, since this solution will not be robust, scalable... . I am feeling that doing it in this way is more a sort of programming exercise.\",1647224360.241129,1647251161.460009,U02LQMEAREX\\n8855dc71-1af3-4369-a437-54a2a415736f,U02UBQJBYHZ,,,<@U02LQMEAREX> that sounds interesting. By implementing you mean you are writing your own join function?,1647224360.241129,1647251331.902129,U02TATJKLHG\\nb7d2a8e7-0e65-4aeb-abd2-f2c432db2d32,U02UBQJBYHZ,,,\"You are right, basically implementing the algorithm from the video. Dealing with windowing and storing messages internally and so on. I do not know if this is the purpose of the last question, but since it seems like an open solution, I am trying it. However, I believe that in production, java API will be used.\",1647224360.241129,1647251667.816589,U02LQMEAREX\\n4e0e03f1-5868-42ce-98e4-30c2450fa0b1,U02UBQJBYHZ,,,Also may I ask which video are you referring to?,1647224360.241129,1647252295.079049,U02TATJKLHG\\nf14120af-185b-437b-b77d-96d49ade3dc7,U02UBQJBYHZ,,,\"<https://www.youtube.com/watch?v=dTzsDM9myr8&amp;list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&amp;index=66>\\nStream to stream join, outer join.\\nalso assuming that the question \"\"Create two streams with same key and provide a solution where you are joining the two keys\"\" means \"\"join the two streams\"\"\",1647224360.241129,1647253389.706569,U02LQMEAREX\\n643c9d73-a2bb-4d1d-a4b4-4b1fcb2d6c17,U02UBQJBYHZ,,,\"Still not sure, but I think one of the easiest solutions to join streams would be to use Java or Scala.\\n<https://kafka.apache.org/23/documentation/streams/developer-guide/dsl-api.html#joining>\",1647224360.241129,1647256200.714759,U02DNSC6Z52\\n1d4531e7-5f87-42e8-849f-b2c0611cb22c,U02UBQJBYHZ,,,\"<@U02DNSC6Z52> I agree, it is just I did not want to jump into java world just for the last question :sweat_smile:.\",1647224360.241129,1647259553.321509,U02LQMEAREX\\n2c8c6529-da70-472d-a85d-9a5b09eada83,U02UBQJBYHZ,,,Thanks I only have a few hours today and I\\'m skeptical that I can install Spark without problems since many previous installations have not been successful first try. It\\'s discouraging. I think I might instead try to do the join by hand using windows.,1647224360.241129,1647260665.500539,U02UBQJBYHZ\\nb85d05cb-d5c4-4f44-80eb-dc674cf19d1f,U01T2HV3HNJ,,,\"<@U02TATJKLHG> could you provide some more insights into how you make it work with spark streaming in python. From what I understand, I need to add packages as argument. but I cannot seem to make this work. I always get something like: \"\"Error: Missing application resource.\"\" or \"\"Provided Maven Coordinates must be in the form \\'groupId:artifactId:version\\'. \"\"\",1646731420.426559,1647261816.246879,U01T2HV3HNJ\\nc7cccebe-0ac6-4e33-94a5-4e48744f2af4,U01T2HV3HNJ,,,\"Hi <@U01T2HV3HNJ> sure! I used the below command to run my spark job. I earlier used a package meant for Spark 3.2.1 I guess and I faced some errors. We are using Spark 3.0.3 so a package for that version would have to be used if you are not already doing that.\\n\\n```spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.3 stream_taxi_json.py```\",1646731420.426559,1647262276.197689,U02TATJKLHG\\neb33f4c7-ddac-4f7a-9204-cb4afe4d1d36,U01T2HV3HNJ,,,\"thank you, unfortunately, that is identical to what I tried :pensive:\",1646731420.426559,1647262600.177419,U01T2HV3HNJ\\n51ebed90-2a1f-473a-bf3a-4170fec642fe,U01T2HV3HNJ,,,\"and I have the same versions, as well\",1646731420.426559,1647262630.314739,U01T2HV3HNJ\\n9b19b532-3e75-4e67-8cab-3368af8375fa,U02UBQJBYHZ,,,Look at alternative libraries or languages. If you are stuck for long please ignore it. It was meant to highlight that all libraries are not advance and sometime we need to look into other libraries,1647224360.241129,1647262889.332199,U01DFQ82AK1\\naac9b4f0-fa67-48d4-a31c-7f304add2291,U01T2HV3HNJ,,,\"Can you paste the command you are using?\\n\\n&gt; Provided Maven Coordinates must be in the form \\'groupId:artifactId:version\\'.\\nThis makes me wonder whether the command has some typo in it\",1646731420.426559,1647262917.346679,U02TATJKLHG\\nd9ffa774-0b0e-4fee-bc61-ad1c4501f0a3,U02UBQJBYHZ,,,\"&gt; You are right, basically implementing the algorithm from the video. Dealing with windowing and storing messages internally and so on. I do not know if this is the purpose of the last question, but since it seems like an open solution, I am trying it. However, I believe that in production, java API will be used\\n<@U02LQMEAREX> Please don\\'t implement streaming engine. That might take some time. Look for alternative python libraries (example Spark) or use Java\",1647224360.241129,1647262961.897339,U01DFQ82AK1\\n1fd48993-676c-46ab-86ad-511b7ee4574e,U02UBQJBYHZ,,,Roger that! Thanks!,1647224360.241129,1647268173.636209,U02LQMEAREX\\n7c1386d1-26a2-41c8-aff2-aeee4fd924d8,,5.0,,No YouTube Q&amp;A today?,1647270368.346079,1647270368.346079,U02TNEJLC84\\nb1dd0b86-7c1d-44de-8f75-bfd51641f050,U02TNEJLC84,,,I was asking myself. I don\\'t see any activity.,1647270368.346079,1647270469.004719,U02DG8S854K\\nf6411c46-4a7d-4e65-b91a-02c55c6a7e93,U02TNEJLC84,,,at 12 pm EST. They don\\'t use day light saving,1647270368.346079,1647270605.338139,U02VBG59VQ9\\n22a6f8e1-083a-462b-bd90-d1ffca05f572,U02TNEJLC84,,,<@U02TNEJLC84> <@U02DG8S854K>,1647270368.346079,1647270627.688349,U02VBG59VQ9\\nbda42e92-47cf-4a1e-b341-85cdc1bf5197,,,,today\\'s office hour is @ 12 pm EST everyone.,,1647270666.502559,U02VBG59VQ9\\n8a9a57b5-9e8b-4a9f-bb1f-e2a3307bf464,U02TNEJLC84,,,Ah that makes sense. Thank you <@U02VBG59VQ9>! Going to miss these once they come to an end.,1647270368.346079,1647270697.388909,U02TNEJLC84\\na1c662d0-750b-4344-8051-3aa71bdfe012,U02TNEJLC84,,,thanks sisu!,1647270368.346079,1647270810.693619,U02DG8S854K\\n4b16002a-e15f-41f4-8112-d4e7e3067bae,,,,\"Hello all,\\nwhat do you guys use to create a workflow or pipeline image like this one\\n<https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/images/architecture/arch_1.jpg>\",,1647271177.392779,U02VBG59VQ9\\n620a2c41-469c-4cbc-924c-379f14f2a9ff,,,,I created this on miro <https://miro.com/app/dashboard/>,,1647271243.293589,U01DFQ82AK1\\n7511F204-DBDE-4D13-9841-20FA8E0B0A67,,,,\"Hello all, an unrelated question…does anyone have familiarity with running Postgres on kubernetes? I have a few questions about running queries \",,1647275955.260209,U02V1TMH4M6\\n989f45b8-0ced-4f2a-8f89-8365d9f69e40,,1.0,,\"Can we use our own dataset for the project? and if the answer is yes, then can it be one that has less than 500 rows ?\",1647279758.798239,1647279758.798239,U02TMP4GJEM\\n10e1e0fa-819d-4915-bfad-dce127e22314,,,,\"Two questions I have: \\n• When is the deadline for the Project Submission \\n• Is there another cohort planned for after this cohort perhaps one could commence and end with that cohort for those that just joined or starting the program today?\",,1647280579.549079,U02TSQTE67R\\n9CD9E1F1-B5D2-4384-BCF9-6D334F5F50A5,U02TMP4GJEM,,,\"You can. But with 500 rows the data is not enough to use tech like spark, Kafka, or complicated pipelines. I would suggest to explore more datasets \",1647279758.798239,1647281213.558629,U01DFQ82AK1\\nf1aa3043-6bb9-4cf0-b61a-de133413c2d7,U02U5G0EKEH,,,\"• restart all docker images\\n• give more memory to docker\\n• check if broker is up\\n• check logs of control centre\",1646118173.058739,1646127786.014499,U01DFQ82AK1\\n4f2af7e5-1115-46f3-be73-2b7a74ae7ff6,U02ULP8PA0Z,,,Definitely worth a read :+1:,1646099641.598249,1646127808.712319,U01DFQ82AK1\\n559c4ca6-1190-456f-ab41-d0401cf49bf9,U02CD7E30T0,,,\"if i’m not wrong, u could filter by pickupdate to be exactly 15th Feb, and no need to consider anything for dropoffdate.\",1646090410.914779,1646132298.837109,U02ULMHKBQT\\nf6f1310a-90e5-4df5-bffe-9933a2d99bb3,U02R6QGM3R6,,,My bad. I started spark in the wrong way.,1645967412.188089,1646147108.997749,U02R6QGM3R6\\nbaac9a5d-e4da-41c7-b276-2b4075e9b648,U02R6QGM3R6,,,what was the mistake? how did you try to start it?,1645967412.188089,1646147342.182799,U01AXE0P5M3\\n132c71a5-e593-4c53-903c-db01b720b7c5,,1.0,,Hello! What is the Deadline for HW6?,1646148218.609659,1646148218.609659,U02R2PU9NLD\\nbdf7c1b5-7b48-46b3-9add-798170d29ec5,,2.0,,\"For whatever reason, Airflow cannot find my credentials:\\n```FileNotFoundError: [Errno 2] No such file or directory: \\'/.google/credentials/google_credentials.json\\'```\\nBut I know that they are there:\\n```dan@DESKTOP-92OUFMB:~/code/dataeng$ ls ~/.google/credentials/\\ngoogle_credentials.json```\\nAnd the docker-compose.yaml file in the week3 directory omits the ~ in front of the filepath - should that be there? My docker compose has the following:\\n```GOOGLE_APPLICATION_CREDENTIALS: /.google/credentials/google_credentials.json\\nAIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT: \\'google-cloud-platform://?extra__google_cloud_platform__key_path=/.google/credentials/google_credentials.json\\'```\",1646152163.350089,1646152163.350089,U02TVGE99QU\\nb94cea36-af9f-4e36-9243-c8ad3a7eab0f,U02TVGE99QU,,,\"That should work. Is it listed in the volumes of your docker compose?\\n```    - ~/.google/credentials/:/.google/credentials:ro```\\nThis might be useful:\\n<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1643411641340209>\",1646152163.350089,1646152627.890729,U02TNEJLC84\\n2fcd6a60-e9c6-4cf7-8d7f-70dc63e906e2,U02TVGE99QU,,,\"Thanks Michael, I didn\\'t have it in my volumes. Much obliged!\",1646152163.350089,1646154095.198329,U02TVGE99QU\\nb6bb00dc-a4a6-4181-b176-0b489dc9e84e,,3.0,,\"Hi everyone, for week 5 homework, is it very normal to take &gt;5 minutes to execute a query for question no.4 (Day with the longest trip)?\\nI\\'m using a local PC in WSL mode (20GB RAM i7 7th gen SSD). I\\'ve already repartitioned the dataframe to 24 partitions and it takes very long &gt;10 minutes to execute a simple query like this.\",1646156221.540489,1646156221.540489,U02BRPZKV6J\\n1370d57d-385a-4d7c-8c22-c14a63640e54,U02BRPZKV6J,,,\"Not too familiar with WSL and not in front of my PC at the moment, but that run time seems a bit excessive. Do you have anything else running at the same time which may be hogging resources?\",1646156221.540489,1646156913.986689,U02TNEJLC84\\n9c7e7a76-0c56-4d24-83d7-2eff208bc337,,,,Can we have an extension on <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_5_batch_processing/homework.md|HW5> deadline until Friday or Monday?,,1646157690.923169,U029JNJBPED\\nd1f75799-ee49-4dd4-9b62-f143bf11d2c4,U02BRPZKV6J,,,I remember some 4 minutes for few queries when running in the VM in GCp,1646156221.540489,1646157852.208239,U0308MF3KUH\\n437e004a-27dd-4314-af7e-bffcd013a20f,U02BRPZKV6J,,,It\\'s my bad. I didn\\'t re-read the dataframe from parquet. It becomes much faster reading the dataframe from the repartitioned parquet.,1646156221.540489,1646158150.307889,U02BRPZKV6J\\n0eaf551f-ee7d-4149-b38d-dd29995294c1,U02CKHHRY6Q,,,I just added all of the proposed datasets to the FAQ :+1:,1646074398.401919,1646162383.572609,U02BVP1QTQF\\nd9a81166-8c79-45b6-b8b3-6e9b3ffed890,,8.0,,\"Hi everyone, I am still in week2 and got this error .I would really appreciate anyone\\'s help\\n```Broken DAG: [/opt/airflow/dags/data_ingestion_gcs.py] Traceback (most recent call last):\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py\"\", line 1481, in set_downstream\\n    self._set_relatives(task_or_task_list, upstream=False, edge_modifier=edge_modifier)\\n  File \"\"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py\"\", line 1419, in _set_relatives\\n    task_object.update_relative(self, not upstream)\\nAttributeError: \\'function\\' object has no attribute \\'update_relative\\'```\\n\",1646163862.906909,1646163862.906909,U02T70K8T61\\n037d59e8-4d19-414a-91b7-cb10687e9dff,U02T70K8T61,,,\"Hi, Anilla! I hope to catch up a bit later this week and early next week: maybe we can be study buddies!\",1646163862.906909,1646164156.680249,U02CCSM8WSY\\n52b28304-ed8b-406f-a213-b42d9d37257a,,2.0,,\"Trying to get to Alexey\\'s level with the VS Code short cuts. In case this is a pain point for people to format the schema I threw this together. Just pass the spark.createDataFrame(df_pandas).schema to the function and it should print it nicely. Hope it helps some. <https://paste-bin.xyz/41069> Also, if <@U01AXE0P5M3> wanted to make a YouTube video of his favorite VS Code shortcuts I would give it a watch or three.....Just sayin. :-)\",1646164207.385069,1646164207.385069,U02TNEJLC84\\nf910b9cd-4203-4195-975e-4c47060489ed,U02T70K8T61,,,<@U02T70K8T61> Is that the entire error? What command or action are you doing when this is displayed?,1646163862.906909,1646164289.791629,U02TNEJLC84\\na6afe7b2-b36b-4d85-bd85-8948499e93a4,U02T70K8T61,,,<@U02T70K8T61> Can you share your DAG?,1646163862.906909,1646164326.460469,U02SUUT290F\\nC9BE936F-5701-4CFE-926B-3EB653BDFBD5,U02TNEJLC84,,,:100: for Alexey’s vs code short cut video!,1646164207.385069,1646164753.509449,U02UM74ESE5\\n704e69a9-54f6-4d24-a158-2faae8b25ee0,U02CKHHRY6Q,,,Thank you Alvaro!,1646074398.401919,1646167371.561259,U01AXE0P5M3\\neacc60b1-2f7a-4fed-b065-85946d22a9a8,U02CKHHRY6Q,,,Gracias Alvaro :),1646074398.401919,1646172803.228149,U02CD7E30T0\\nf1b36963-955f-40fb-a190-4f927b7ea2cb,U02TNEJLC84,,,\"Am in for the video, already learning some of them on my own\",1646164207.385069,1646174300.805609,U02T0CYNNP2\\nbc29697c-ecf0-49e2-82d2-a01ee41d33e8,U02U5G0EKEH,,,Thank you!,1646118173.058739,1646179538.894169,U02U5G0EKEH\\n2baec77d-5ec5-44b9-a77c-2f6d36ef7ffd,,1.0,,I\\'m sort of lost in Week 5. I can\\'t find the job for the join of green and yellow revenue on localhost:4040. I just have old jobs listed. I\\'m watching the join video and I just have old stage diagrams. But the join is working and populating the report directory with parquet files.,1646190015.093249,1646190015.093249,U02UBQJBYHZ\\n34016226-8ab6-4571-a821-1248b8acc46e,U02UBQJBYHZ,,,\"Never mind, of course as soon as I wrote that I realized I was running two notebooks and the second one started on port 4041.\",1646190015.093249,1646190186.436849,U02UBQJBYHZ\\n13184539-da28-4f0d-8ae3-aa8df3d1abd9,U02R2PU9NLD,,,Deadline is 9th March. But if week 5 deadline is extended we will extend deadline for week 6 also,1646148218.609659,1646197113.380659,U01DFQ82AK1\\n823892b2-3fde-47cc-9929-375f535ce375,U02T70K8T61,,,<@U02TNEJLC84> yes that is the entire error when trying to upload the DAG to the airflow,1646163862.906909,1646206604.449709,U02T70K8T61\\n77064a09-b9f0-47dd-be05-19204e0f0a4a,U02T70K8T61,,,<@U02SUUT290F> I just restarted the server and now it is working lol,1646163862.906909,1646206654.671469,U02T70K8T61\\nacf411ed-24e8-4e7e-bd4e-f1d31a59ea4d,U02T70K8T61,,,so weirs,1646163862.906909,1646206660.148809,U02T70K8T61\\n726e89d3-59c6-48a1-ab6f-cbb75eda36a2,U02T70K8T61,,,weird,1646163862.906909,1646206664.026359,U02T70K8T61\\n8e09d5fc-5694-419b-9c84-d129989a42c0,U02T70K8T61,,,Here is the picture nonetheless,1646163862.906909,1646206773.473759,U02T70K8T61\\nb151f4bc-1fed-4688-814d-c1c59dcdebb1,,,,\"I found the simple tutorial for setup spark cluster\\n<https://medium.com/codex/setup-a-spark-cluster-step-by-step-in-10-minutes-922c06f8e2b1>\",,1646385087.090939,U02SQQYTR7U\\na4981f93-e5e1-4197-866d-413308422006,U0330DCCDN1,,,<@U02TNEJLC84> it\\'s not:slightly_smiling_face:,1646329299.706539,1646386620.870769,U034Q6ZKR9C\\n332f1fd2-fb83-4964-88ba-65412ca634fa,U034Q6ZKR9C,,,\"My current personal cards didn\\'t get through, but turned out that I have credentials for and old card that I didn\\'t for sometime and it worked. Do not have any solution, at this point, just luck I guess\",1646369026.483969,1646386751.417029,U034Q6ZKR9C\\n4F1B0332-422C-47D7-B1FE-6C9A9041B119,U02T9GHG20J,,,I guess I might have to run some experiments to figure this one out,1646367680.437609,1646401493.796539,U02T9GHG20J\\nbfdf98e7-8a22-469c-bcb0-ebc86792b85d,,1.0,,\"hi all anyone managed to solve this `googleapi: Error 403: The billing account for the owning project is disabled in state absent, accountDisabled`\",1646404258.477979,1646404258.477979,U035VQMUY9E\\n2cf66d7d-0dd8-47bd-a15a-52490bf76661,,10.0,,Why step RUN pip install --no-cache-dir -r requirements.txt  takes so long to process?,1646411072.603079,1646411072.603079,U034Q6ZKR9C\\ne751af53-e3b5-4f30-b213-e79b1fac9c43,U034Q6ZKR9C,,,,1646411072.603079,1646411227.193569,U034Q6ZKR9C\\nb36cb825-2014-4be8-b659-c3f47a665d55,,6.0,,\"Hello everyone!\\nI\\'m thinking about one of the best practices for Bigquery:\\n• Do not threat `WITH` clauses as prepared statements.\\nI don\\'t understand this sentence. How can I threaten (or not) `WITH` clauses as prepared statements?? Can someone help me with some examples?\\n\\nThanks :simple_smile:\",1646414115.669799,1646414115.669799,U02URV3EPA7\\n86610c95-e878-4c6f-9aef-1dce83843a18,U02URV3EPA7,,,\"I think it\\'s supposed to say \"\"Don\\'t _treat_ WITH clauses as prepared statements.\"\" Sometimes I wish I could threaten my code though!!\",1646414115.669799,1646417887.008449,U02U3E6HVNC\\n90ebc172-56d3-4eb9-8105-4056ca06986b,U02URV3EPA7,,,\"heheh yep. \"\"Treat\"\" is the correct word but I still don\\'t understand.\",1646414115.669799,1646418046.455089,U02URV3EPA7\\nbbd3d134-007b-450f-95dc-1af1d2d6ffbe,,10.0,,\"I\\'m having trouble with the final question on homework 6, and I want to make sure I\\'m on the right track. We should be using faust.joins, right?\",1646418691.258709,1646418691.258709,U02U3E6HVNC\\nec63645a-ffc4-4873-9c8c-a89fab9d692e,U032Q68SGHE,,,thanks <@U02U34YJ8C8>,1646341588.083949,1646420457.256439,U02T0CYNNP2\\n8addac68-8741-444e-ac63-9c7233a5d7ea,U032Q68SGHE,,,<@U02U34YJ8C8> are you on LinkedIn lets connect,1646341588.083949,1646420550.629469,U02T0CYNNP2\\n125f5854-f57b-4a83-9b8c-d8f8fba72c00,U02U3E6HVNC,,,Hi <@U02U3E6HVNC> I believe the last question is trying to get you to combine everything that we\\'ve learnt so far. In this case I believe that you will need both joins and a particular type of group by in order to get the answer. Make sense?,1646418691.258709,1646434525.769709,U02U5SW982W\\nDAFD7865-7913-4D04-A900-4A4C6E78B4AA,,4.0,,Curious if we will go over data testing and validation in this course? Thank you!,1646436323.584369,1646436323.584369,U02UX5MHB40\\nfa57af50-8f94-4684-a917-1c8be51c02e9,U02UX5MHB40,,,Hi <@U02UX5MHB40> I believe <@U01AXE0P5M3> said that we will not be covering this. I think it was in one of his earlier videos but can\\'t put my finger on it.,1646436323.584369,1646437947.749709,U02U5SW982W\\n0fc3e099-37e1-4d3c-a3a5-c6bb6a52c0f1,,,,\"Has anyone else received the error\\n```  File \"\"producer.py\"\", line 3, in &lt;module&gt;\\n    from kafka import KafkaProducer\\n  File \"\"/home/gary/.local/lib/python3.8/site-packages/kafka/__init__.py\"\", line 23, in &lt;module&gt;\\n    from kafka.producer import KafkaProducer\\n  File \"\"/home/gary/.local/lib/python3.8/site-packages/kafka/producer/__init__.py\"\", line 4, in &lt;module&gt;\\n    from .simple import SimpleProducer\\n  File \"\"/home/gary/.local/lib/python3.8/site-packages/kafka/producer/simple.py\"\", line 54\\n    return \\'&lt;SimpleProducer batch=%s&gt;\\' % self.async```\\nafter pip installing kafka and attempting to run producer.py?\\n\\nSolution: I guess it needs kafka-python and not kafka. If you get this just run\\n```pip uninstall kafka\\npip install kafka-python```\\n\",,1646438591.483779,U02TNEJLC84\\n63512ed9-9635-4ed3-a27e-5591a48d7c00,U02UX5MHB40,,,Hi <@U02UX5MHB40> - I just stumbled on it then. <@U01AXE0P5M3> said in Week 2: DE Zoomcamp 1.2.1 in relation to Docker that \\'Note with integration tests above we won’t cover this in our course – you can look up what CI/CD is – but we will usually use things like GitHub Actions GitLab CI/CD or Jenkins. Things like that. So you can take a look at that (yourselves) if you are interested. I would recommend you learn about this.\\' - this was from my transcript/notes of his video on my blog <https://learningdataengineering540969211.wordpress.com/2022/01/26/week-2-de-zoomcamp-1-2-1/|post> (in case you were wondering about the context etc.). Hope that helps?,1646436323.584369,1646439468.892319,U02U5SW982W\\n7af08557-b480-4c05-bd7a-596c175b0d4d,U02U3E6HVNC,,,\"My issue is, I\\'m not sure how to execute the join properly in Python. The sample code in the Confluent docs looks like Java: <https://docs.confluent.io/platform/current/streams/developer-guide/dsl-api.html#kstream-kstream-join>\\n\\nAnd the docs for the faust.joins classes isn\\'t much to go off of: <https://faust.readthedocs.io/en/latest/reference/faust.joins.html>\",1646418691.258709,1646440649.547429,U02U3E6HVNC\\na7a1ab8a-a513-46f6-9890-2b9d5f23de52,U02ULP8PA0Z,,,<@U02U5SW982W> So glad you found it useful! :blush:,1646099641.598249,1646441275.493969,U02ULP8PA0Z\\n07D8816D-97AA-4DAC-8590-70051A096096,,1.0,,Do we have Kafka homework?,1646448560.078759,1646448560.078759,U02U6DR551B\\n455b4f65-296d-41f5-9e19-ae482dc4bb7b,,,,My schema-registry and broker services always exit when I run the compose file with `docker-compose up -d` but seems to work fine when I run it without a `-d` flag. Has anybody else faced this issue?,,1646461004.072629,U02TATJKLHG\\na9735ddb-5ab8-47cf-b804-f4cdf5199777,U033VBQCGQK,,,\"Thanks everyone, sorted this by enabling virtualization on windows\",1646235795.512249,1646462975.671799,U033VBQCGQK\\n72a77788-21cb-462f-95c4-4cf836fc2172,U0300EGP2EL,,,\"thank you ankur, that helps\",1646320735.860349,1646463039.414809,U0300EGP2EL\\nb0f0199a-0506-4e6b-9367-55522a4f571f,U034Q6ZKR9C,,,Holy shit that\\'s close to 40 minutes isn\\'t it. Maybe issues with the internet?,1646411072.603079,1646463345.171179,U02TATJKLHG\\ne71ca7ba-febe-4b2d-a284-1fc2e64595a7,U034Q6ZKR9C,,,\"Nope, internet speed is great\",1646411072.603079,1646463690.132569,U034Q6ZKR9C\\n9e603aee-71e3-4346-b6e2-66d434bbac30,U034Q6ZKR9C,,,Still having this issue,1646411072.603079,1646463704.170549,U034Q6ZKR9C\\nf736ec76-6df5-4656-bd36-c11f0ba89d03,U034Q6ZKR9C,,,Are you trying on your local machine? Do you have a VM?,1646411072.603079,1646463971.366399,U02TATJKLHG\\n1d7e2c88-1cbd-45f8-aa3b-9c9436ebc8d8,U02U6DR551B,,,\"in the down of the week 6 readme there is a link to the hw form.\\n<https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_6_stream_processing>\",1646448560.078759,1646465829.585419,U02R2PU9NLD\\neae75b63-6d17-45d8-b34a-62fd5cb4229c,U034Q6ZKR9C,,,\"Yeah, I\\'m doing it locally\",1646411072.603079,1646466034.742509,U034Q6ZKR9C\\n3b4ec578-4371-4b0f-a7b2-73ca61b013c1,,,,\"For folks who are trying out the coding question in this weeks homework, Spark streaming could be your way to go as Faust has pretty much nothing to offer for joins. You can conveniently connect Spark to your Kafka broker and then just write dataframe operations as you would do in batch. I did this and it\\'s working for me. You can check out the structured streaming guide here :smile:\\n\\nLearn Spark Streaming - <https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html>\\nKafka + Spark Integration - <https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html>\",,1646988685.297649,U02TATJKLHG\\n53f950d4-1fa4-4336-8176-70a5d762ce76,U01T2HV3HNJ,,,\"I used Spark streaming instead and was able to perform the joins :smile:. Here\\'s the official spark guide.\\n<https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html>\",1646731420.426559,1646988746.043329,U02TATJKLHG\\n7e3ccc4e-0162-4eda-8fff-df8e4ec5820d,,4.0,,\"hi all, could i ask which container do i need to enter in order to run below command line? aim is to adjust the partitions without using control centre UI. thanks!\\n`./bin/kafka-topics.sh --create --topic demo_1 --bootstrap-server localhost:9092 --partitions 2`\",1646993308.399819,1646993308.399819,U02ULMHKBQT\\n034b8f10-8a21-4708-85b4-e9319a17dbd8,U02ULMHKBQT,,,i already have docker compose up and running.,1646993308.399819,1646993447.877089,U02ULMHKBQT\\na82b69f5-687e-4506-8af4-25717da3f4c5,,2.0,,\"Just wondering if there’s a typo with one of the questions in week 6? Was it supposed to ask about joining _*kstream*_ with ktable, or ktable with ktable?\\n`When joining KTable with KTable partitions should be`\",1646996078.393879,1646996078.393879,U02ULMHKBQT\\na5b98afe-cce7-4b0e-8ecf-bff6746be296,U02ULMHKBQT,,,server container,1646993308.399819,1646996329.797539,U01DFQ82AK1\\n21014292-79b2-42e3-ac1b-fc26b4ef94fc,U02ULMHKBQT,,,you can follow this <https://datatalks-club.slack.com/archives/C01FABYF2RG/p1646816575358089?thread_ts=1646686750.451479&amp;cid=C01FABYF2RG>,1646993308.399819,1646996360.953919,U01DFQ82AK1\\ndefc9dcf-f840-4589-9884-ef9a9341aabe,U02ULMHKBQT,,,\"thanks a lot, i must hv missed that thread out!\",1646993308.399819,1646996660.776859,U02ULMHKBQT\\nd5c787ff-5cb1-49a6-83ac-b291f0243eb1,U02TVGE99QU,,,yep,1646831287.988489,1647004286.764259,U02TVGE99QU\\n346981b7-2c98-4f67-ad17-614d95bdc3c3,U02A3RTSE5D,,,\"kindly whats the sulution i can have for gcp or an alternative resource, now that im facing a challenge\",1646825218.588189,1647014799.406749,U02AUCL9ZQF\\n72b856a7-904f-435b-8d26-e6ae032b3efe,U01DFQ82AK1,,,\"OK thanks, I wasn\\'t sure of the right order to do this. The logs are impossible. They are pages and pages long. I will see if I can pipe them into a file if I keep having problems.\",1646904198.505189,1647024819.982239,U02UBQJBYHZ\\nacf05b6a-d395-4539-9d12-1316ccc26e43,,2.0,,\"Week 6 - last question. Is it possible to complete this question in Kafka with kafka-python? I haven\\'t actually attempted it yet but given the flurry of questions here on Slack I wanted to just check that it IS possible to complete the question in kafka-python before I start chasing my tail... I am assuming that it is (what would be the point of setting an impossible task after all), but thought I\\'d better check.\",1647038172.532389,1647038172.532389,U02U5SW982W\\nF5D1BDA4-6C17-4097-869F-C96FEA4C8CE4,U02U5SW982W,,,Yes. U can even choose any other library or language for it ,1647038172.532389,1647039864.129169,U01DFQ82AK1\\n53eaddb6-b8f2-4869-ba0b-b36946771bfb,U02U5SW982W,,,Awesome - thanks <@U01DFQ82AK1> :slightly_smiling_face:,1647038172.532389,1647042363.431979,U02U5SW982W\\n0dba1ef7-007b-48c8-813f-9beae9faec28,,6.0,,\"I still can\\'t get kafka to run on my environment. I updated the OS, thought that would help. It is Mac OS 12.2.1. I will post the errors in the comments.\",1647046781.674949,1647046781.674949,U02UBQJBYHZ\\n27d1588c-7f44-4dd2-a1ac-4a08ca6a233b,U02UBQJBYHZ,,,\"`control-center   | #`\\n`control-center   | # A fatal error has been detected by the Java Runtime Environment:`\\n`control-center   | #`\\n`control-center   | #  SIGSEGV (0xb) at pc=0x000000400c36c180, pid=1, tid=0x000000412fa93700`\\n`control-center   | #`\\n`control-center   | # JRE version: OpenJDK Runtime Environment (Zulu 8.38.0.13-CA-linux64) (8.0_212-b04) (build 1.8.0_212-b04)`\\n`control-center   | # Java VM: OpenJDK 64-Bit Server VM (25.212-b04 mixed mode linux-amd64 compressed oops)`\\n`control-center   | # Problematic frame:`\\n`control-center   | # J 4308 C1 org.apache.kafka.common.record.DefaultRecord.sizeOfBodyInBytes(IJLjava/nio/ByteBuffer;Ljava/nio/ByteBuffer;[Lorg/apache/kafka/common/header/Header;)I (42 bytes) @ 0x000000400c36c180 [0x000000400c36c180+0x0]`\\n`control-center   | #`\\n`control-center   | # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try \"\"ulimit -c unlimited\"\" before starting Java again`\\n`control-center   | #`\\n`control-center   | # An error report file with more information is saved as:`\\n`control-center   | # //hs_err_pid1.log`\\n`control-center   | #`\\n`control-center   | # If you would like to submit a bug report, please visit:`\\n`control-center   | #   <http://www.azulsystems.com/support/>`\\n`control-center   | #`\\n`control-center   | qemu: uncaught target signal 6 (Aborted) - core dumped`\\n`broker           | [2022-03-12 00:45:29,483] INFO [GroupCoordinator 1]: Member _confluent-controlcenter-5-4-0-1-29469009-bba2-43cd-860a-53cd9658ae95-StreamThread-7-consumer-5fa737cd-1f4f-4613-8622-14d55a8acdd6 in group _confluent-controlcenter-5-4-0-1 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)`\\n`broker           | [2022-03-12 00:45:29,488] INFO [GroupCoordinator 1]: Preparing to rebalance group _confluent-controlcenter-5-4-0-1 in state PreparingRebalance with old generation 3 (__consumer_offsets-18) (reason: removing member _confluent-controlcenter-5-4-0-1-29469009-bba2-43cd-860a-53cd9658ae95-StreamThread-7-consumer-5fa737cd-1f4f-4613-8622-14d55a8acdd6 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)`\\n`broker           | [2022-03-12 00:45:30,297] INFO [GroupCoordinator 1]: Member _confluent-controlcenter-5-4-0-1-29469009-bba2-43cd-860a-53cd9658ae95-StreamThread-5-consumer-becf9af9-6caf-406a-ac1b-aac616f2f4b7 in group _confluent-controlcenter-5-4-0-1 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)`\\n`broker           | [2022-03-12 00:45:30,300] INFO [GroupCoordinator 1]: Member _confluent-controlcenter-5-4-0-1-29469009-bba2-43cd-860a-53cd9658ae95-StreamThread-1-consumer-c2ba3a19-b1e9-4ee9-95f9-d7262978a352 in group _confluent-controlcenter-5-4-0-1 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)`\\n`broker           | [2022-03-12 00:45:30,310] INFO [GroupCoordinator 1]: Member _confluent-controlcenter-5-4-0-1-29469009-bba2-43cd-860a-53cd9658ae95-StreamThread-8-consumer-c0cdb1ae-d383-4e1b-bcc4-833b67e4199e in group _confluent-controlcenter-5-4-0-1 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)`\\n`broker           | [2022-03-12 00:45:32,014] INFO [GroupCoordinator 1]: Member _confluent-controlcenter-5-4-0-1-command-e48a384e-3293-4323-a61b-9f1af503f746-StreamThread-1-consumer-6c6bef7b-2b01-4682-ad7c-57465e2bdcab in group _confluent-controlcenter-5-4-0-1-command has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)`\\n`broker           | [2022-03-12 00:45:32,022] INFO [GroupCoordinator 1]: Preparing to rebalance group _confluent-controlcenter-5-4-0-1-command in state PreparingRebalance with old generation 1 (__consumer_offsets-14) (reason: removing member _confluent-controlcenter-5-4-0-1-command-e48a384e-3293-4323-a61b-9f1af503f746-StreamThread-1-consumer-6c6bef7b-2b01-4682-ad7c-57465e2bdcab on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)`\\n`broker           | [2022-03-12 00:45:32,036] INFO [GroupCoordinator 1]: Group _confluent-controlcenter-5-4-0-1-command with generation 2 is now empty (__consumer_offsets-14) (kafka.coordinator.group.GroupCoordinator)`\\n`broker           | [2022-03-12 00:45:32,245] INFO [GroupCoordinator 1]: Member _confluent-controlcenter-5-4-0-1-29469009-bba2-43cd-860a-53cd9658ae95-StreamThread-6-consumer-95020efd-88f3-4cca-a8a2-e444a3b1249f in group _confluent-controlcenter-5-4-0-1 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)`\\n`broker           | [2022-03-12 00:45:32,248] INFO [GroupCoordinator 1]: Member _confluent-controlcenter-5-4-0-1-29469009-bba2-43cd-860a-53cd9658ae95-StreamThread-3-consumer-4d24c8b9-ccec-4577-a250-7766a32506a3 in group _confluent-controlcenter-5-4-0-1 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)`\\n`broker           | [2022-03-12 00:45:32,273] INFO [GroupCoordinator 1]: Member _confluent-controlcenter-5-4-0-1-29469009-bba2-43cd-860a-53cd9658ae95-StreamThread-4-consumer-14731ed3-4e22-475b-b554-33112faf899e in group _confluent-controlcenter-5-4-0-1 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)`\\n`broker           | [2022-03-12 00:45:32,310] INFO [GroupCoordinator 1]: Member _confluent-controlcenter-5-4-0-1-29469009-bba2-43cd-860a-53cd9658ae95-StreamThread-2-consumer-778103cb-a6fb-404c-b4dc-11571bbcb7c8 in group _confluent-controlcenter-5-4-0-1 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)`\\n`broker           | [2022-03-12 00:45:32,312] INFO [GroupCoordinator 1]: Group _confluent-controlcenter-5-4-0-1 with generation 4 is now empty (__consumer_offsets-18) (kafka.coordinator.group.GroupCoordinator)`\",1647046781.674949,1647047367.573649,U02UBQJBYHZ\\nb85a545b-2633-4569-b331-dfe6fbbc5ed0,U02UBQJBYHZ,,,I\\'m not sure this is the problem. I have about 8000 lines of info and warning from running docker-compose up. Does anyone think I might have processes running that I need to kill? I\\'ve restarted my computer. I don\\'t know what\\'s going wrong and I would like to progress beyond repeating the same frustrating situation that I\\'ve experienced for the last seven days.,1647046781.674949,1647047891.601809,U02UBQJBYHZ\\nC863090A-9FDA-439E-A85E-1A6706C5456A,U02UBQJBYHZ,,,\"Hi cris, could U pls confirm that u have installed all the packages from requirements.txt before running docker compose?\",1647046781.674949,1647048663.800909,U02ULMHKBQT\\n473d7a54-461a-4892-a141-1bfa7f8b706c,U02UBQJBYHZ,,,I got the impression from Ankush reply to me on another thread that I run the requirements.txt in a virtual environment after I get the docker images running.,1647046781.674949,1647048877.413549,U02UBQJBYHZ\\na953f76a-e9c9-4257-b2d2-5192220fa7f1,U02UBQJBYHZ,,,\"I might have fixed it!!!! I went into docker desktop and there were active docker images that didn\\'t appear when I typed \"\"docker ps\"\". I deleted them and now I get control-center up with all the topics. Fingers crossed that the virtual environment works to run the python file.\",1647046781.674949,1647049098.376489,U02UBQJBYHZ\\nb9c94bfe-c779-4720-b42e-a4a3f0a2a859,U01DFQ82AK1,,,\"OMG it works, I had to delete some docker files from airflow week that didn\\'t show up in docker ps. I could see them in docker desktop and I deleted them from there.\",1646904198.505189,1647049265.951799,U02UBQJBYHZ\\n51b5bb7c-aa59-40c4-b8f4-e90edd976b38,U02UBQJBYHZ,,,Yes fixed. Zombie docker files.,1647046781.674949,1647049850.653379,U02UBQJBYHZ\\n8ff837aa-bfe8-40a6-9178-e8695bc4686c,U02UBQJBYHZ,,,\"Hi, Sorry for so many threads - I had some docker images from Week 4 still running that did not show up on \"\"docker ps.\"\" I could see them in Docker desktop and I removed them from there, then everything worked as expected and I was able to run producer and consumer scripts, and get the same results as in the video.\",1646836356.583069,1647051114.242239,U02UBQJBYHZ\\n,,,channel_join,<@U01AXE0P5M3> has joined the channel,,1606078378.000200,U01AXE0P5M3\\nc750d7d6-2e19-4604-ba5d-b9bcabaa1dfc,,,,\"Hello everyone!\\n\\nLet\\'s talk about our idea to create a course on data engineering. I\\'ve started a document: <https://docs.google.com/document/d/1TI1co3aRcYYBB9keAMPgrnQd84juN-i8bZiiXW_1oUo/edit?usp=sharing>\",,1606078694.001300,U01AXE0P5M3\\n,,,channel_topic,set the channel topic: <https://docs.google.com/document/d/1TI1co3aRcYYBB9keAMPgrnQd84juN-i8bZiiXW_1oUo/edit?usp=sharing>,,1606078707.001500,U01AXE0P5M3\\n,,,channel_join,<@U01DHB2HS3X> has joined the channel,,1606078713.001700,U01DHB2HS3X\\n,,,channel_join,<@U01F78474M9> has joined the channel,,1606078713.001900,U01F78474M9\\n9d6c86dc-3215-4a80-a03d-2a603ceddd7a,,,,\"I think we should figure out\\n\\n• who the course is for - software engineers interested in data engineering, data scientists/analysts, or graduates without experience? \\n• the objective - what kind of outcome we want to have? being able to work as a data engineer after completing it? \\n• the main topics we want to cover given the first two things \",,1606078852.004300,U01AXE0P5M3\\n457cc8d2-5ba1-4db3-b53f-484e89873f00,,,,I\\'d say software engineers and data scientists/analysts could be good target audience,,1606078935.005900,U01AXE0P5M3\\necf66f14-456f-46fb-94be-5d8c7f1c284b,,1.0,,this is a good starting point for the list of topics: <https://github.com/DataTalksClub/data-paths/blob/main/career_paths/data_engineer.md> (thanks <@U01DHB2HS3X>!),1606078995.006400,1606078995.006400,U01AXE0P5M3\\n93eba8e0-4df8-4103-a2c6-97d9784dbc9e,,,,\"the way I imagine it - at the end we come up with 6-7 topics, find 6-7 people to cover them, write some a couple of articles for each topic, and then cover each topic in a live webinar\",,1606079097.008700,U01AXE0P5M3\\nec56ce21-26e2-4d23-9967-aa71b402ebaa,U01AXE0P5M3,,,I have collected some more resources. I\\'ll update them by tomorrow.,1606078995.006400,1606079200.010400,U01DHB2HS3X\\n7f6e6a01-7b6e-4b45-9eb3-841012f1552c,,,,\"perhaps we should split the course or to have some kind of entrance criteria(extra topics to be learn for an Analyst for example), cause SDE vs Analyst will have different experience. wdyt?\",,1606079203.010600,U01F78474M9\\n3419cfb6-9229-4280-abec-32b24f981984,,,,\"yes, I agree! I added a \"\"prerequisites\"\" section\\n\\nMaybe then data analysts shouldn\\'t be the main focus audience - we can do some prep course later for them\",,1606079381.013100,U01AXE0P5M3\\n28d13ad5-44e7-4b20-a828-d81f5e3c0825,,,,\"The reason I got this idea is I see that\\n• many data scientists want to get into data engieering to be more independent \\n• many software engineers are interested in this topic\",,1606079426.014500,U01AXE0P5M3\\nd4a24615-ac40-405c-8916-4b5f67bd2bf7,,,,the question is - how much overlap they have in terms of pre-requisites,,1606079454.015400,U01AXE0P5M3\\ne8e63a99-0c88-43e0-ae92-bdaca95a786e,,,,\"yeah, from my experience DA mostly going in direction of DS, rather than Data Engineering, but there are a lot of SDE who is tired of doing backend and want to get into the data :slightly_smiling_face:\",,1606079478.015900,U01F78474M9\\nfdbfaa9f-aad5-4993-bc18-62f7f8e407fb,,1.0,,\"for example a course might be a good base for creating some kind of retraining from SDE to Data Engineer. it’s quite common in big companies such as Amazon, Spotify(ex head of data engineering in spotify was an author of Data Engineering academy in the company, which totally makes sense)\",1606079641.017900,1606079641.017900,U01F78474M9\\n,,,,\"I had a chat with Ben, he\\'d be pretty interested in such a course\",,1606079646.018000,U01AXE0P5M3\\n50db9704-ead3-4905-a2bf-eb0aca0e1881,,,,interesting :slightly_smiling_face:,,1606079704.018700,U01F78474M9\\ne151ed30-b9b7-49cf-af08-23b04187b209,,,,Ben told me that he wants to do this bootcamp: <https://www.dataengineering.academy/curriculum> - they have a comprehensive curriculum,,1606079727.019100,U01AXE0P5M3\\n3f9ece31-a1a4-46b9-8e5d-fcd0b9f2921f,,,,which reminds me that we should probably add streaming to the list :smiley:,,1606079751.019500,U01AXE0P5M3\\n3d385c29-18c9-4dc3-9d88-959de8be8a78,U01F78474M9,,,could be indeed the right target audience! the demand is definitely there,1606079641.017900,1606079810.020000,U01AXE0P5M3\\n6a32bc04-3caf-4186-9982-2a3a57c39d39,,,,\"oh, yeah true!\",,1606079868.021100,U01F78474M9\\n29510708-ef36-41a1-828f-24a67f965d37,,,,\"so after we come up with a rough plan, we can chunk it into lessons and write what exactly we want to cover in each lesson\",,1606079873.021200,U01AXE0P5M3\\nba164111-0d60-4cfd-bc85-e2756d12cf1a,,,,sounds good,,1606079884.021400,U01F78474M9\\n93f553a1-010d-4843-b345-109983afb887,,,,\"I\\'ve also asked <@U01DFQ82AK1> if he wants to join, and there are a couple more data engineers already in this slack which we can also talk with\",,1606079932.022100,U01AXE0P5M3\\n48b7a764-60f4-400c-bba6-696272de21f1,,,,so for now we can just do some sort of brainstorming - come up with many different topics/technologies and then narrow it down,,1606080020.023100,U01AXE0P5M3\\n7e3aab16-5548-4733-80fb-507b8c70d52c,,,,Sounds great!,,1606080337.023500,U01DHB2HS3X\\ne2eed2c1-0cb2-4825-a0b5-9812f754c43c,,,,<https://datatalks-club.slack.com/archives/C01B0JGRBMY/p1604556725002400> this thread could be useful,,1606080521.023700,U01AXE0P5M3\\n2f1fc82e-d348-4b34-a095-1c1c7ee4d023,,,,\"what do you think about the order? I\\'m not sure where to put the \"\"cloud\"\" section\",,1606080767.024400,U01AXE0P5M3\\n2bde12cb-72f9-4af4-aad3-a99e3ce71de2,,,,another idea - perhaps each lesson should have two parts: theory where we just show things and practice - like a demo/walkthrough of how some particular technology could be used,,1606081001.025700,U01AXE0P5M3\\n843e9401-73b9-43f3-91b3-f1a69257a0a9,,,,\"most of the tools from known clouds overlap and I would say it’s better to know a concept, but the tools itself could be just for general knowledge\",,1606081055.026600,U01F78474M9\\n231c4a21-28cc-4e30-b616-f713a28dd6ea,,,,\"I like the idea. in general any courses I’m more in favour for practice, rather than a lot of reading\",,1606081086.027600,U01F78474M9\\ncb8745c2-0956-4089-92be-f23aabf363f3,,,,I know only AWS :see_no_evil:,,1606081088.027700,U01AXE0P5M3\\nc2193b7a-907a-475c-a5f9-e1db015e5e1b,,,,\"I know both aws and gcp, azure is another planet, but as far as I know all those clouds share the same concepts\",,1606081133.029000,U01F78474M9\\nc9ee42e8-413a-428b-b9b4-3c0a2780bd0c,,,,do you think it makes sense to keep it as a separate lesson?,,1606081246.029900,U01AXE0P5M3\\n9c287849-2547-4af7-85b3-4160d18880b6,,,,I mean the whole cloud thing,,1606081256.030100,U01AXE0P5M3\\n6ef23a97-cf71-4cb8-bc05-2bd6d3bdfa7f,,,,\"mmm probably not, all the parts of the clouds will be a part of one of he bigger sections like streaming, storage, etc\",,1606081322.030800,U01F78474M9\\nb6248d19-c5b6-44e9-9c5a-ac206b028230,,2.0,,\"How about we design the course such that the audience can build end-to-end data products or pipelines? If we\\'re aligned on that, then we can have a module for each project, using a certain stack of technologies. We could start explaining the concepts of a pipeline design (streaming, storage etc.) using basic/self-hosted technologies like Spark+Kafka. And then move onto the next module for a Cloud-based project to demonstrate a pipeline design using AWS/GCP stack.\",1606084093.039600,1606084093.039600,U01DHB2HS3X\\nf0b2ab77-f07c-4449-96f8-89830913c1ca,,,,that\\'s a great idea! do you have any project in mind?,,1606084702.042600,U01AXE0P5M3\\n51a1aeb8-b49d-410b-bb1c-167e873937b9,,,,I can look something up for AWS/serverless. Could be something simple like clickstream...,,1606085596.002500,U01DHB2HS3X\\n,,,channel_join,<@U01DFQ82AK1> has joined the channel,,1606115756.000200,U01DFQ82AK1\\n8b9b5155-b478-443c-aec9-807fb667a449,,,,\"can some body help....I am booted out of ssh of my virtual machine, I couldnt login from yesterday. I need to submit homework. all code is in vm\",,1646212988.978599,U0300EGP2EL\\n247cbf6f-2212-4455-8b44-26a8055c9b07,,,,error is :  Permission denied (publickey).,,1646213009.862159,U0300EGP2EL\\n70aad65f-cc79-4b23-a63c-b2c4ed71b044,,6.0,,\"I have tried by adding new pair of key and tried again, still facing with same issue\",1646213045.645589,1646213045.645589,U0300EGP2EL\\n3f87cdc1-c070-4126-b062-1e53c7efb341,,5.0,,I keep getting timeout error when running the download task . How can i sort it out ?,1646214976.685759,1646214976.685759,U02T70K8T61\\nba374986-0dbb-4795-a89a-99836688d5ae,U02T70K8T61,,,I think you need try download from browser to make sure internet connection is no issue,1646214976.685759,1646215119.772029,U02SQQYTR7U\\nf0d0f42e-e140-4560-83d0-956f62782de2,U02T70K8T61,,,its downloading perfectly from the browser,1646214976.685759,1646217791.165949,U02T70K8T61\\nd119a6d6-e48c-4a6f-b557-338cd3d42b8a,U02T70K8T61,,,is there a way  i can increase the timeout \\'time\\',1646214976.685759,1646217817.688669,U02T70K8T61\\nc3d900a9-ae97-4d2b-acbe-d21136c60965,U02T70K8T61,,,,1646214976.685759,1646218269.848229,U02T70K8T61\\n29ad05d0-ce6f-4720-bb0d-14c847175c4d,U0300EGP2EL,,,did you change or regenerate your ssh keys? Check how many keys do you have in `.ssh` folder,1646213045.645589,1646220524.220169,U01DFQ82AK1\\n6bdbb2c4-c2db-4e1f-8004-a2b1562a88a5,U0300EGP2EL,,,\"If you restarted the VM, the IP address would be changed. Worth to check\",1646213045.645589,1646223113.122209,U02C2M83SE5\\n86A91A2D-0DBD-4782-B2A6-88B2223A95D8,U02T70K8T61,,,\"<@U02T70K8T61> this is from course repo ..please try  it out ..\\n\\n# WORKAROUND to prevent timeout for files &gt; 6 MB on 800 kbps upload speed.\\n    # (Ref: <https://github.com/googleapis/python-storage/issues/74|https://github.com/googleapis/python-storage/issues/74>)\\n    storage.blob._MAX_MULTIPART_SIZE = 5 * 1024 * 1024  # 5 MB\\n    storage.blob._DEFAULT_CHUNKSIZE = 5 * 1024 * 1024  # 5 MB\\n    # End of Workaround\",1646214976.685759,1646224546.441679,U02AGF1S0TY\\n40c07b7c-df13-4597-9b10-8475385640d3,,9.0,,\"I have a query related to the Question 6 `Most common locations pair`  in week-5 Homework. Their are two columns in HVFHW dataframe PULocationID and DOLocationID , we have to join it the Taxi_Zones dataset. I\\'m joining my dataframe trips_hvhhw column  PUlocation with Taxi_Zones Location ID and then saving it to new dataframe.\\nThen again joining the my new dataframe that I got from last step with the DOLocationID with taxi_zones LocatonID.\\nAt the end after filtering and creating a new column with DOLOCATION and PULocation zones names and then grouping that new columns and sorting it.\\nBut I\\'m not matching  any options given in question 6.\\nIs my way of doing it is correct or I\\'m doing some mistake?\\nCan we do the same thing sparksql query?\",1646227839.083989,1646227839.083989,U0297ANJF6F\\n6df3e905-e690-480a-a3ab-c6c561b98866,U0297ANJF6F,,,\"Can you show your code?\\n\\nUsing sql is okay, I used it in my solution\",1646227839.083989,1646227933.880599,U01AXE0P5M3\\n74415e75-3cda-44ae-b740-62594ddb55e9,U0300EGP2EL,,,\"I have identified the possible reason: information in error trace is as folows\\n```systemd-logind.service: Failed to run \\'start\\' task: No space left on device systemd-logind.service: Failed with result \\'resources\\'.```\\nI remember I have consumed all the space in disk. Can I come out of this situation, I just want to login once, then I will clear space.\",1646213045.645589,1646227977.175629,U0300EGP2EL\\n792edc22-05c1-4543-a933-708c24c8e5bf,U0300EGP2EL,,,\"I couldnt even login with cloud console ssh due to the same reasons. I have resized the disk to 100GB, same is getting reflected in VM instance in GCP. but still couldnt login with same reason.\",1646213045.645589,1646228112.415339,U0300EGP2EL\\nc1d5d52f-a645-4429-a1cd-00ad803a0622,U0297ANJF6F,,,You can consider using sql and apply a self join with regard to PULocationID and DOLocationID from trips_hvhhw and LocationID from taxi_zones.,1646227839.083989,1646228709.998559,U02SEH4PPQB\\nd1ca21a1-56a4-4674-8deb-4feb52e21d8a,U0297ANJF6F,,,\"<@U01AXE0P5M3> I will share my code later, it\\'s on my PC at home, currently in university\",1646227839.083989,1646230665.834939,U0297ANJF6F\\ne0b5412b-0079-4895-b349-f53d6309a2f1,U0297ANJF6F,,,\\'Some code\\' from previous week might help :stuck_out_tongue:,1646227839.083989,1646232893.631949,U02HB9KTERJ\\n79299270-8047-4ea0-82bf-2aa0058d1858,U0300EGP2EL,,,\"I have faced the same issue with same method at week2. Increasing the disk size, restarting the VM and changing the ip addresses at config file. But I could not reach the VM and setup a new VM. I hope you will find a solution and we can learn the solution.\",1646213045.645589,1646234214.332929,U02TPL8JXS5\\n00c84753-5ff1-4301-b669-11a5fa551a32,,4.0,,\"Hi Everyone,\\n\\nPlease I am new here, and wanted to start off with docker.\\n\\nBut found out my machine BIOS Mode has legacy, which throws up the HyperV error message when loading Docker.\\n\\nIs there a work-around?\\nHow can one move forward from this thanks.\",1646235795.512249,1646235795.512249,U033VBQCGQK\\nee30e309-ec0f-46e0-8365-ceee2cfd1290,U033VBQCGQK,,,\"Not sure your model, but F10 on start up to get to bios and make sure virtualization is enabled. That should work.\",1646235795.512249,1646236900.846369,U02TNEJLC84\\n9906a369-fb37-4084-a088-ca307610895f,U0297ANJF6F,,,You can join to the same table more than once using different columns. Just alias the tables...just sayin.,1646227839.083989,1646237060.946419,U02TNEJLC84\\n88d0ddd7-cb66-47f0-8fa4-dcff9f7e9bfe,U033VBQCGQK,,,Other options could be installing Ubuntu or renting a VM in the cloud,1646235795.512249,1646238164.393619,U01AXE0P5M3\\ne53e4bc2-8283-4aad-8e74-e32cd4242b87,,1.0,,\"Hello everyone.\\nHere it is a little help for everyone working with SQL on Spark:\\n<https://spark.apache.org/docs/latest/api/sql/index.html>\\n:)\",1646242686.037309,1646242686.037309,U02CD7E30T0\\nf14f2add-7f54-4081-b475-653a9b0e9a5e,U02CD7E30T0,,,Good job <@U02CD7E30T0> - as always you are keeping on top of things :slightly_smiling_face:.,1646242686.037309,1646253034.483099,U02U5SW982W\\n9097c047-0083-4a83-8a40-71938bf0e073,U0297ANJF6F,,,\"<@U01AXE0P5M3> This is my code\\n```df_zones = spark.read.parquet(\\'zones/\\')\\n\\ndf_result = df.join(df_zones, df.PULocationID == df_zones.LocationID,how = \\'left\\')\\n\\ndf_result = df_result.select(\\'hvfhs_license_num\\', \\'dispatching_base_num\\', \\'Zone\\', \\'DOLocationID\\')\\ndf_result = df_result \\\\\\n    .withColumnRenamed(\\'Zone\\', \\'PU_Zone\\')\\n\\ndf_result = df_result.join(df_zones, df_result.DOLocationID == df_zones.LocationID,how = \\'left\\')\\n\\ndf_result = df_result.select(\\'hvfhs_license_num\\', \\'dispatching_base_num\\', \\'PU_Zone\\',\\'Zone\\')\\ndf_result = df_result \\\\\\n    .withColumnRenamed(\\'Zone\\', \\'DO_Zone\\')\\n\\nfhhv_zones =df_result \\\\\\n    .withColumn(\\'zones\\', F.concat(F.trim(df_result.PU_Zone),F.lit(\"\" / \"\"),F.trim(df_result.DO_Zone))) \\\\\\n    .select(\\'hvfhs_license_num\\', \\'dispatching_base_num\\', \\'zones\\')\\n\\nfhhv_zones = fhhv_zones.groupBy(\\'zones\\').count()\\n\\nfhhv_zones.coalesce(1).write.parquet(\\'/output/\\', mode=\\'overwrite\\')```\",1646227839.083989,1646260136.191019,U0297ANJF6F\\n44e1abe3-ad3b-4249-8647-78092b824616,U02TC704A3F,,,\"Hi <@U02TC8X43BN>\\n\\nThe problem I faced was that every \"\"read\"\" function used with Spark wasn\\'t being able to find my files, for some reason it looked like it was trying to find the data inside the created cluster. So I got the tip *<http://holowczak.com/getting-started-with-pyspark-on-google-cloud-platform-dataproc/5/|from this tutorial>* and used HDFS command to copy data from and to the cluister.\\n\\nI\\'m reading *<https://www.aosabook.org/en/hdfs.html|this paper here>* and some Spark documentation, trying to understand exactly why I had to do this and how HDFS works with clusters.\",1645659834.521759,1646280618.040409,U02TC704A3F\\n3e71dd25-c38f-460e-9b0d-b67934c6bc62,U0300EGP2EL,,,I started new VM and doing work in that. I couldnt login to old VM.,1646213045.645589,1646284686.149669,U0300EGP2EL\\n11f01fc1-71aa-4e5b-8b84-67209141259c,U01AXE0P5M3,,,\"Thanks for this thread. I have issues installing docker on my windows10 pro PC. I bought a pre-owned PC and virtualization was disabled, I tried to enable it through BIOS but the PC has bios setup administrator\\'s password and I dont know what the password looks like. Through my research, I noticed that I can retrieve it by using HP spare key but I dont know how to get that. I need help regarding this <@U01AXE0P5M3>. Is there any possible solution?\",1642186903.450800,1642235817.457600,U02TMEUQ7MY\\n488f80ea-a6e4-4262-8885-37ee08cc5b07,U01AXE0P5M3,,,,1642186903.450800,1642238683.458500,U02TMEUQ7MY\\n58e89632-87b7-4365-8e74-2a37ded4d57e,U01AXE0P5M3,,,\"<@U01AXE0P5M3>  I thought Since we\\'re going to work on GCP, we don\\'t need to install docker on windows.:sweat_smile:\",1642186903.450800,1642241093.459600,U02QGA57GRY\\na00349ef-fa4f-4efe-beb7-8f448a185760,U01AXE0P5M3,,,Okay so you want to see how to rent a computer in GCP and install everything there?,1642186903.450800,1642242060.459800,U01AXE0P5M3\\n4663b240-1c2f-42d5-95a2-6618abbe2cdf,U01AXE0P5M3,,,Got it. I\\'ll see if I can record a tutorial for that,1642186903.450800,1642242090.460000,U01AXE0P5M3\\n34B3A264-885F-40A3-B2CD-9D2924947F0E,,,,What are the machine requirements for windows and for Mac? I have both I am not sure which to use. ,,1642251185.462200,U02BE1CRHQV\\n5301bbd6-395e-4580-a49d-9508030eaf98,U01AXE0P5M3,,,Thank you :relaxed:,1642186903.450800,1642252292.462300,U02QGA57GRY\\nbe04253d-d57d-4849-ac30-191044f64989,,14.0,,I\\'m having issues with the installation of Docker on ubuntu.. `sudo docker run hello-world` isn\\'t working,1642270989.467100,1642270989.467100,U02T9JQAX9N\\ndc46fbef-8243-4a45-9ee8-8b0edd7a4e60,U02T9JQAX9N,,,\"If you copy-paste the error message to Google, it\\'ll probably help\",1642270989.467100,1642272385.468000,U01AXE0P5M3\\nd5e8810b-c4ec-42f3-993e-088afa40f856,U01AXE0P5M3,,,<@U02TMEUQ7MY> This might help: <https://www.repairwin.com/how-to-reset-bios-password-hp-probook-elitebook-pavilion-laptop/|https://www.repairwin.com/how-to-reset-bios-password-hp-probook-elitebook-pavilion-laptop/>,1642186903.450800,1642273987.469100,U02S848271C\\n7c53a71c-179d-4897-8107-028d903d0909,U02T9JQAX9N,,,\"```sudo service docker status```\\nshould show it as Running\",1642270989.467100,1642274420.469400,U029JNJBPED\\nd4b61886-4a3e-4b74-8bfb-3d140c0880b6,U02T9JQAX9N,,,you have to add your user to the `docker` group in linux: <https://docs.docker.com/engine/install/linux-postinstall/>,1642270989.467100,1642274474.469600,U029JNJBPED\\nBE1F91FF-3C6A-4CBE-A14F-B53A7011BB91,U02T9JQAX9N,,,The instructions in this documentation work perfectly for me,1642270989.467100,1642278939.471300,U02SDHJMASG\\n8484EBCE-775D-42B9-A171-2D7FD3463108,U02T9JQAX9N,,,\"I\\'m also new to Linux. One thing I\\'ve learned, you can\\'t go wrong with a simple to Google search of any error.\",1642270989.467100,1642279046.473300,U02SDHJMASG\\n08e48ca5-ab01-4f75-904b-38adcfefa1e7,U02T9JQAX9N,,,\"You only have to add your user to the group `docker` if you can\\'t use sudo or are uncomfortable with running it with sudo. Normally, containerized apps should not require root, and there are security issues\\n\"\"Warning;\\nThe\\xa0`docker`\\xa0group grants privileges equivalent to the\\xa0`root`\\xa0user. For details on how this impacts security in your system, see\\xa0_<https://docs.docker.com/engine/security/#docker-daemon-attack-surface|Docker Daemon Attack Surface>_.\"\"\",1642270989.467100,1642281176.473700,U02GVGA5F9Q\\n29d62d4b-be41-4648-8eeb-12c7eb7ded72,U01AXE0P5M3,,,\"<@U02TMEUQ7MY>, you can get rid of that BIOS setup password resetting its configuration completely. You may need to reconfigure it, but I\\'d give it a try:\\n\"\"On the computer motherboard, locate the BIOS clear or password jumper or DIP switch and change its position. This jumper is often labeled CLEAR, CLEAR CMOS, JCMOS1, CLR, CLRPWD, PASSWD, PASSWORD, PSWD or PWD. To clear, remove the jumper from the two pins currently covered, and place it over the two remaining jumpers.\"\"\\n<https://www.computerhope.com/issues/ch000235.htm>\",1642186903.450800,1642281910.474100,U02GVGA5F9Q\\nc8724912-ef42-4c54-9623-3d413b1b859a,U01AXE0P5M3,,,\"Ok, I may not have added any value from <@U02S848271C>\\'s answer...\",1642186903.450800,1642282160.474500,U02GVGA5F9Q\\na9df6318-5dfc-4cd3-9ba1-413414120531,U01AXE0P5M3,,,\"Thank you all for your response, really appreciate. I will try the methods one after the other.\",1642186903.450800,1642285627.475200,U02TMEUQ7MY\\nd5f5ca89-fe70-486b-9e48-27cd122fff92,,1.0,,\"I got 404 on the link\\n```Continue here: week_1_basics_n_setup/1_terraform_gcp/terraform```\",1642292964.476100,1642292964.476100,U0297MFTTM1\\n8daae89f-a0ad-4755-9da4-0a3d488dac93,,1.0,,\"Hello everyone, I\\'m Laura from Mexico :)\",1642297122.476900,1642297122.476900,U02UKSVF88H\\n3e2abe92-2576-4ab6-a2f5-72cece56ede6,U02UKSVF88H,,,:flag-mx:,1642297122.476900,1642297895.477000,U0297MFTTM1\\n0aee3739-0db6-4aa7-8cb5-a6f34739d386,,,,\"Hi everyone, I\\'m Jakachai from Thailand :D\",,1642307248.478200,U02U7CUPRRR\\n384c73a2-4b96-4cb7-a1b7-0a7f26ddbf44,,,,\"Hi Everyone, I\\'m Osmani from the Dominican Republic!! Nice to meet you all!\",,1642313208.480000,U02U4G7U3GV\\n35615BDC-2CFB-4938-B88D-A9E3FF499773,,,,\"Hello everyone :) I’m Lyes from Montreal, Canada :flag-ca: Very excited to be here! \",,1642314626.482400,U02UX664K5E\\n844b2efc-2307-469b-b03c-e33d4a6800aa,U02T9JQAX9N,,,\"Hi :wave:  what are the course prerequisites that  we need to install or sign up for before the course..\\nAs complete beginners\",1642270989.467100,1642317631.482600,U02ECUGHWG4\\n39f5d855-aa89-444b-8190-3c3bbf7fe3c4,,3.0,,Hi what are the course prerequisites for DE for complete beginners,1642317893.483700,1642317893.483700,U02ECUGHWG4\\n0667d7c6-8517-4635-b1c5-52573db4635d,U02ECUGHWG4,,,Both modules and accounts needed to sign up in,1642317893.483700,1642317938.483800,U02ECUGHWG4\\n1e0c3a3d-e14f-4e43-a4ad-197932562724,U0297MFTTM1,,,\"thanks, we\\'ll fix that\",1642292964.476100,1642319292.484000,U01AXE0P5M3\\n60c268e0-c86f-4f39-847f-e1c72cb28d1d,U02ECUGHWG4,,,\"GCP is recommended, you won\\'t be able to do the big query module without it\\n\\nother things shouldn\\'t require a registration\",1642317893.483700,1642319368.484600,U01AXE0P5M3\\nefd1a7cb-2a09-4765-82b5-ea2f64100a3f,,3.0,,\"Hello everyone, I was unable to follow the course since week 3 and I\\'m wondering if anyone knows what is the approximate timeline until the final Project needs to be delivered? I know there were some small deviations from the original timeline and I would like to calculate and see how much time I need to dedicate to catch up and also deliver the final project. Thanks!\",1645870386.727719,1645870386.727719,U02QPBZ3P8D\\n3c6c896b-3b3f-466a-a2b4-efa1d7f45358,U02QPBZ3P8D,,,The homework for spark is due next Wed. Then we\\'ll have a week for Kafka. And after that we\\'ll have a project,1645870386.727719,1645870587.862039,U01AXE0P5M3\\nd5575283-56ec-495e-9b12-caac1a2277e6,U02QPBZ3P8D,,,\"<@U01AXE0P5M3> so Spark homework due on the 2nd of March, then Kafka and 3 more weeks to work on the project if I remember correctly? So the final project is expected to be delivered by the 4th of April (the earliest - if there are no other calendar changes)?\",1645870386.727719,1645875157.450289,U02QPBZ3P8D\\n0130898e-8bb9-419d-912b-351afc786522,U02QPBZ3P8D,,,Yes it sounds correct,1645870386.727719,1645875850.738139,U01AXE0P5M3\\nd0a754c1-6e90-41a8-b20c-b14472323ebc,,1.0,,\"<@U01B6TH1LRL> <@U01AXE0P5M3> - For the Week4, I did provide the code github links (my DBT project and SQL code project) details while submitting the homework, however the valid_code_link marked was \"\"FALSE\"\" in the leaderboard dashboard. Can you check on this when you get a chance? Thanks.\",1645886703.300479,1645886703.300479,U02S4A9TA5Q\\n1be2977d-6e29-4339-bbc8-43e24a8c3f63,,2.0,,\"Hello everyone, regarding homework form-  for Question 3: Records on Feb 2022 *. is it typo for 2021? Please confirm.\\n\\nFrom homework on github- its\\nQuestion 3. Count records\\nHow many taxi trips were there on February 15?\\nConsider only trips that started on February 15.\",1645896772.281429,1645896772.281429,U02NSF7RYP4\\n34ce6c6e-99d6-428d-89d5-ef5cb7062d28,U031HNNSW3A,,,\"Thanks everyone! Sorry for the slow reply, as I am not able to log in during the week usually. I will try this tomorrow when I continue with the course\",1645210766.400019,1645897499.169919,U031HNNSW3A\\n3f3924e8-4519-43b7-97e7-a3c2a17ae23e,U02R09ZR6FQ,,,\"Thank you, I will try it!\",1645803682.736359,1645898712.618259,U02R09ZR6FQ\\n1dceec32-076a-4e62-8cb8-84942512a5f8,U02S4A9TA5Q,,,Please dm me your email,1645886703.300479,1645898738.881539,U01AXE0P5M3\\n18fd7a43-5aff-4092-b563-21ced00f4532,U02NSF7RYP4,,,Yes it\\'s a typo. Thanks!,1645896772.281429,1645898768.840759,U01AXE0P5M3\\nbcc49318-d93d-4091-91ac-5a32539e6ffe,U02TC704A3F,,,\"I completely unfamiliar with hdfs, you mention it can be an issue, when exactly does the problem may arise. I see in the notebook you linked they upload data from csv as well so then it has more to do with the way it is saved?\",1645659834.521759,1645905287.159459,U02TC8X43BN\\nd6c868aa-64ae-48f7-8d7c-1d525e3ccd31,,2.0,,\"<#C01FABYF2RG|course-data-engineering>\\nHi I\\'m getting below error while  ingesting data to postgres DB . any help .\",1645908184.146689,1645908184.146689,U033WHDKP0T\\ndb508bf6-45e5-4689-a1a9-0468d8ed2c4e,,1.0,,\"Hi everyone, I am in the first week where we try to ingest data into the database and connect it to pgAdmin. Anytime I stopped the container, my database is deleted and pgcli always returns that the database does not exist, even though I used -v when starting the container. What can I do\",1645908362.047859,1645908362.047859,U034VEFGMSM\\n5df662be-24e2-4911-814a-f6e69df53b46,U033WHDKP0T,,,This is not an error. All good,1645908184.146689,1645908851.497709,U01AXE0P5M3\\n06d94531-3cce-4926-ab7c-b0702cc79518,U034VEFGMSM,,,Check FAQ,1645908362.047859,1645908885.545669,U01AXE0P5M3\\ndd355ea3-7728-41b6-b43b-afecf2abf078,U033WHDKP0T,,,Thanks Alexey,1645908184.146689,1645910167.990009,U033WHDKP0T\\n1e1aebf1-66b2-4415-b655-1f28c164e684,,3.0,,\"Hi All,\\nA tip for MAC OS users:\\nI was trying to run `spark-shell` after following the MAC OS instructions on <https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_5_batch_processing/setup/macos.md|Week 5 Github page> but was getting \"\"unable to locate a java runtime\"\" error. So realized I had not setup JAVA_HOME path. It should be done right after installing JAVA or before doing `brew install scala@2.11` .\\nUse following command:\\n`JAVA_HOME=/usr/local/Cellar/openjdk@11/11.0.12`   ** make sure JAVA was installed to this location by going to Open Finder &gt; Press Cmd+Shift+G &gt; paste \"\"/usr/local/Cellar/openjdk@11/11.0.12\"\" **\\n`PATH=\"\"$JAVA_HOME/bin/:$PATH\"\"`\",1645917095.457459,1645917095.457459,U02UM74ESE5\\n72c8ea27-da4d-4051-9e7e-a6b60f546872,U02NSF7RYP4,,,Thanks.,1645896772.281429,1645935434.863459,U02NSF7RYP4\\n77d584fa-fcd4-4f17-b6e9-de286dd5c4ea,,6.0,,Spark and Dask question,1645937133.963009,1645937133.963009,U02U5SW982W\\nbc3e1db2-0ff7-403b-89dc-04f6008ce28d,U02U5SW982W,,,\"Hi All,\",1645937133.963009,1645937163.581819,U02U5SW982W\\n7f8f4239-c0e8-4f8d-b68c-c4d91c421e5c,U02U5SW982W,,,\"The only thing I\\'ve had experience with really is Dask in terms of doing the types of things that we are doing with Spark. I was trying to figure out exactly how these differed and trying to figure out exactly why we are using Spark. Just wondering if this is a good summary: Conclusion\\n• Spark is mature and all-inclusive. If you want a single project that does everything and you’re already on Big Data hardware, then Spark is a safe bet, especially if your use cases are typical ETL + SQL and you’re already using Scala.\\n• Dask is lighter weight and is easier to integrate into existing code and hardware. If your problems vary beyond typical ETL + SQL and you want to add flexible parallelism to existing solutions, then Dask may be a good fit, especially if you are already using Python and associated libraries like NumPy and Pandas.\\nIf you are looking to manage a terabyte or less of tabular CSV or JSON data, then you should forget both Spark and Dask and use <https://www.postgresql.org/|Postgres> or <https://www.mongodb.org/|MongoDB>.\",1645937133.963009,1645937262.477949,U02U5SW982W\\ndbb4ed20-53ab-45ae-8eba-ce1fc0a8c980,U02U5SW982W,,,Sorry and that is from here <https://docs.dask.org/en/stable/spark.html> (apologies in advance - I\\'ve gone trigger happy on the Enter key :/),1645937133.963009,1645937314.036339,U02U5SW982W\\n4fba33e9-0c04-4f57-aabd-f85cfedfbe88,,,,\"Hello all,\\nSo in my experience (might be biased) generally backed developers get interested in data working on data products.\\nSo it might be interesting to introduce data engineering from the perspective of data products\",,1606123555.006300,U01DFQ82AK1\\n85254cb0-d865-4273-8f8c-3cd2afd1d1c4,U01DHB2HS3X,,,great idea! I like it!!,1606084093.039600,1606141301.008200,U01F78474M9\\nf52ad9b4-c05d-43e4-949a-37862768806c,,3.0,,\"<https://datatalks-club.slack.com/archives/C01FABYF2RG/p1606085596002500>\\nwe can think about getting some know public datasets such as <https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page>   and work on it\",1606141358.009000,1606141358.009000,U01F78474M9\\n3880a4fa-729b-4e4c-b8da-a2567c5a15cd,U01F78474M9,,,\"Thanks, Nick! Yeah, sounds good to me.\",1606141358.009000,1606143741.009300,U01DHB2HS3X\\ndc53a74d-f5ca-4b89-b369-f02ce1050e9e,U01F78474M9,,,\"Thanks, that looks really geat!\",1606141358.009000,1606144589.009500,U01AXE0P5M3\\n7a2e1a01-813a-4892-906d-e2062022d112,,,,\"I was also thinking about common crawl, but it might be too difficult and too specific. The taxi trips dataset is much nicer example\",,1606144633.010400,U01AXE0P5M3\\n60418bd5-5028-4180-8237-39f59c713deb,U01DHB2HS3X,,,:grinning:,1606084093.039600,1606148097.010600,U01DHB2HS3X\\n1901602d-9a87-4b72-ad1b-be3b19ea1fe4,,1.0,,\"I\\'m looking at this taxi data and it looks awesome! What kind of technologies do you think we can cover with it? The most obvious one is Spark, but I\\'m thinking if it can cover some other thing s\",1606150991.012000,1606150991.012000,U01AXE0P5M3\\n0d26627f-3450-4cdc-83b3-a249e9b2c134,,,,what about streaming? something like Twitter might be an interesting option,,1606151011.012500,U01AXE0P5M3\\n97f873b4-df69-471e-b77b-6ea1954f1a59,,4.0,,or we can pump the data from CSVs into kafka to emulate a real-time use case :slightly_smiling_face:,1606151234.013000,1606151234.013000,U01AXE0P5M3\\n40761d47-da51-4433-a17a-4086f14d5814,U01AXE0P5M3,,,\"definitely an option, but how will we make a realtime use case out of that. Specially as the data is historical?\",1606151234.013000,1606152242.013200,U01DFQ82AK1\\ncb97c89d-97f1-4e51-bbba-fd6f4ff3492e,U01AXE0P5M3,,,Maybe... we pretend it\\'s not historical? =),1606151234.013000,1606153068.013400,U01AXE0P5M3\\n54c86ff1-c3c2-40a4-8c4f-d81dbb14270c,,,,\"I found some data here: Question on @Quora: Where can I find public or free real-time or streaming data sources? <https://www.quora.com/Where-can-I-find-public-or-free-real-time-or-streaming-data-sources|https://www.quora.com/Where-can-I-find-public-or-free-real-time-or-streaming-data-sources>\\n\\nThey mention financial and cryptocurrency data and wikipedia edits\",,1606153322.015300,U01AXE0P5M3\\n0673255a-f3db-4303-8aa7-49fe150fb4a8,,2.0,,\"Twitter is great, but I recently tried it and API is closed as far as I know, you need to request access. the easies solution is  to find an alternative or to build a simple script which would generate random data(like a stream of users) and you need to subscribe via socket(it might be even better if we want to customise something or keep our course independent from any streaming services)\",1606155181.016900,1606155181.016900,U01F78474M9\\n5dab35f7-dcac-4dde-960c-8afdd6dece1c,U01AXE0P5M3,,,\"I  worked on it a while ago and it’s really great for DE task, cleaning and exploration. we can add visualisation component to it. What I noticed, a lot of DE does not really have experience in visualising data - but it could be really handy and there are some tools which are far better than just terminal :smile: such as Jupyter notebooks, Zepelin notebooks etc  :slightly_smiling_face:\",1606150991.012000,1606155286.017000,U01F78474M9\\n02d48a2c-eb23-4ad8-abbd-b35a6189a7e7,U01AXE0P5M3,,,\"kafka might be a bit overhead in this case, although we can use  some docker image, however a script which will  push constantly data to kafka could emulate anything :slightly_smiling_face:\",1606151234.013000,1606155749.017300,U01F78474M9\\n8ed113d0-8718-4210-8f32-a15f5c0986b3,,,thread_broadcast,\"Yeah, I think simulating a stream ourselves could be a safer option than integrating with 3rd-party services. I\\'m not sure if a lot of our audience would be comfortable otherwise.\",1606155181.016900,1606158896.017600,U01DHB2HS3X\\n8343c5f1-68db-464c-9755-59b350877c6e,U01F78474M9,,,\"I once tried to conduct a workshop using Stripe API as the source, and not many were ok to register, due to GDPR concerns. But this is probably just my personal experience.\",1606155181.016900,1606158903.017900,U01DHB2HS3X\\nf64812b2-4ae1-4700-95ad-dd986607875a,,5.0,,\"At OLX we have something similar to that:\\n\\n- we have a central tracking module that collects data from our websites: new listings, edits, views, search queries, etc. Basically all kinds of events\\n- all the data from there goes to our datalake. Some of the data is personalized, so only certain teams can  access it, the data is in S3 and accessible with Athena/Presto\\n- from the data lake, the BI team computes some aggregate tables, so it\\'s easier for analytics to use it, pushes it back to the lake and to redshift. For scheduling we use airflow\\n- from this lake, data scientists use data to compute some features with Athena, presto, spark and aws batch\\n- we usually train models with batch and kubernetes jobs\\n- often, the predictions are pushed back to the lake for other teams to use it\\n\\nHow do pipelines look for you?\\n\\n\\nSo I guess we can come up with a case like that, design it together during the lesson and implement it\\nWhat do you think? There could be several cases\",1606161683.032300,1606161683.032300,U01AXE0P5M3\\nbe2b7f2c-d6f6-4914-a661-7bd90da43ada,U01AXE0P5M3,,,\"I like the idea of working on a single case throughout the whole course. smth like, let’s start from batch data collection, processing cleaning, you need to setup airflow etc, then streaming and so on and so force.\\n\\nfor the beginning we can just take a dataset or an idea of a case and just add different components based on our plan for the course\",1606161683.032300,1606165438.032500,U01F78474M9\\na578988b-5263-437c-aa44-436ac32374f3,U01AXE0P5M3,,,\"that can definitely be a use case, will look into data regarding this\",1606151234.013000,1606201994.000100,U01DFQ82AK1\\n15767f8b-4414-4db5-aaad-51d854c9f93e,U02UX5MHB40,,,We do a bit of that in week 4 with dbt. We will also have a workshop with <https://www.soda.io/|Soda> soon where we\\'ll cover these topics in more details,1646436323.584369,1646469524.181099,U01AXE0P5M3\\nca0660de-d3df-4dca-b49c-39fc322e04d8,U034Q6ZKR9C,,,Can you try on VM. It\\'s anyways better to do it on VM because the file downloads will be extremely slow on your local. Like extremely.,1646411072.603079,1646471624.677859,U02TATJKLHG\\n55d6aa1f-70bc-4ccd-b4d9-5923d6f2a2ba,U034Q6ZKR9C,,,\"<@U02TATJKLHG>, thanks, tried running it on VM, everything works fine, but will I still be able to ingest data to postgres?\",1646411072.603079,1646471811.567099,U034Q6ZKR9C\\n44772584-0138-4601-a1a6-1bee2af0b7fb,U034Q6ZKR9C,,,\"I mean, to local postgres db\",1646411072.603079,1646471828.784309,U034Q6ZKR9C\\n26df9550-d27d-496c-bab5-c9ea3698689a,U034Q6ZKR9C,,,You can use BigQuery if that\\'s an option,1646411072.603079,1646472384.922119,U02TATJKLHG\\na08c9d3b-92c2-4c01-b8d8-b5473b765bac,U02UX5MHB40,,,nice,1646436323.584369,1646472612.928649,U02V90BSU1Y\\n8D65173E-932E-49E8-B5D3-FFCCF470FE30,U02URV3EPA7,,,\"Sorry for the typo. Will fix it \\nThanks for pointing out \",1646414115.669799,1646478649.299909,U01DFQ82AK1\\n1a950247-1561-4f81-9dc2-84604c099cc2,,3.0,,\"```docker run -it \\\\  \\n-e POSTGRES_USER=\"\"root\"\" \\\\    \\n-e POSTGRES_PASSWORD=\"\"root\"\" \\\\     \\n-e POSTGRES_DB=\"\"ny_taxi\"\" \\\\     \\n-v d:/Github/Data-Engineering-Zoomcamp/01_basics_n_setup/2_docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\  \\n-p 5432:5432 \\\\ \\npostgres:13```\\ndocker: invalid reference format.\\n\\nI keep getting this issue..\\nI tried  a bunch of things from the FAQs\\nCan anyone help?\",1646479522.551359,1646479522.551359,U0233FB1SB1\\n470111be-8e3f-4219-a200-54efe9cf5e5c,U0233FB1SB1,,,\"Change this to one line or add  \\\\ at the end of line as in the rest of lines?\\n\\n -v d:/Github/Data-Engineering-Zoomcamp/01_basics_n_setup/2_docker_sql/ny_taxi_postgres_data:/var/lib/\\npostgresql/data\",1646479522.551359,1646479847.562769,U0308MF3KUH\\n94cedb6c-7ae4-48f4-b500-e28f5eb28f6e,U0233FB1SB1,,,Thanks alot GerryK :slightly_smiling_face:  bad mistake on my part :p,1646479522.551359,1646481308.755879,U0233FB1SB1\\n4577f7a9-7d11-4775-9c90-3875beb5b1c9,,19.0,,\"When I enter the password, Nothing happens, I again get a bash prompt :disappointed:\\n\\n$ pgcli -h localhost -p 5432 -u root -d ny_taxi\\nPassword for root:\\n\\nDid it anyone face the same issue?\",1646486970.082019,1646486970.082019,U0233FB1SB1\\n3672bac0-3bbb-4fa4-b98f-65e7428bbbcd,U0233FB1SB1,,,\"Try using windows terminal or terminal in VS code. If it resolves your problem, can you please add it to FAQ?\",1646486970.082019,1646487289.071589,U01AXE0P5M3\\n71bc4b5b-a976-4681-948e-9f1d7531c96a,U0233FB1SB1,,,For the previous one as well please,1646486970.082019,1646487300.389669,U01AXE0P5M3\\nd7a2136b-61ab-494e-a251-6a7e2f679d26,U0233FB1SB1,,,Sure Alexey!,1646486970.082019,1646488844.462099,U0233FB1SB1\\nc5ce1ec4-c972-4ca0-9039-8fa354d7f2b4,U0233FB1SB1,,,\"Solved that issue but now I get\\n\\nFATAL:  password authentication failed for user \"\"root\"\"  :p\",1646486970.082019,1646489271.245159,U0233FB1SB1\\nCC4966FA-D392-438D-8DAA-8EE71626FD18,,1.0,,\"I came across this playlist from Seattle Data Guy about planning out a good data engineering project. The playlist isn\\'t finished, but should give us some good things to think about \\n\\n<https://youtube.com/playlist?list=PLXRKPZRrlvE4ttHhis2CLRnOR58ojfFSS|https://youtube.com/playlist?list=PLXRKPZRrlvE4ttHhis2CLRnOR58ojfFSS>\",1646518466.238809,1646518466.238809,U02T9GHG20J\\n4c86f296-c76b-442c-928c-b0dbae19ade0,U02T9GHG20J,,,Love your work <@U02T9GHG20J>!,1646518466.238809,1646525252.538319,U02U5SW982W\\n7ac8d139-6d51-494e-985d-f6b317873f31,U0233FB1SB1,,,\"I\\'m getting the same error: \"\"FATAL: password authentication failed for user \"\"root\"\" in VS terminal. Were you able to resolve it? <@U0233FB1SB1>\",1646486970.082019,1646530453.356149,U032TP2AS2Z\\n424c9610-7801-4f45-8ba3-f1fcea6e1806,,4.0,,Week 6 DE Zoomcamp 6.1.2 - consumer.py question...,1646538059.326969,1646538059.326969,U02U5SW982W\\nf9b31448-1072-4f2a-b749-abb54278096b,U02U5SW982W,,,Just wondering when the condition for the while loop gets set to False? Am I missing something? It seems it\\'s just always True which is perhaps why it never successfully exits the program for me - and just hangs there? :woman-shrugging:,1646538059.326969,1646538139.462469,U02U5SW982W\\ned28f07e-6152-4fc4-a99d-cc305851379d,U02U5SW982W,,,And maybe is that what it is supposed to be doing? My understanding of what it is supposed to be doing might be off...,1646538059.326969,1646538982.691389,U02U5SW982W\\n66fb8ec3-2483-4dde-ab9d-5a45346964ca,U02U5SW982W,,,\"I guess you are right. Ideally the consumer should keep on looking for any news messages to the broker. Hence, the `while true`.\",1646538059.326969,1646546494.157319,U02TATJKLHG\\n11ec3e8b-25f9-4aa6-b842-d59a303582b1,U01AXE0P5M3,,,\"Sure, I like this idea too.\",1606161683.032300,1606219837.000100,U01DHB2HS3X\\nbea56d60-6acc-498c-b1ee-fd9442f7aa88,,5.0,,\"As a suggestion- Shall we start with preparing a glossary of terminologies that we would like to cover? For example, also defining what are pipelines, workflows etc., besides just the main concepts like, Streaming and Batch Processing.\\nThe reason I mention this is because I think the usage of the right vocabulary for a use-case is also important to avoid misunderstanding in a working environment.\\n\\nBased on that, we could think of how can we connect these terms to form use-cases/projects, and which could possibly be a complete product.\",1606230464.007100,1606230464.007100,U01DHB2HS3X\\n8d8717cd-dabe-45b1-aabb-76cafcac9c1e,U01DHB2HS3X,,,That\\'s a great idea! I tend to use jargon quite often without realizing that not everyone understands it. So your suggestion is spot on,1606230464.007100,1606231729.007400,U01AXE0P5M3\\n251b2264-c657-4b9a-a256-3e636f7fcb17,U01DHB2HS3X,,,What do you think is the best way of doing it?,1606230464.007100,1606231781.007700,U01AXE0P5M3\\n455f3d93-5ce2-4176-804f-49444a705038,U01AXE0P5M3,,,Do you have an idea about a use case? Should we come up with something taxi related? I\\'m not an expert in this kind of data. Maybe <@U01DFQ82AK1> you have some suggestions?,1606161683.032300,1606231922.007900,U01AXE0P5M3\\n1a5d1252-bdaa-4f02-9b3a-ee95d23ebb5e,,,thread_broadcast,\"Yeah, happens with me too! And I just realized I did something like this at work today, so I posted this :sweat_smile:\\n\\nI think we can create a repo to begin with, and just keep adding in a markdown file, all the terminologies we use on a daily basis. \\nAnd also, some problems we regularly encounter at work. I think this could also be helpful for the audience.\",1606230464.007100,1606235153.008100,U01DHB2HS3X\\n434eeb7b-fb07-4079-8efe-ecafc95256f8,U01F78474M9,,,found the same data in google <https://console.cloud.google.com/marketplace/product/city-of-new-york/nyc-tlc-trips?filter=solution-type:dataset&amp;q=taxi&amp;project=evocative-tube-275410&amp;authuser=1&amp;folder=&amp;organizationId=&amp;supportedpurview=project|big query>,1606141358.009000,1606235929.008400,U01DFQ82AK1\\n8babafe8-64ed-4a5a-8942-1ffa5d487e73,U01AXE0P5M3,,,\"So this data is for completed rides. I believe we can answer few questions in this regards (I guess they are some of the very basic ones, not sure if they are useful):\\nFor batch:\\n• Most famous pickup/dropoff area per hours\\n• Fare price change over hours (rush vs non-rush hours)\\n• Max pickup request per hour\\n• Trip distance average per hour per vendor per area\\n• Seasonality effect\\nFor realtime:\\n• Most requested pickup/dropoff areas\\n• Busy vs free taxis per vendor (for vendor monitoring)\\nIf I can think of some more I will add here\",1606161683.032300,1606236572.008700,U01DFQ82AK1\\nfa17b65e-f305-4a92-b285-8dbf24fb702b,U01DHB2HS3X,,,\"Yes, that\\'s great! Both glossary, and writing down the problems we have at work.\\n\\nPerhaps we can start with the google doc? It\\'s a bit easier in my opinion, and then we can move it to markdown. What do you think?\",1606230464.007100,1606238202.009000,U01AXE0P5M3\\n0a44d96d-1550-470c-b4db-c957cda726ae,,5.0,,\"Per <@U01DHB2HS3X>’s suggestion I added a few sections to the google doc:\\n• use cases from work\\n• glossary \\nWe can move it to some other place (e.g. a git repo)  if you\\'d like\",1606238422.010200,1606238422.010200,U01AXE0P5M3\\ncaba4d43-a4c3-4b25-a68c-98b32ad7c60c,U01AXE0P5M3,,,\"Thanks <@U01AXE0P5M3>! :) yeah, a google doc also works for now. We can move it to a repo later.\",1606238422.010200,1606238501.010300,U01DHB2HS3X\\nb68aceec-81e2-4f97-b7cc-2a66cab98de6,U01DHB2HS3X,,,\"Yup, a google doc sounds fine.\",1606230464.007100,1606238852.011800,U01DHB2HS3X\\n84ea26f0-c82e-4c1c-8a7b-7ae8c7024bad,,2.0,,\"By the way, this topic came up today in our discussion with Sejal, I wanted to write about it in the channel as well.\\nI don\\'t have a timeline in mind, but if we can prepare this course by March 2021, that would be great. I\\'d like to organize a conference in February (or early March) and announce it there. What do you think about it? Is it realistic?\\n\\nSo in March we can start the live lessons (no need to pre-record anything)\",1606238914.012900,1606238914.012900,U01AXE0P5M3\\n9f854a6e-40a8-4467-a053-e48c0af1f839,U01AXE0P5M3,,,\"Sure, that\\'s plenty of time!\",1606238914.012900,1606238980.013200,U01DHB2HS3X\\ne52af1da-9435-4aec-9b03-e9cc496075f7,U01AXE0P5M3,,,Thanks <@U01DFQ82AK1>! I\\'ve added your idea to the doc,1606161683.032300,1606241563.013600,U01AXE0P5M3\\n,,,channel_join,<@U01FGCNT6CS> has joined the channel,,1606259458.000200,U01FGCNT6CS\\nd111dcd8-3935-4673-9762-38d14627ac6c,U01HNFLKQ3X,,,Does this mean that it wouldn\\'t be recommended to start your career (for a recent CS graduate) as a DE but rather start as a SWE or DS then after some time consider moving to data engineering?,1609810261.023800,1641547904.007600,U02RZUKDBU6\\ne636674f-9a8c-422a-897b-427706090f93,,,,\"Hello everyone! My name is Rinat, I live in Russia. very happy to join the Data Engineer course, thank you for this opportunity! :slightly_smiling_face:\",,1641553723.008800,U02T2T1CZBN\\n4aaddec0-e155-4354-906b-20230d8df645,,10.0,,Cant wait to start learning....when exactly is it beginning,1641565510.010100,1641565510.010100,U02RTJPV6TZ\\n67b3f710-dafc-4e09-86cd-9410de6838bc,U02RTJPV6TZ,,,One more week of waiting and then we start!,1641565510.010100,1641566244.010400,U01AXE0P5M3\\nb3123f20-d935-4dd3-9830-67a6bb6c6e77,,24.0,,\"Here\\'s the list of things you\\'ll need to have for the course:\\n\\n• GCP account\\n• Docker\\n• Terraform\\n• Python 3\\nYou probably will also need an IDE. For that you can use VS code or PyCharm or any other one\",1641566368.013200,1641566368.013200,U01AXE0P5M3\\nd40ed9aa-3c46-4b7b-9a68-127115e1f513,U01AXE0P5M3,,,\"We\\'ll try to make sure everything will run locally, but it\\'s still recommended to have an account in GCP\",1641566368.013200,1641566414.013300,U01AXE0P5M3\\nda9f8d37-0bbe-4252-b2a9-4c8da2e3969f,U01AXE0P5M3,,,\"Also, the first week will be devoted to setting up the environment. You don\\'t have to do it right now\",1641566368.013200,1641567455.013700,U01AXE0P5M3\\n3e2aaf95-1151-485c-88c1-9bae49f7817a,U01AXE0P5M3,,,It is possible to do a free 1 year GCP account?,1641566368.013200,1641574456.014100,U02CD7E30T0\\n58254000-62a1-487f-af0b-876fe179ae51,U01AXE0P5M3,,,You get $300 on your account for free when you register. That\\'s more than enough for the course,1641566368.013200,1641576351.014400,U01AXE0P5M3\\n351eeba6-b6eb-4a2a-9f3f-c4b0a5feba8a,U01AXE0P5M3,,,\"Note that one can\\'t have enjoyed that free tier bonus before. If this course were to be on Azure, I wouldn\\'t be able to subscribe a new account for free, as I already played with it before.\",1641566368.013200,1641589030.015100,U02GVGA5F9Q\\n27e360fe-adf8-45ea-b4d4-4b345a9e755e,U02RHT0M3M5,,,\"<@U02RHT0M3M5> do we have the mini workshop tomorrow ( Jan 9 th).. can you please send the invite, if there is one\",1641159804.189100,1641609305.015800,U02RPEGKUVA\\n2fb8e540-f7b6-438f-88bb-ecf551b71746,U01AXE0P5M3,,,do we need the email confirmation to participate? assuming we already registered of course.,1641566368.013200,1641620142.016300,U02S8K9JBD0\\n23327d64-59d0-4bed-ab2b-42b042772c19,,2.0,,\"Hello All,\\nIs it normal that I didn\\'t get the confirmation email for the course yet?\",1641283992.208400,1641283992.208400,U02R7EBBYKX\\n755b0f4a-498b-4b0f-9f82-67f4babca072,U02R7EBBYKX,,,Yep. You\\'ll get it soon =),1641283992.208400,1641284046.208500,U01AXE0P5M3\\n82675024-0293-4948-aa1d-f8490e370cff,U02R7EBBYKX,,,<@U01AXE0P5M3> I think you should pin an announcement about confirmation emails :sweat_smile:,1641283992.208400,1641286714.209900,U02Q7JMT9P1\\nae94fc4e-e1d0-4919-b3bd-c4014eddf848,U02SFFC9FPC,,,Hi fellow scientist. Just curious - what is your field of study? :slightly_smiling_face:,1641275845.206400,1641286900.210100,U02Q7JMT9P1\\n703fc5e8-c855-4607-96e5-280b048432c8,U02RHT0M3M5,,,Interested and weekend would be cool,1641159804.189100,1641287081.210500,U02R78WNLER\\n4cbd00b7-4544-472d-b210-4427f89fdea1,U02RHT0M3M5,,,\"Great Guys!:v: Lets do it in next weekend on 9  Jan, and after finishing our first WS we can talk about the next one which. May be next weekend on 15 Jan.\\n\\nI will send you Google hangout link for 9 Jan at 5 PM Grmany time , please confirm guys if that is works well with you by click love on the comment.\",1641159804.189100,1641287429.210700,U02RHT0M3M5\\na80b2dd5-fbf4-4b5c-a2a3-15ff77079576,U02RHT0M3M5,,,what is the timezone for ur 5PM?,1641159804.189100,1641287905.210900,U02R65WFT16\\n21f2a885-0c52-4cf9-8c03-b8957ff2652e,U02RHT0M3M5,,,Sorry Alex I meant Germany 5PM,1641159804.189100,1641288211.211100,U02RHT0M3M5\\n7722b8b5-0781-4499-a781-995e2df4e07c,U02RHT0M3M5,,,\"I am in SGT time zone, Germany 3pm before works better for me\",1641159804.189100,1641295710.212200,U02R65WFT16\\nd3a60758-dda5-43ce-bc2e-84a23ac12836,U02RHT0M3M5,,,\"<@U02RHT0M3M5> I am interested in the Jan 9th meeting, can you include me in the hangout meeting, I am in EST thanks\",1641159804.189100,1641296918.212800,U02SFNCRUAY\\n0380f7e5-c855-454e-a9b3-cd391a1de898,,1.0,,Let me know if you need any help <@U01AXE0P5M3>! Trying to do something similar internal where I work.,1641303654.214200,1641303654.214200,U01LU36K58R\\nbe3d34a6-32f6-4e64-bf34-823c432a6b54,U01LU36K58R,,,\"Great, thanks! I\\'m sure many people here will have questions, so we\\'ll be happy if you can help us with answering them\",1641303654.214200,1641316354.215400,U01AXE0P5M3\\n71e94919-c86b-4eea-aec0-588030d0c50f,U02RHT0M3M5,,,\"<@U02RHT0M3M5> Do you know what setup we need to prepare? I saw we need to setup GCP and download Terraform for Week 1, is there any other local setup?\",1641159804.189100,1641318116.215700,U02SLJ38N2F\\na82ea8bc-97a6-413c-813f-b858d129e074,U02RHT0M3M5,,,\"We\\'ll cover most of the setup in Week 1, which is a preparation/introduction week. Of course you can already do a few things:\\n\\n• register for GCP\\n• install docker and terraform\\n\",1641159804.189100,1641331035.217000,U01AXE0P5M3\\n52f5a6f2-5b35-423c-b007-c4615b1810ca,U02RHT0M3M5,,,\"We\\'ll also need Airflow, DBT and Spark, so let me get back to you with a list of all the things we\\'ll need to install\",1641159804.189100,1641331114.217200,U01AXE0P5M3\\n63bde4b1-6fc9-4294-a91f-4e1ea55c388a,U02CD7E30T0,,,Thank you Alexey! :slightly_smiling_face:,1639666651.046000,1639735144.052000,U02CD7E30T0\\n64b8231a-5ceb-4339-9e97-6c8f00770576,,,,Hello everyone I am glad to be here...and am a beginner in the data field  I think this course will be very useful for my career.,,1639743047.055100,U02R59SH3FD\\n1998f188-b723-4505-a2bd-3de03c6ceea1,,,,Hello everyone! I’m excited to be here!,,1639746242.056700,U02QR4FBG9M\\n8131ab99-0885-4d3c-88ca-247070953280,,,,\"Hello everyone! Looking forward to taking part in the course :slightly_smiling_face:\\nWish you a great day ahead!\",,1639746280.057600,U02R5P5N6M8\\nc6871768-392b-4ed8-8c16-5a415251aa51,,,,\"heyyy, hello everyone\",,1639746369.058100,U01T8E1HVK9\\n613627f4-5b41-4012-b625-0313698b60e5,,5.0,,are there any links for zoomcamp,1639746392.058500,1639746392.058500,U01T8E1HVK9\\n670a9f98-4890-4d5f-ba4b-28c2bb3a18e7,U01T8E1HVK9,,,This is all I have: <https://github.com/DataTalksClub/data-engineering-zoomcamp>,1639746392.058500,1639746490.058800,U02R5P5N6M8\\nb4379bf6-a26c-47da-8016-8247e0dd7678,U01T8E1HVK9,,,oh I\\'m just worried that I might lost the sessions because of timezone,1639746392.058500,1639746785.059200,U01T8E1HVK9\\n21f117c0-2f1f-44e4-b27b-de4ea3b2604c,U01T8E1HVK9,,,\"They\\'ll be recorded though, no?\",1639746392.058500,1639747399.059600,U02R5P5N6M8\\ne6131c2b-cc38-407d-8011-cb29c6f6e059,U01T8E1HVK9,,,they are and after a second sight I saw that the meetings will be usually held on 17 CET,1639746392.058500,1639747893.060000,U01T8E1HVK9\\n56cd746e-c12e-4d1d-be50-c90f89eca38b,U01T8E1HVK9,,,All sessions would be recorded,1639746392.058500,1639749817.060800,U01DFQ82AK1\\n0d28249f-6d00-4cac-83f1-6f3d06df593a,,,,\"Hello Everyone. I’m Mario from Colombia, looking forward to learn with you! :snake::nerd_face:\",,1639770494.066700,U02RA1FFLDA\\n445f33fe-b15d-41e8-9d94-0d0bb2024847,,1.0,,\"Hi All\\nWhen does this course starts? Jan 17th?\",1639810365.069300,1639810365.069300,U02R8V9MF6E\\n835c8f48-ead5-415a-a3d6-e24ef9e4241c,,1.0,,<https://www.mihaileric.com/posts/we-need-data-engineers-not-data-scientists/>,1610644380.000400,1610644380.000400,U01HNFLKQ3X\\n5d560adc-3591-4ccc-bdac-0f57186a449a,U01HNFLKQ3X,,,\"Thanks for sharing! I\\'ve seen this link everywhere today, and now, when you shared it as well, it feels I really have to read it :smiley:\",1610644380.000400,1610659066.001000,U01AXE0P5M3\\n821fddff-e29e-4f4d-bd15-afde2f081ed8,,,,\"Hello everyone! Sorry for the radio silence from my side, I got into too many projects :sweat_smile:\\n\\nI had a chat with <@U01DFQ82AK1> the other day about the tech we should use for the course, and we thought it would be a good idea to:\\n\\n• focus more on the architecture and fundamentals rather than specific technologies\\n• implement the pipelines with some technologies that can be set up locally, and also discuss the alternatives  in the cloud\\n• talk also about the internals of these technologies, maybe not in-depth, but just to explain the difference between some things   \\n\",,1610659611.004900,U01AXE0P5M3\\n57fdf75a-d63c-407a-9610-939173148004,,10.0,,Another piece of news - we can potentially collaborate with AWS and they can give us some free credits for the course. Is it something interesting? Should I talk more to them about it?,1610659654.005700,1610659654.005700,U01AXE0P5M3\\n443451ad-c5cc-4542-816b-df9b06f69114,U01AXE0P5M3,,,That’s definitely interesting! Will these be free credits for the services already on Free Tier or other ones as well?,1610659654.005700,1610659725.005900,U01DHB2HS3X\\n707e3852-a37d-4096-ad48-b64d004d7302,U01HNFLKQ3X,,,\"Thanks for sharing. In your opinion, how important for data engineers to know the internals of databases? In this article, the author refers to Alex Petrov\\'s talk about it.\\n\\nWhile interesting, I\\'m curious if it\\'s something that is super important and should be included in the course, or it\\'s something more advanced and designing pipelines would be a more important skill?\\n\\nI think that designing pipelines might be a more important skill\",1610389402.031400,1610659830.006300,U01AXE0P5M3\\n6a90c16c-0b2c-48e2-a7a1-dabc618191b9,U01AXE0P5M3,,,it\\'s like they can deposit $50 on the account and you can spend them in any way you want. something like that,1610659654.005700,1610659886.006600,U01AXE0P5M3\\n93baa09f-699d-4e6b-af55-a50e0b22087e,U01AXE0P5M3,,,but they want concrete suggestions,1610659654.005700,1610659897.006800,U01AXE0P5M3\\nf2a13034-5a1e-4e2f-ad58-fdf056a44be9,U01AXE0P5M3,,,\"like \"\"this is the course, it teaches these things, and this is what we need\"\"\",1610659654.005700,1610659928.007100,U01AXE0P5M3\\n7414a37a-307a-4443-96e9-84832c3590a5,U01AXE0P5M3,,,\"Awesome! We should be able to consolidate the list soon. Also, I have a free tier account recently created (for the serverless workshop) in case anyone wants to run tests.\",1610659654.005700,1610660176.007300,U01DHB2HS3X\\nc92e3a9e-7668-4f6c-8a39-fa9255b7a2b3,U02GVGA5F9Q,,,We are aiming to cover everything in the first free 300$ allotted for new users,1640814719.123600,1640854397.135100,U01DFQ82AK1\\nf4a98bcd-4dd3-4f76-adbc-58101ce3a5fd,,2.0,,\"Hello Sicelo Ntombana from  Joburg, SA , looking forward to the learning experience\",1640859253.140000,1640859253.140000,U02SC5JGJKW\\n51e75c3c-08b2-4a9c-b792-635640c53b67,,,,\"Hello everyone, I am Gagan from India.. Looking forward to the course and interacting with like minded souls\",,1640860581.142100,U02S9SYDY2E\\nb731c435-adc3-4cda-913f-10be4ff387b5,,,,\"Hi everyone\\nChoukouriyah from Benin Republic\\n\\nGlad to be here and looking forward to learning with you :slightly_smiling_face:\",,1640860610.142800,U02SC725TL4\\n33cd74c0-4aeb-4ca6-a3c3-f4f69c921e46,U02GVGA5F9Q,,,\"Excellent Ankush! This is an important point, and the use of GCP only makes it better!\",1640814719.123600,1640862878.144500,U02GVGA5F9Q\\n687F1E66-F2F1-4ABC-97BF-426765683CE9,,,,\"Hello everyone, \\nDerrick vincent from Nigeria \\n\\nLooking forward to learning with you all \",,1640865454.147000,U02SZQTB2BA\\nd936d270-fe0b-47e1-b098-1592a870979d,,,,\"Hi, \\nI am Adi from India. Excited to learn with you.\",,1640873465.150600,U02S9SSURMH\\n5fa4f2b7-79d5-4657-9aa6-5ceac1794a96,,,,\"Hello.\\nI am Usman Abdulkareem\\nLooking forward to learn and grow\",,1640879999.153000,U02S7PS6G0M\\nB861B080-CEE6-4411-8C50-E1DA2BA66B6D,,,,\"Hi Everyone, Obinna from Nigeria \\nGreat to be here \",,1640893131.156300,U02SBA0U2H1\\n8575ad9c-99cd-4ada-916a-161cfa0af2dd,,,,Hi. I am Chandrahaas! Looking forward to learn and contribute.,,1640894479.157900,U02RTU7PJ87\\n57003947-aa7e-4aa2-8abe-4e55ebea455d,,,,\"hallo, I am syamsul from Indonesia\",,1640919060.160300,U02S95HHCRZ\\n78190ed5-5a35-4c11-8636-9a5a1f194630,,,,\"Hello, I\\'m Shrishail. I looking forward to this exciting course.\",,1640928562.162400,U02SQNXK5MF\\n619169b1-1f05-4274-9f33-64279d0ca035,,1.0,,\"Hi Everyone, excited for this year\",1641120601.186900,1641120601.186900,U02QQCG250X\\n27a2dd70-c6b3-46e9-8609-46f98543ff43,U02QY444V8E,,,\"Hi Khanyi, good to see a fellow Mzansinian in the thread\",1641053970.182800,1641120690.187000,U02QQCG250X\\ndf2a2fac-51c9-4d2e-a7fa-6249cd221476,,,,Hello all I\\'m Biola and new to this Journey.looking forward to learn and cross pollinate ideas,,1641152423.188400,U02S301MSLE\\naddfcac1-f757-4e22-948f-153c56a505f3,,18.0,,\"Hi Data friends :earth_asia:\\n\\nHope you are all doing great and have a happy new begining in 2022!!\\n\\nI suggest to do mini workshop to help each other in preparing the needed tools/installations/setups etc of espiacaly for thous who have limited experiance in data proccessing tools like spark,airflow,kafka etc..\\n\\nPlease let us arrange that and do it before 17 Jan to fully benefit from the zoomCamp \\n\\nPlz Like the post if you are in, then post best date and time to setup the online workshop link.\\nThanks all:hearts:\\nBasem\",1641159804.189100,1641159804.189100,U02RHT0M3M5\\ncdc0b2a9-9c3b-42e3-ba5d-2f89ecff3665,,,,\"Hello, this Jay and exited to begin this journey:+1:\",,1641161849.190300,U02SSP7C4SD\\n7e32e80a-a9f8-42b4-bb42-4068a2d4a9b2,,,,\"Hi everyone! :slightly_smiling_face: I am Anaida from Germany, working as software engineer and looking forward to expand my knowledge in data engineering\",,1641168348.191900,U02SU51P4SD\\n,,,channel_join,<@U02SFFC9FPC> has joined the channel,,1641168484.192100,U02SFFC9FPC\\n37bd6df6-5f6f-4ca9-9f48-7e7bc1f0cb21,U02QY444V8E,,,Great seeing you too :clap:,1641053970.182800,1641185056.195100,U02QY444V8E\\n289d6112-06e9-42bc-8c61-2aeeeb56f69d,U02RHT0M3M5,,,\"Hi there, great idea. How long to you think the preparation would ideally take? That way we know whether to set a date for a week or two before it starts\",1641159804.189100,1641185199.195400,U02QY444V8E\\nf32967c5-2174-4c83-9b46-604f3a477eb7,U01AXE0P5M3,,,\"It\\'ll be similar to what we do in <#C0288NJ5XSA|course-ml-zoomcamp> \\n\\nThere will be a playlist on YouTube with all the videos and we\\'ll put the links to github\",1641566368.013200,1641629050.017200,U01AXE0P5M3\\na84cd5bb-9c66-4aed-b0d8-d0a80dd5ac06,U01AXE0P5M3,,,This is how it looks for ML zoomcamp: <https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp>,1641566368.013200,1641633453.017900,U01AXE0P5M3\\n153b8dfe-86c6-4ef7-9a35-01130533e60c,,7.0,,I could use either a Windows or Mac for this course - is one obviously easier to set up than the other? thanks,1641638240.019000,1641638240.019000,U01UMAXUPSQ\\n06415111-d671-47d1-b587-a58738fa5ae9,U01AXE0P5M3,,,<@U02S8K9JBD0> no you don\\'t need a confirmation,1641566368.013200,1641638794.019200,U01AXE0P5M3\\n0763b00f-f22b-43f2-8925-06b63a00812d,U01AXE0P5M3,,,<@U01AXE0P5M3> How to apply to <#C01FABYF2RG|course-data-engineering> and will it be on zoom platform or what is the way course will go on ?,1641566368.013200,1641640558.019600,U02T76JHS4S\\nbbddbf2d-98b7-4f95-90c2-3ad01aa4b716,U01AXE0P5M3,,,We\\'ll put the videos to our YouTube channel,1641566368.013200,1641640726.019900,U01AXE0P5M3\\n8445fdac-7391-404b-90d1-25d6e19fa72f,U01AXE0P5M3,,,\"You don\\'t need to apply, since you\\'re already here in slack, you wont miss anything\",1641566368.013200,1641640768.020100,U01AXE0P5M3\\nE89C4454-435E-4343-9379-0C23001B70CB,U01UMAXUPSQ,,,\"I personally use Mac because I\\'ve been using it for more than 10 years already for unixy stuff and I\\'m comfortable with it, but it seems that Windows with WSL is pretty good. I know that many people on <#C0288NJ5XSA|course-ml-zoomcamp> use this setup and it seems to work, but you will always find little quirks and issues which are platform-specific\",1641638240.019000,1641640967.023300,U02BVP1QTQF\\n48d00854-0db7-42b2-8f88-b540ac3e50af,U01UMAXUPSQ,,,:+1:,1641638240.019000,1641642038.023500,U01UMAXUPSQ\\n498e09f8-9504-46a7-a3b7-baff6c7053dd,U01UMAXUPSQ,,,I\\'m planning to use an Ubuntu machine,1641638240.019000,1641644213.023800,U02GVGA5F9Q\\n76398914-36e1-4076-ac0c-f0c0c3feb47e,U02AUCL9ZQF,,,<@U01AXE0P5M3> what\\'s the alternative of gcp,1646817149.444839,1647073314.548599,U02AUCL9ZQF\\n214ad728-f81d-4e58-81e8-3b878d067f15,U02AUCL9ZQF,,,You can do things locally. Not all of them though,1646817149.444839,1647074782.389459,U01AXE0P5M3\\n,USLACKBOT,2.0,tombstone,This message was deleted.,1647098758.213029,1647098758.213029,USLACKBOT\\n2bd8b01e-9a1f-4157-af43-fa4461aa1f74,USLACKBOT,,,\"Hey all, another airflow question - in the Dockerfile for airflow, airflow home is set here: `ENV AIRFLOW_HOME=/opt/airflow` . I\\'m trying to figure out airflow more in depth and in a sample with a simple BashOperator that I set up for myself, the run was succesful but this is from the log:\\n```[2022-03-12, 15:01:16 UTC] {subprocess.py:74} INFO - Running command: [\\'bash\\', \\'-c\\', \\'curl -sS {url_to_my_file}.zip &gt; /opt/***/{myFile}.zip\\']```\\nThe file, however, is not in my airflow folder, even though it has supposedly run succesfully. This was specified in the dag:\\n```path_to_local_home = os.environ.get(\"\"AIRFLOW_HOME\"\", \"\"/opt/airflow/\"\")```\\nand the task is simply:\\n```    download_dataset_task = BashOperator(\\n        task_id=\"\"download_dataset_task\"\",\\n        bash_command=f\"\"curl -sS {dataset_url} &gt; {path_to_local_home}/{dataset_file}\"\"\\n    )```\\nthere\\'s no airflow dir in my /opt/ folder and I had thought that setting it like this would make the airflow home default to my /project/airflow directory. I\\'m not overly knowledgeable here. Thanks to anybody who can help me understand a little bit.\",1647098758.213029,1647099499.753079,U02TVGE99QU\\n361c248e-3f2e-404d-ad1f-f1e9cd264e0f,,2.0,,Hi. Can I create external table in BigQuery without some columns?,1647099681.034019,1647099681.034019,U02QL1EG0LV\\nf858eaf0-f5ce-4111-b7ee-af32a856b76c,USLACKBOT,,,\"<https://airflow.apache.org/docs/apache-airflow/stable/start/local.html|https://airflow.apache.org/docs/apache-airflow/stable/start/local.html>\\n\\nThis seems to describe how to run it locally without docker\",1647098758.213029,1647106944.704449,U01AXE0P5M3\\nE3EFED66-5668-49C6-929D-6C3CC4A9B43B,,1.0,,Deadline for final project?,1647123564.636869,1647123564.636869,U02U6DR551B\\ndbc2007f-c1e6-40bb-9e5f-6b245401cd99,U02TC704A3F,,,I would recommend Confluent\\'s Youtube playlist as well,1646865736.378139,1647136726.028609,U02T941CTFY\\nfd1ffb84-9d3d-4c8b-a502-653373f58cd7,U02QL1EG0LV,,,\"Hi, Dimitriy. I had a similar issue. I created the external table then selected the columns I needed into a materialized table. Also, I was able to set partitioning and clustering on the second table.\",1647099681.034019,1647149567.222869,U02UUU3PUDA\\ncff699e1-d09c-4b34-9501-842acfa6c813,,4.0,,\"<@U01AXE0P5M3> I see in the doc that the target audience is both software engineers and data scientists, but aren\\'t those two groups coming from very different backgrounds? How do you plan to target both?\",1611826298.004100,1611826298.004100,U01GL9D4H7Z\\n0fe71626-84e2-45db-bf05-ae7c9bf33b08,U01GL9D4H7Z,,,\"They are, indeed. I think it\\'s better to target software engineers, not data scientists. But data scientists will probably also find it beneficial\",1611826298.004100,1611826478.006700,U01AXE0P5M3\\nf96c04e1-4d7c-474b-81dd-6a2fc0c9459f,U01GL9D4H7Z,,,\"Thanks for pointing it out, I\\'ll change it\",1611826298.004100,1611826514.007500,U01AXE0P5M3\\n7d79096d-1043-4eaa-825f-73582a076122,,3.0,,\"Also re each lesson having two parts, theory and practice, I agree 100%. I know that the standard way of doing things is to first have the theory and then the practice, but I\\'ve seen a different educational model that is much more successful: first you show the end result so that the student has an overall idea of what\\'s going on and what they\\'re learning about and what the \"\"goal\"\" is.\\n\\nFor example if you\\'re showing how to create a relational database structure, first you show the database (a simple version), and then you show a short demo where you create the database so they have an idea of what\\'s expected of them. Also, when you then teach them the theory, it won\\'t just be \"\"in the luft\"\" - they\\'ll have a better idea of what you\\'re talking about. Then after the theory you go back and do a more in-depth demo, and give them an exercise to do it themselves.\\n\\nSo the structure would be: final product (simple version), short demo, theory, in-depth demo, exercise.\\n\\nAlternately, you could just keep it theory and then demo, and just assume that the students will review the lessons.\\n\\nJust my two cents.\",1611826652.009700,1611826652.009700,U01GL9D4H7Z\\n5b7528a7-8bf2-455b-b155-7caedc3d5b3e,U01GL9D4H7Z,,,\"I\\'m sure anyone would find it beneficial, it looks like a great course!\",1611826298.004100,1611826765.009800,U01GL9D4H7Z\\n30e8e78b-7522-43e2-b56e-4a04763f5ad2,U01GL9D4H7Z,,,\"Also I think that software engineers are a great target because it is a really powerful combination to have both programming skills and AI, and surprisingly not so many software engineers are going into it.\",1611826298.004100,1611826930.010000,U01GL9D4H7Z\\n4798fc91-dd4f-406e-84fe-341ee2560657,U01GL9D4H7Z,,,\"Thank you for your ideas and your feedback! Yes this is how it also looks like in my mind as well - that it\\'s not just theory, but also showing things in action.\",1611826652.009700,1611869889.010500,U01AXE0P5M3\\n80ca8f8b-f53c-441a-9abd-37869dfdb818,U01GL9D4H7Z,,,Great! I\\'m sure it\\'ll be a success - good luck with it!,1611826652.009700,1611869969.010700,U01GL9D4H7Z\\nc6e8be34-d70f-49d8-bd6e-370e3e79f123,U01GL9D4H7Z,,,thank you!,1611826652.009700,1611870020.010900,U01AXE0P5M3\\n896c16d8-e088-4e9b-9f41-d07346ee0de9,,,,Hello hello :slightly_smiling_face:,,1639391878.009800,U02CD7E30T0\\n9ebc4aac-a868-439d-ac24-84a3f90ebe0e,,4.0,,How can we know if we are already register at the DE Zoomcamp?,1639391954.010400,1639391954.010400,U02CD7E30T0\\n33d2dcb5-d475-4c88-89d0-0fda499efc83,U02CD7E30T0,,,\"You\\'ll receive an email soon if you have registered. But since you\\'re already in this channel, you won\\'t miss it for sure.\",1639391954.010400,1639392043.010600,U01AXE0P5M3\\nc9ff57c2-f952-4b32-a27a-bb12414f22e1,U02CD7E30T0,,,\"If you want to be super certain, you can leave your email here - <https://airtable.com/shr6oVXeQvSI5HuWD>\",1639391954.010400,1639392055.010900,U01AXE0P5M3\\n,,,channel_topic,set the channel topic: <https://github.com/DataTalksClub/data-engineering-zoomcamp>,,1639392069.011200,U01AXE0P5M3\\nea5ab288-1dc8-42c8-8b66-99b37673083e,U02CD7E30T0,,,Done :slightly_smiling_face:,1639391954.010400,1639392313.011300,U02CD7E30T0\\ncd55c071-7f19-4342-9b80-534e4a0189a4,U02CD7E30T0,,,I am excited by this. Thank you Alexey,1639391954.010400,1639392339.011500,U02CD7E30T0\\n,,,channel_join,<@U02QDR8TF9U> has joined the channel,,1639441916.021000,U02QDR8TF9U\\n40eeedf0-a8db-4d58-bb22-7b03033b1f7e,,,,\"Hello i just wanna ask, does this course cover from the basics? since i\\'m a fresher\",,1639442724.022100,U02RA8F3LQY\\n94c80b01-d198-4613-adcf-d4b062f93d4b,,3.0,,if there any prerequisites to follow this course?,1639442770.022700,1639442770.022700,U02RA8F3LQY\\nc44b5bc5-3178-434f-a467-30eb9f5252c2,,,,\"Hello Folks,  I\\'m Mahesh from USA. Currently working as systems Engineer.\",,1641223899.199300,U01HNBLMT2N\\nc51dd36d-5bc5-4940-b739-b5f59a3e9cb4,U02RHT0M3M5,,,\"Thanks Khanyisile! I think it may take two workshops , 2 hours for each included the discussions between the attendance \\n\\nWhat do you think?\",1641159804.189100,1641237591.201900,U02RHT0M3M5\\n5095db39-efbe-4a1d-ab82-559e2e505e84,U02RHT0M3M5,,,Hi Its a great initiative and I am interested to join!,1641159804.189100,1641270309.204100,U02R65WFT16\\ncf25134b-6a03-46c6-b02a-969f097de80b,U02RHT0M3M5,,,I can generally do on weekends,1641159804.189100,1641270331.204300,U02R65WFT16\\n3d9d7bb3-4330-4746-97b0-d7a2fe680094,U02RHT0M3M5,,,Interested and available on weekends,1641159804.189100,1641271706.204700,U02RPEGKUVA\\n0d705633-762a-4593-9267-f6492aa699b0,,5.0,,\"Greetings Charles here, adapting nascent skills and knowledge from the DS/AI industry to aid in my academic field, Theoretical Physics. Pleasure to make your acquaintance! :blush:\",1641275845.206400,1641275845.206400,U02SFFC9FPC\\n76c4b8c8-7b03-4ad2-ad17-b7c06d124db4,U02RHT0M3M5,,,Yes two workshops sound and weekends seem better suited for most generally,1641159804.189100,1641276880.206900,U02QY444V8E\\n4ae8d45c-dbaf-4985-b1a9-91e3841db124,U02RHT0M3M5,,,Agreed with weekends too,1641159804.189100,1641282099.207300,U02SJE5M867\\nc5802210-87cc-43c6-8ef5-4234f37b47be,,,,Hi. I\\'m Ayoade Abel from Nigeria. Looking forward to learn,,1640946836.165900,U02S5R82DCN\\n4616b6aa-0ed1-4b63-90c2-3fecb44af0d0,,,,Hi all Ayanlowo Babatunde Ayanlola From Nigeria. looking forward in addition to learning Great collaboration,,1640958224.168600,U02RUUJ2TV5\\ne366788a-c84d-4dab-8f84-f60e3b11fccb,U02SC5JGJKW,,,\"Hi Sicelo, I\\'m from SA also maybe we should connect and help one another during the bootcamp.\",1640859253.140000,1640967928.170000,U02QVFVHD3N\\nc80bb88c-0f8f-4665-b0d4-58a2db13513a,,4.0,,Greetings everyone! I am Anton! I have registered for the Data Engineering Zoomcamp a month back. But I didn\\'t receive any updates yet! How can I check if I have registered properly?,1640970379.172000,1640970379.172000,U02JRJJHFL6\\n87dfce83-e9a1-40bb-bf66-b8fa49a1b4f1,U02JRJJHFL6,,,\"You\\'re already in slack, so you have registered properly\\n\\nBut I\\'ll be sending confirmational emails next week, if you don\\'t receive it, let me know. I\\'ll write in slack when I send it\",1640970379.172000,1640971483.172400,U01AXE0P5M3\\n8BDECEC5-9FB5-45C0-9F15-FBD3F63A7509,,,,Hey guys Any students from Germany ? ,,1640984038.174500,U02S6KXPH8W\\n3967c7b1-5b9f-4abe-bcd3-b37edc6c3c98,U02JRJJHFL6,,,\"Thank you so much, Alexey! Eagerly waiting for the course.\",1640970379.172000,1640995390.175500,U02JRJJHFL6\\nedd94933-0655-400b-915a-27eccf46e53e,,,,\"Hello everyone, can\\'t wait to take part in the course. Have a nice day!\",,1639660864.044200,U02QKF292LF\\n95c8ef34-8fe8-4d79-a5dd-2bda4ffc5fb7,,4.0,,Never been to a Bootcamp before. How does this Zoomcamp work? You provides some videos and projects and we should do by week?,1639666651.046000,1639666651.046000,U02CD7E30T0\\n357bc073-233b-4232-9a04-627c7c8c5450,U02CD7E30T0,,,\"You can check <#C0288NJ5XSA|course-ml-zoomcamp>, it\\'ll probably be more or less similar\\n\\n<https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp|https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp>\",1639666651.046000,1639667105.046100,U01AXE0P5M3\\n210f48d5-2dec-410c-9aac-c2a4dc7ada23,U02CD7E30T0,,,<@U01AXE0P5M3> I see that there are youtube videos uploaded. Would there also be weekly zoom calls/interactive sessions as a group? Or is it like learning tracks for individuals.,1639666651.046000,1639668456.046500,U02QPTB3PU5\\n4d9e1275-5e21-409c-a488-b60eb66fc267,U02CD7E30T0,,,In ML zoomcamp we have lessons + office hours. It\\'ll be probably similar for DE zoomcamp. The office hours are those interactive group level things,1639666651.046000,1639668514.046700,U01AXE0P5M3\\n0346b962-ba8d-47c2-a554-a8279ac463ef,,,,Hi Everyone. I am glad to be here and can\\'t wait to be part of this course.,,1639696065.049200,U02QNPWGKHD\\n1269b320-2171-4cd3-8764-e66f9c7ba37f,,4.0,,I am a data scientist by title but I am curious person do you think me trying to learn data engineering is it a waste of time or will it help me improve my data science? I want to be 10x data scientist. What would be the bare minimum data engineering I need to learn and would love to keep the stack only in Python :slightly_smiling_face:,1614354418.004000,1614354418.004000,U01L9SP7UBT\\n3cae9f05-e373-4e41-85cb-3335f7060179,,,,\"How to decide which one to use between airflow, luigi, prefect?\",,1614354622.004600,U01L9SP7UBT\\n7147a059-597a-4574-989e-e0896d30add7,,,,What does a regular day of a data engineer look like?,,1614354668.005100,U01L9SP7UBT\\n6df3ba62-0abb-45f6-8757-0d596173925d,,,,Does one really need to know all kinds of Databases for Data Engineer?,,1614354738.005500,U01L9SP7UBT\\n476a4483-195a-42c0-a489-ac7295916ac0,,2.0,,<https://www.notion.so/Ride-duration-prediction-Architecture-7c580d19d2774fb7a0e30abb16b3a1dc> In this document what do you mean by Metadata Store? What is a metadata store?,1614357894.006200,1614357894.006200,U01L9SP7UBT\\n6DECCED5-EA21-41AE-83C2-ABD4D2FFB170,U01L9SP7UBT,,,\"Up, I want to know too the pros and cons of only using python :slightly_smiling_face:\",1614354418.004000,1614396628.007900,U01P4K4PBFA\\nb0959957-f632-4cbd-a634-91e80dae1885,U02RA8F3LQY,,,\"Hi Fikri,\\nWe do have a week of basic covering SQL, Docker and Terraform basics.\\n<https://github.com/DataTalksClub/data-engineering-zoomcamp#week-1-introduction--prerequisites>\\nBut it would be nice to know SQL and python in advance\",1639442770.022700,1639469213.027400,U01DFQ82AK1\\n0847306d-3433-4a72-8234-d5b39cb5bb10,U02RA8F3LQY,,,Docker and Terraform are basics? :/ Wow I am really newbie.,1639442770.022700,1639471511.028100,U02CD7E30T0\\n7d92bf66-cbb9-4bd1-a4b1-6e22ef01a138,U02RA8F3LQY,,,\"We will use docker and terraform from time to time in the course. U can obviously skip that part (it won\\'t be necessary).\\nIn the introduction section we will share basic knowledge around all those topics\",1639442770.022700,1639472056.028400,U01DFQ82AK1\\n08062fbc-7403-45e5-8c53-7cb3d1f4779b,,,,\"Hello folks, I\\'m glad to be here, thanks for sharing and nice to meet you! :heart:\",,1639481192.030700,U02QQMKB2LU\\ncf6b2a42-33f0-484e-8f5c-0ca07e53f499,,1.0,,Hello. Should I know Java/Scala for this course or Python/SQL will be enough?,1639489906.034200,1639489906.034200,U02QL1EG0LV\\nce1b609d-ec90-4853-9837-419b176a7e96,U02QL1EG0LV,,,\"All courses will be in python/SQL\\nWe can provide alternative languages to attempt test in\",1639489906.034200,1639490715.034500,U01DFQ82AK1\\n559433de-fd1e-403e-9f44-adc19b4d22b4,U01AXE0P5M3,,,we need to think about as well about how to handle the cases and cloud providers? perhaps not everyone can setup a free tier,1607373721.008600,1607593492.009700,U01F78474M9\\ne8f54a3f-4a18-4572-825e-ea0e316329bc,U01AXE0P5M3,,,There are ways to mimic a cloud environment locally as well. I recently used LocalStack for a project.,1607373721.008600,1607594023.009900,U01DHB2HS3X\\na4b66a57-239a-4f82-b0b8-b7fe521baa7a,U01AXE0P5M3,,,\"yeah, I used it as well - pretty cool stuff. but, it just mimics the api’s, so not sure how valuable it could be for someone who is conducting the course.\",1607373721.008600,1607594100.010100,U01F78474M9\\n520cb492-0c62-4415-b3df-62d8bb67114a,U01AXE0P5M3,,,\"I once conducted a workshop using that. I like how it doesn\\'t just mock the environment for the tests, but also generates that kind of environment locally along with the s3 file-system etc. So that when I ran the lambda functions I could actually view the output files!\",1607373721.008600,1607594610.010300,U01DHB2HS3X\\n3d092d15-1314-427c-a344-2aea89801a47,U01AXE0P5M3,,,<https://github.com/sejalv/serverless-workshop|https://github.com/sejalv/serverless-workshop>,1607373721.008600,1607594721.010500,U01DHB2HS3X\\n3ef25986-9949-47e5-8e2c-459366f09a11,U01AXE0P5M3,,,\"But yeah, on the other hand, we may possibly need to inform them which particular sections would require the Free Tier as a pre-req. I don\\'t think we may be able to adjust on that.\",1607373721.008600,1607594825.010900,U01DHB2HS3X\\n693dc439-7677-4a91-84aa-dc9c4987ae98,U01DFQ82AK1,,,I changed the style of the design doc a bit. I realized that we have too many alternative technologies to choose from. That requires a bit more insights into what we want to achieve and how much time + money are we willing to spend on this,1607944135.003800,1608309605.006700,U01DFQ82AK1\\n4538e39b-4fcc-4aa2-ab5a-486e62ef6eb1,U01DFQ82AK1,,,\"With gcp we can get free credits, right?\",1607944135.003800,1608309790.006900,U01AXE0P5M3\\nc3d4ef6e-5220-4a20-93f8-33095c318d3d,U01DFQ82AK1,,,\"With AWS it\\'s a bit tricky, <@U01DHB2HS3X> suggested localstack\",1607944135.003800,1608309878.007100,U01AXE0P5M3\\n54c8a252-3c2c-4053-aef8-1c40ca1456e3,U01DFQ82AK1,,,\"Probably we should focus on one or two things and if there\\'s a lot of interest in a particular alternative, we can invest more time in it\",1607944135.003800,1608309981.007300,U01AXE0P5M3\\nc61d81d1-f5e7-4e77-8eb8-676a6b7d7c5d,U01DFQ82AK1,,,\"Regarding LocalStack, here\\'s a conversation from an earlier thread, if you\\'re interested: <https://datatalks-club.slack.com/archives/C01FABYF2RG/p1607373721008600?thread_ts=1607373721.008600&amp;cid=C01FABYF2RG|https://datatalks-club.slack.com/archives/C01FABYF2RG/p1607373721008600?thread_ts=1607373721.008600&amp;cid=C01FABYF2RG>\\n\\nI used LocalStack offline for development and testing, and the online AWS resources only on deployment and to show how things look on Cloud. But this was for a serverless/lambda-based application. We would need to check if they support more resources.\\nBut in general, if people would be open to using the Free Tier, then that could possibly be an option too…\\n\\nBut I also agree that we don\\'t have to use a certain technology if we don\\'t see it fit for a use-case. Eg. Lambda is a nice entry point to AWS, but if it doesn\\'t feel necessary/scalable to have a Lambda-based application, it\\'s fine. AWS Batch, however, is quite popular among companies. So if we have a batch job as a use-case, we could possibly consider AWS as a provider in such a situation… (just my 2 cents)\",1607944135.003800,1608312222.007600,U01DHB2HS3X\\n9e9dd6e0-15a4-405b-a663-2e91f7c750ba,,3.0,,\"I took another look at the docs from <@U01DFQ82AK1>. Thanks for putting it together! I left a few comments\\n\\nProbably to help us structure the content, we should think what will the the outcome\\n\\nPossible options:\\n• we teach the fundamental concepts vs \\n• we teach how to use particular technology in aws/gcp/etc\\nI like the first option:\\n• we could focus on designing the system (how the data pipeline looks like)\\n• discuss possible options for implementing it\\n• talk about some internals of some of the tools\\n• then implement it with something relatively simple (even if it means running everything locally) \\n• also outline how it can be implemented with alternative technologies\\nFor the last step, if we use kafka in docker + postgres as a \"\"reference implementation\"\", we can say that typically you\\'d do it with kafka + big query (and talk for a few slides about how you\\'d do it) or kinesis + glue + athena (also a few slides)\",1608498792.018700,1608498792.018700,U01AXE0P5M3\\n8f2dcc5d-e19f-4d42-854d-02484cbb4bc1,U01AXE0P5M3,,,\"Also, coming back to this:\\n\\n&gt; Probably we should focus on one or two things and if there\\'s a lot of interest in a particular alternative, we can invest more time in it\\nSo we can initially try to keep it simple and see which technologies people are actually interested in, and then do a follow-up if there\\'s enough interest\",1608498792.018700,1608499076.018800,U01AXE0P5M3\\n09faf65d-e1ab-4eb5-892f-c15bea201e76,U01AXE0P5M3,,,\"Here’s the list of main technologies I think we can expect to make use of (maybe not all, but on a higher estimate):\\n1. S3\\n2. Batch\\n3. ECR\\n4. ECS / EC2\\n5. RDS\\n6. Kinesis / Managed Kafka\\n7. SQS, SNS\\n8. Lambda\\n9. DynamoDB\\n10. Glue\\n11. Athena\\n12. EMR\\n13. CloudFormation &amp; CloudWatch (by default)\",1610659654.005700,1611663854.001100,U01DHB2HS3X\\n6934ed43-ce2a-4890-a685-1ba943bb15f0,U01AXE0P5M3,,,What about Sagemaker?,1610659654.005700,1611698237.001600,U01GL9D4H7Z\\nd537d8e3-a229-4a65-b9ed-c827b35b97e1,U01AXE0P5M3,,,\"Usually it\\'s something used by data scientists, and it\\'s probably less interesting for data engineers\",1610659654.005700,1611733553.001800,U01AXE0P5M3\\n7710647e-42cb-40b5-8504-c47ee97fee96,,7.0,,*HI INEED SOME BEST DE PROJECT WITH CODE PLZ!!!!* ,1623502158.009700,1623502158.009700,U01H4CB36HK\\naff951e9-e82e-43eb-81e7-2ecc7360c286,U01H4CB36HK,,,Can you be a bit more specific?,1623502158.009700,1623502225.009900,U01AXE0P5M3\\neda17e28-3352-429b-8101-45cada3774c9,U01H4CB36HK,,,ineed some project of DE for practice ETL or ELT PROCESS with cloud tools like S3 AND Glue,1623502158.009700,1623502509.010100,U01H4CB36HK\\n4751f8a6-dd92-41f6-9719-74fd595cbd57,U01H4CB36HK,,,\"I\\'ve googled \"\"aws glue end-to-end project example\"\" and there are a couple of interesting videos. Have you checked them?\",1623502158.009700,1623502692.010300,U01AXE0P5M3\\ndb56a7e4-adf3-42f8-bd26-649485f33de6,U01H4CB36HK,,,YES SEND ME SEE,1623502158.009700,1623502720.010500,U01H4CB36HK\\n04a1897a-9b7d-45c1-bf3f-62a31f91a00b,U01H4CB36HK,,,OR ANY ONE HER IN GRP HAVE SOME PROJECTS DO IT IN GITHUB...,1623502158.009700,1623502953.010700,U01H4CB36HK\\nEEB9FE2E-6C36-4502-B757-10D5950AA54B,,,,Hi everyone. I\\'m Gloria a software developer and data analyst. Looking forward to fully become a data engineer,,1641371711.220100,U02T91CL14Y\\n9e634287-7053-4f18-af3f-03bf7668ee7f,,,,\"Hy mates, my name is flo. I\\'m happy to get the opportunity to learn and discuss with you methods and application to analyze data streams.\",,1641402523.224800,U02SB1T3J2G\\n122ab1c8-c14a-458c-8926-03060fb6a360,,3.0,,\"Hv a question on course syllabus:\\n1. I see the course covers Spark, will it cover PySpark as well?\\n2. will the course cover relational DB &amp; NoSQL? (e.g. normalisations, rules of thumbs for designing db schema, how to optimise db performance, tips for writing performant SQL query … etc)\",1641433476.001000,1641433476.001000,U02R65WFT16\\nb0f0d367-54d0-489c-b5ee-dc2474e17252,,,,\"Hi guys\\nI hope This year I can Shift my caarreer to data enggineer\\nAlso You all guys\",,1641435727.002500,U02SQQYTR7U\\n37f10cab-6d45-4591-9a5e-5fd950cdfffe,U01AXE0P5M3,,,sounds  reasonable!,1606238914.012900,1606292650.000100,U01F78474M9\\na8caaafa-e9c2-45c9-974c-2e4b6290c5e6,,2.0,,\"what do you think guys about making a case study  rather a story then a list of tasks. e.g.: you as  data engineer joining ride-hailing company, your first task set up batch data  pipelines….and then during the course you involve and learning together  with a “product”, kind of make it more interactive and close to the idea that as Data Engineer you are building a  product and not just a set of pipelines?\",1606292801.002700,1606292801.002700,U01F78474M9\\n630c454e-65e5-4873-9434-3913e0c34411,,,,That\\'s an awesome idea!,,1606295686.003300,U01AXE0P5M3\\nc89ede73-0b73-4a91-9c3c-02f2eedfa5d0,U01F78474M9,,,Very interesting! Like an RPG. Here\\'s more to add to your idea: <https://mystery.knightlab.com/|https://mystery.knightlab.com/>,1606292801.002700,1606304440.003700,U01DHB2HS3X\\ne6943aa3-ee92-4047-9850-d073c773356d,U01F78474M9,,,oh my good!!! it’s  so cool!!! I love detectives !,1606292801.002700,1606305395.004100,U01F78474M9\\nbaeaa5f1-1b65-4c80-abdd-fb0b22adfbef,,5.0,,\"Hi folks. We offer a data engineering course. AFAIK it think it was the first offering in Europe not tied to a vendor, but I might be wrong. It is merely a side activity for us, and I will not attempt to promote it here, but let me know if you have questions or want information or feedback. The course description is here: <https://www.scling.com/courses/>\\n\\nI read the course doc. How many days of training are you aiming for?\",1611997607.015100,1611997607.015100,U01LEF7FU1G\\n3405a7b3-8cc3-4760-9dbc-9abc4adfdf5a,,2.0,,Iwant some Data Engineer project roles do script code Python for build ETL with pyspark and S3,1612013239.016800,1612013239.016800,U01H4CB36HK\\n791036d6-9dce-43d4-8d89-57fd8da4b868,U01H4CB36HK,,,\"sorry, it\\'s not clear what you mean\",1612013239.016800,1612027688.017700,U01AXE0P5M3\\ndbd62b91-77c7-4f7d-b792-797e503b7dba,U01LEF7FU1G,,,\"Hey Lars, I took a short look, but it definitely looks very useful and gives some info.\\n\\nRegarding the number of days, I don\\'t know to be honest. With practice, I don\\'t think it\\'ll be more than 8 hours (spread across multiple weeks, e.g. 2 hours each week)\",1611997607.015100,1612027827.017900,U01AXE0P5M3\\n13347708-cb1a-4a40-a8fd-894dd685b578,U01LEF7FU1G,,,\"for your course, it\\'s 5 full days (8 hours each day)?\",1611997607.015100,1612027875.018100,U01AXE0P5M3\\ne0a30bbd-5022-4227-b4c2-358e9a53f620,,,,Hello ! plz anyone her see doc <https://docs.google.com/document/d/1TI1co3aRcYYBB9keAMPgrnQd84juN-i8bZiiXW_1oUo/edit#> and modified for more information what u know for we can share knowledge !,,1624993789.017200,U01H4CB36HK\\n10a12adc-2e01-4233-90aa-8d5c2b040eef,,3.0,,*Hello ! plz anyone her see doc\\xa0<https://docs.google.com/document/d/1TI1co3aRcYYBB9keAMPgrnQd84juN-i8bZiiXW_1oUo/edit#>\\xa0and modified for more information what u know for we can share knowledge !*,1624995888.017400,1624995888.017400,U01H4CB36HK\\n4fc9ed76-6c46-46da-b96f-dc9255bf16bf,U01H4CB36HK,,,Thanks for the effort. You should have asked before modifying this document,1624995888.017400,1624995965.017500,U01AXE0P5M3\\n4b268132-45e2-4692-8f73-0cca3252d430,U01H4CB36HK,,,\"I\\'ve made a copy of the doc with your changes - here they are - <https://docs.google.com/document/d/14NR8M2sdGTpDurCyWxrjcXKkjZfCjAqs-sJSc00IXtY/edit?usp=sharing>\\n\\nI\\'ll revert the original document\",1624995888.017400,1624996248.017700,U01AXE0P5M3\\n0a82d837-709c-4473-8ac9-8edab39f73ef,U01H4CB36HK,,,:slightly_smiling_face:,1624995888.017400,1625001303.017900,U01H4CB36HK\\n9eeea5af-b3f5-47ec-9b1b-a7e341fde487,U02R8V9MF6E,,,\"Hi Ashish, yes it starts 17th Jan\",1639810365.069300,1639818680.070000,U01DFQ82AK1\\n0C97DA1C-A30B-43EF-91F8-6EFBEE9EFAC3,,4.0,,Hi guys! I have not received any email with a confirmation after I submitted registration form . Is it ok ?,1639831010.073000,1639831010.073000,U02R461R2Q5\\n77d82919-d304-4a68-ace1-9087481c6ec7,U02R461R2Q5,,,\"Hi! Yes, it\\'s not automated\",1639831010.073000,1639831051.073100,U01AXE0P5M3\\n3581CA6D-B17B-4FB5-BDC2-35505A027E71,U02R461R2Q5,,,\"Ok, thanks ! Could u describe please how will I understand that I will be able to participate the course ?)\",1639831010.073000,1639831179.074800,U02R461R2Q5\\nffc3e407-c10e-4714-92a1-ee5ce6be92a8,U02R461R2Q5,,,Everyone will be able to participate =),1639831010.073000,1639835827.077900,U01AXE0P5M3\\n6B024677-862F-4EA9-8453-DDAAA49E5900,U02R461R2Q5,,,Thanks ),1639831010.073000,1639847609.079000,U02R461R2Q5\\nd3347310-15b8-47d0-ac42-3ff2ae669fb4,,9.0,,\"Hello all,\\nSorry for being away, was busy with some personal responsibilities and interviews.\\nFollowing from Alexeys messages above, I have started with a technical design document (WIP) for <https://docs.google.com/document/d/1a0ONPuhktezUHO5JjChWY8IBsE9CmjQnxQww0PzNzhI/edit?usp=sharing|Data streaming and collection>. I have also added a section in the <https://docs.google.com/document/d/1TI1co3aRcYYBB9keAMPgrnQd84juN-i8bZiiXW_1oUo/edit#heading=h.9uh91gxvs21y|initial google doc>.\\nI will like to request some one to either be my partner or first reviewer in it? Any one interested? :slightly_smiling_face:\\n\\nMy aim is to get initial feedback about the approach and architecture.\\nWill be nice if we can also discuss if technical design document is the right approach here. For me technical design document really help to get a project rolling.\",1607944135.003800,1607944135.003800,U01DFQ82AK1\\nbd12fb62-967a-4c56-99d9-f5cc7f77c3a5,U01DFQ82AK1,,,\"Hi Ankush, just gave it a glance. It looks great. I would be happy to discuss/review it. But I’m only free from next week…\",1607944135.003800,1607944448.004800,U01DHB2HS3X\\n0984ec42-e838-462d-a021-7a06340d3350,U01DFQ82AK1,,,\"Great, i can also add more text until then. Will be nice to have more eyes looking into it :slightly_smiling_face:\",1607944135.003800,1607944540.005100,U01DFQ82AK1\\n,,,channel_join,<@U01H2JVDX7W> has joined the channel,,1607984251.005700,U01H2JVDX7W\\n79b2ae90-24c1-4f5f-b5f2-dce0a082663c,U01HNFLKQ3X,,,Thanks Sam for your feedback and also for your comprehensive answer!,1609810261.023800,1609943839.025500,U01AXE0P5M3\\nddb60744-1be6-4872-8b5f-0510112e0372,U01HNFLKQ3X,,,\"<@U01HNFLKQ3X> thanks for your input! it was nice and detailed and good to know that foundations is a core requirement for such positions. My take away based on all you said is that a candidate must show these essential qualities but in terms of in what format can vary. I understand that perhaps doing course led projects may not be the best way since so many ppl have already done them, but outside of having a formal university training in this field, how can an individual demonstrate this? Only with prior work experience? Or is there no truly defined way and this is defined on case to case basis?\",1609810261.023800,1609947996.025900,U01CPLS6EDS\\nf6c82b1e-2a88-42ca-a30d-d02c0eadf44a,U01HNFLKQ3X,,,\"<@U01HNFLKQ3X> Thanks for bringing this up!\\nwe  were in doubts at the beginning regarding who should be our primary audience and perhaps we should indeed try to target those people with experience who wants to move to Data Eng field with engineering or analytics background. <@U01AXE0P5M3> wdyt?\",1609810261.023800,1609961237.026100,U01F78474M9\\n0d98134b-3fe3-41e1-a612-380e7c020a58,U01HNFLKQ3X,,,\"Yes, definitely. I think it makes our job a bit easier and there\\'s aslo demand for that\",1609810261.023800,1609963646.026300,U01AXE0P5M3\\n71c1595b-171c-4a3f-8f16-e6e6d3aacfc8,U02QELRUA3X,,,\"Hi <@U02QELRUA3X>, maybe this is helpful: <https://github.com/DataTalksClub/data-engineering-zoomcamp>\",1640706567.109900,1640797499.116400,U022D2X5E9M\\n90b09fab-beee-4eb2-8a9a-66e3366dfcea,,,,\"Hello everyone, I\\'m Bhaskar from NY. I\\'m looking forward to learn with you.\",,1640806721.120200,U02SAM9H8TW\\nf0c7a624-c769-48eb-b084-176da4fc3c41,,2.0,,\"Will this course require an active GCP subscription? Will we be able to use free tier services? I see extensive use of GCP tools on GitHub. Whatever the answer, I\\'m looking forward to follow this course! You\\'re top!\",1640814719.123600,1640814719.123600,U02GVGA5F9Q\\n58d70d5c-9d61-412c-af48-264c1dbb7ce2,,,,\"Hi All, I am Nirmal from Germany. Currently I work as a data engineer and I would like to strengthen my skills with this zoomcamp. Looking forward to the course eagerly!\",,1641031651.178700,U02GU7PJ76E\\ne84b64f0-8543-4bcd-91cd-5d16b4f9b39e,,3.0,,\"Hi everyone. I\\'m Khanyi from Joburg, working towards becoming a junior DE/BI Developer. Excited to learn more and from each other.\",1641053970.182800,1641053970.182800,U02QY444V8E\\n829f703b-80f7-45ef-9c8e-ed8c521e47d9,,,,\"hello guys , excited to start the journey :slightly_smiling_face:\",,1641059877.184100,U02P038T8P2\\n2260ce45-50b3-490a-a631-9ffdc49af66c,,,,Funny that I still can see all the history from my phone :sweat_smile: but not from the computer,,1622144730.003300,U01AXE0P5M3\\nebab73df-f334-407d-b5ac-048597d87e8e,,,,Would be nice to pick this topic up at some point!,,1622144771.003400,U01AXE0P5M3\\n7296c4b6-ae3e-477a-95bf-5f984bfb21a6,,1.0,,What are some of the topics previously discussed that people would want to learn?,1622144913.004100,1622144913.004100,U020571PNMT\\ndb038cdb-4b9c-408b-ad48-e2cd314b9952,U020571PNMT,,,\"There\\'s a doc linked in the channel, I think it\\'s open for everyone\",1622144913.004100,1622144950.004200,U01AXE0P5M3\\na1746ce7-88d2-4bbc-9c9c-3722d514262a,,8.0,,\"Hey everyone! I had a conversation with Alexey about the course on LinkedIn and I wanted to share some general feedback with the group as a data engineering hiring manager. Overall I like it. But I\\'m not sure I would hire someone just because they\\'ve taken a course like this. Maybe at a Junior-to-Mid level if they have previous technical experience. Having a real interest in the data landscape and understanding of key concepts has been a better signal to me than completing a project. While it\\'s good to be hands on and all, I think you should definitely emphasize that the concepts. It won\\'t matter if you just do the work, but really understand the ideas behind it.\",1609810261.023800,1609810261.023800,U01HNFLKQ3X\\n0cf8415c-95fb-481b-be10-95490937028d,U01HNFLKQ3X,,,\"<@U01HNFLKQ3X> thanks for the input, its really valuable to have it coming from a hiring manager. From the key factors that you pointed out, have a genuine interest in data landscape and understanding of key concepts, would this mean having a degree in CS is preferable? How can an applicant demonstrate these qualities if not with projects or courses? If applicants get passed the initial CV screening, they’ll be tested in the tech challenge? How can an applicant make this apparent in the initial screening stage (CV)?\",1609810261.023800,1609830731.023900,U01CPLS6EDS\\n3534fe28-a397-4d00-b596-4d67b5d52aaa,,1.0,,What is difference between Data Engineer project and Data Scientist project in team of data in company? I would like someone also send me many important data Engineer projects for practice my skills,1610372044.030800,1610372044.030800,U01H4CB36HK\\n97505b80-e286-4437-93d1-cf9a3329ae71,U01H4CB36HK,,,\"So, a very simplified way to look at it is - if there\\'s ML in a project, then it\\'s a data science project\",1610372044.030800,1610374181.030900,U01AXE0P5M3\\ncd237639-224e-4999-989e-bfb76d90b2d4,,3.0,,<https://khashtamov.com/en/how-to-become-a-data-engineer/>,1610389402.031400,1610389402.031400,U01HNFLKQ3X\\n1c291526-710c-40fe-bb14-ae4ec88df12a,,8.0,,\"Hi all, sorry for the radio silence\\n\\nI gave it some thought and these are the cases I came up with:\\n\\nCase 1:\\nHelp analysts build a dashboard. Metrics: number of rides, avg duration, avg distance, revenue (if possible). There\\'s no data infra yet, but the app from drivers sends all these events to a central location, where we can capture them.\\nSo here we\\'ll need to build the pipeline from scratch using things like kafka/kinesis, airflow, and then build a data warehouse with s3/redshift/snowflake\\n\\nCase 2:\\nSurge detection. If there\\'s a sudden increase in demand in some area, we want to know that to increase prices for rides\\nHere we can build on top of infra from the previous step and add some real-time aggregations (not sure how that works to be honest)\\n\\nCase 3:\\nRide duration prediction. When a customer orders a taxi, we want to tell them how much time it will take\\nHere we can build a pipeline for training a ML model, and then also think how we can serve it\\n\\n\\nWhat do you think? Which other use cases we can include?\",1607373721.008600,1607373721.008600,U01AXE0P5M3\\n45960833-fd8a-41bf-9b78-38a3a59bbaa8,U01AXE0P5M3,,,\"I like how you\\'ve covered one use-case from each area, i.e. Business Intelligence, Data Science, and ML, that DEs generally work on\",1607373721.008600,1607374383.008700,U01DHB2HS3X\\nc909a8fc-5008-466f-8d91-d14825ae4bf3,U01HNFLKQ3X,,,\"I don\\'t think a CS degree is completely necessary - i\\'ve hired great data engineers who have worked their way up from the call center and from law - but I do think a study of computer science and engineering concepts is a foundation that a lot of data engineers miss. I would need to sit down and write out a whole list - but things like algorithms (~leetcode medium), software patterns, high level networking concepts, terminal fu. I did have formal training in this space, but I am not hands on anymore and keep myself fresh by reading code, digging into how it works, and asking questions. At an entry-level, hiring managers should not expect people to be able to stand up a big data system from scratch. They should (and I) look for someone who has a strong foundation and some experience that they can leverage in the role. That is typically tested through coding problems - which end up being leetcode-esque and SQL challenges - moreso than systems design\",1609810261.023800,1609855849.024500,U01HNFLKQ3X\\nbd63052f-f717-4182-aa9a-516760cfc223,U01HNFLKQ3X,,,\"As you move up in your career, you WILL need to understand systems design, but a lot of that learning comes from the systems you are working on and doing targeted deep dives in new technologies. for example, we were evaluating redshift at one point in my career and I read pretty much every piece of documentation on redshift, took notes, thought about how it would fit into our stack, etc. That exercise was hugely helpful in understanding MPP databases and I apply the concepts I learned from that exercise to this day.\",1609810261.023800,1609855945.024700,U01HNFLKQ3X\\nce41eae8-d06f-4d70-8f8b-bba77ad84864,U01AXE0P5M3,,,\"Hi Alexey, I\\'m not sure if I missed it. But do you have the link to this Google Doc?\",1606238422.010200,1606832675.000100,U01DHB2HS3X\\nfd10ce88-ad72-4090-9e71-64e063d02658,U01AXE0P5M3,,,<https://docs.google.com/document/d/1jq0XKvtFlubGcuPozvYGrxDRXlIer7iaDMZfo8ZF5uk/edit?usp=drivesdk|https://docs.google.com/document/d/1jq0XKvtFlubGcuPozvYGrxDRXlIer7iaDMZfo8ZF5uk/edit?usp=drivesdk> here it is,1606238422.010200,1606834277.000400,U01AXE0P5M3\\n41242f92-4e74-4695-9f97-ac484b20998a,U01AXE0P5M3,,,It\\'s also in the topic of this channel,1606238422.010200,1606834291.000600,U01AXE0P5M3\\na231c6bb-70f4-4e71-a588-cef5c6645848,U01AXE0P5M3,,,Got it. thank you!,1606238422.010200,1606838290.000800,U01DHB2HS3X\\n589f3fcb-e4c1-4003-8298-747879cefc63,U02JRJJHFL6,,,Hey <@U01AXE0P5M3> I\\'m yet to get any confirmation email?,1640970379.172000,1641540074.006700,U02TNA9H75E\\nf81a1fb2-1d04-47ef-a7d7-6cfe65404962,U02JRJJHFL6,,,Oh right. I\\'ll send them this week. Still need to do it,1640970379.172000,1641540124.006900,U01AXE0P5M3\\n75325d9e-4752-456b-9a10-78dd6ff61c1b,U02R65WFT16,,,\"1. Yes\\n2. I don\\'t think so. <@U01DFQ82AK1> can you please confirm?\",1641433476.001000,1641540278.007200,U01AXE0P5M3\\n2bd7f86f-1b4a-4855-bc72-99e1d4927109,,2.0,,Does the course cover infromation for being junior/trainee de?,1640706567.109900,1640706567.109900,U02QELRUA3X\\n9AFAB08D-4105-46B8-84D8-CD5F17311C88,U02QELRUA3X,,,\"Hello Rusal, you can check the course content on GitHub. The course is aimed for all levels, you can obviously skip some part which might be advance for you and come back to them when you are more comfortable. \",1640706567.109900,1640719263.111700,U01DFQ82AK1\\n1c6aa3b4-fecf-40ac-9683-d3d30b6283f4,,,,\"Hi everybody, I\\'m Kenechukwu.\\n\\nExcited to be here and looking forward to co-learn with you!\",,1640757925.112900,U02S6AX9KJ6\\n,,2.0,tombstone,This message was deleted.,1616537631.004700,1616537631.004700,USLACKBOT\\nc332e177-6653-4b86-9cdd-f1e38ac0c2c6,U01H4CB36HK,,,\"Hi Mustapha\\n\\nThanks for being active here in this community. I just wanted to suggest you to read this - <https://datatalks.club/slack/guidelines.html#homework-help>\\n\\nPerhaps it\\'ll help you get more answers\",1616537631.004700,1616538342.005000,U01AXE0P5M3\\nb66b368b-1c3b-4ed3-a1a6-97e7639b840d,U01H4CB36HK,,,Also this channel is for discussing a data engineering course (which I hope we\\'ll do eventually),1616537631.004700,1616538397.005300,U01AXE0P5M3\\nd6ee8935-86fd-4b81-9099-3f1c8ae5e517,,1.0,,\"<@U01AXE0P5M3> can you\\'ll have some document like this for analytics, data science, mlops also please? This is a wonderful document.\",1625048497.019000,1625048497.019000,U01L9SP7UBT\\n41e4fb7d-8e4e-4e57-87c4-b4864fc416dc,,,,also regarding the ride sharing <https://www.notion.so/Ride-duration-prediction-Architecture-7c580d19d2774fb7a0e30abb16b3a1dc> is there a Github repo on this with some implementation?,,1625061993.019800,U01L9SP7UBT\\n3941b8ba-26cb-48bf-87fc-6a5dde1d0737,U01LEF7FU1G,,,\"I have run it over 3 days x 8 hours. I got feedback that shorter days would be preferable. Next instance will be 5 days x 6 hours, with some more material on data engineering principles + machine learning engineering.\",1611997607.015100,1612606691.021500,U01LEF7FU1G\\nce89692f-5a69-4880-b950-19e9f8db7f30,U01LEF7FU1G,,,\"You aim for too large a scope for 8 hours. You won\\'t be able to make progress on each subject beyond a shallow intro, which can be found in conference presentations on YouTube instead. I suggest pruning everything that is not strictly data engineering, e.g infrastructure and general software engineering.\",1611997607.015100,1612607105.021700,U01LEF7FU1G\\n43e6c4ec-9126-46e6-a1c0-00707c85b091,U01LEF7FU1G,,,\"Thanks! Indeed, that makes total sense\",1611997607.015100,1612607757.021900,U01AXE0P5M3\\n6cce498c-0338-42e2-832d-7479d4f582f8,U01L9SP7UBT,,,\"It\\'s definitely not a waste of time and learning data engineering will make you more productive. The bare minimum is probably knowing how to get the data you need in an automatic way, not just from your jupyter notebook\",1614354418.004000,1614590742.009400,U01AXE0P5M3\\n145d78d6-c651-4cc6-ae02-8fac03a3a791,U01L9SP7UBT,,,So just knowing kafka and airflow is enough?,1614354418.004000,1614595020.009600,U01L9SP7UBT\\nebe6c2bc-93b2-44cb-aa72-aa882ee1aebe,U01L9SP7UBT,,,Also some database and some cloud provider,1614354418.004000,1614597315.009800,U01AXE0P5M3\\nad50f86a-10ea-4819-825a-75abb27467e6,U01L9SP7UBT,,,\"Hi, I looked through it again, and I don\\'t think I\\'ve mentioned Metadata Store anywhere. It\\'s a Feature Store that holds the metadata along with other data. For more info on that, I\\'ve cited other references. And in general, you can find plenty of good articles on Feature Stores anyway.\",1614357894.006200,1614425817.008300,U01DHB2HS3X\\nd5d204f1-5149-43a0-834a-3ba0adfc32ca,U01L9SP7UBT,,,\"Also please note, this channel was created to coordinate on a course in future. A lot of documents here are still in a draft mode  and not complete enough to be discussed.  But if you\\'re still interested in going through them, it\\'ll be best to reach out to the authors of the documents directly so that the content is interpreted correctly.\",1614357894.006200,1614425875.008600,U01DHB2HS3X\\n8707034d-95fe-443a-b42d-384f05c339f4,,,,Hi Everyone! I am Basem from Palestine :christmas_tree: glad to be here and can\\'t wait to be part of this course.:innocent:,,1640446131.103900,U02RHT0M3M5\\n22bbd83f-2fbb-4c8b-b40a-1a4bbd7aae7b,,,,\"Hi Everyone, I am excited to join this course.\",,1640478138.105700,U02S14MHV3N\\n311b6cef-2463-4e99-ad0b-7f63af979396,U01HNFLKQ3X,,,\"Yeah I think designing pipelines is more important. You are not going to get quizzed on DB internals for a job interview unless you are interviewing for an adminstration role. You *should* pick up some DB skills as you work with upstream data sources, but that\\'s clearly a side affect of the role than a core skill.\",1610389402.031400,1610893787.008100,U01HNFLKQ3X\\n8be0fa10-2c2c-4624-a5e3-18d94aaab3c6,U01HNFLKQ3X,,,Thanks! That was my impression as well,1610389402.031400,1610894549.008300,U01AXE0P5M3\\n4b671cf2-2390-4030-a6d8-e761af422b77,U01AXE0P5M3,,,I didn\\'t know that there was such a strong split between the two! I guess in smaller companies one person fulfills both roles.,1610659654.005700,1611739155.002100,U01GL9D4H7Z\\n868d8038-8e24-4e40-9e81-40a350166b6a,U01AXE0P5M3,,,\"As a DE, I don’t find it less interesting :sweat_smile:  But yeah, more specialized in its use-case to fit into a general design.\",1610659654.005700,1611739357.002300,U01DHB2HS3X\\n1055f889-0b7a-4a70-be9f-46f92b1a50c7,,,,\"Hello guys, excited for the DE zoomcamp :partying_face:\",,1639914306.083100,U02HYRDSYQY\\n79612833-60c2-4eca-a5b6-727c13bee3b0,,,,\"hello guys , excited to start the journey :)\",,1639954847.085400,U02D8JTTSMC\\n47611e53-ef9f-4fd7-92b2-a456f8204565,,,,\"Hello, glad to join the course.\",,1639995660.088700,U02RYUWG4CQ\\nfc465fbd-9345-4b29-9a9a-bc7805e1bedc,,,,\"Hello everyone, here to join the course as well\",,1640033055.090900,U02RHKC7PGC\\nba1db9f4-e55b-4873-b60c-13440efb4ce9,,1.0,,what is python &amp; SQL knowledge needed necessary  for ETL or ELT in job of data engineering?,1623671289.012500,1623671289.012500,U01H4CB36HK\\n07aca557-aa52-4755-abf6-54ca5757c1d0,U01H4CB36HK,,,Yes,1623671289.012500,1623672966.012700,U01AXE0P5M3\\nc029995b-6550-4167-b2c4-523ebd6080bb,U01AXE0P5M3,,,\"&gt; we teach the fundamental concepts vs\\nlove it. I guess a lot of courses these days miss this point :slightly_smiling_face:\",1608498792.018700,1608652386.019200,U01DFQ82AK1\\ne04dd544-72ef-4e66-89f0-2d8b8dc0ac57,U01AXE0P5M3,,,The tricky part is how to teach fundamental concepts while keeping it practical =),1608498792.018700,1608655446.019400,U01AXE0P5M3\\n,,,channel_join,<@U01HB4W8G5P> has joined the channel,,1608046120.005900,U01HB4W8G5P\\nc651ab1b-3a2d-42a4-8013-0925b88c2e19,U01DFQ82AK1,,,\"Sorry for the delay, checking the doc now! Looks really great!\",1607944135.003800,1608066622.006000,U01AXE0P5M3\\nc33dd71d-0a25-4601-9e83-8981dd911e6d,U01DFQ82AK1,,,\"I left a few comments, but it really looks great, thanks for taking the time to do it!\",1607944135.003800,1608067085.006200,U01AXE0P5M3\\n84c17937-e08a-4ae8-9bf8-111c363389be,U01H4CB36HK,,,<@U01AXE0P5M3> when to use something like aws glue?,1623502158.009700,1623579854.011200,U01L9SP7UBT\\n2ae484cf-a283-48aa-b7da-bda01ed38781,U01H4CB36HK,,,WHEN WE DO BIG DATA PROCESSING WITH PYSPARK FOR ETL PROCESS,1623502158.009700,1623591503.011500,U01H4CB36HK\\n58163a80-5ebd-47da-a7da-50b2a44ffb0f,,,,<https://www.linkedin.com/feed/update/urn:li:activity:6778216146022744064|https://www.linkedin.com/feed/update/urn:li:activity:6778216146022744064>,,1616066292.001200,U01H4CB36HK\\ne63ccd6a-98f4-4689-81d8-629e121bd072,,,,\"Great stuff <@U01AXE0P5M3>, trying to take a break from doing too much stuff at the same time, you are not helping me!! :rolling_on_the_floor_laughing:\",,1640341124.101000,U01LP0ZLJU9\\n86d93cca-7c53-43e4-b83f-35e7e779fc72,,,,\"Hello everyone, I glad to join this course.:slightly_smiling_face:\",,1640182348.096600,U02QVRMFQMD\\n443f766f-2df8-4fe8-91d0-c9a73037ae77,U01AXE0P5M3,,,\"looks good, thanks for doing it! I would say we can start for this draft, assign different parts of the topics we want to cover and then take a look on the gaps, maybe some more cases\",1607373721.008600,1607455252.009200,U01F78474M9\\n95f8a60b-5c25-4449-982f-a49576db6b55,U01H4CB36HK,,,What do Data Engineer is created code Python for ETL process?,1612013239.016800,1612310712.020300,U01H4CB36HK\\n222e70f4-84dd-4d1a-960c-c54f49ed5624,U01L9SP7UBT,,,At some point! Now I hope at least to finish this course =),1625048497.019000,1626288400.022700,U01AXE0P5M3\\n2faadcb9-e438-4433-88c4-e2bb68a95b24,,,,\"Hi everybody,\\n\\nLooking forward to co-learn with you!\",,1640553390.108000,U01R7FD7BFB\\n14f53d2c-9132-43bc-bb46-545dc92734c9,,,,Someone have good data engineering project WITH Scala?,,1615657675.001900,U01H4CB36HK\\n95b10f50-62ef-4fff-9347-4c345851502d,,,,Some one have good project real for data Engineer job!,,1612371192.020600,U01H4CB36HK\\n,,,channel_join,<@U01HN7S4ASE> has joined the channel,,1609059234.020000,U01HN7S4ASE\\n,,,channel_join,<@U01HFK3MHSA> has joined the channel,,1609081197.020300,U01HFK3MHSA\\n,,,channel_join,<@U01H7LADJ4F> has joined the channel,,1609089983.020600,U01H7LADJ4F\\n,,,channel_join,<@U01JCBHKFK2> has joined the channel,,1609095312.020900,U01JCBHKFK2\\n,,,channel_join,<@U01HNPSC5LJ> has joined the channel,,1609123754.021200,U01HNPSC5LJ\\n,,,channel_join,<@U01CPLS6EDS> has joined the channel,,1606910929.000200,U01CPLS6EDS\\n,,,channel_join,<@U01FSQ31MNZ> has joined the channel,,1606927565.000400,U01FSQ31MNZ\\n,,,channel_join,<@U02RJ0MJW03> has joined the channel,,1640091410.093500,U02RJ0MJW03\\n,,,channel_join,<@U01BVF8489F> has joined the channel,,1607527167.009500,U01BVF8489F\\n,,,channel_join,<@U01ENFKMTU5> has joined the channel,,1607784533.011300,U01ENFKMTU5\\n,,,channel_join,<@U01GL9D4H7Z> has joined the channel,,1608114154.006500,U01GL9D4H7Z\\n,,,channel_join,<@U01H4CB36HK> has joined the channel,,1608591242.019100,U01H4CB36HK\\n,,,channel_join,<@U01HXRYSD09> has joined the channel,,1608776149.019700,U01HXRYSD09\\n,,,channel_join,<@U01HGDYJZUN> has joined the channel,,1609150354.021500,U01HGDYJZUN\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0a97584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1. Check all columns\n",
    "sorted(all_messages.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc860b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88511314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "dbfa0934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(subtype='tombstone'),\n",
       " Row(subtype=None),\n",
       " Row(subtype='thread_broadcast'),\n",
       " Row(subtype='channel_join'),\n",
       " Row(subtype='channel_topic')]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_cde.select('subtype').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c79deec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(type='message')]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_cde.select('type').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "dd06a31d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(topic=None),\n",
       " Row(topic='<https://docs.google.com/document/d/1TI1co3aRcYYBB9keAMPgrnQd84juN-i8bZiiXW_1oUo/edit?usp=sharing>'),\n",
       " Row(topic='<https://github.com/DataTalksClub/data-engineering-zoomcamp>')]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_cde.select('topic').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "18ccb1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 3. Clean Unwanted subtypes\n",
    "messages_cde = messages_cde.where((col(\"subtype\").isNull()) | ((col(\"subtype\") != \"thread_broadcast\") & (col(\"subtype\") != \"channel_join\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8d37ff9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+-----+--------------------+\n",
      "|       client_msg_id|           name|count|               users|\n",
      "+--------------------+---------------+-----+--------------------+\n",
      "|d751f16a-ca2a-403...|         scream|    2|[U01AXE0P5M3, U02...|\n",
      "|780394d7-0647-499...|   raised_hands|    1|       [U02V24WAZRN]|\n",
      "|425a4e1f-fd43-496...|   raised_hands|    1|       [U02U8FXSG1Y]|\n",
      "|aa607d5d-55fe-45f...|             +1|    1|       [U02U34YJ8C8]|\n",
      "|a72b3aa0-db7b-488...|             +1|    3|[U02U34YJ8C8, U02...|\n",
      "|1ade0883-ba49-4d3...|   raised_hands|    1|       [U0308865C0H]|\n",
      "|1ade0883-ba49-4d3...|             +1|    1|       [U0308865C0H]|\n",
      "|29f12de9-4834-405...|             +1|    1|       [U02UY1QTGHW]|\n",
      "|53f8696b-acd2-476...|             +1|    1|       [U02HFP7UTFB]|\n",
      "|3a7e75e9-1603-4b5...|   raised_hands|    1|       [U02QBJYQFK9]|\n",
      "|071697F4-5AA5-417...|             +1|    1|       [U02RHT0M3M5]|\n",
      "|0F73517E-6509-47F...|             +1|    1|       [U0308MF3KUH]|\n",
      "|760094a6-756d-464...|             +1|    1|       [U02U34YJ8C8]|\n",
      "|ce038456-8a58-48b...|             +1|    3|[U01DFQ82AK1, U02...|\n",
      "|2350ecb7-e8fa-46d...|              v|    1|       [U02UB59DKDL]|\n",
      "|2350ecb7-e8fa-46d...|+1::skin-tone-6|    1|       [U02UHJB6CJ0]|\n",
      "|28426f6b-271d-422...|             +1|    1|       [U0290EYCA7Q]|\n",
      "|daa79e17-4ea0-4ac...|           eyes|    1|       [U02U790S2NA]|\n",
      "|3f716653-c593-483...|             +1|    3|[U02Q51Y4MM5, U02...|\n",
      "|74856b73-7410-466...|      thank_you|    1|       [U01AXE0P5M3]|\n",
      "+--------------------+---------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Extract reactions into new dataframe\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "reactions_df = messages_cde.where((col(\"reactions\").isNotNull())) \n",
    "reactions_df = reactions_df.select(['client_msg_id','reactions'])\n",
    "# reactions_df.select(['client_msg_id','reactions']).distinct().collect()\n",
    "reactions_df = reactions_df.withColumn('reaction', explode('reactions')).drop(\"reactions\")\n",
    "# reactions_df.select(['client_msg_id','reaction']).head()\n",
    "reactions_df = reactions_df.select(['client_msg_id',\"reaction.name\", \"reaction.count\", \"reaction.users\"])\n",
    "reactions_df.select(['client_msg_id',\"name\", \"count\", \"users\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e744d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2. Unique Reactions..\n",
    "unique_reactions = reactions_df.select('name').distinct().rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "fc55b7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# 5 Clean Message from some unwanted characters\n",
    "def clean_message_text(text):             \n",
    "    user_pattern = re.compile(r'<@(.+?)>')\n",
    "    link_pattern_text = re.compile(r'<(http.+?)\\|(.+?)>')\n",
    "    link_pattern = re.compile(r'<(http.+?)>')\n",
    "    \n",
    "    text = text.replace('\\xa0', ' ').replace('•', '-').replace('\\n\\n', '\\n').replace(\"'\", '').replace(\"`\", '')\n",
    "    text = re.sub('\\n', ' ', text) \n",
    "    text = user_pattern.sub(\"\", text)\n",
    "    text = link_pattern_text.sub(\"\", text)\n",
    "    text = link_pattern.sub(\"\", text)\n",
    "    return text\n",
    "\n",
    "udf_clean_message_text = udf(lambda x:clean_message_text(x),StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "257a251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_cde = messages_cde.withColumn(\"text\",udf_clean_message_text(col(\"text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "d397be10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(text='hi , i had a lot of problems when trying to convert the .ipynb file to a py script. i copied exactly the code in the video and i kept getting the error no template sub-directory with name script can be found. after a bit of googling, i found that if i uninstalled nbconvert module and then reinstalled it with a lower version, the command would work. i was close to throwing my laptop out of the window by that point! obviously we all have different computers, os and python versions, so mentioning the module version would be very helpful!')"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_cde.select(\"text\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "d16e6fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 Convert epoch to human readable datetime\n",
    "from pyspark.sql.functions import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "20e8807b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(ts=datetime.datetime(2022, 1, 24, 9, 25, 48)),\n",
       " Row(ts=datetime.datetime(2022, 1, 24, 9, 27, 13)),\n",
       " Row(ts=datetime.datetime(2022, 1, 24, 9, 29, 34)),\n",
       " Row(ts=datetime.datetime(2022, 1, 24, 9, 30)),\n",
       " Row(ts=datetime.datetime(2022, 1, 24, 9, 33, 10)),\n",
       " Row(ts=datetime.datetime(2022, 1, 24, 9, 34, 32)),\n",
       " Row(ts=datetime.datetime(2022, 1, 24, 9, 38, 52)),\n",
       " Row(ts=datetime.datetime(2022, 1, 24, 9, 42, 27)),\n",
       " Row(ts=datetime.datetime(2022, 1, 24, 9, 45, 13)),\n",
       " Row(ts=datetime.datetime(2022, 1, 24, 9, 47, 25))]"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType, DateType\n",
    "df = messages_cde.withColumn(\"ts\", messages_cde[\"ts\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"ts\",udf_epoch_2_datetime(col(\"ts\")))\n",
    "df=df.withColumn(\"ts\",to_timestamp(col(\"ts\")))\n",
    "\n",
    "df.select([\"ts\"]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "c26d4902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_2_datetime(epoch):\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(epoch))\n",
    "\n",
    "udf_epoch_2_datetime = udf(lambda x: epoch_2_datetime(x),StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "2793bfac",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required (got type str)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [498]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m time\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocaltime\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1643012748.273300\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required (got type str)"
     ]
    }
   ],
   "source": [
    "time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(\"1643012748.273300\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "f39c366a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-03-22 23:00:06'"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_2_datetime(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "4ad6a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=messages_cde.withColumn(\"ts_timestamp\", (F.col(\"ts\")/1000).cast(\"timestamp\"))\n",
    "# x.select([\"ts\", \"ts_timestamp\"]).show()\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "messages_cde = messages_cde.withColumn(\"x\",udf_epoch_2_datetime(col(\"thread_ts\")))\n",
    "# messages_cde = messages_cde.withColumn(\"ts_\",udf_epoch_2_datetime(col(\"ts\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "3dbb169b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ts: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages_cde.select(['ts']).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab40077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import types as t\n",
    "messages_cde = messages_cde.withColumn('ts', f.date_format(df.epoch.cast(dataType=t.TimestampType()), \"yyyy-MM-dd\"))\n",
    "messages_cde = messages_cde.withColumn('thread_ts', f.date_format(df.epoch.cast(dataType=t.TimestampType()), \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e571d0a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c21fa97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232e878e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a8bfb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "22067192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All_messages  :(9087, 10)\n",
      "Root_messages :(1862, 10)\n",
      "Thread Replies:(7225, 10)\n"
     ]
    }
   ],
   "source": [
    "# 6. Split Root messages and Threaded messages\n",
    "root_messages = messages_cde.where((col(\"parent_user_id\").isNull()))\n",
    "thread_replies = messages_cde.where((col(\"parent_user_id\").isNotNull()))\n",
    "\n",
    "print(f\"All_messages  :{(messages_cde.count(), len(messages_cde.columns))}\")\n",
    "print(f\"Root_messages :{(root_messages.count(), len(root_messages.columns))}\")\n",
    "print(f\"Thread Replies:{(thread_replies.count(), len(thread_replies.columns))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9cd55dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A3.1 see which columns are all empty\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def columns_not_in_use(df):\n",
    "    nonNull_cols = [c for c in df.columns if df.filter(F.col(c).isNotNull()).count() > 0]\n",
    "    null_cols = [c for c in df.columns if df.filter(F.col(c).isNotNull()).count() == 0]\n",
    "    return (nonNull_cols, null_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cea744fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['inviter', 'parent_user_id', 'root', 'x_files']\n",
      "-----\n",
      "['inviter', 'root', 'topic']\n"
     ]
    }
   ],
   "source": [
    "drop_columns_root_messages = columns_not_in_use(root_messages)[1]\n",
    "drop_columns_thread_replies = columns_not_in_use(thread_replies)[1]\n",
    "print(drop_columns_root_messages)\n",
    "print(\"-----\")\n",
    "print(drop_columns_thread_replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1d1749b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A3.2 Define interested columns based on A1.analysis\n",
    "\n",
    "interest_columns= [\"client_msg_id\", \"parent_user_id\", \"reply_count\", \"subtype\", \"text\",\n",
    "                   \"thread_ts\",\"ts\", \"user\"]\n",
    "\n",
    "drop_columns = [\"attachments\",\"blocks\", \"display_as_bot\", \"edited\", \"files\", \"hidden\", \"inviter\", \"is_locked\", \n",
    "                \"last_read\",\"latest_reply\", \"reactions\", \"replies\", \"reply_count\", \"reply_users\",\n",
    "                \"reply_users_count\", \"root\" \"source_team\", \"subscribed\", \"team\", \"topic\" , \"type\",\n",
    "                \"upload\", \"user_profile\", \"user_team\", \"x_files\"]\n",
    "\n",
    "len(interest_columns + drop_columns) # 30 \n",
    "len(messages_cde.columns) #32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e0e06eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Drop unused columns  \n",
    "messages_cde=messages_cde.drop(*drop_columns)\n",
    "root_messages=root_messages.drop(*drop_columns)\n",
    "thread_replies=thread_replies.drop(*drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1016481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373aff76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971cfcb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87708ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d992eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759ceb21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe2d374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6723f63f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b44889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some text cleanup practice..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424ab82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Approach 1 Alexey ...\n",
    "# things to clean up, code-blocks\n",
    "# punctuations\n",
    "# \\n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "18fa3956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_message_text(text): \n",
    "    # clean message text from usernames, links, etc\n",
    "            \n",
    "    user_pattern = re.compile(r'<@(.+?)>')\n",
    "    link_pattern_text = re.compile(r'<(http.+?)\\|(.+?)>')\n",
    "    link_pattern = re.compile(r'<(http.+?)>')\n",
    "    \n",
    "    text = text.replace('\\xa0', ' ').replace('•', '-').replace('\\n\\n', '\\n').replace(\"'\", '').replace(\"`\", '')\n",
    "    text = re.sub('\\n', ' ', text) \n",
    "    text = user_pattern.sub(\"\", text)\n",
    "    text = link_pattern_text.sub(\"\", text)\n",
    "    text = link_pattern.sub(\"\", text)    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "07eb3a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql.types import StringType\n",
    "udf_clean_message_text = udf(lambda x:clean_message_text(x),StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "658937bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_messages_cleaned1 = root_messages.withColumn(\"text\",udf_clean_message_text(col(\"text\")))\n",
    "thread_replies_cleaned1 = thread_replies.withColumn(\"text\",udf_clean_message_text(col(\"text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "2a25369f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/usr/local/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 663, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(text='Also , do you have to submit homework for all the 6 weeks to be eligible for the project stage ?'),\n",
       " Row(text='i am getting some error in SQL query tool, i am tring to find a specific location in the \"zones\" data set but i keep getting the ERROR:  column \"borough\" does not exist LINE 7: Borough=Queens heres an example that i tried SELECT  * FROM zones WHERE Borough=Queens'),\n",
       " Row(text='Hey all, I want to ask something about home work. So, besides yellow_taxi_data we need to import dataset about zones right. Do I need to create python script to ingest this dataset to the database or can I just import it using sql script or using import feature in pgadmin? Thanks:raised_hands:'),\n",
       " Row(text='Week 1: seems to me that the README.md is a little confusing, if i were to follow the steps documented there, as some steps are duplicated? understand there are multiple ways to achieve the same task, but perhaps they can be marked as alternatives?'),\n",
       " Row(text='Im having this error while trying to run the data ingestion script. Traceback (most recent call last):   File \"/app/ingest_data.py\", line 67, in &lt;module&gt;     main(args)   File \"/app/ingest_data.py\", line 30, in main     df = next(df_iter)   File \"/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1187, in __next__     return self.get_chunk()   File \"/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1280, in get_chunk     return self.read(nrows=size)   File \"/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1250, in read     index, columns, col_dict = self._engine.read(nrows)   File \"/usr/local/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 225, in read     chunks = self._reader.read_low_memory(nrows)   File \"pandas/_libs/parsers.pyx\", line 817, in pandas._libs.parsers.TextReader.read_low_memory   File \"pandas/_libs/parsers.pyx\", line 861, in pandas._libs.parsers.TextReader._read_rows   File \"pandas/_libs/parsers.pyx\", line 847, in pandas._libs.parsers.TextReader._tokenize_rows   File \"pandas/_libs/parsers.pyx\", line 1960, in pandas._libs.parsers.raise_parser_error pandas.errors.ParserError: Error tokenizing data. C error: Expected 1 fields in line 3, saw 2'),\n",
       " Row(text='After running docker-compose up -d, Im trying to connect to pgadmin through localhost:8080, but its just loading for too long..'),\n",
       " Row(text='Possible to highlight Central Park in the Fifth question on google forms? What was the most popular destination for passengers picked up in *central park* on January 14? Enter the zone name (not id). If the zone name is unknown (missing), write \"Unknown\"'),\n",
       " Row(text='Question 6: Most expensive route * Whats the pickup-dropoff pair with the largest average price for a ride (calculated based on total_amount)? Enter two zone names separated by a slashFor example:\"Jamaica Bay / Clinton East\"If any of the zone names are unknown (missing), write \"Unknown\". For example, \"Unknown / Clinton East\".'),\n",
       " Row(text='what column constitutes a ride in yellow_taxi_trips data'),\n",
       " Row(text='the pickup and drop location IDs?')]"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_messages_cleaned1.select(\"text\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e43b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Approach 2 nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "53a8383a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/iremertuerk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/iremertuerk/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/iremertuerk/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def clean_message_text2(sentence):        \n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    sentence = sentence.replace('\\xa0', ' ').replace('•', '-').replace('\\n\\n', '\\n')\n",
    "    sentence = sentence.replace(\"'\", ' ').replace(\"`\", ' ')\n",
    "    sentence = sentence.replace('{html}',\"\")\n",
    "    \n",
    "    sentence = re.sub('\\n', '', sentence) \n",
    "    sentence = re.sub(r'http\\S+', '',sentence) # remove url\n",
    "    sentence = re.sub('[0-9]+', '', sentence) # \n",
    "    \n",
    "    return sentence.strip()\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)  \n",
    "    # Remove filterwords and remove the worlds that lenght less than 2\n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "    \n",
    "    stemmer = PorterStemmer() \n",
    "    stem_words=[stemmer.stem(w) for w in filtered_words] # set root form of the word\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "ff0e49f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "udf_clean_message_text2 = udf(lambda x:clean_message_text2(x),StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "64a800d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_messages_cleaned2 = root_messages.withColumn(\"text\",udf_clean_message_text2(col(\"text\")))\n",
    "thread_replies_cleaned2 = thread_replies.withColumn(\"text\",udf_clean_message_text2(col(\"text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "1078504c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/usr/local/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 663, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(text='also <@uaxepm>, do you have to submit homework for all the  weeks to be eligible for the project stage ?'),\n",
       " Row(text='i am getting some error in sql query tool, i am tring to find a specific location in the \"zones\" data set but i keep getting the error:  column \"borough\" does not exist  line : borough= queens  here s an example that i tried   select *fromzoneswhereborough= queens'),\n",
       " Row(text='hey all, i want to ask something about home work. so, besides  yellow_taxi_data  we need to import dataset about zones right. do i need to create python script to ingest this dataset to the database or can i just import it using sql script or using import feature in pgadmin? thanks:raised_hands:'),\n",
       " Row(text='week : seems to me that the readme.md is a little confusing, if i were to follow the steps documented there, as some steps are duplicated?understand there are multiple ways to achieve the same task, but perhaps they can be marked as alternatives?'),\n",
       " Row(text='i m having this error while trying to run the data ingestion script.   traceback (most recent call last):  file \"/app/ingest_data.py\", line , in &lt;module&gt;    main(args)  file \"/app/ingest_data.py\", line , in main    df = next(df_iter)  file \"/usr/local/lib/python./site-packages/pandas/io/parsers/readers.py\", line , in __next__    return self.get_chunk()  file \"/usr/local/lib/python./site-packages/pandas/io/parsers/readers.py\", line , in get_chunk    return self.read(nrows=size)  file \"/usr/local/lib/python./site-packages/pandas/io/parsers/readers.py\", line , in read    index, columns, col_dict = self._engine.read(nrows)  file \"/usr/local/lib/python./site-packages/pandas/io/parsers/c_parser_wrapper.py\", line , in read    chunks = self._reader.read_low_memory(nrows)  file \"pandas/_libs/parsers.pyx\", line , in pandas._libs.parsers.textreader.read_low_memory  file \"pandas/_libs/parsers.pyx\", line , in pandas._libs.parsers.textreader._read_rows  file \"pandas/_libs/parsers.pyx\", line , in pandas._libs.parsers.textreader._tokenize_rows  file \"pandas/_libs/parsers.pyx\", line , in pandas._libs.parsers.raise_parser_errorpandas.errors.parsererror: error tokenizing data. c error: expected  fields in line , saw'),\n",
       " Row(text='after running  docker-compose up -d , i m trying to connect to pgadmin through localhost:, but it s just loading for too long..'),\n",
       " Row(text='<@uaxepm> possible to highlight central park in the fifth question on google forms?what was the most popular destination for passengers picked up in *central park* on january ? enter the zone name (not id). if the zone name is unknown (missing), write \"unknown\"'),\n",
       " Row(text='question : most expensive route *what s the pickup-dropoff pair with the largest average price for a ride (calculated based on total_amount)? enter two zone names separated by a slashfor example:\"jamaica bay / clinton east\"if any of the zone names are unknown (missing), write \"unknown\". for example, \"unknown / clinton east\".'),\n",
       " Row(text='what column constitutes a ride in yellow_taxi_trips data'),\n",
       " Row(text='the pickup and drop location ids?')]"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_messages_cleaned2.select(\"text\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafc43f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Approach 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "73f5a9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "\n",
    "import re, string # Regular Expressions, String\n",
    "from nltk.corpus import stopwords # stopwords\n",
    "from nltk.stem.porter import PorterStemmer # for word stemming\n",
    "from nltk.stem import WordNetLemmatizer # for word lemmatization\n",
    "# soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "# set of stopwords to be removed from text\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# update stopwords to have punctuation too\n",
    "stop.update(list(string.punctuation))\n",
    "\n",
    "def clean_message_text3(text_list):\n",
    "    \n",
    "    # Remove unwanted html characters\n",
    "    re1 = re.compile(r'  +')\n",
    "    x1 = text_list.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "    'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "    '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n",
    "    ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n",
    "    text = re1.sub(' ', html.unescape(x1))\n",
    "    \n",
    "    # remove non-ascii characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    \n",
    "    # strip html\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    # remove between square brackets\n",
    "    text = re.sub('\\[[^]]*\\]', '', text)\n",
    "    \n",
    "    # remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # remove twitter tags\n",
    "    text = text.replace(\"@\", \"\")\n",
    "    \n",
    "    # remove hashtags\n",
    "    text = text.replace(\"#\", \"\")\n",
    "    \n",
    "    # remove all non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z ]', '', text)\n",
    "    \n",
    "    # remove stopwords from text\n",
    "    final_text = []\n",
    "    for word in text.split():\n",
    "        if word.strip().lower() not in stop:\n",
    "            final_text.append(word.strip().lower())\n",
    "    \n",
    "    text = \" \".join(final_text)\n",
    "    \n",
    "    # lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()    \n",
    "    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    text = \" \".join([lemmatizer.lemmatize(word, pos = 'v') for word in text.split()])\n",
    "    \n",
    "    # replace all numbers with \"num\"\n",
    "    text = re.sub(\"\\d\", \"num\", text)\n",
    "    \n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "8974cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql.types import StringType\n",
    "udf_clean_message_text3 = udf(lambda x:clean_message_text3(x),StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "2c32cc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_messages_cleaned3 = root_messages.withColumn(\"text\",udf_clean_message_text3(col(\"text\")))\n",
    "thread_replies_cleaned3 = thread_replies.withColumn(\"text\",udf_clean_message_text3(col(\"text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "e33d2edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(text='also uaxepm submit homework week eligible project stage'),\n",
       " Row(text='get error sql query tool tring find specific location zone data set keep get theerror column borough existline boroughqueensheres example triedselect fromzoneswhereboroughqueens'),\n",
       " Row(text='hey want ask something home work besides yellowtaxidata need import dataset zone right need create python script ingest dataset database import use sql script use import feature pgadmin thanksraisedhands'),\n",
       " Row(text='week seem readmemd little confuse follow step document step duplicatedunderstand multiple way achieve task perhaps mark alternative'),\n",
       " Row(text='im error try run data ingestion scripttraceback recent call last file appingestdatapy line mainargs file appingestdatapy line main df nextdfiter file usrlocallibpythonsitepackagespandasioparsersreaderspy line next return selfgetchunk file usrlocallibpythonsitepackagespandasioparsersreaderspy line getchunk return selfreadnrowssize file usrlocallibpythonsitepackagespandasioparsersreaderspy line read index column coldict selfenginereadnrows file usrlocallibpythonsitepackagespandasioparserscparserwrapperpy line read chunk selfreaderreadlowmemorynrows file pandaslibsparserspyx line pandaslibsparserstextreaderreadlowmemory file pandaslibsparserspyx line pandaslibsparserstextreaderreadrows file pandaslibsparserspyx line pandaslibsparserstextreadertokenizerows file pandaslibsparserspyx line pandaslibsparsersraiseparsererrorpandaserrorsparsererror error tokenizing data c error expect field line saw'),\n",
       " Row(text='run dockercompose im try connect pgadmin localhost load long'),\n",
       " Row(text='uaxepm possible highlight central park fifth question google formswhat popular destination passenger pick central park january enter zone name id zone name unknown miss write unknown'),\n",
       " Row(text='question expensive route whats pickupdropoff pair largest average price ride calculate base totalamount enter two zone name separate slashfor examplejamaica bay clinton eastif zone name unknown miss write unknown example unknown clinton east'),\n",
       " Row(text='column constitute ride yellowtaxitrips data'),\n",
       " Row(text='pickup drop location id')]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_messages_cleaned3.select(\"text\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e29683b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e26a4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group 2:\n",
    "def epoch_2_datetime(epoch):\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(epoch))\n",
    "\n",
    "\n",
    "def clean_message_text(text):\n",
    "    user_pattern = re.compile(r\"<@(.+?)>\")\n",
    "    link_pattern_text = re.compile(r\"<(http.+?)\\|(.+?)>\")\n",
    "    link_pattern = re.compile(r\"<(http.+?)>\")\n",
    "\n",
    "    text = (\n",
    "        text.replace(\"\\xa0\", \" \")\n",
    "        .replace(\"•\", \"-\")\n",
    "        .replace(\"\\n\\n\", \"\\n\")\n",
    "        .replace(\"'\", \"\")\n",
    "        .replace(\"`\", \"\")\n",
    "    )\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    text = user_pattern.sub(\"\", text)\n",
    "    text = link_pattern_text.sub(\"\", text)\n",
    "    text = link_pattern.sub(\"\", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "udf_epoch_2_datetime = udf(lambda x: epoch_2_datetime(x), types.StringType())\n",
    "udf_clean_message_text = udf(lambda x: clean_message_text(x), types.StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6a84143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, to_timestamp, udf\n",
    "\n",
    "def transform_message_data(spark_session: SparkSession, prefix):\n",
    "    \n",
    "    # transform 1: all_message_data for given year-month\n",
    "    message_data = spark_session.read.json(f'{DATA_SOURCE_ROOT}/{COURSE_CHANNEL}/{prefix}-*.json', multiLine=True)\n",
    "    \n",
    "    # transform 2: clean unwanted subtype (thread_broadcast, channel_join)\n",
    "    # this step cause, all values for 'inviter' and 'root' become null\n",
    "    message_data = message_data.where((col(\"subtype\").isNull()) | ((col(\"subtype\") != \"thread_broadcast\") & (col(\"subtype\") != \"channel_join\")))\n",
    "    \n",
    "    # transform 3: cleanup the text column in messages.\n",
    "    # some logic should come here ..\n",
    "    udf_clean_message_text = udf(lambda x:clean_message_text(x),types.StringType())\n",
    "    message_data = message_data.withColumn(\"text\",udf_clean_message_text(col(\"text\")))\n",
    "    \n",
    "    message_data = message_data.select(*interest_columns)\n",
    "    return message_data.toPandas()\n",
    "\n",
    "#     # transform 4: split messages in : root_level and thread_level messages\n",
    "#     root_level_messages = message_data.where((col(\"parent_user_id\").isNull()))\n",
    "#     thread_level_messages = message_data.where((col(\"parent_user_id\").isNotNull()))\n",
    "\n",
    "#     # transform 5:  drop unrelevant columns from root_level and thread_level messages\n",
    "#     root_level_messages = root_level_messages.select(*interest_columns)\n",
    "#     thread_level_messages = thread_level_messages.select(*interest_columns)\n",
    "#     return root_level_messages,thread_level_messages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72b55729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "x = transform_message_data(spark, \"2020-11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d82e1864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_msg_id</th>\n",
       "      <th>parent_user_id</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>subtype</th>\n",
       "      <th>text</th>\n",
       "      <th>thread_ts</th>\n",
       "      <th>ts</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c750d7d6-2e19-4604-ba5d-b9bcabaa1dfc</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Hello everyone! Lets talk about our idea to cr...</td>\n",
       "      <td>None</td>\n",
       "      <td>1606078694.001300</td>\n",
       "      <td>U01AXE0P5M3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>channel_topic</td>\n",
       "      <td>set the channel topic:</td>\n",
       "      <td>None</td>\n",
       "      <td>1606078707.001500</td>\n",
       "      <td>U01AXE0P5M3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9d6c86dc-3215-4a80-a03d-2a603ceddd7a</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>I think we should figure out - who the course ...</td>\n",
       "      <td>None</td>\n",
       "      <td>1606078852.004300</td>\n",
       "      <td>U01AXE0P5M3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>457cc8d2-5ba1-4db3-b53f-484e89873f00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Id say software engineers and data scientists/...</td>\n",
       "      <td>None</td>\n",
       "      <td>1606078935.005900</td>\n",
       "      <td>U01AXE0P5M3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ecf66f14-456f-46fb-94be-5d8c7f1c284b</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>this is a good starting point for the list of ...</td>\n",
       "      <td>1606078995.006400</td>\n",
       "      <td>1606078995.006400</td>\n",
       "      <td>U01AXE0P5M3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>37f10cab-6d45-4591-9a5e-5fd950cdfffe</td>\n",
       "      <td>U01AXE0P5M3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>sounds  reasonable!</td>\n",
       "      <td>1606238914.012900</td>\n",
       "      <td>1606292650.000100</td>\n",
       "      <td>U01F78474M9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>a8caaafa-e9c2-45c9-974c-2e4b6290c5e6</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>None</td>\n",
       "      <td>what do you think guys about making a case stu...</td>\n",
       "      <td>1606292801.002700</td>\n",
       "      <td>1606292801.002700</td>\n",
       "      <td>U01F78474M9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>630c454e-65e5-4873-9434-3913e0c34411</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Thats an awesome idea!</td>\n",
       "      <td>None</td>\n",
       "      <td>1606295686.003300</td>\n",
       "      <td>U01AXE0P5M3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>c89ede73-0b73-4a91-9c3c-02f2eedfa5d0</td>\n",
       "      <td>U01F78474M9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Very interesting! Like an RPG. Heres more to a...</td>\n",
       "      <td>1606292801.002700</td>\n",
       "      <td>1606304440.003700</td>\n",
       "      <td>U01DHB2HS3X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>e6943aa3-ee92-4047-9850-d073c773356d</td>\n",
       "      <td>U01F78474M9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>oh my good!!! it’s  so cool!!! I love detectiv...</td>\n",
       "      <td>1606292801.002700</td>\n",
       "      <td>1606305395.004100</td>\n",
       "      <td>U01F78474M9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           client_msg_id parent_user_id  reply_count  \\\n",
       "0   c750d7d6-2e19-4604-ba5d-b9bcabaa1dfc           None          NaN   \n",
       "1                                   None           None          NaN   \n",
       "2   9d6c86dc-3215-4a80-a03d-2a603ceddd7a           None          NaN   \n",
       "3   457cc8d2-5ba1-4db3-b53f-484e89873f00           None          NaN   \n",
       "4   ecf66f14-456f-46fb-94be-5d8c7f1c284b           None          1.0   \n",
       "..                                   ...            ...          ...   \n",
       "71  37f10cab-6d45-4591-9a5e-5fd950cdfffe    U01AXE0P5M3          NaN   \n",
       "72  a8caaafa-e9c2-45c9-974c-2e4b6290c5e6           None          2.0   \n",
       "73  630c454e-65e5-4873-9434-3913e0c34411           None          NaN   \n",
       "74  c89ede73-0b73-4a91-9c3c-02f2eedfa5d0    U01F78474M9          NaN   \n",
       "75  e6943aa3-ee92-4047-9850-d073c773356d    U01F78474M9          NaN   \n",
       "\n",
       "          subtype                                               text  \\\n",
       "0            None  Hello everyone! Lets talk about our idea to cr...   \n",
       "1   channel_topic                             set the channel topic:   \n",
       "2            None  I think we should figure out - who the course ...   \n",
       "3            None  Id say software engineers and data scientists/...   \n",
       "4            None  this is a good starting point for the list of ...   \n",
       "..            ...                                                ...   \n",
       "71           None                                sounds  reasonable!   \n",
       "72           None  what do you think guys about making a case stu...   \n",
       "73           None                             Thats an awesome idea!   \n",
       "74           None  Very interesting! Like an RPG. Heres more to a...   \n",
       "75           None  oh my good!!! it’s  so cool!!! I love detectiv...   \n",
       "\n",
       "            thread_ts                 ts         user  \n",
       "0                None  1606078694.001300  U01AXE0P5M3  \n",
       "1                None  1606078707.001500  U01AXE0P5M3  \n",
       "2                None  1606078852.004300  U01AXE0P5M3  \n",
       "3                None  1606078935.005900  U01AXE0P5M3  \n",
       "4   1606078995.006400  1606078995.006400  U01AXE0P5M3  \n",
       "..                ...                ...          ...  \n",
       "71  1606238914.012900  1606292650.000100  U01F78474M9  \n",
       "72  1606292801.002700  1606292801.002700  U01F78474M9  \n",
       "73               None  1606295686.003300  U01AXE0P5M3  \n",
       "74  1606292801.002700  1606304440.003700  U01DHB2HS3X  \n",
       "75  1606292801.002700  1606305395.004100  U01F78474M9  \n",
       "\n",
       "[76 rows x 8 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b994ebc4",
   "metadata": {},
   "source": [
    "# dag logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "578865ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_schema = types.StructType([\n",
    "    types.StructField(\"client_msg_id\",types.StringType(),False),\n",
    "    types.StructField(\"parent_user_id\",types.StringType(),True),\n",
    "    types.StructField(\"text\",types.StringType(),True), #-> \n",
    "    types.StructField(\"type\",types.StringType(),True),\n",
    "    types.StructField(\"subtype\",types.StringType(),True),\n",
    "    types.StructField(\"user\",types.StringType(),True), #-> id and user fk.\n",
    "    types.StructField(\"ts\",types.StringType(),True), #-> epoch to human readable format\n",
    "    types.StructField(\"thread_ts\",types.StringType(),True),\n",
    "    types.StructField(\"reply_count\",types.IntegerType(),True),\n",
    "    types.StructField(\"reactions\",types.ArrayType(types.StructType([\n",
    "        types.StructField(\"count\",types.LongType(),True),\n",
    "        types.StructField(\"name\" ,types.StringType(),True),\n",
    "        types.StructField(\"users\" ,types.ArrayType(types.StringType(),True),True)\n",
    "        ])\n",
    "    ),True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a47c61db",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_data = spark.read.schema(message_schema) \\\n",
    "        .json(f'{DATA_SOURCE_ROOT}/{COURSE_CHANNEL}/2020-11-*.json', multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "690d0523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# message_data = spark.read.json(f'{DATA_SOURCE_ROOT}/{COURSE_CHANNEL}/2020-11-*.json', multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5e33cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- client_msg_id: string (nullable = true)\n",
      " |-- parent_user_id: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- subtype: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- ts: string (nullable = true)\n",
      " |-- thread_ts: string (nullable = true)\n",
      " |-- reply_count: integer (nullable = true)\n",
      " |-- reactions: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- count: long (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- users: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "message_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "668b0f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(client_msg_id=None, parent_user_id=None, text=None, type=None, subtype=None, user=None, ts=None, thread_ts=None, reply_count=None, reactions=None),\n",
       " Row(client_msg_id=None, parent_user_id=None, text=None, type=None, subtype=None, user=None, ts=None, thread_ts=None, reply_count=None, reactions=None),\n",
       " Row(client_msg_id=None, parent_user_id=None, text=None, type=None, subtype=None, user=None, ts=None, thread_ts=None, reply_count=None, reactions=None),\n",
       " Row(client_msg_id=None, parent_user_id=None, text=None, type=None, subtype=None, user=None, ts=None, thread_ts=None, reply_count=None, reactions=None)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49b300dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode,col\n",
    "message_data = message_data.where((col(\"subtype\").isNull()) | ((col(\"subtype\") != \"thread_broadcast\") & (col(\"subtype\") != \"channel_join\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ebf3efbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, lit\n",
    "message_data = message_data.withColumn(\"channel_name\", lit(COURSE_CHANNEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "71e782ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reactions_data(df):\n",
    "    reactions_df = df.where((col(\"reactions\").isNotNull()))\n",
    "    reactions_df = reactions_df.select([\"client_msg_id\", \"user\", \"channel_name\", \"reactions\"])\n",
    "    reactions_df = reactions_df.withColumnRenamed(\"user\", \"msg_owner\")\n",
    "\n",
    "    reactions_df = reactions_df.withColumn(\"reaction\", explode(\"reactions\")).drop(\n",
    "        \"reactions\"\n",
    "    )\n",
    "    reactions_df = reactions_df.select(\n",
    "        [\"client_msg_id\", \"msg_owner\", \"reaction.name\", \"reaction.count\", \"reaction.users\", \"channel_name\"]\n",
    "    )\n",
    "    reactions_df = reactions_df.withColumn(\"msg_reactor\", explode(\"users\"))\n",
    "    drop_cols = [\"users\", \"count\"]\n",
    "    reactions_df = reactions_df.drop(*drop_cols)\n",
    "\n",
    "    return reactions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0131a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# message_data.select(\"reactions\").head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4fd2843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reactions_df = extract_reactions_data(message_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4f6cf79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['client_msg_id', 'msg_owner', 'name', 'channel_name', 'msg_reactor']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reactions_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a0582a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+----------------+--------------------+-----------+\n",
      "|       client_msg_id|  msg_owner|            name|        channel_name|msg_reactor|\n",
      "+--------------------+-----------+----------------+--------------------+-----------+\n",
      "|c750d7d6-2e19-460...|U01AXE0P5M3|            wave|course-data-engin...|U01DHB2HS3X|\n",
      "|9d6c86dc-3215-4a8...|U01AXE0P5M3|              +1|course-data-engin...|U01F78474M9|\n",
      "|9d6c86dc-3215-4a8...|U01AXE0P5M3|              +1|course-data-engin...|U01DHB2HS3X|\n",
      "|93eba8e0-4df8-410...|U01AXE0P5M3|              +1|course-data-engin...|U01DHB2HS3X|\n",
      "|28d13ad5-44e7-4b2...|U01AXE0P5M3|    raised_hands|course-data-engin...|U01TKRLE998|\n",
      "|fdbfaa9f-aad5-499...|U01F78474M9|            eyes|course-data-engin...|U02THT8U1GC|\n",
      "|f52ad9b4-c05d-43e...|U01F78474M9|              +1|course-data-engin...|U01DFQ82AK1|\n",
      "|f52ad9b4-c05d-43e...|U01F78474M9|              +1|course-data-engin...|U01AXE0P5M3|\n",
      "|f52ad9b4-c05d-43e...|U01F78474M9|              +1|course-data-engin...|U01DHB2HS3X|\n",
      "|54c86ff1-c3c2-40a...|U01AXE0P5M3|white_check_mark|course-data-engin...|U02TRHFHDH7|\n",
      "|f64812b2-4ae1-470...|U01AXE0P5M3|              +1|course-data-engin...|U01DHB2HS3X|\n",
      "|8babafe8-64ed-4a5...|U01DFQ82AK1|              +1|course-data-engin...|U01DHB2HS3X|\n",
      "|8babafe8-64ed-4a5...|U01DFQ82AK1|              +1|course-data-engin...|U01AXE0P5M3|\n",
      "|e52af1da-9435-4ae...|U01AXE0P5M3|            pray|course-data-engin...|U01DFQ82AK1|\n",
      "|a8caaafa-e9c2-45c...|U01F78474M9|              +1|course-data-engin...|U01DFQ82AK1|\n",
      "|a8caaafa-e9c2-45c...|U01F78474M9|              +1|course-data-engin...|U01AXE0P5M3|\n",
      "|a8caaafa-e9c2-45c...|U01F78474M9|              +1|course-data-engin...|U01DHB2HS3X|\n",
      "|e6943aa3-ee92-404...|U01F78474M9|        squirrel|course-data-engin...|U01DHB2HS3X|\n",
      "+--------------------+-----------+----------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reactions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1799ca4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+\n",
      "|       client_msg_id|  msg_owner|            reaction|\n",
      "+--------------------+-----------+--------------------+\n",
      "|c750d7d6-2e19-460...|U01AXE0P5M3|{1, wave, [U01DHB...|\n",
      "|9d6c86dc-3215-4a8...|U01AXE0P5M3|{2, +1, [U01F7847...|\n",
      "|93eba8e0-4df8-410...|U01AXE0P5M3|{1, +1, [U01DHB2H...|\n",
      "|28d13ad5-44e7-4b2...|U01AXE0P5M3|{1, raised_hands,...|\n",
      "|fdbfaa9f-aad5-499...|U01F78474M9|{1, eyes, [U02THT...|\n",
      "|f52ad9b4-c05d-43e...|U01F78474M9|{3, +1, [U01DFQ82...|\n",
      "|54c86ff1-c3c2-40a...|U01AXE0P5M3|{1, white_check_m...|\n",
      "|f64812b2-4ae1-470...|U01AXE0P5M3|{1, +1, [U01DHB2H...|\n",
      "|8babafe8-64ed-4a5...|U01DFQ82AK1|{2, +1, [U01DHB2H...|\n",
      "|e52af1da-9435-4ae...|U01AXE0P5M3|{1, pray, [U01DFQ...|\n",
      "|a8caaafa-e9c2-45c...|U01F78474M9|{3, +1, [U01DFQ82...|\n",
      "|e6943aa3-ee92-404...|U01F78474M9|{1, squirrel, [U0...|\n",
      "+--------------------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reactions_df = reactions_df.withColumn(\"reaction\", explode(\"reactions\")).drop(\"reactions\")\n",
    "reactions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b0c6ce8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+\n",
      "|       client_msg_id|  msg_owner|            reaction|\n",
      "+--------------------+-----------+--------------------+\n",
      "|c750d7d6-2e19-460...|U01AXE0P5M3|{1, wave, [U01DHB...|\n",
      "|9d6c86dc-3215-4a8...|U01AXE0P5M3|{2, +1, [U01F7847...|\n",
      "|93eba8e0-4df8-410...|U01AXE0P5M3|{1, +1, [U01DHB2H...|\n",
      "|28d13ad5-44e7-4b2...|U01AXE0P5M3|{1, raised_hands,...|\n",
      "|fdbfaa9f-aad5-499...|U01F78474M9|{1, eyes, [U02THT...|\n",
      "|f52ad9b4-c05d-43e...|U01F78474M9|{3, +1, [U01DFQ82...|\n",
      "|54c86ff1-c3c2-40a...|U01AXE0P5M3|{1, white_check_m...|\n",
      "|f64812b2-4ae1-470...|U01AXE0P5M3|{1, +1, [U01DHB2H...|\n",
      "|8babafe8-64ed-4a5...|U01DFQ82AK1|{2, +1, [U01DHB2H...|\n",
      "|e52af1da-9435-4ae...|U01AXE0P5M3|{1, pray, [U01DFQ...|\n",
      "|a8caaafa-e9c2-45c...|U01F78474M9|{3, +1, [U01DFQ82...|\n",
      "|e6943aa3-ee92-404...|U01F78474M9|{1, squirrel, [U0...|\n",
      "+--------------------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ec8f78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(client_msg_id='222e70f4-84dd-4d1a-960c-c54f49ed5624', parent_user_id='U01L9SP7UBT', text='At some point! Now I hope at least to finish this course =)', type='message', subtype=None, user='U01AXE0P5M3', ts='1626288400.022700', thread_ts='1625048497.019000', reply_count=None, reactions=None)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c59c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_data = message_data.toPandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "695ffbc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_msg_id</th>\n",
       "      <th>parent_user_id</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>subtype</th>\n",
       "      <th>user</th>\n",
       "      <th>ts</th>\n",
       "      <th>thread_ts</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>reactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>222e70f4-84dd-4d1a-960c-c54f49ed5624</td>\n",
       "      <td>U01L9SP7UBT</td>\n",
       "      <td>At some point! Now I hope at least to finish t...</td>\n",
       "      <td>message</td>\n",
       "      <td>None</td>\n",
       "      <td>U01AXE0P5M3</td>\n",
       "      <td>1626288400.022700</td>\n",
       "      <td>1625048497.019000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          client_msg_id parent_user_id  \\\n",
       "0  222e70f4-84dd-4d1a-960c-c54f49ed5624    U01L9SP7UBT   \n",
       "\n",
       "                                                text     type subtype  \\\n",
       "0  At some point! Now I hope at least to finish t...  message    None   \n",
       "\n",
       "          user                 ts          thread_ts  reply_count reactions  \n",
       "0  U01AXE0P5M3  1626288400.022700  1625048497.019000          NaN      None  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1777c2da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "client_msg_id      object\n",
       "parent_user_id     object\n",
       "text               object\n",
       "type               object\n",
       "subtype            object\n",
       "user               object\n",
       "ts                 object\n",
       "thread_ts          object\n",
       "reply_count       float64\n",
       "reactions          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_data.info\n",
    "message_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23ba46f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_message_text(text):\n",
    "    user_pattern = re.compile(r\"<@(.+?)>\")\n",
    "    link_pattern_text = re.compile(r\"<(http.+?)\\|(.+?)>\")\n",
    "    link_pattern = re.compile(r\"<(http.+?)>\")\n",
    "\n",
    "    text = (\n",
    "        text.replace(\"\\xa0\", \" \")\n",
    "        .replace(\"•\", \"-\")\n",
    "        .replace(\"\\n\\n\", \"\\n\")\n",
    "        .replace(\"'\", \"\")\n",
    "        .replace(\"`\", \"\")\n",
    "    )\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    text = user_pattern.sub(\"\", text)\n",
    "    text = link_pattern_text.sub(\"\", text)\n",
    "    text = link_pattern.sub(\"\", text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bba8c0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_data['text'] = message_data['text'].apply(lambda x: clean_message_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "786977a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop_cols = ['attachments', 'blocks', 'edited', 'inviter','is_locked', 'last_read', 'latest_reply', 'reactions', \n",
    "#              'replies', 'reply_users' , 'reply_users_count', 'root','source_team', 'subscribed', \n",
    "#              'team','topic','type', 'user_profile',  'user_team'\n",
    "#             ]\n",
    "\n",
    "\n",
    "# interest_col = ['client_msg_id', 'parent_user_id', 'reply_count', \"subtype\", \"text\", \"thread_ts\", \"ts\", \"user\" ]\n",
    "\n",
    "# len(message_data.columns) # 27\n",
    "\n",
    "# # len(drop_cols + interest_col) #29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9d7eebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# message_data= message_data[interest_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e66b264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(message_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42fc9dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_replies = message_data[message_data.parent_user_id.notnull()]\n",
    "root_messages = message_data[message_data.parent_user_id.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e72e932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_msg_id</th>\n",
       "      <th>parent_user_id</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>subtype</th>\n",
       "      <th>user</th>\n",
       "      <th>ts</th>\n",
       "      <th>thread_ts</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>reactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>222e70f4-84dd-4d1a-960c-c54f49ed5624</td>\n",
       "      <td>U01L9SP7UBT</td>\n",
       "      <td>At some point! Now I hope at least to finish t...</td>\n",
       "      <td>message</td>\n",
       "      <td>None</td>\n",
       "      <td>U01AXE0P5M3</td>\n",
       "      <td>1626288400.022700</td>\n",
       "      <td>1625048497.019000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          client_msg_id parent_user_id  \\\n",
       "0  222e70f4-84dd-4d1a-960c-c54f49ed5624    U01L9SP7UBT   \n",
       "\n",
       "                                                text     type subtype  \\\n",
       "0  At some point! Now I hope at least to finish t...  message    None   \n",
       "\n",
       "          user                 ts          thread_ts  reply_count reactions  \n",
       "0  U01AXE0P5M3  1626288400.022700  1625048497.019000          NaN      None  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thread_replies.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7e7d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def epoch_2_datetime(epoch):\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "073b764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_replies = thread_replies.astype({\"ts\": float , \"thread_ts\": float})\n",
    "\n",
    "thread_replies['ts'] = thread_replies['ts'].apply(lambda x: epoch_2_datetime(x))\n",
    "thread_replies['thread_ts'] = thread_replies['thread_ts'].apply(lambda x: epoch_2_datetime(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b902515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_messages = root_messages.astype({\"ts\": float , \"thread_ts\": float})\n",
    "\n",
    "root_messages['ts'] = root_messages['ts'].apply(lambda x: epoch_2_datetime(x))\n",
    "# root_messages['thread_ts'] = root_messages['thread_ts'].apply(lambda x: epoch_2_datetime(x)) ## hmm thread_ts causing issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc72d0a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_msg_id</th>\n",
       "      <th>parent_user_id</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>subtype</th>\n",
       "      <th>user</th>\n",
       "      <th>ts</th>\n",
       "      <th>thread_ts</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>reactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>222e70f4-84dd-4d1a-960c-c54f49ed5624</td>\n",
       "      <td>U01L9SP7UBT</td>\n",
       "      <td>At some point! Now I hope at least to finish t...</td>\n",
       "      <td>message</td>\n",
       "      <td>None</td>\n",
       "      <td>U01AXE0P5M3</td>\n",
       "      <td>2021-07-14 20:46:40</td>\n",
       "      <td>2021-06-30 12:21:37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          client_msg_id parent_user_id  \\\n",
       "0  222e70f4-84dd-4d1a-960c-c54f49ed5624    U01L9SP7UBT   \n",
       "\n",
       "                                                text     type subtype  \\\n",
       "0  At some point! Now I hope at least to finish t...  message    None   \n",
       "\n",
       "          user                   ts            thread_ts  reply_count  \\\n",
       "0  U01AXE0P5M3  2021-07-14 20:46:40  2021-06-30 12:21:37          NaN   \n",
       "\n",
       "  reactions  \n",
       "0      None  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thread_replies.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56db6f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_msg_id</th>\n",
       "      <th>parent_user_id</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>subtype</th>\n",
       "      <th>user</th>\n",
       "      <th>ts</th>\n",
       "      <th>thread_ts</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>reactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [client_msg_id, parent_user_id, text, type, subtype, user, ts, thread_ts, reply_count, reactions]\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_messages.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383a3e42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
