{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e24b0a41",
   "metadata": {},
   "source": [
    "In this notebook, we are interested to figure out correct data transformation strategy\n",
    "1. [x] User Data\n",
    "2. [x] Message Data\n",
    "3. [x] Message Data within Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e964288d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Constants and Common Functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71582288",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Source Constants\n",
    "DATA_SOURCE_ROOT = \"../../dtc-capstone-data/slack-data\" \n",
    "COURSE_CHANNEL = \"course-data-engineering\"\n",
    "WELCOME_CHANNEL = \"welcome\"\n",
    "USERS_DATA = \"users.json\"\n",
    "# book-of-the-week\n",
    "# announcements-course-data-engineering\n",
    "# shameless-content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ce93364",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Target Constants\n",
    "PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\", 'dtc-capstone-344019')\n",
    "BUCKET = os.environ.get(\"GCP_GCS_BUCKET\", 'dtc_capstone_data-lake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35d028e5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77830202",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('dtc-capstone') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d6dfcf",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Users-Data\n",
    "\n",
    "We have single JSON file which store all Users information for the complete Data Talks Club Slak group.\n",
    "This data contains, different set of attributes, where some of them repeatative and uninformative.\n",
    "Therefore in this section we will decide columns that we are interested and continue the development by using that subet of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40601a6d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 1. Read JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbd72ff",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "users = spark.read.json(f'{DATA_SOURCE_ROOT}/{USERS_DATA}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512470c6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "users.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fa50d2",
   "metadata": {
    "code_folding": [],
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2. Cleanup Unnecessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60867321",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['profile', 'color', 'who_can_share_contact_card', 'team_id', '_corrupt_record']\n",
    "users = users.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e81b20a4",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- deleted: boolean (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- is_admin: boolean (nullable = true)\n",
      " |-- is_app_user: boolean (nullable = true)\n",
      " |-- is_bot: boolean (nullable = true)\n",
      " |-- is_email_confirmed: boolean (nullable = true)\n",
      " |-- is_invited_user: boolean (nullable = true)\n",
      " |-- is_owner: boolean (nullable = true)\n",
      " |-- is_primary_owner: boolean (nullable = true)\n",
      " |-- is_restricted: boolean (nullable = true)\n",
      " |-- is_ultra_restricted: boolean (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- real_name: string (nullable = true)\n",
      " |-- tz: string (nullable = true)\n",
      " |-- tz_label: string (nullable = true)\n",
      " |-- tz_offset: long (nullable = true)\n",
      " |-- updated: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(deleted,BooleanType,true),StructField(id,StringType,true),StructField(is_admin,BooleanType,true),StructField(is_app_user,BooleanType,true),StructField(is_bot,BooleanType,true),StructField(is_email_confirmed,BooleanType,true),StructField(is_invited_user,BooleanType,true),StructField(is_owner,BooleanType,true),StructField(is_primary_owner,BooleanType,true),StructField(is_restricted,BooleanType,true),StructField(is_ultra_restricted,BooleanType,true),StructField(name,StringType,true),StructField(real_name,StringType,true),StructField(tz,StringType,true),StructField(tz_label,StringType,true),StructField(tz_offset,LongType,true),StructField(updated,LongType,true)))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check schema after cleanup\n",
    "users.printSchema()\n",
    "users.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e99d6c5f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "user_schema = types.StructType([\n",
    "    types.StructField(\"deleted\",types.BooleanType(),True),\n",
    "    types.StructField(\"id\",types.StringType(),True),\n",
    "    types.StructField(\"is_admin\",types.BooleanType(),True),\n",
    "    types.StructField(\"is_app_user\",types.BooleanType(),True),\n",
    "    types.StructField(\"is_bot\",types.BooleanType(),True),\n",
    "    types.StructField(\"is_email_confirmed\",types.BooleanType(),True),\n",
    "    types.StructField(\"is_invited_user\",types.BooleanType(),True),\n",
    "    types.StructField(\"is_owner\",types.BooleanType(),True),\n",
    "    types.StructField(\"is_primary_owner\",types.BooleanType(),True),\n",
    "    types.StructField(\"is_restricted\",types.BooleanType(),True),\n",
    "    types.StructField(\"is_ultra_restricted\",types.BooleanType(),True),\n",
    "    types.StructField(\"name\",types.StringType(),True),\n",
    "    types.StructField(\"real_name\",types.StringType(),True),\n",
    "    types.StructField(\"tz\",types.StringType(),True),\n",
    "    types.StructField(\"tz_label\",types.StringType(),True),\n",
    "    types.StructField(\"tz_offset\",types.IntegerType(),True),\n",
    "    types.StructField(\"updated\",types.LongType(),True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08de9139",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 2.2 (optional) After deciding schema we can read the json file again with that schema definition\n",
    "users = spark.read.schema(user_schema).json(f'{DATA_SOURCE_ROOT}/{USERS_DATA}') \n",
    "users.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7cd5aa",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 3. Create SubSets for User Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6e49182",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def format_user_data(d):\n",
    "    identity = [\"name\", \"real_name\"]\n",
    "    location = [\"tz\",\"tz_label\", \"tz_offset\"]\n",
    "    status = [\"deleted\", \"is_admin\", \"is_owner\", \"is_primary_owner\",\n",
    "              \"is_restricted\",\"is_ultra_restricted\",\"is_bot\", \n",
    "              \"is_email_confirmed\"]\n",
    "    \n",
    "    user = {}\n",
    "    for key, value in {\"identifiers\": identity, \"location\":location, \"status\":status}.items():\n",
    "        user[key]={}\n",
    "        user[key][\"id\"] = d[\"id\"] # each group will have the id column\n",
    "        for v in value:\n",
    "            if v in d:\n",
    "                user[key][v]=d[v]\n",
    "            else:\n",
    "                user[key][v]=\"None\"   \n",
    "                \n",
    "    return user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fef40f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 4. Upload Clean Data in 4 different files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0e0e8f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 5.Create 4 External Table from the Buckets and changed files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec13a7c4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Message-Data\n",
    "\n",
    "In that section, we will check the datastructure of `course-data-engineering` channel. The slack-data for the channel consist of daily json files. Therefore we need to ensure how we ingest data to cloud. In below, you will see that we are going to handle messages monthly basis rather than running ingestion job daily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c7da51",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 1. Read All JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "baac0d9e",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2020-11-22', '2022-03-14')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all related files \n",
    "files = sorted(glob(f'{DATA_SOURCE_ROOT}/{COURSE_CHANNEL}/*.json'))\n",
    "\n",
    "# Analyse start and end of the data set\n",
    "start, end = files[0].split(\"/\")[-1].split(\".\")[0], files[-1].split(\"/\")[-1].split(\".\")[0]\n",
    "(start,end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4ffeff7c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 204:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9136, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 1. Load the data into pyspark dataframe\n",
    "messages_cde = spark.read.json(f'{DATA_SOURCE_ROOT}/{COURSE_CHANNEL}/*.json', multiLine=True) \n",
    "print((messages_cde.count(), len(messages_cde.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d21cb4",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2. Analyse the Column Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1ee50723",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- client_msg_id: string (nullable = true)\n",
      " |-- parent_user_id: string (nullable = true)\n",
      " |-- reactions: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- count: long (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- users: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |-- reply_count: long (nullable = true)\n",
      " |-- subtype: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- thread_ts: string (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages_cde.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dbfa0934",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(subtype='tombstone'),\n",
       " Row(subtype=None),\n",
       " Row(subtype='thread_broadcast'),\n",
       " Row(subtype='channel_join'),\n",
       " Row(subtype='channel_topic')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_cde.select('subtype').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c79deec9",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(type='message')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_cde.select('type').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd06a31d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(topic=None),\n",
       " Row(topic='<https://docs.google.com/document/d/1TI1co3aRcYYBB9keAMPgrnQd84juN-i8bZiiXW_1oUo/edit?usp=sharing>'),\n",
       " Row(topic='<https://github.com/DataTalksClub/data-engineering-zoomcamp>')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_cde.select('topic').distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b18507f",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 3. CleanUp Unnecessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "80fe883f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "18ccb1fa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 3.1 Remove Rows with Unwanted subtypes\n",
    "messages_cde = messages_cde.where((col(\"subtype\").isNull()) | ((col(\"subtype\") != \"thread_broadcast\") & (col(\"subtype\") != \"channel_join\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "395c31d5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 3.2 Keep the columns that makes sense for the rest\n",
    "interest_columns = [\n",
    "    \"client_msg_id\",\n",
    "    \"parent_user_id\",\n",
    "    \"reactions\",\n",
    "    \"reply_count\",\n",
    "    \"subtype\",\n",
    "    \"text\",\n",
    "    \"thread_ts\",\n",
    "    \"ts\",\n",
    "    \"user\",\n",
    "]\n",
    "messages_cde = messages_cde.select(interest_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d60654",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 4. Transform Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12bdbd0",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 4.1 Extract the Reactions from the Message data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8d37ff9f",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+---------------+-----+--------------------+\n",
      "|       client_msg_id|       user|           name|count|               users|\n",
      "+--------------------+-----------+---------------+-----+--------------------+\n",
      "|d751f16a-ca2a-403...|U02UEJMRB8X|         scream|    2|[U01AXE0P5M3, U02...|\n",
      "|780394d7-0647-499...|U02QBJYQFK9|   raised_hands|    1|       [U02V24WAZRN]|\n",
      "|425a4e1f-fd43-496...|U02QJJ30E05|   raised_hands|    1|       [U02U8FXSG1Y]|\n",
      "|aa607d5d-55fe-45f...|U02U809EAE7|             +1|    1|       [U02U34YJ8C8]|\n",
      "|a72b3aa0-db7b-488...|U02U809EAE7|             +1|    3|[U02U34YJ8C8, U02...|\n",
      "|1ade0883-ba49-4d3...|U0290EYCA7Q|   raised_hands|    1|       [U0308865C0H]|\n",
      "|1ade0883-ba49-4d3...|U0290EYCA7Q|             +1|    1|       [U0308865C0H]|\n",
      "|29f12de9-4834-405...|U02R09ZR6FQ|             +1|    1|       [U02UY1QTGHW]|\n",
      "|53f8696b-acd2-476...|U02U34YJ8C8|             +1|    1|       [U02HFP7UTFB]|\n",
      "|3a7e75e9-1603-4b5...|U01AXE0P5M3|   raised_hands|    1|       [U02QBJYQFK9]|\n",
      "|071697F4-5AA5-417...|U02DY0L6PHV|             +1|    1|       [U02RHT0M3M5]|\n",
      "|0F73517E-6509-47F...|U02DY0L6PHV|             +1|    1|       [U0308MF3KUH]|\n",
      "|760094a6-756d-464...|U02T25ULN9W|             +1|    1|       [U02U34YJ8C8]|\n",
      "|ce038456-8a58-48b...|U02RREQ7MHU|             +1|    3|[U01DFQ82AK1, U02...|\n",
      "|2350ecb7-e8fa-46d...|U02U34YJ8C8|              v|    1|       [U02UB59DKDL]|\n",
      "|2350ecb7-e8fa-46d...|U02U34YJ8C8|+1::skin-tone-6|    1|       [U02UHJB6CJ0]|\n",
      "|28426f6b-271d-422...|U02RTJPV6TZ|             +1|    1|       [U0290EYCA7Q]|\n",
      "|daa79e17-4ea0-4ac...|U0290EYCA7Q|           eyes|    1|       [U02U790S2NA]|\n",
      "|3f716653-c593-483...|U0290EYCA7Q|             +1|    3|[U02Q51Y4MM5, U02...|\n",
      "|74856b73-7410-466...|U0290EYCA7Q|      thank_you|    1|       [U01AXE0P5M3]|\n",
      "+--------------------+-----------+---------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.1 Extract reactions into new dataframe\n",
    "# In here caution that user columns is the owner of message where users put reaction.\n",
    "reactions_df = messages_cde.where((col(\"reactions\").isNotNull())) \n",
    "reactions_df = reactions_df.select(['client_msg_id',\"user\",'reactions'])\n",
    "reactions_df = reactions_df.withColumn('reaction', explode('reactions')).drop(\"reactions\")\n",
    "reactions_df = reactions_df.select(['client_msg_id',\"user\",\"reaction.name\", \"reaction.count\", \"reaction.users\"])\n",
    "reactions_df.select(['client_msg_id',\"user\",\"name\", \"count\", \"users\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e744d2c2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "unique_reactions = reactions_df.select('name').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "# unique_reactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce068d54",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 4.2 Cleanup Messages from unwanted characters with Spark UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fc55b7ef",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# 4 Clean Message from some unwanted characters\n",
    "def clean_message_text(text):             \n",
    "    user_pattern = re.compile(r'<@(.+?)>')\n",
    "    link_pattern_text = re.compile(r'<(http.+?)\\|(.+?)>')\n",
    "    link_pattern = re.compile(r'<(http.+?)>')\n",
    "    \n",
    "    text = text.replace('\\xa0', ' ').replace('•', '-').replace('\\n\\n', '\\n').replace(\"'\", '').replace(\"`\", '')\n",
    "    text = re.sub('\\n', ' ', text) \n",
    "    text = user_pattern.sub(\"\", text)\n",
    "    text = link_pattern_text.sub(\"\", text)\n",
    "    text = link_pattern.sub(\"\", text)\n",
    "    return text\n",
    "\n",
    "udf_clean_message_text = udf(lambda x:clean_message_text(x),StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "257a251e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "messages_cde = messages_cde.withColumn(\"text\",udf_clean_message_text(col(\"text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d397be10",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/usr/local/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 663, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/Cellar/apache-spark/3.2.1/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(text='Hi , I had a lot of problems when trying to convert the .ipynb file to a py script. I copied exactly the code in the video and I kept getting the error No template sub-directory with name script can be found. After a bit of googling, I found that if I uninstalled nbconvert module and then reinstalled it with a lower version, the command would work. I was close to throwing my laptop out of the window by that point! Obviously we all have different computers, OS and python versions, so mentioning the module version would be very helpful!')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_cde.select(\"text\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74d46ea",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 4.3 Convert time columns from Epoch to Timestamp format with Spark UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d16e6fb0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf,col, to_timestamp\n",
    "from pyspark.sql.types import StringType,IntegerType, DateType\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c26d4902",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def epoch_2_datetime(epoch):\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(epoch))\n",
    "\n",
    "udf_epoch_2_datetime = udf(lambda x: epoch_2_datetime(x),StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "20e8807b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "messages_cde = messages_cde.withColumn(\"ts\", messages_cde[\"ts\"].cast(IntegerType()))\n",
    "\n",
    "messages_cde = messages_cde.withColumn(\"ts\",udf_epoch_2_datetime(col(\"ts\")))\n",
    "messages_cde = messages_cde.withColumn(\"ts\",to_timestamp(col(\"ts\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "07a3e545",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(client_msg_id='9e77c6e5-e288-4060-a6e6-3ec9185345c2', parent_user_id='U02TMP4GJEM', reactions=None, reply_count=None, subtype=None, text='Hi , I had a lot of problems when trying to convert the .ipynb file to a py script. I copied exactly the code in the video and I kept getting the error No template sub-directory with name script can be found. After a bit of googling, I found that if I uninstalled nbconvert module and then reinstalled it with a lower version, the command would work. I was close to throwing my laptop out of the window by that point! Obviously we all have different computers, OS and python versions, so mentioning the module version would be very helpful!', thread_ts='1642995076.257900', ts=datetime.datetime(2022, 3, 28, 17, 30, 2), user='U02TMP4GJEM')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_cde.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65894c66",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 4.4 Split Root messages and Threaded messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "22067192",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All_messages  :(9087, 9)\n",
      "Root_messages :(1862, 9)\n",
      "Thread Replies:(7225, 9)\n"
     ]
    }
   ],
   "source": [
    "root_messages = messages_cde.where((col(\"parent_user_id\").isNull()))\n",
    "thread_replies = messages_cde.where((col(\"parent_user_id\").isNotNull()))\n",
    "\n",
    "print(f\"All_messages  :{(messages_cde.count(), len(messages_cde.columns))}\")\n",
    "print(f\"Root_messages :{(root_messages.count(), len(root_messages.columns))}\")\n",
    "print(f\"Thread Replies:{(thread_replies.count(), len(thread_replies.columns))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9cd55dca",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Check which columns are all empty\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def columns_not_in_use(df):\n",
    "#     nonNull_cols = [c for c in df.columns if df.filter(F.col(c).isNotNull()).count() > 0]\n",
    "    null_cols = [c for c in df.columns if df.filter(F.col(c).isNotNull()).count() == 0]\n",
    "    return null_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cea744fe",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 270:==================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['parent_user_id']\n",
      "-----\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "columns_2_drop_root_messages = columns_not_in_use(root_messages)\n",
    "dcolumns_2_drop_thread_replies = columns_not_in_use(thread_replies)\n",
    "print(columns_2_drop_root_messages)\n",
    "print(\"-----\")\n",
    "print(dcolumns_2_drop_thread_replies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b994ebc4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Message Data - Airflow DAG approach\n",
    "\n",
    "The idea is run exactly similiar steps described in Message-Data section but with different order and with different tool set (read with pyspark but do majority of transformation with pandas)\n",
    "\n",
    "There are couple of reasons for that.\n",
    "1. Need to ensure data I am dealing with always have same format, and if there is mismatch then enforce it as null through schema. By doing that, I do not need write bunch of if checks within the function.\n",
    "2. The Airflow has some issues to pickle UDFs, therefore I needed a different approach to apply transformations on column level and need to switch pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215b590a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 1. Read JSON files with the Predefined Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "578865ef",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "message_schema = types.StructType([\n",
    "    types.StructField(\"client_msg_id\",types.StringType(),False),\n",
    "    types.StructField(\"parent_user_id\",types.StringType(),True),\n",
    "    types.StructField(\"text\",types.StringType(),True), #-> \n",
    "    types.StructField(\"type\",types.StringType(),True),\n",
    "    types.StructField(\"subtype\",types.StringType(),True),\n",
    "    types.StructField(\"user\",types.StringType(),True), #-> id and user fk.\n",
    "    types.StructField(\"ts\",types.StringType(),True), #-> epoch to human readable format\n",
    "    types.StructField(\"thread_ts\",types.StringType(),True),\n",
    "    types.StructField(\"reply_count\",types.IntegerType(),True),\n",
    "    types.StructField(\"reactions\",types.ArrayType(types.StructType([\n",
    "        types.StructField(\"count\",types.LongType(),True),\n",
    "        types.StructField(\"name\" ,types.StringType(),True),\n",
    "        types.StructField(\"users\" ,types.ArrayType(types.StringType(),True),True)\n",
    "        ])\n",
    "    ),True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a47c61db",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "message_data = spark.read.schema(message_schema) \\\n",
    "        .json(f'{DATA_SOURCE_ROOT}/{COURSE_CHANNEL}/2020-11-*.json', multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a5e33cfd",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- client_msg_id: string (nullable = true)\n",
      " |-- parent_user_id: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- subtype: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- ts: string (nullable = true)\n",
      " |-- thread_ts: string (nullable = true)\n",
      " |-- reply_count: integer (nullable = true)\n",
      " |-- reactions: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- count: long (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- users: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "message_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ebf3efbe",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, lit\n",
    "message_data = message_data.withColumn(\"channel_name\", lit(COURSE_CHANNEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f738075b",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2. Extract the Reactions from Message Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "71e782ff",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def extract_reactions_data(df):\n",
    "    reactions_df = df.where((col(\"reactions\").isNotNull()))\n",
    "    reactions_df = reactions_df.select([\"client_msg_id\", \"user\", \"channel_name\", \"reactions\"])\n",
    "    reactions_df = reactions_df.withColumnRenamed(\"user\", \"msg_owner\")\n",
    "\n",
    "    reactions_df = reactions_df.withColumn(\"reaction\", explode(\"reactions\")).drop(\n",
    "        \"reactions\"\n",
    "    )\n",
    "    reactions_df = reactions_df.select(\n",
    "        [\"client_msg_id\", \"msg_owner\", \"reaction.name\", \"reaction.count\", \"reaction.users\", \"channel_name\"]\n",
    "    )\n",
    "    reactions_df = reactions_df.withColumn(\"msg_reactor\", explode(\"users\"))\n",
    "    drop_cols = [\"users\", \"count\"]\n",
    "    reactions_df = reactions_df.drop(*drop_cols)\n",
    "\n",
    "    return reactions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4fd2843d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "reactions_df = extract_reactions_data(message_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4f6cf79c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['client_msg_id', 'msg_owner', 'name', 'channel_name', 'msg_reactor']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reactions_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e6a34f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 3. Transform Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da2e24d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 3.1 Remove Rows with specific Column Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "49b300dd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode,col\n",
    "message_data = message_data.where((col(\"subtype\").isNull()) | ((col(\"subtype\") != \"thread_broadcast\") & (col(\"subtype\") != \"channel_join\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9af27cb",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 3.2 Convert Data to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9c59c27f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "message_data = message_data.toPandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1777c2da",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "client_msg_id      object\n",
       "parent_user_id     object\n",
       "text               object\n",
       "type               object\n",
       "subtype            object\n",
       "user               object\n",
       "ts                 object\n",
       "thread_ts          object\n",
       "reply_count       float64\n",
       "reactions          object\n",
       "channel_name       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_data.info\n",
    "message_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8bd978",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 3.3 Cleanup Messages from unwanted characters  with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "23ba46f5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def clean_message_text(text):\n",
    "    user_pattern = re.compile(r\"<@(.+?)>\")\n",
    "    link_pattern_text = re.compile(r\"<(http.+?)\\|(.+?)>\")\n",
    "    link_pattern = re.compile(r\"<(http.+?)>\")\n",
    "\n",
    "    text = (\n",
    "        text.replace(\"\\xa0\", \" \")\n",
    "        .replace(\"•\", \"-\")\n",
    "        .replace(\"\\n\\n\", \"\\n\")\n",
    "        .replace(\"'\", \"\")\n",
    "        .replace(\"`\", \"\")\n",
    "    )\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    text = user_pattern.sub(\"\", text)\n",
    "    text = link_pattern_text.sub(\"\", text)\n",
    "    text = link_pattern.sub(\"\", text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "bba8c0d1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "message_data['text'] = message_data['text'].apply(lambda x: clean_message_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdc7920",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 3.4 Split Root messages and Threaded messages¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "42fc9dcf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "thread_replies = message_data[message_data.parent_user_id.notnull()]\n",
    "root_messages = message_data[message_data.parent_user_id.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2505918e",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 3.5 Convert time columns from Epoch to Timestamp format with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c7e7d606",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def epoch_2_datetime(epoch):\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "073b764f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "thread_replies = thread_replies.astype({\"ts\": float , \"thread_ts\": float})\n",
    "\n",
    "thread_replies['ts'] = thread_replies['ts'].apply(lambda x: epoch_2_datetime(x))\n",
    "thread_replies['thread_ts'] = thread_replies['thread_ts'].apply(lambda x: epoch_2_datetime(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b902515c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "root_messages = root_messages.astype({\"ts\": float , \"thread_ts\": float})\n",
    "\n",
    "root_messages['ts'] = root_messages['ts'].apply(lambda x: epoch_2_datetime(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
